,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507
s0957417413009615, 1 Introduction Facial expression is the explicit transformation of the human face due to the automatic responses to the emotional instability,In most situations it is spontaneous and uncontrollable,The automatic facial expression involves the application of an artificial intelligent system to recognize the expressions of the face under any circumstance,"Today, the studies of facial expressions have gained keen interest in pattern recognition, computer vision and its related fields","Mainly, such facial expressions are the seven prototypical ones, namely; anger, fear, surprise, sad, disgust, happy, and neutral",Research into automatic facial expression recognition is very important in this modern society of technological age,"For instance, the technology is applied in a wide variety of contexts, including robotics, digital signs, mobile applications, and medicine",It is reported that “some robots can operate by first recognizing expressions” of humans ( ),The AIBO robot for instance is a biologically-inspired robot that can show its emotions via an array of LEDs located in the frontal part of the head ( ),"In addition to this, the robot can also display ‘happiness’ feeling when it detects a face","In behavioral sciences and medicine for instance, expression recognition is effectively applied for intensive care monitoring ( )","Currently, there are developing systems that are capable of making routine examinations of facial behavior during pain in clinical settings",In infants the Neonatal Facial Coding System (NFCS) has been employed for real-time assessment within 32 to 33 week post-conceptional age infants who are undergoing a heel lance,The technology is being used in more advanced settings to reduce accidents through the implementation of automated detection of driver drowsiness in public transports,This system relays information about the drivers’ emotional states to observers for effective surveillance leading to necessary awareness,The hallmark of every facial expression system is accuracy and to some extent the speed of execution,"However most of the existing systems produce poor performances in terms of accuracy; as for execution speed, most of the systems are even silent to give a hint",Some few examples;   proposed a neural based facial expression recognition system that used principal component analysis (PCA) to reduce the feature vectors,The features were fed into a feed-forward neural network that was trained by a back-propagation network,In this system an average recognition of 84.5% was reported on the Yale facial expression database – an achievement which is not very encouraging.   described a neural network classification facial expression recognition system that employs Gabor feature extraction and feature reduction by PCA to distinguish 7-class facial expression recognition on the JAFFE database,"In this system they specified 20 inputs, 40 to 60 hidden layers and seven output feed-forward neural networks","Again, the 60–70% recognition accuracy they obtained by their procedure is not encouraging to befit the expectations of a real-time system","Recently,   extracted features of the face using Affine Moment Invariants and performed the classification using feed-forward neural network",The expression recognition obtained was 93.8% on the JAFFE database.   extracted the facial features using a Sobel filter,"In their experiment they reserved the maximum connected component to reduce the wrinkles and noises and conducted 7-class classification on JAFFE facial expression database through the application of Elman network with two hidden layers, each layer containing fifteen neurons","With this approach the average accuracy of automatic facial expression recognition is 84.7%.   extracted the expressive face by using Gabor filters, feature reduction by PCA and expression classification by neural network",In this method an average facial recognition of 93.4% was recorded in the JAFFE facial expression database.   also extracted facial features by Gabor techniques and reduced the features by PCA,"The expression classifier was neural network and the average expression recognition was 94.5% ± 0.7 on the seven prototypical facial expressions, however the facial expression database was not mentioned",Most of these studies advocate the use of Neural Network as the expression classifier and extracted the facial features by Gabor filters and reduced the features via PCA,The displeasing thing is that all the results were not very encouraging,"This study persists in exploring the potentials of neural networks to execute this kind of assignment, trying to esteem some biological constraints, utilizing the capabilities of modular systems","Though many techniques have been used to extract the facial features, Gabor feature extraction remains a high-quality choice; there are other alternatives but they are not very promising",Just a few examples:   utilized the Haar technique to extract facial features which were used as input to the neural network for classifying 8 facial expressions,"The Haar wavelet extraction is very fast ( ), however the wavelets are too huge to result to effective classification when used as input to classifiers in facial expression recognition ( ); in other words it is a potential cause to misclassifications and poor performance",Distance-based feature extraction methods are also one of the largely applied techniques used for feature extraction in both 2D and 3D static faces,The idea behind these procedures is that the muscle deformations which are the major causes of changes in facial expression from normal expression results in variations of the Euclidean distances between facial landmarks or points,"These points, as well as their distances, have been widely employed for static facial expression analysis ( )",Among the most successful ones is feature extraction based on the Bhattacharyya distance ( ),"However, despite some advantages of this method, the degree of computational complexity is unacceptably high",The matching of even a small model shape with a normal image can take half an hour on an eight-processor Sun SPARCServer 1000 ( ),The Patch based feature extraction method is another alternative widely exploited for facial expression biometrics.   for instance represented extracted patches from facial surfaces by sets of closed curves and then applied a Riemannian framework to obtain the shape analysis of the extracted patches,"However, the patch-based features also have numerous drawbacks","First, particular representations cannot be applied to other solutions without major modifications: the majority of the techniques have only been utilized to a single class","Also, most methods do not exploit the large amounts of available training data ( )","Thus on the basis of these we still considered Gabor features as the best approach, not because it does not have drawbacks, but the drawbacks can be easily managed","The Gabor filter is a superior model of simple cell receptive fields in cat striate cortex ( ), and it grants exceptional basis for object recognition and face recognition ( )","Again, the Gabor methods are superior to all the above-mentioned methods because it extracts the maximum information from local image regions ( ), and it is invariant against, translation and rotations ( )","In this work, the data were reduced in dimensions by Bessel transform ( ) and then after extraction of the face by Gabor methods, the features were further reduced via an AdaBoost-based ( ) feature reduction technique",The selected features which represented the facial deformation patterns were then fed into a 3-layer feed-forward neural network that is trained by a back-propagation algorithm,It is interesting to note that Bessel down-sampling techniques have never been adopted for facial expression recognitions,"Again, the combinations of Bessel down-sampling and the formulated AdaBoost-based algorithm is an innovation that reduces the expression dataset to enhance accuracy and speed","Finally, the construction of the feed-forward neural network is influential to bring about successful results",The rest of the work is arranged as follows,Section   discusses face detection and image down-sampling,Section   discusses Gabor feature extraction,Section   discusses feature selection,Section   discusses the multilayer feed-forward neural network (MFFNN),Results and analysis are presented in Section  ,"The final conclusions of the work are drawn in Section  . 2 Face detection and down-sampling The face detection component was implemented by the adoption of Viola and Jones system ( ).  
                       shows sample face detection by the Viola–Jones classifier",The size of the image is rescaled to a window of size 20 × 20 pixels by the use of Bessel down-sampling,Methods like bilinear interpolations have been utilized by several authors for this task in particular but interpolations are prone to aliasing problems ( ),Bessel down-sampling reduces the size of the image and preserves the details and perceptual quality of the original image ( ),"The down-sampling image signal  
                      ( 
                      , 
                      
                      ) is expressed as: where   and   refer to the respective image size,  
                      − 
                       and  
                      − 
                       are the required reduced size of the image;   and   are positive integers that represent the reduction values,   is the number of low-frequency DCT coefficients,   and   are zero order Bessel functions,  ( 
                      , 
                      
                      ) are Bessel coefficients computed from the first order Bessel function,  
                       and  
                       are chosen such that   and  ","Interested readers are referred to ( ). 3 Gabor feature extraction The 2-D Gabor filters are spatial sinusoids localized by Gaussian window, and they can be created to be selective for orientation, localization, and frequency as well","It is very flexible to demonstrate images by Gabor wavelets because the details about their spatial relations are preserved in the process. where   is a complex number representing the square root of −1. 
                      
                      
                      = 
                      
                      cos 
                       and  
                      
                      = 
                      
                      sin 
                      ,   is the spatial frequency of the band pass,   is the spatial orientation of the function G, (x, y) specify the position of light impulse in the visual field,   is the standard deviation of 2-D Gaussian envelop","In this Gabor family, we chose eight orientations   and five scales   In order to give added robustness to illumination we turned the Gabor filter to zero DC (direct current) by the expression where,   is the size of the filter, given by  
                      = (2 
                      + 1) .  
                       shows the Gabor filter image","The sample points of the filtered image is coded to two bits, real bit  
                       and imaginary bit  
                       such that, 
                      
                   where   is subimage of the expressional face,   and   are the real and the imaginary parts of each Gabor kernel, * is the convolution operator","With this coding, only the phase information in the facial expressions image is stored in the feature vector of size 256 bytes","The final magnitude response which is used to represent the feature vectors is computed by 
                   
                      
                       shows the magnitude response of a template image. 4 Feature selection Due to the large size of the Gabor wavelets, it is not practically possible to use all the wavelets as input to our classifier, for fear of misclassification and possible system crash",The AdaBoost feature reduction algorithms have special speed advantage in increasing classification process ( ),Thus we formulated an AdaBoost-based algorithm to select a few deserving portions of the wavelets,"Assuming the extracted Gabor features are represented by a total of  
                      ∊ (1, 2,..., 
                      ) appearance features","Then the image   is represented as   configured by the parameters  ,  ,  ","The positive sets  
                       and the negative sets  
                       are denoted by   and   respectively, where   is the  th data sample containing   features, and   is its corresponding class label","To train the vectors || ||, which is denoted by  
                      
                      
                      
                      
                      
                      
                       over a distribution  , we simply determined the weights of all the feature vectors   This gives us a threshold   which indicated the decision hyperplane.   is computed as: 
                   A sample is positive or client if it is located at the positive half of   (which is the majority decision), otherwise it is a negative or an imposter",The status is reversed if the minority of the positive instances is rather located in the positive half space,Let   be denoted by clients and   be the imposters,"For a given training dataset containing both positive and negative samples, where each sample is ( , 
                      );  
                      ∊ { ± 1} represents the corresponding class label, the feature selection algorithm is formulated as follows: 
                      
                   The selected features represent samples of the facial deformation patterns of the expressive face",The datasets which were images from the JAFFE and Yale databases were partitioned into training and testing by leave-one-out cross validation ( ). 5 Multilayer feed-forward neural network (MFFNN) classifier The selected features are fed into the constructed neural network to train it to identify the seven universal facial expressions,The architecture is a 3-layer feed-forward neural network and trained by a back-propagation algorithm ( ),The back propagation algorithm basically replicates its input to its output via a narrow conduit of hidden units,The hidden units extract regularities from the inputs because they are completely connected to the inputs,Every network was trained to give the maximum value of 1 for exact facial expression and 0 for all other expressions,"The construction is shown in  
                      ","The input layer has 7 nodes, each for each facial expression whiles the hidden layer had 49 neurons; each expression for 7 neurons",We chose 7 neurons to compensate for the target output of seven facial expressions,"This was the case for the seven prototypical facial expressions, which was validated by the use of the JAFFE facial expression database","Since the experiment was also validated using the Yale database, where four expressions were used there was a slight modification in the construction of the network for this application","Here, the hidden layer neurons were settled at 16 each facial expression dedicated for 4 neurons and 4 neurons in the output layer","The input vectors of the network represented by  
                      = [ 
                      , 
                      
                      ,..., 
                      ] ","The output layers are denoted by  
                      = [ 
                      , 
                      
                      ,..., 
                      ] ","The optimization model is formulated as  
                      : 
                      
                      → 
                      ","The output dataset of each layer of the network is denoted by   where  
                      − 1 corresponds to the total hidden layers and   represents the total output layers","We denote the target datasets and its additive white noise by ( 
                      , 
                      
                      ,..., 
                      ) and  
                      = ( 
                      , 
                      
                      ,..., 
                      ), respectively",The variable   represents the total patterns of the network,"The corresponding vectors of the hidden units are denoted by  
                      = ( 
                      , 
                      
                      ,..., 
                      
                      )","The sigmoid activation function  
                      = ( 
                      , 
                      
                      ,..., 
                      ) of each layer is  
                      , 
                      
                      ,..., 
                      
                      ","The weights of the network are updated by  
                      , 
                      
                      ,..., 
                      ",The training epochs are 1000 and the target of error is 0.001,"The training algorithm is modeled as: Subject to the constraints 
                   The process of training involves weight initialization, calculation of the activation unit, adjustment, weight adaptation, and testing for convergence of the network",Assuming   represents the weight between the  th hidden unit and  th input unit; and   represents the weight between the  th output and the  th hidden unit,"The activation unit is then calculated sequentially, starting from the input layer","The activation of hidden and output unit is calculated as: 
                      where   is the activation of the  th hidden unit and   is the activation of the  th output unit for the pattern,  .   is a sigmoid function.   is the total number of output units,   is the total number of input units and   is the total number of hidden units.   is the weight connected to the bias unit in the hidden layer,  
                      = −1 and  
                      = −1","We adjusted the weights, starting at the output units and recursively propagated error signals to the input layer","The detected output   is compared with the corresponding target value   which is a facial image, over the entire training set using the sigmoid function to express the approximation error in the network’s target functions. 
                   The minimization of the error  
                      
                      
                      , requires the partial derivative of  
                      
                      
                       with respect to each weight in the network to be computed","The change in weight is proportional to the corresponding derivative. 
                      where,   is the learning rate, normally between 0 and 1, we set it to 0.9",The function   is also set to 0.9,"The last term is a function of the previous weight change. 
                       Therefore, The weights are updated by, 
                      where,   is equal to the current time step. Δ  and Δ  are the weight adjustments",We repeated the process once from the equation   in order to achieve the desired output. 6 Results and analysis The facial expression recognition was validated with the JAFFE and Yale facial expression databases,The JAFFE database contains 213 images of 10 female Japanese persons,"Each respondent in the database posed three or four examples of each of the seven facial expression prototypes (happy ( ), sad ( ), anger ( ), disgust ( ), fear ( ), surprise ( ), and neutral ( )). 2 images of each individual from each class of expression are randomly selected for training, leading to a total of 140 images corresponding to 65.7%, the rest was preserved for testing",The trial was performed using tenfold cross-validation to obtain the average recognition rate,"In order to create distinct datasets for cross-validation, none of the sets in the training folder appear in any of the remaining folders",The Yale facial expression database contains 165 grayscale images in GIF format of 15 individuals,There are 11 images per subject,"Each subject exhibited one of the six facial expressions;  ,  ,  , sleepy ( ), surprise ( ) and wink ( )","In this database we manually extracted 130 images corresponding to  ,  ,  , and  ",The datasets in this database were also partitioned into training and testing by using the same method described for the JAFFE,"In due course about 77% of the images were used for training and the remaining, for testing","We recorded an average recognition rate of 96.83% in JAFFE and 92.22% in Yale on Intel(R) Core(TM) 2 Duo CPU P8400 @ 2.26 GHz (2 CPUs) – 2.3 GHZ and 2.0 GHz RAM computer running on Windows 7 Ultimate 64-bit (6.1, Build 7601) (see  
                       and  
                       for the confusion matrices)","The best recognitions were detected in  ,   and  , where we obtained almost 100% in the JAFFE database",We saw that facial images with extreme exhibited expressions recorded the best results,"Generally, the performance of   was the weakest; about 92.23% in JAFFE and 86.16 in Yale",The results show that the deformations of the muscles around the mouth and the eyes are the most reliable determinants for automatic facial expressions,This accounts why recognition in the neutral face is poor,Thus the increase in these muscle deformations increases the accuracies of automatic recognitions,"The execution time for a pixel of size 100 × 100 is 14.5 ms.  
                       shows the comparative performance of execution time with other neural network classifiers",The proposed method was compared with three different classifiers to access its performance in terms of recognition accuracy and execution speed,The system was also tested with some real life images from the World Wide Web,"The results indicate that the proposed method is statistically better ( 
                      < 0.05) both in accuracy and speed","The three methods are described as follows: Method I (same as   method): The feature detector is by a discrete cosine transform (DCT), pruning technique is used to reduce the input size of the network, the training algorithm is back-propagation procedure, the expression classifier is a feed-forward neural network with one hidden layer","Method II (same as   method): The feature detector is Gabor method, the feature dimensionality is principal component analysis (PCA) and the expression classifier is a feed-forward neural network","Method III (same as  ): The feature detector is discrete cosine transform, the feature reduction is by principal component analysis (PCA), and the classifier is a feed-forward neural network","All the classifiers were trained and tested with the same datasets used for the proposed method.  
                      
                       shows the average recognition rates of various expressions in JAFFE and Yale respectively; the intent is not to compare the performances of the two databases but to investigate for the robustness of the system in diverse databases.  
                       also shows the comparison of the overall average recognition rates of various descriptors in JAFFE and Yale.  
                       shows sample real-time expression recognitions by the system","The average recognition rates are also compared with other methods that employed the same datasets in their experiment to give a general idea of the performance of the proposed method (see  
                       and  
                       for details)",However this does not signify a direct comparison because the experiments were not conducted under the same environment,The results show that the proposed method is very encouraging,"Though performance in the Yale facial expression database is reduced as compared to that in the JAFFE facial expression database, it is far better than all the performances in Yale we compared with. 7 Conclusions This study employs many advanced techniques to improve the recognition rate and execution time of facial expression recognition system",Face detection was carried out by the application of Viola–Jones descriptor,Detected faces were down-sampled by the Bessel transform,This approach reduced the image dimensions and preserved the perceptual quality of the original image,An AdaBoost based algorithm was formulated to select a few hundreds of Gabor wavelets from the several thousands of the extracted features to reduce the computational cost and to avoid misclassification as well,The selected features were fed into a well-designed multilayer feed-forward neural network classifier,The network is thus trained with sample datasets from both JAFFE and Yale facial expression databases,The remaining datasets from the two databases and some images from the World Wide Web were used to test for the system,The execution time for a pixel of size 100 × 100 is 14.5 ms; the average recognition rate in JAFFE database is 96.83% and that in Yale is 92.22%,The proposed method is compared with several methods and the performance is outstanding,"The results of the study also show that automatic expression recognitions are very accurate in surprise, disgusts and happy; about 100%","Mild expressions like sad, fear and neutral have lower accuracies",However fear can be very accurate when it is at the peak because accuracies in recognitions largely depend on the magnitude of facial deformations around the mouth and eyes,"To advance towards 100% efficiency we believe the development of natural databases would be of more help since many artificial databases have many confused scenarios among facial expressions in sad, neutral and mild anger",Again future improvements of recognition accuracies will look at the possibility of increasing the number of hidden neurons to expressions that recorded lower values.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s2590188519300162, 1 Introduction Predicting the future price of the stock market is extremely important for the investors as knowing the appropriate movement of the stock price will reduce the risk for determining future trends so as to invest money,"It's a challenging issue to know the future movements due to its highly fluctuating nature, hence, suitable computational methods are required to forecast the stock price movement",Regarding stock market predictability ( ) many controversies have been rising from decades long,"Initially, the movement of the stock price was characterized by the random walk theory ( ) and later the research on price changes was carried out based on the Efficient Market Hypothesis (EMH) ( )","According to their perspective, the future price movement does not depend on the present and past value and also from their point of view future stock price prediction is impossible","In another way, various studies have made effort to experimentally disprove the EMH and the observational evidences have delineated that the stock market can be predicted to some extent","Numerous traditional approaches have been developed by the researchers in the field of forecasting the stock price movements such as Autoregressive Moving Average (ARMA), Autoregressive Integrated Moving Average (ARIMA) etc., but ( ) these methods are having some limitations, they are not capable to handle the nonlinear relationship exist in time series data as they make the assumption of a linear form of the structure of the model","Furthermore, they are presumed to have constant variance, whereas, the financial time series data are very noisy by nature","Later the issues regarding the existence of non linearity in time series data has been solved by the approach of so many machine learning techniques such as an Artificial Neural Network (ANN), Functional Link Artificial Neural Network (FLANN), Support Vector Machine (SVM) etc","In the last decades, numerous researchers have explored the prediction ability of neural networks for financial market prediction",This study emphasizes on increasing the ability of the prediction model through feature reduction techniques,From the immense literature study it has observed that various kinds of neural networks have been used for stock market prediction,"Since the literature study hardly divulges the efficiency of Extreme Learning Machine (ELM), Online Sequential Extreme Learning Machine (OSELM) and Recurrent Back Propagation Neural Network (RBPNN) using both statistical and optimized feature reduction techniques in the domain of financial market prediction","In this study, a comparison has made on the outcome of the prediction model considering statistical based feature reduction along with swarm evolutionary based feature reduction technique","The proposed OSELM and firefly with an evolutionary framework strategy model has not used in the financial domain, specifically in stock market prediction and this work basically focused on prediction of the future value of the stock price, which is entirely application oriented and not reported or published yet in the financial domain",An empirical comparison is established among the performance of prediction models using both statistical and optimized feature reduction techniques,"For statistical based feature reduction Principal Component Analysis (PCA) and Factor Analysis (FA) and for optimized feature reduction Firefly Optimization (FO), Genetic Algorithm (GA) and Firefly algorithm with evolutionary framework are considered for the experimental work","In the group of heuristic algorithm FO is considered as the most interesting and novel population based heuristic algorithm, which follows the behavior of fireflies","The beauty of firefly is that, each firefly supposed to be attracted ( ) by each other firefly irrespective of their sex, here the attraction between the fireflies is directly proportional to their brightness that means the lesser brighter is attracted towards the more brighter one and here in experimental point of view the brightness is decided by calculating the fitness function","On the other hand to generate new fireflies with better performance genetic algorithm is employed on FO, which is termed as Firefly algorithm with evolutionary framework in this study","The Firefly algorithm with evolutionary framework, which is the combination of FO and GA explores the optimal solutions more exactly than individual FO and GA","All these features reduction techniques are applied to the experimented prediction models such as ELM, OSELM and RBPNN","Empirically the model validation is realized over four stock market datasets such as BSE Sensex, NSE Sensex, S&P 500 index and the FTSE index within a given period of time","The dataset is regenerated using technical indicators and statistical measures, based on the available features such as open price, close price, maximum price, minimum price and change of price considering the window size seven",The proposed work provides better prediction ability than the other evaluated learning techniques and prediction models used in this experimental work,"The rest of the paper is organized as follows:   contains the related work which basically focuses on the promising techniques used by various researchers in financial domain to get best possible results.   covers the details of data samples and the dataset regeneration part.   introduces all the predictive models in details and   covers the particulars of statistical and optimized based feature reduction techniques.   provides the result analysis, which includes a schematic layout of the proposed prediction model, parameter setup, detailed steps of the proposed algorithm, the analysis of the experimental outcome in the form of simulation graph and the training result in MSE along with the performance evaluation measures","Finally,   and   present the overall work analysis and conclusions respectively. 2 Related work In the development of various prediction models, ANN is realized as a promising technique from the significant contributions of various researchers, basically in the field of financial domain","Predicting future stock price through the application of various neural network technologies such as ANN, Back propagation Neural Network (BPNN), FLANN, ELM etc. has proven to be efficient, making the provision of the days ahead from 1 day to 1month to be predicted",Some researchers have explored ANN for the prediction of ( ) NASDAQ stock value considering the input data on NASDAQ stock index value,"Though, ANN shows its potential for solving the existence of non linearity in financial data, but the random selection of weights may create some non optimal solutions","So, for this issue so many optimization techniques have been developed and applied to ANN by the researchers","Though, ANN is having very distinguished learning ability, but due to the availability of the noise data it may perform very inconsistently and unpredictably","Hence,   proposed GA to optimize the weights between the layers and selecting the task for the relevant instances",Zhang et al. modified bacterial chemotaxis optimization (BCO) ( ) and named it as Improved BCO and integrated to BPNN for getting the better performance and realized the validation over S&P 500,"In addition to this FLANN is efficiently used in the financial domain, as it is less complex and simultaneously able to solve the dynamic and computational complexity problem",Majhi et al. proposed FLANN as a predictor and to train the weight of the prediction ( ) model least mean square and recursive least square algorithms is used and the efficiency of the model is evaluated by applying over DJIA and S&P 500,"To iteratively adjust the parameters of the FLANN, PSO and GA and the gradient descent are used by Naik et al. and compared the ( ) result with the result of individuals like FLANN, GA adjusting the parameter of FLANN, FLANN adjusting the parameter by PSO","Tuning of parameter is one of the important issues in the working of NN, many researchers have introduced various learning techniques to tune the parameters",Bebarta et al. projected meta-heuristic Firefly algorithm to train ( ) FLANN and compared with FLANN trained with the learning rule of conventional BPNN,In another research ( ) article Artificial Fish Swarm Algorithm (AFSA) has used to learn the RBFNN and compared the result with other widely accepted optimization algorithm such as Particle Swarm Optimization (PSO) and GA,"Arévalo et al. introduced a dynamic scheme ( ), where the author basically focus on the loss to be stopped and the profit to be taken, which is updated on the basis of quarterly","In addition to this the author also gives emphasis on technical analysis indicator, which is calculated both for 15 min and 1 day time frame","Similarly exploring on time convergence theorem, Xu et al. finally achieved success ( ) on fix time controlling on stock price fluctuation system","More over by analyzing over hybridization model,   stated hybridized model is always having the best predictive performance compared to single algorithm",Fluctuation leads to rise or fall of the trading market,"Hence, based on the concept of rise or fall, bullish or bearish market strategies arises, which define the thought of the trader about future","Tsinaslanidis focused on this concept of bullish and bearish ( ), where the author analyzed bearish class predictions, which has generated on average gives significantly maximum potential profits, whereas the performance of bullish does not give significant profit","Though, traditional BPNN is having the potential to solve a complex problem, but it may suffer from trapping into local minima and slow convergence speed",Another batch learning algorithm called ELM has developed by   to train single layer feed forward neural network and the beauty of ELM is that the parameters of hidden layer need not to be tuned at each iteration and the output weights are generated analytically ( ),For many real applications ELM has shown good generalization performance,"Apart from the advantages of ELM, some of the issues regarding random choosing of number of nodes in the hidden layer also arise in ELM",Comparing to other conventional neural network ELM needs more number of hidden layer nodes,"For predicting the improved outcomes feature reduction technique is addressed in so many research articles, as high dimensionality may cause high computational cost at the time of learning the network","Furthermore, most relevant features can also give more accurate results in predicting the outcome","Dimensionality reduction is process, where the number of attributes is reduced and a set of principal attributes is obtained, to get better performance result in the working of the network","Initially statistical based methods were used for the purpose of feature reduction, but now optimized based feature reduction is widely used in machine learning approach",PCA is a statistical technique for the reduction of dimension with the aim of minimizing the loss of variance in the ( ) original data,"For extracting features, it's a domain of independent technique which can be applied to a wide variety of data",Through the calculation of eigenvectors of the covariance matrix belongs to the original inputs PCA transforms linearly the high dimensional input vector to the low dimensional vector with ( ) uncorrelated components,"There is another statistical data reduction method called FA, which explains about the existence of correlations among the multiple outcomes as a result of one or more number of underlying factors",FA involve in data reduction technique representing a set of variables with a smaller number,"Many optimized based feature reduction techniques have been developed by the researchers basically to improve the learning algorithm, feature selection and optimization of the topology",Kim et al. proposed ANN hybridized with ( ) GA for feature discretization and feature discretization is associated very closely ( ) to the dimensionality reduction,GAs is mostly appropriated for higher dimension and the ( ) stochastic problems with the available nonlinearities and discontinuities,Likewise firefly is one popular algorithm extensively used in numerous area of research,Xiong et al. used Firefly algorithm with ( ) support vector regression (SVR) in forecasting stock price index,In addition to this Kazem et al. proposed chaotic firefly ( ) algorithm to optimize the hyperparameters of SVR and evaluated the performance using the stock market index,"The result of predicted outcome is compared with the SVR optimized with firefly, where chaotic Firefly algorithms with SVR proved its efficiency over firefly with SVR","Firefly is a population based algorithm work on meteheuristic approach, developed by  ) inspired by the behavior of flashing characteristics of fireflies",The flashing light is developed in a way that it is related to the objective ( ) function to be optimized,"Simultaneously the concept of chaos theory is added to the optimization algorithm design, as in recent years, researchers have grown their interest on chaos theory and its features for better performance","This Firefly algorithm comes under stochastic algorithm, which uses the process of randomization at the ( ) time of searching for a set of solutions. 3 Data acquisition For empirical evaluation of ideas four stock indices such as BSE Sensex, NSE Sensex, S&P 500 index and FTSE index have considered in this study covering the range 24th February 2011to 27th November 2017 for BSE, 27th October 2000 to 27th November 2017 for NSE, 2nd January 2008 to 16th March 2018 for S&P500 and 2nd January 2008 to 16th March 2018 for FTSE100",The total number of samples collected from the time duration of the respective years is derived through a process of windowing by providing 7 as the window size,"The experimentation is carried out for a different prediction time horizon of 1 day, 3 days, 5 days, 7 days, 15 days and 30 days in advance, so accordingly the dataset is reset for the respective time horizon",Out of this total number of sample patterns 70% data are considered for training and 30% data are considered for testing,"All the experimented datasets with its details are expressed in  
                      ","The experimental model has used various technical indicators and statistical measures, which transform the price data by adding some additional value and then use this price data as input to the model",The technical indicators and the statistical measures are chosen based on the usage of previous literature ( ) and the combination of available data with technical indicator and statistical measures is able to capture the unsteady properties of financial market,"A growing number of technical indicators and statistical measures are available to study, but a few numbers are considered in this study for experimentation are detailed on  
                      . 4 Prediction models 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   4.1 ELM ELM works on the principle of Single Layer Feed-forward Neural Network (SLFN), with the basic difference is that unlike SLFN the hidden layer nodes and biases of ELM need not to be tuned at each iteration","In almost all practical learning algorithms of feed forward neural network, it has been marked that the input weights and hidden layer biases ( ) are needed to be adjusted at each iteration","Comparing to SLFN, the learning speed of ELM is very faster with smallest training ( ) error and smallest norm of the weights","For developing ELM, Moore-Penrose generalized inverse takes an important task","For   number of ( ) samples, let   are the input to ELM and   are the target output for the respective input",Let   and  ( ) is set as the number of hidden nodes and activation function respectively,"The input weight vector  is calculated by connecting input nodes with the  th hidden nodes and the output weight vector   is generated analytically, which connects  th hidden nodes to the output nodes",Let   is the bias of  th hidden node,"The equation of ELM can be written as where   is the hidden layer output matrix,   is the output weight matrix and   is the target vector and these three values are calculated by using the  
                         
                         
                      The choice of   and   is random, which is important for ( ) accelerating the rate of learning.   is calculated by using the Moore-Penrose generalized inverse of  ","The beauty of ELM is that it avoid multiple iterations as the ( ) total number of training inputs is trained at a time and also it is having the better generalization ability with faster learning rate. 4.2 OSELM From the batch learning approach of ELM, OSELM is generated ( )",OSELM can learn the training data in both the ways such as one-by-one and chunk by chunk ( ) and the size of the chunk can be fixed or can be varied,The training observations are discarded as soon as the learning procedure is completed,"Unlike ELM, at any time, only the newly arrived observations that may be single or chunk is sequentially learned in OSELM","Here, the learning algorithm does not have any prior knowledge about the number of training observations","The major difference between batch ELM and OSELM is that in batch ELM all the training data available at the time of training, whereas in real application training data may arrive one by one or chunk by chunk, hence this problem can be solved through the online sequential process ( )","Let  
                          is set as the chunk of an initial set of total   training samples where the number of hidden layer nodes  is less than  
                         ","Considering the batch algorithm, it needs to minimize  , where 
                         
                      For minimizing  , the solution is given by 
                      Let  
                          is another chunk of data of total   training samples 
                      Again to minimize 
                      The output   is calculated by considering both the training datasets such as  
                          and  
                         , where 
                      By solving the  , it can observed that 
                      
                         
                          and  
                         is calculated as follows 
                         
                      
                          is derived and rewritten as 
                      The equation can be generalized and represented in the form of (  + 1)th chunk data, Let it be  chunk of data and the   value is calculated 
                      4.3 RBPNN RBPNN is having an activation feedback with the short term memory, where the hidden ( ) layer is updated by both external input and activation from the previous forward propagation","Using some set of weights the feedback is modified so as to enable the usual adaption through the network learning. 5 Feature reduction techniques When the data of all the features are given to the prediction model, the prediction models, here in this study are neural network models, cannot be well trained properly","Hence, the feature reduction technique is introduced in this study to get the most relevant features of original datasets, for improving the performance of the model",Both statistical based and optimized based feature reduction methods are employed to the dataset,"The details of which are described in this section. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   5.1 Statistical based feature reduction 
                         
                         
                         
                      
                         
                         
                         
                      5.1.1 PCA PCA used in multivariate data analysis as a standard tool for reducing the dimension",The basic aim is to consider the reduced number of data variables keeping the intact ( ) information available in the original data,"As it requires only the eigen decomposition or singular decomposition, hence it is most popular in the perception of computation","To obtain the PCA solution, two alternative optimization approach exists ( ) such as variance maximization and minimum error formulation","For feature extraction, PCA is a very prominent method, which calculate the eigenvectors of the covariance matrix belongs to the original inputs and transforms linearly the high ( ) dimension input vector to low dimension with uncorrelated components","The step wise procedure of PCA is given below ( ): 
                         5.1.2 FA FA accepts the observed variables are the linear combination of unobservable factors and among these factors some are common to two or more variables and some expect unique to each variable","The factors which are unique do not contribute to the covariance between the variables or it ( ) can be said among the observed variables the common factors, which are smaller in number than the observed variable numbers contribute to the covariation","The basic difference of PCA from FA is that, in PCA, the principal components are having certain mathematical functions of the observed variables, while the combinations of observed variables is not able to express the common factors, Whereas, the main objective of FA is to determine the common factors which can produce the correlations among the observed variables satisfactorily. 5.2 Optimized based feature reduction 
                         
                         
                         
                      
                         
                         
                         
                         
                      5.2.1 GA GA is an evolutionary algorithm based on Darwinian rules of ( ) evolutionary dynamics and can be used efficiently to solve problems with many possible solutions","Initially, the random population is generated and then through the process of crossover, mutation and survival of the fittest, the individuals are selected to go to ( ) the next generation until the particular stopping condition is satisfied","The following steps are the evolutionary stage of GA: 
                         5.2.2 FO technique FO is a metaheuristic algorithm, developed by Xin-Shi Yang, based on the flashing characteristics of fireflies","The flashing characteristics can be idealized ( ) by using these following rules: 
                         Basically the light intensity variation and attractiveness formulation are the two important issues in Firefly algorithm","Simply we can say the attractiveness is determined by the brightness or the intensity of the light, which is successively associated with the objective function","The computational complexity of the ( ) algorithm is considered to be  ( 
                            ) but the large population size may lead to substantial increase in the time complexity. 6 Result analysis This section covers the details of experimental outcome through various steps",At first the schematic layout of the proposed model along with its step wise description is analyzed,Further the appropriate parameter value set for the techniques used in this experimental work is studied,Then the step wise description of the proposed algorithm is designed,The simulation result is delineated in the form of actual versus predicted graph along with the Mean Square Error (MSE) during the training phase,"In addition to this the performance evaluation part is figured out, considering the test outcome of the prediction models through different performance measures. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   6.1 The system framework In the process of planning to take decision for investing money in financial markets, prediction is a powerful tool","Three prediction models such as ELM, OSELM and RBPNN are considered in this study and the efficiency of the prediction models are evaluated by four stock market datasets, BSE Sensex, NSE Sensex, S&P 500 index and FTSE index","The framework of the proposed prediction model is shown in  
                         ","The experimental work has done phase wise; in the first phase the dataset has regenerated, where the regeneration occurs by adding some new features such as technical indicators and statistical measures to the existing stock market datasets",The dimension of the input has increased with the expectation of getting closer predicted value towards the actual value,The technical indicators and statistical measures are generated by using the mathematical formula taking into account 7 as the window size,"In the second phase, data preprocessing occurs that involves normalization and feature reduction","For normalization, min-max normalization is well thought-out in this study, which scales the data between 0 and 1","In case of feature reduction two types of feature reduction occurs, one is statistical based feature reduction and another one is optimized based feature reduction","PCA and FA are used for statistical based feature reduction whereas GA, FO and Firefly algorithm with evolutionary framework are designed for optimized feature reduction","Now the reduced data are ready for input to ELM, OSELM and RBPNN prediction models","In optimized based feature reduction GA, FO and Firefly algorithm with evolutionary framework are considered for reducing features before giving input to the models","FO is an interesting population based heuristic algorithm, where each brighter firefly attracts its partner in spite of their sex, which help to explore the search space efficiently","But to explore the search space more precisely for getting optimal solution, GA is added to Firefly, which is designed as Firefly algorithm with ( ) evolutionary framework is designed",The reduced features obtained from of all the feature reduction techniques are individually set input to the prediction models,An empirical comparison is carried among all the prediction models for all types of feature reduction techniques using all the datasets,"From the simulation result of all the experimental work, the proposed OSELM based on Firefly Algorithm with evolutionary framework optimized feature reduction model proves its efficiency over the rest of the combinations of feature reduction approach with prediction models. 6.2 Parameter setup This section discussed the parameters of all the experimented methods one by one","ELM works on the principle of SLFN, hence it has one hidden layer",The number of nodes in the hidden layer is considered as 15 and the execution is carried out for 100 number of iterations,"Apart from the parameters of ELM, OSELM is having one extra parameter such as the number of batches",This study considered 30 numbers of batches to get better predicted outcome,"Similarly, for the execution of RBPNN, this experimental work set threshold value and learning rate to 0.1 and momentum coefficient to 0.05",Further the parameters of two optimized methods such as GA and FO is analyzed,"GA is found to have two basic parameters, one is crossover rate and another is mutation rate",The parameter value 0.8 for crossover rate and 0.1 for mutation rate is most optimal according to the convergence time,"In addition to this, the basic parameters of FO, which controls the algorithm are noticed",They are attraction coefficient and absorption coefficient,The appropriate values for these two parameters has to be chosen correctly to get optimal solutions,"This study set attraction coefficient ( 
                         ) to 1.0 and absorption coefficient ( ) to 0.8","Apart from the individual algorithm specific parameters, population size and number of iterations are two common controlling parameters, which are kept fixed for all the experimentation of this work","Population size has a great role in determining the optimal solutions as a small population may miss the part of the solution space and the large populations may increase the computational time, hence a 100 number of populations is considered for a standard comparison","In addition to this, number of iterations is another parameter which effect the running time to get a good solution, hence number of iterations is also fixed to 100 for all the experimental work. 6.3 Detailed steps of stock exchange prediction using Firefly algorithm and Firefly algorithm with evolutionary framework applying on OSELM prediction model 
                         
                      
                         : The best population, which is obtained out of this FO technique is the transformation matrix","This transformation matrix is multiplied to the input data, by which the reduced features are obtained","Instead of all the data only the data of those reduced features are set input to the OSELM prediction model. 
                      
                         : Some improvements are added here to this FO technique ( ) for transforming the current solutions to some enhanced solutions","The concept of GA id added to the updated solutions moving towards the best solutions, and for this the basic operation of GA such as selection, crossover and mutation is carried out",The basic reason behind this modification is ability to find good solutions in the reasonable time and avoiding of trapping in local minima,"Hence, the two current best populations or in the concept of firefly, two populations of higher light intensity are selected through an attractiveness process","Crossover mechanism is applied over these two current best populations and two best solutions are selected based on their fitness value using the fitness function, eliminating the rest among the parent and children feasible solutions","In the next step, the mutation operation is performed by choosing one random population",If the new solution or new firefly is feasible and giving better solution through the functional value then the new one replaced the old solution,The process is continued till the termination condition is satisfied,"Through, this improve method a best solution is generated which is a transformation matrix","The above transformation matrix is multiplied to the input data, through, which the reduced features are obtained","Now the data of these reduced features are considered as input to the OSELM prediction model. 6.4 Experimental outcome analysis Four datasets such as BSE Sensex, NSE Sensex, S&P 500 index and FTSE index are used to explore the forecasting ability of the prediction models",Each dataset is divided into training and testing in the ratio of 7:3,"The actual versus predicted graph of all the experimented datasets using Firefly algorithm with an evolutionary framework applying on ELM, OSELM and RBPNN for 1 day ahead closing price is depicted in  
                         ,   and  
                         
                          respectively",From these figures it can be clearly figured out that the performance of RBPNN in predicting the above mentioned stock value is poorer than ELM and OSELM,"The simulated graph of ELM, OSELM and RBPNN for BSE Sensex data considering six time horizon such as 1 day, 3 days, 5 days, 7 days, 15 days and 30 days ahead to be predicted is depicted in  
                         
                         
                         , where PCA feature reduction technique is set for reducing feature before giving stock data input to the prediction model","In the experimental outcome though all the simulated graph of actual versus predicted is achieved but due to lack of space, in some experimentation 1 day, 5 days and 15 days ahead is given, whereas in other experimentation 3 days, 7 days and 30 days ahead prediction result has shown but the information can be retrieved from the table through MSE result during training","Similarly,  
                         ,   and  
                         
                          shows the actual versus predicted graph of the same data using FA feature reduction technique applying on ELM, OSELM and RBPNN respectively","Further the actual versus predicted graph of ELM, OSELM and RBPNN based on GA is pictured in  
                         ,   and  
                         
                          respectively",From the figure it can be visualized that the predicted graph of GA is more closer to actual than the statistical based feature reduction method,"The same result is reflected in the actual versus predicted graph presented in  
                         ,   and  
                         
                         , for firefly optimized feature reduction techniques applied on ELM, OSELM and RBPNN respectively","Moreover,  
                         ,   and  
                         
                          represents the actual versus predicted graph of Firefly algorithm with evolutionary framework for the prediction models ELM, OSELM and RBPNN respectively, which shows the predicted graph of all the datasets considered here in this study with respect to the experimented time horizon is more closer to the actual graph in comparison to the rest of the feature reduction techniques","The experimental work covers the six different time horizon such as 1 day, 3 days, 5 days, 7 days, 15 days and 30 days of BSE, NSE, S&P 500 and FTSE dataset, but due to the limitation on space only the actual verses predicted graph of BSE for 1 day to 30 days time horizon is given in this study","The total result of all the prediction models reducing features with the statistical as well as optimized feature reduction techniques for BSE, NSE, S&P 500 and FTSE datasets is shown in the form of MSE during the training phase, which is given in the respective tables","The experimental results obtained from the MSE during the training phase of PCA based ELM, OSELM and RBPNN for 1 day, 3 days, 5 days, 7 days, 15 days and 30 days ahead to be predicted using BSE, NSE, FTSE and S&P500 dataset are given in the  
                         ,  
                          and  
                          respectively","The most notable result that we observed from the above tables is that, for 1 day ahead prediction, PCA-ELM is showing minimum error than the rest of days ahead to be predicted",As the number of days ahead prediction increases the performance of the model is significantly decreases,"The above result helps to visualize that, through this model short term investment is more predictable than long term investment","Furthermore, the MSE result of PCA-OSELM exhibit that BSE and FTSE during 3 days, NSE during 5 days and S&P 500 during 30 days ahead to be predicted is outperformed over the rest time horizon for the respective datasets","In addition to this, the MSE result of PCA-RBPNN is analyzed, which make to know that the performance of RBPNN is worse for all the time horizon of all the experimented dataset. 
                         
                         
                         
                          shows the prediction accuracy for the same dataset during the training period considering the time horizon of 1 day, 3 days, 5 days, 7 days, 15 days and 30 days ahead","For all the datasets it can be observed that like PCA-ELM, FA-ELM is showing better performance for 1 day ahead prediction than the rest of the days ahead considered here in this experimental work","In addition to this, the MSE result of FA-OSELM is analyzed and found that, for BSE prediction the above model is showing better performance for 1 day, for NSE the performance is better for 7 days whereas for FTSE and S&P500 the model is giving better prediction accuracy on 3 days ahead prediction","Further, FA-RBPNN, which is used for a comparison, as it is a widely accepted model, is showing the worst performance for all the datasets irrespective of the time horizon","The ability of the firefly optimized feature reduction for training the prediction models are shown in  
                         
                         
                         ","According to the MSE result of the above model, it can be seen that, firefly based feature reduction well performed in 1 day ahead prediction for ELM and OSELM and predicting better compared to the statistical based feature reduction method, in the regard of minimum MSE value","The MSE result of ELM and OSELM is comparatively less than the PCA and FA irrespective of the different experimented time horizon, applicable for all the datasets","The Firefly algorithm is very efficient in generating optimized features, which implies that its success rate to find the global optima is more than the statistical based feature reduction techniques","Though RBPNN result is worse compared to ELM and OSELM, but for a comparison purpose, this study has included RBP neural network model, which is a bench mark model to the analysis of financial market forecasting","The training phase result of GA in the term of MSE is illustrated in  
                         
                         
                         ","The performance of ELM and OSELM using GA as feature reduction is better for one day ahead prediction, same as FO, than the rest of time horizon ahead to be predicted","According to the result, if we consider the MSE of one day ahead, for BSE, FTSE and S&P dataset, GA is presenting lesser error than FO in ELM prediction model, whereas for NSE dataset FO is performing better","Similarly, by establishing the comparison of FO and GA applying on OSELM, in case of NSE dataset, FO is found to be performing better than GA but for the rest of the dataset GA is performing better",The   result shows it worse performance compared to ELM and OSELM regardless of all the feature reduction technique,"The performance of Firefly algorithm with evolutionary framework, which is the combination of FO and GA is better than the individual performance of FO and GA for all the prediction models for the same datasets, which is briefly illustrated in  
                         
                         
                         ",The aspect of combining FO with GA is to explore the search space more particularly than the individual explore of search space by FO,"FO can search more efficiently but, whenever the concept of GA is added, which is described briefly in the algorithm part, it is able to explore more precisely","The result shows that Firefly algorithm with evolutionary framework, proficiently predict 1 day ahead data than the rest of the days ahead prediction, which is seen in both ELM and OSELM","Comparing all the prediction outcome of ELM and OSELM with respect to both statistical and optimized based feature reduction techniques, it is observed that not only for 1 day but also for 3 days, 5days, 7 days, 15 days and 30 days the performance of Firefly algorithm with evolutionary framework is better than PCA, FA, FO and GA","Though the result of RBPNN is worse than ELM and OSELM but one thing can be noticed that the result of Firefly algorithm with evolutionary framework based on RBPNN is found to be performing significantly better than the rest of the feature reduction methods based on RBPNN. 6.5 Performance evaluation criterion The performance of the predicted models is evaluated by calculating the deviation of predicted stock price and the actual stock price through the calculation of Root Mean Square Error (RMSE), Mean Absolute Percentage Error (MAPE), Mean Absolute Error (MAE), Theil's U and Average Relative Variance (ARV)","The above measures are used to assess the performance ( ) of the prediction model during the testing phase, whose mathematical formulation is described in  . 
                         
                         
                         
                         
                      Here,   and   are the  th day actual and predicted stock price and   is the number of total experimented samples","Lesser the value of RMSE, MAPE, MAE, Theil's U and ARV leads to a better prediction model. 
                         
                          shows the summary of the verification result obtained from different performance measures during the testing phase",The performance of different feature reduction techniques with respect to the prediction models is analyzed briefly,"In case of PCA feature reduction technique, PCA-OSELM is showing better results in RMSE, MAPE, Theil's U and ARV, except MAE where PCA-ELM is having minimum value","Similarly, in FA, the performance of FA-OSELM is better in terms of minimum value of MAPE, Theil's U and ARV",The rest two measures such as RMSE and MAE is minimum for FA-ELM,"In addition to this the performance of firefly and GA with respect to all the prediction model is examined, where it is found that Firefly-OSELM is showing minimum value in terms of MAPE, Theil's U, MAE and ARV error, except RMSE","Similarly GA-OSELM is giving a minimum of RMSE, Theil's U, MAE and ARV error except MAPE, GA-ELM is giving minimum value","Further the efficiency of Firefly algorithm with evolutionary framework is viewed through the result of performance measures presented in  , where it is clearly depicted that the proposed Firefly algorithm with evolutionary framework-OSELM model is outperformed over all the experimented model with the minimum value of RMSE, MAPE, MAE, Theil's U and ARV",All the above performance measures are calculated for 1 day ahead to be predicted for BSE Sensex dataset,"The convergence graph of Firefly, GA and Firefly algorithm with evolutionary framework using MSE for 1 day, 3 days, 5 days, 7 days, 15 days and 30 days ahead for BSE Sensex data is depicted in  
                         ","The figure itself clearly outlined that with the increase of days ahead, the convergence speed decreases, it means 1day and 3 days ahead is converging faster than the rest of days considered here in this study.  
                          addresses the convergence speed of BSE, NSE, S&P 500 and FTSE 100 using MSE for Firefly algorithm with evolutionary framework","The figure shows that BSE Sensex dataset is converging at about 6, NSE is at 10, S&P 500 is at 25 and FTSE is at 51 number of iterations","The convergence speed of BSE, NSE, S&P 500 and FTSE 100 using MSE for GA, Firefly and Firefly algorithm with evolutionary framework is produced in  
                         , where the convergence speed of different optimized feature reduction techniques can be marked","From the convergence graph it can be clearly figured that the Firefly algorithm with evolutionary framework, which is the combination of Firefly and GA, is converging faster than the individual converging rate of Firefly and GA optimization algorithm. 7 Overall work analysis Analyzing the stock market is one of the important task, where many methods are proposed by the researchers to predict stock price with the expectations of getting predicted results closer to the actual result","The empirical results in predicting four different stock values using the feature reduction methods for prediction models are shown in this experimental work. 
                   For all the optimized feature reduction techniques the common controlling parameters such as number of populations and number of iterations is set to 100 to make the comparison standardize",Apart from this common controlling parameters each algorithm is having some specific parameters,"Hence, for a particular algorithm proper parameter value set is based on the architecture of the algorithm, as tuning of this algorithm specific parameter has a vital role in the performance of the model","For GA the two basic parameters, crossover rate and mutation rate, where the crossover rate value at 0.8 and the mutation rate at 0.1 is giving more optimal result according to the time of convergence","Similarly the attraction coefficient and absorption coefficient are two specific parameters of Firefly algorithm, where the attraction coefficient with 1 and absorption coefficient with 0.8 configurations achieved the best result in the course of executing test runs",The simulation result of GA and firefly with respect to the number days in terms of MSE is nearly equal for all the experimented stock market data,In both these optimized feature reduction based method ELM and OSELM are giving better results than RBPNN,"In comparison to firefly and GA, Firefly algorithm with the evolutionary framework showing better performance irrespective of the prediction models and datasets","The features reduced by Firefly algorithm with the evolutionary framework for OSELM outperforms over all the combinations of both statistical and optimized feature reduction methods for all the experimented prediction models, whose relative improvements result for BSE Sensex is shown in  
                      
                      
                      
                      
                      ","The relative improved results of the proposed Firefly algorithm with evolutionary framework for OSELM in predicting BSE Sensex follow-up days ahead by, 
                   The aforementioned remarks about the performance of Firefly algorithm with evolutionary framework for OSELM should be accompanied by a conclusion that the proposed model is significantly improved and better than other feature reduction through transformation method applied on the prediction models used for the experimentation by this study. 8 Conclusions This study presents the Firefly algorithm with evolutionary framework based feature reduction through transformation model for OSELM to predict the future stock price","The proposed model's performance is evaluated using four stock market dataset such as BSE Sensex, NSE Sensex, S&P 500 index and FTSE index",The objective on feature reduction focused on two factors: statistical based feature reduction and optimized feature reduction,"PCA and FA are considered for statistical based feature reduction, whereas Firefly, GA and Firefly algorithm with evolutionary framework are used to reduce the features using the respective optimization technique","After reducing the features the datasets are employed to three prediction models such as ELM, OSELM and RBPNN, exploring the predicted outcome of six time horizon","Though, all the feature reduction methods have the ability to reduce features with the expectation of getting better result, but the potential of the proposed Firefly algorithm with evolutionary framework is very high and also delineating outstanding performance for OSELM prediction model","In future, this work can be extended to other financial aspects with analyzing their influencing factors","CRediT authorship contribution statement 
                       Conceptualization, Data curation, Formal analysis, Methodology.   Supervision, Validation.   Writing - review & editing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0957417418306535, 1 Introduction Arms races alternate between incremental and disruptive moves like the stockpiling of armaments and the invention of airplanes,The malware detection/evasion arms race is no exception,Its history exhibits periods of minor moves and counter-moves like tweaking malware to avoid known signature of disruptive moves like the transition to polymorphic concealment,Our core contribution is to show how to use search to restrict the adversary to only making disruptive moves,"Given an evasion or detection technique, we use machine learning to search for transformations that produce variants that force the adversary to make expensive, disruptive moves",The specific detection and evasion techniques we consider use information theoretic entropy,"To conceal their malware, black hats often rewrite it",Polymorphism hides malware by encoding it and decoding it at runtime,"Because it is trivially semantic preserving, it is the dominant way black hats conceal their malware — in 2016, Webroot reported that 97% of malware is polymorphic ( )",There are two main classes of polymorphism: those that encrypt the malware and those that compress it,"Both encryption and compression increase the entropy of their input, when forced to produce output whose size is not much larger than the input",Neither Trojans that are large relative to their hosts nor disk-resident malware that hide themselves polymorphically can violate this constraint without risking becoming too large and therefore less viable,"In this case, polymorphism introduces an entropy signature ( ) that distinguishes it from many classes of benign-ware","The promise of entropy as a malware detector is that it works on executables as binary strings, without needing pre-processing, disassembly, dynamic analysis, reverse engineering, or manual analysis",Structural Entropy ( ) ( ) took the first step toward effectively exploiting entropy to detect malware when treating executables solely as binary strings,"When a file is separated into chunks, the   of that file is its per-chunk entropy.   computes the entropy signature of a file over small, fixed size chunks","After computing a file’s entropy signature,   segments this signature into sequences of chunks, treats these segments as symbols, then uses Levenshtein to compare the resulting string against the segmentation strings it extracts from other files",It has two severe limitations,"First, it does not scale: it relies on the pairwise comparison ( ( 
                      )) of a suspicious program with a zoo of known malware and benign-ware","Second, its decomposition of an entropy signature into segments, whose entropy it compares, relies on six parameters, three of which are implicit and baked into   technique itself ( )","The authors do not elucidate the settings of these implicit parameters, nor is it obvious from first principles how they should be set and whether they can be learned","There are other three explicit parameters: the chunk size that divides the file into blocks to measure their entropy, the number of chunks or blocks used and a noise threshold","To overcome these limitations, we present a disruptive detection move called EnTS (Entropy Time Series), a new entropy-based malware detector",EnTS scales and requires only three explicit parameters that can be learned from a corpus,EnTS constructs a metric space for entropy signatures,"EnTS considers an entropy signature as a time series, then applies wavelet analysis to clean the signature and extract a simplified signature from the amplitude and longitudinal variation in a file’s entropy",EnTS then treats this signature as a point,"In this way, EnTS constructs a metric space, while   resorts to segmentation and then to the pairwise computation of edit distance to construct its metric space ( )",Eliminating  ’s segmentation step removes the implicit parameters that   requires,"The zoo defines EnTS’ three explicit parameters: the chunk size, number of chunks, and noise threshold ( )",We designed EnTS to defeat polymorphism,"On a corpus that includes the Kaggle and the VirusShare training sets and an equal number of benign-ware from   , EnTS achieves 82% accuracy when maximizing 100% precision and 93.9% accuracy when maximizing accuracy","EnTS surpasses the quality of   in terms of scalability: it is more than 15 times faster and linear in its time and memory consumption in contrast to  ’s  ( 
                      )",EnTS’ accuracy is between 2 to 5 points higher than  ’s in all cases,"EnTS is also good at detecting metamorphic malware, not just polymorphic, because metamorphic malware often has compressed or encrypted regions",It detect all the metamorphic variants from our test corpus,"EnTS outperforms all 56 VirusTotal AV engines 
                       applied to the same data, the best of which achieved only 40.6% accuracy","EnTS’ time complexity is linear in the number of files being classified; it is 3000 times faster than its main competitor in accuracy, another information theoretic technique named normalized compression distance ( ) that we built as a baseline ( )","Clearly, the next disruptive evasion to defeat EnTS must control the entropy of packed regions","For this task, we developed   (the evolutionary packer or ‘El Empaquetador Evolutivo’).   is a polymorphic engine that controls the entropy of packed regions under a tight space budget, increasing the size of its input at most 1% ( ).   creates a packed variant with a specific entropy signal by creating chunks with a specific entropy and injecting into the packed binary","To decide position, entropy, and size of these chunks,   leverages evolutionary computation.   is an instance of adversarial machine learning; it uses search to exploit the vulnerabilities of a entropy-based detector in order to fool it ( ). 
                       defeats  , EnTS, and other state of the art binary-based detection techniques ( ).   explodes the false negative (FN) rates of these techniques: Prior to  ’s application, these technique’s FN rates range over [0%–9.4%]; after  ’s application, their FN rates range over [90.8–98.7%].   rapidly learns to defeat these tools; It takes two generations to defeat the weakest ones and eight to defeat the strongest","To its credit, EnTS resists   better than the other techniques ( )",EnTS embeds binaries into entropy metric space and uses machine learning to detect malware and to advance the state of the art,"Defeating it requires  , a disruptive entropy-based evasion technique that uses search to control the entropy of the binary it is concealing","The main contributions of this paper are: 
                   EnTS,  , and the corpus on which we evaluated them will be available online 
                      
                   2 EnTS: Entropy Time Series Analysis The dominant forms of polymorphism hide their payloads using either compression or encryption",These are string transformations that change the entropy of their input string,The promise of string-based malware detection is that it can distinguish benign-ware from polymorphic malware based on the differences in the entropy of their binaries,This is a potentially disruptive move that could obsolete the current state of the art polymorphic engines,"In pursuit of this game changing malware detection, we present EnTS 
                      , which we designed to advance the state of the art in the scalability and accuracy of string-based malware detection",Our goal is to define entropy-based signatures for code,"Shannon entropy ( ) is defined over an event sequence, hence, we must convert strings to event sequences","First, we define the event space as the byte sequence within a string, and measure their entropy counting the byte frequency",This defines the entropy signature,"This method can be applied to any string-based information source, like source code",EnTS instantiates this idea for binaries,"We consider each file as a stream of chunks (fixed length segments), each with an associated entropy value",The entropy of each chunk is calculated from the byte frequencies of that chunk from which a probability distribution on the bytes is calculated,"As we want to compare strings of different lengths, we normalize the signature to a fixed length",This signature is noisy so we use wavelet analysis to clean it,"Finally, EnTS leverages machine learning to classify the signatures",EnTS exploits time series analysis,"Time series have been widely studied in the literature ( ), applied in many fields, and have often been used for prediction","Typically, time series are either analysed to estimate the next value ( ) or grouped by similarity ( )","Here, we focus on similarity in our design of Entropy Time Series or EnTS","Like other machine learning malware detection techniques, EnTS requires labelled data ( )",It compares a suspicious binary   against a zoo containing labelled malware and benign-ware,It considers   to be a malicious if it is more similar to malware than benign-ware,"EnTS creates the classification space from these binaries as follows: 
                   This  -vector of smoothed entropies forms the time series for each file","We can then interpret each time series as a coordinate in an  -dimensional space and train a machine learning classifier to distinguish malware and benign-ware. 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   2.1 File division We compute the entropy signature of a file,  , as a Discrete Haar wavelet Transformation ( )","This requires that the entropy signature length and, as a consequence, the number of chunks,  , be a power of 2,   
                          for some  : where  |  is the zoo’s benign-ware,  |  is the zoo’s malware and   is the  ",The ceiling operator produces an integer between the two median lengths for the two sets of binaries,"Then, for each program  , we divide its binary representation into chunks of size  ",The chunk size is a critical parameter for EnTS,Chunks are file segments but we also considered sliding windows as an alternative,This was quickly rejected because it adds redundant information into the entropy signature,"Given that the atomic constituents of chunks are bytes, it is easy to see that a chunk size of 256 bytes is optimal with respect to the amplitude of entropy variation",There are   possible different bytes,"According to   the entropy is maximum if and only if the distribution is uniform,   the entropy of a chunk will be maximal when every possible byte has equal probability","This corresponds to a uniform distribution, where every element has the same appearance probability","To measure the probability of appearance of these 256 elements, we need, at least, 256 samples composing a chunk","Hence, the minimum chunk size that allows the maximal possible variation in entropy (from 0 to 8 bits) is 256",On the other hand we want as many chunks as possible in each file so we also want the length of chunks to be as small as possible,"These chunks provide more information about the entropy signature, showing granular variations within it. 
                         
                      
                            
                            
                         Example Consider a zoo of just two binary files,   and  , and a chunk size of  ","These programs, considered as binary strings, are divided in chunks",Suppose that   and  ,"Each chunk is related to a wavelet coefficient, therefore, the number of coefficients would be 20 for   and 6 for  ","However, the Haar wavelet requires 2  coefficients","Next section shows how to adapt the width. 2.2 The entropy signature Once we have the chunk division for a file, we need to reduce or increment the number of chunks to  , in order to fit the mother wavelet, which is explained next in  ",The selection process of the chunks is equidistant,The first and last chunks have special status because file head and tail are usually relevant parts in malware analysis,"To choose the rest, we calculate an increment value   to get the next chunk index using the floor of the accumulation of this factor as the next chosen index","For each chosen chunk, we calculate its Shannon entropy on the basis of the byte frequencies of the chunk: where  ( ) is the probability of byte   within the  th chunk,  , of program  , calculated from its frequency count within the chunk",The concept of chunk generates a local entropy computation,Therefore repeated chunks will not reduce the entropy,"For example, imagine a string (00011011) over the alphabet  ","This string has maximum local entropy,  , chunk entropy","However, the global entropy  ((00011011) ) tends to 0, as   → ∞ with Σ fixed","In this case, local or chunk entropy is maximised, while global entropy is minimized","This problem did not occur in our dataset and can be easily checked. 
                         
                      
                            
                            
                         Example Following the example of  , we need to adapt the width of   and   to the Haar wavelet coefficients, which are 2 ",Suppose that we choose   then we need   coefficients,For   we need to contract the number of chunks from 20 to 8 and for   we need to increase the number of chunks from 6 to 8,"In order to choose these chunks, we generate a subset of the current chunks using a jump factor for each file","The chunk index is initially set to 0, and it is incremented in every step by   for   and   for  ","The indices are selected using the floor of the accumulated jump value, so the chosen indices will be: After, we only need to calculate the entropy of each chunk, defining an N-vector of entropy values for each file which is considered as an entropy time series. 2.3 Wavelet denoising This last step smooths the entropy signal using wavelets","In our case, the mother wavelet is defined by: where   corresponds with the dimensions of the final  -vector space,   is a shifting parameter,  ( ) are the entropy values, | | is the total number of chunks,   is the current chunk   in the sequence and  ( ) is the Haar wavelet defined by: The Haar wavelet is chosen because it approximates a step function from the original function ( )","Other wavelets, such as Daubechies or Biorthogonal wavelets were considered, but their performance was worse and their results were similar to the Haar wavelet","As EnTS focuses on the variation patterns, a noise free step function provides all the information it needs about the most relevant entropy variations","Then, we calculate the discrete Haar wavelet transformation",Each iteration in the process is divided into two parts: calculating the scale coefficients and calculating the detail coefficients,The scale coefficients contain the most relevant information about the signal while the detail coefficients contain information about the small variations,"In each iteration, the coefficients used are the scale coefficients for the previous iteration,   in iteration number 2 only the scale coefficients of iteration 1 are used to calculate the scale and detail coefficients of iteration 2, and the other wavelet coefficients are not modified","According to the Haar wavelet equations, a scale coefficient is calculated by: and a detail coefficient is calculated by equations: The scale coefficients are positioned at the beginning of the wavelet and the detail coefficients after the scale coefficients","For example, with   the iterations generate the coefficients as follows: 
                      In the final iteration the wavelet,  , has been constructed","We can use it to reduce the noise from the entropy time series, using a  , on the wavelet coefficients in this final iteration",Those values that are below the threshold are set to 0,This process improves the performance of the classification task by eliminating minor variations in the original signature,"Lastly, we apply the inverse wavelet transformation to reconstruct the entropy signature without the noise. 
                         
                      The resulting coefficients vary between 0 and 8 because of the choice of chunk size and will be used as coordinates of the entropy time series in the classification space","This space allows the creation of scalable models based on machine learning classifiers, and significantly improves the speed of the classification process","The model scales linearly (as we discuss in   and show in  ) because it does not require a pairwise comparison between every element on the training data, as other state of the art algorithms such as Structural Entropy do ( )","The classifier will infer a way of discriminating the malware and benign-ware files within the zoo, focused on targeting 100% precision, that is one of our main goals. 
                         
                      
                            
                            
                         Example Following the examples of   and  , We need to remove noise and simplify each time series by obtaining the reconstruction coefficients","Now, for purposes of illustration, we focus on  ",We apply the discrete Haar wavelet transformation,"Assume that the entropy values for   are (4,5,4,1,1,2,1,2) then the wavelet transformation process will give us: We apply the threshold, in this example it is 0.75, to   and we get Then, we apply the reconstruction process to   and we get the reconstructed signal as (4.5,4.5,4,1,1.5,1.5,1.5,1.5)","These values are the coordinates of  ’s signature in the space. 3 Experimental data We want to study EnTS’ performance on encryption-only, compression-only and both encrypting and compressing polymorphic engines, so we use three malware datasets: Kaggle ( ) malware competition dataset 
                      , packed ( ) malware from VirusShare 
                      , and  , a dataset that we construct from   and  ","EnTS requires labelled benign-ware (Benign) to operate ( ) so, for each of these cases, we collect 3 corresponding benign-ware datasets  ,  , and  . 
                       contains two subsets: train and test","Kaggle’s test subset is not labelled, so we train and test on the train subset","It is composed of 10,869 Malware files","The dataset contains 9 malware families whose features are summarised in  
                      ",The families are useful for understanding how EnTS works,There are two files per malware: a byte representation (hexdump) and an asm file with IDA Pro-information from the disassembly process,"We used  
                      
                       to convert the hexdumps to binary executables","According to Kaggle description, these binaries are not packed","This dataset was published February 2015, and it has become a benchmark on malware analysis, used in more than 50 research papers ( ). 
                       contains Win32 malware whose packing system was known, so we focused on malware uploaded to VirusShare in January 2016","This database is composed of approximately 131,000 Malware files, covering different types of malware",We filter them using Linux file command,Only those files identify as PE software passed this filter,"By combining Yara 
                       with packer rules extracted from the YaraRules project 
                      ","We extracted 10,000 malware with known packers","Around 70 specific packing systems were detected by Yara, however, several of them came from the same family, so we focused on the most frequently occurring families ( 
                      ). 
                       synthesises   and   by sampling: one joining different types of malware and benign-ware and the second for distinguishing packed and non-packed software","The former is formed by   from Polymorphic data ( ),   from Metamorphic ( ) and   from  ","In industry, white hats often must analyse different kinds of malware at the same time",This dataset aims to emulate this scenario,"The second mixed dataset,  , is composed by   packed and   non-packed files",This dataset aims to evaluate our abilities discriminating packed and non-packed,"For Benign, we collected 2000 packed benign ( ) and 2000 non-packed benign files ( )","For   the benign-ware mixed as   non-packed and   packed (to keep packed and non-packed proportions), in the first case and   packed,   packed for the second case",The resulting datasets have 2000 malware and 2000 benign instances,The benign files were collected from  ,"All the benign-ware was submitted to VirusTotal 
                       to ensure that no anti virus detect it as malware",All the files are also PE executables and the packed files are discriminated using Yara,EnTS is a classifier ( ),The class imbalance problem is the bane of classifiers ( ),"In our corpus, malware outnumbers benign-ware so we uniformly sampled 10,000 files from the   and   dataset without replacement and randomly divided it into five partitions with equal size (  and  ). 4 Evaluating EnTS We built EnTS to be accurate and scalable, here we demonstrate that it achieves both ends, its accuracy is 93.9% improving over   by 1.5 points, and the other state of the art up to 8.9 points, and it scales linearly",The evaluation consists of four steps,The first is data selection,"In every experiment, we have chosen the same number of malware and benign-ware instances",The training data consists of two thirds of the instances and the remaining third comprises the test data,The test data is always fresh data for either approach and is randomly selected by uniform sampling at the beginning of the process,The second step consists of the feature space generation for classification in EnTS,"Once this is prepared, we train a classifier as appropriate","After, we evaluate malware detection on the test set, recording the accuracy and the false positive rate. 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   4.1 Algorithms and Parameters for EnTS study To compare against EnTS, we implemented three other information theoretic features of binary strings from the literature as baselines",The first is the compression rate (CR) which calculates the ratio of the compressed length to the uncompressed length for a given file compressor and is related to the Kolmogorov complexity of the specific file ( ),"We chose LZMA2 as the compressor and its maximum compression parameters and the maximum windows size,  , 4GB, using the package 7zip ","The second is the Normalised Compression Distance (NCD), which approximates the Normalised Information Distance ( ), a universal, generic, information theoretic metric","Formally, NCD is where   are strings,   is their concatenation, and  ( · ) is the compressed size function for a specified compressor",NCD also uses LZMA2 as the compressor,"Finally, we compare against Structural Entropy ( )",Sorokin introduced this technique in 2011 ( ) and Baysa et al. applied it to metamorphic malware in 2013 ( ),"It divides a file into chunks, calculates the entropy of each chunk, then groups the chunks into arbitrarily sized segments (the information for each segment is its average entropy and its size)","It generates a similarity matrix, performing a pairwise comparison on the files based on Levenshtein distance","This approach is  ( 
                         ), where   is the number of files","Further, the variable number and variable size of segments in a file means this approach may determine a file with more segments to be totally different from another file with fewer segments even though the overall entropy pattern in the two files is similar",EnTS escapes this problem: it extracts a fixed length entropy time series from a file as a token stream and operates directly on this time series and therefore all of the file’s information at once,"The implicit parameters chosen for this comparison are the same as those used in both Baysa and Sorokin’s work:   
                          and  ","EnTs has three parameters: the chunk size, signature size, and the wavelet threshold",In this experiment we set the chunk size to 256 ( ),"The signature size (  the number of dimensions) is 512 (2 ) because the smaller zoo (packed files) has an average size of 116 KB (  
                         )",The wavelet threshold is 0.5 ( ),"EnTS uses two classic classification algorithms: Random Forest 
                          and Inference Trees 
                         ","To optimize finding the boundary between malware and benign-ware for each measure, we used multiple-learning to combine these two algorithms","Multi-learning divides the learning process, specialising it to different regions of the space","Multi-learning penalises false positives during construction ( ). 4.2 EnTS’ accuracy EnTS was designed to detect polymorphic malware.   Following related work ( ), we consider a detector to be accurate when its accuracy is at least 90%",EnTS uses a classifier ( ),"To determine how much of its performance is due to its machine learning classifier and how much to its similarity metric, we compared EnTS with other information theory similiarity measures, using the same parameters and classifier and ask:  
                      NCD and   generate a similarity matrix while EnTS and CR describe point coordinates for the signatures and the compression rate, respectively",Applying machine learning to EnTS and CR is straightforward because we have the points and we only need to discriminate them,"For NCD and   we consider each row of the training similarity matrix as coordinates, due to the number of files is fixed",This provides the points that the machine learning algorithm uses,"For testing, we will consider the coordinates as the similarities among the test files and the training files","The machine learning algorithms chosen are non-deterministic approaches (they choose a random seed during the initialization process), then, we need to generate different models to measure their median performance ( )","Hence, each experiment has been carried out 100 times, and the median and standard deviation have been provided to compare the results","Furthermore, in order to compare different algorithms, we have applied the Wilcoxon test to evaluate whether there is statistical significance among the results or not",We consider that there is statistical significance when the   value is less than 0.05 using EnTS as benchmark,"In order to reduce the redundancy of correlated variables in the space, we have eliminated those dimensions whose Pearson correlation was higher than 0.8 with respect to other dimension","This reduces the space to the 5% of the original dimension. 
                         
                          shows the direct comparison between the four techniques discriminating malware and benign-ware, according to the accuracy",It divides the results by technique and provides the accuracy of applying each algorithm to the specific datasets described in  ,"For  , EnTS and NCD generally obtain the best results (EnTS is over 97% of accuracy in all cases and NCD over 93%).   is always worse than EnTS and CR is the worst approach","For  , all techniques reduce their accuracy but NCD, which increments its discrimination abilities","EnTS and NCD keep competitive results compare with the rest of the techniques (over 94% and 95% in all cases).   shows that EnTS and   are better discriminating malware and benign-ware than the other techniques (93.9% and 91.7% of accuracy, respectively) when the dataset mixes packed and non-packed binaries",NCD obtains worse results than in the previous cases and CR is the worst technique,"This analysis shows that EnTS and   are the best techniques classifying malware, when no previous information about the malware has been obtained. 
                         
                      
                            
                            
                         Findings We originally asked whether EnTS is accurate","Targeting accuracy, it obtains 98.0% in  , 94.1% in   and 93.9% in  ",We also compared EnTS to the other techniques,"EnTS is more accurate than CR and  , and similar to NCD","These results show that NCD and EnTS are competitive classifiers in all cases, although EnTS scales 3000 times better ( )","CR does not detect any malware that the other techniques do not also detect; EnTS easily defeats  . 4.3 EnTS’ precision One of the main aims of a malware detector is to reduce false positives, and, as a consequence, improve precision ( )",We aim to achieve a precision of 100% (  there are no false positives),Due to the classification nature of EnTS we use the ROC curve to decide a cut-off during its validation process,"We compared EnTS with the other information theory similarity measures, using the same parameters and classifier and ask:  
                      Improving precision is equivalent to reducing false positives","The classifier penalizes false positives during the learning process, as mentioned above, to ensure that the model effectively detects malicious programs",The cut-off or threshold used in the ROC curve also provides a confidence value to the random forest voting system that helps to reduce false positives,"Using 10 cross-fold validation in the training set, we set the cut-off to the most conservative value,   the one that ensures 0 false positives in all validation sets","It is this last model that we apply to the test data. 
                         
                          shows the median results for the ROC curves for all the experiments","In this table, we can see how the threshold variation modifies the true/false positives rates for each dataset","For  , EnTS detects 38% of malware with 100% precision (  0 false positive), NCD detects 67% and   detects 44%","For  , EnTS detection rate is reduced to 18%, NCD to 19% and   to 20%","For  , NCD improves its results significantly (61%), as well as EnTS (64%)","This table shows that EnTS only outperforms all techniques with 100% precision, when the data is mixed, in the other scenarios there are not significant improvements",These results also discard CR as a classifier targeting the 100% precision,"After setting the threshold to the 100% precision, the median accuracy achieved by EnTS for   is 69.0%, for   is 59.0% and for   is 82.0%",We aim to understand why EnTS obtains better results in some specific cases and why it is not performing as well as the other techniques with respect to the precision,"First, the results suggest that EnTS can easily separate non-packed malware and benign-ware",In order to provide an intuition about how this separation is performed we have generated a t-SNE ( ) projection of  ,"This projection aims to show, in a low-level dimensional space (normally 2 or 3 dimensions) the separation among the data instances in their original (usually high-level dimensional) space, according to a specific metric, in our case, the Euclidean distance","During the mapping process, which generates the projection, t-SNE aims to keep coherence among those instances that are close in the high dimensional space, setting them close in the low dimensional space","Compared with other projection techniques like Isomap, Sammon mapping or a Locally Linear-Embedding, t-SNE performs better showing high dimensional data discrimination based on their manifolds structure ( )","It is effective with space of similar dimensions to us, for instance, Maaten and Hinton showed its performance projecting spaces of 784, 10,304 and 1024 dimensions to 2 and 3","Our space contains 512 dimensions ( ), hence our dimensionality is inside the projection bounds. 
                         
                         shows the results of this projection for EnTS and NCD","During the discrimination process, the application of Random Forest helps to discriminate those sections which are fuzzier, due to the multi-learner approach, that sets different trees in these sections to ensure a clearer discrimination","To analyse the false positive rate, we focus on those black instances (benign) invading red clusters (malware)","If a black instance is in the middle of a red cluster, it will be considered as malware with high probability","Therefore, even when the cut-off is more conservative, it will still be misclassified",In   we can see this phenomena in both: EnTS and NCD,In this case of NCD this normally happens closer to the boundary,"The cut-off sets this whole boundary section as benign, to reduce false positives",EnTS has wider red regions free of black instances,"However, the boundary of these regions are problematic, due to they are covered by several black instances","This forces the cut-off to consider the boundary as benign-ware. 
                         
                      
                            
                            
                         Findings We asked whether EnTS is accurate and precise",It   precise: it obtains 100% precision,"However, it falls short of the 90% accuracy bar","It obtains 64% accuracy on  , 38% on   and 18% on  ","Again, EnTS is more accurate than CR and  , and similar to NCD. 4.4 EnTS’ accuracy by family We want to go deeper in the specific concealment strategies used by the families and packing systems and how they affect the performance of each technique","This leads us to ask:  
                      
                         
                         breaks down the results from   in families ( ), packing systems ( ) and strategies ( )",This only increments the granularity of the binary classification in order to detect how different families or packers affect it,"For  , NCD achieves the best performance in almost all cases, followed by EnTS, but NCD’s time and memory performance is significantly lower ( )","All techniques are good discriminating Armadillo system, as well as NET","For   families, we can see that NCD and EnTS outperforms the rest of techniques in all cases",This also shows the effectiveness of EnTS when it is applied to metamorphic malware,Due to metamorphic malware has not intuitive entropy variations we focus on the two specific families: Vundo and Obfuscator.ACY,"Vundo was previously studied by Li et al., who provided a description about the metamorphic engine ( )","This description mentions that the data section is encrypted or compressed, therefore this produces entropy variations that can be detected by EnTS","This fact is also detected in the entropy signature, where there are long sections with higher entropy than others","For Obfuscator.ACY the previous pattern is also frequent in the entropy signature, but in smaller sections, probably related to encrypted or compressed strings","These variation patterns make the metamorphic data totally unique for EnTS, and it is the reason it can easily detect them","For  , the best results are for polymorphic and metamorphic data, applying NCD and EnTS","For  , NCD is the best, followed, in this case, by   which is close to EnTS. 
                         
                      
                            
                            
                         Findings We wondered how different concealment strategies affect EnTS and the other baselines","EnTS and NCD are strong against polymorphism and, surprisingly, metamorphism","They can handle specific families and packers, forcing malware writers to create new ones. 4.5 Packed and non-packed The current state of the art is focused on distinguishing between packed and non-packed software, this leads us to ask:  
                      EnTS and NCD are accurate detecting malware in different packed and non-packed zoos, this section aims to analyse their ability to discriminate between packed and non-packed software.   dataset was designed to fulfil this goal: the dataset contains 50% packed and 50% non-packed binaries, mixing malware and benign-ware in the same proportions","The accuracy values for distinguishing packed and non-packed are: 88.1  ±  0.7 for NCD, 69.4  ±  0.3 for CR, 82.1  ±  0.4 for   and 88.9  ±  0.3 EnTS","According to Wilcoxon test, EnTS and NCD results are not significantly different.  
                          shows the detection percentage considering packed as the detection target","It illustrates that EnTS overcomes the rest of the techniques specially for 0 false positives. 
                         
                      
                            
                            
                         Findings After comparing EnTS packing detection abilities with the other techniques, we discovered that EnTS is more accurate than CR and   and similar to NCD","EnTS also performs better than the other techniques when the target precision is 100%. 4.6 Scalability We explore the scalability by asking:  
                         
                          shows the average time consumption of the techniques for training and testing","The table is divided in three datasets ( ,   and  ), and three specific values: the space generation or training (where the algorithms generate the similarity matrices, entropy signatures or the compressibility values), the classification process and the total average time",EnTS outperforms every single technique,"We can also see that NCD is the most impractical technique, taking 2 days in the best case and 5 in the worse",This shows that NCD is not optimal for malware detection,It is a consequence of the file compression and the pairwise comparison to generate the similarity matrix,The compression process also affects to CR which needs more time to calculate the ratios,The pairwise comparison affects to NCD and  ,"EnTS uses no pairwise comparison, and this improves the time consumption","Besides, the entropy signature generation and the wavelet decomposition are linear processes, they do not generate a bottleneck during the analysis",The memory consumption of each metric grows depending on the space size,"For NCD and  , this space is related to the similarity matrix, which grows as  ( 
                         ) while EnTS grows linearly  ( ) according to the number of programs,  , due to the number of coefficients (or coordinates) used in the space is fixed",CR also grows linearly according to the number of files,The time consumption ranking for the techniques and for datasets containing 2000 malware and 2000 benign-ware starts with NCD consuming more than five days,"It follows with   consuming 43  min, CR consuming 40.3  min and finally EnTS consuming only 2.5  min","The equivalent memory consumption ranking starts with NCD and   consuming a big square similarity matrix ( ( 
                         ))","It follows with EnTS and CR as  ( ) techniques. 
                         
                      
                            
                            
                         Findings EnTS   scale better than NCD,   and CR","It is linear scalable, 10 times faster than the second fastest technique. 4.7 EnTS vs AV Engines This last part of the study was focused on comparing EnTS with commecial tools","We ask:  
                      We have compared EnTS with 56 Anti-Virus Engines","For this comparison, we have sent all the test set from the  ,   and   to Virus Total","In the case of  , all the data was already classified as Malware using this system, but   is fresher and there are a few anti-virus that can detect it.  
                          shows the comparison between the best engines related to accuracy","We can see that EnTS and NCD obtain the best accuracy results. 
                         
                      
                            
                            
                         Findings We find that EnTS   all the 56 AV Engines in term of accuracy up to 69 points for the best anti-virus using the Kaggle data. 4.8 Discussion The results suggest that EnTS quality depends on the sparsity of the data in the space","When the data is more sparse,  , when the entropy signatures are different among them, it is more difficult for the classifier to find a good discrimination, however, in the opposite situation, it is clear that the variants generate small clusters in the space, where the families or the packers are set together","EnTS space is based on the signatures, it does not depend on the data, therefore the classifier can be easily transported to detect other malware or retrain with new malware, keeping no information of the original training data","Zero-day malware, which is totally different to all the previous data and more likely to benign-ware, might be a countermeasure for EnTS, but if black hats aims to repack or re-conceal variants from current malware they will find limitations set by EnTS (we will discuss this fact in the following section)","NCD’s quality roots in the compressor: when NCD assigns a high similarity, the strings have patterns that can be identified by the compressor after the concatenation","However, when NCD sets two strings as different, it is not confident, because if one of the objects is already compressed, the distance will be maximum","NCD space is based on similarities, therefore, the object selection will affect the space construction","In this case, the fact that we work with specific families and packers improve the abilities during the detection process, because they are more likely to be similar among them","On the other hand, the scalability of NCD is extremely problematic if we want to use this technique as an online detector","However, EnTS is almost 3000 times faster than NCD for the zoos we had studied","Next section will be focused on finding a potential countermeasure generating variants for EnTS. 5 
                      : the evolutionary packer EnTS advances the state of the art in entropy-based malware detection, achieving an unprecedented combination of speed and accuracy","Is it a disruptive move? To answer this question, we immediately take the next step in the malware detection arms race and present   (the evolutionary packer or “El Empaquetador Evolutivo”), an EnTS countermeasure.   manipulates the entropy signature of the binaries to create malware variants",It injects controlled entropy regions (CERs) into the binary file and learns how many CERs to create and where to put them,"In so doing,   defeats EnTS and all other frequency-based malware detectors. 
                      
                      shows   workflow","It uses a malware binary and a detector as starting points ( 
                      )","The malware contains the malicious semantics, which is not modified.   changes the malware shape injecting CERs ( )",This produces variants whose new features aim to produce a misclassification in the detector,"Due to the manipulation process might not be enough, we include a learning process, based on genetic algorithms ( ).   learns to create and place the CERs based on the variant’s classification score, which feeds the fitness function of the learning process",Every variant generated is executable and it runs as the original malware after the unpacking process performed in runtime ( ),The adversarial machine learning process of   is embedded into the fitness function,"Every time that EEE generates a new variant with the aim of reducing the classification abilities of the machine learning based malware detector, it is playing adversary to the machine learning algorithm","From an adversarial perspective,   has access to the classifier and can get the classification probability, but it does not know which specific features needs to be modified, that information is learnt during the search process depending on the response from the malware detector. 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   5.1 Controlled entropy regions. 
                          introduces controlled entropy regions (CERs) anywhere in the binary file","A CER is a set of random bytes constructed so that the entropy of the byte distribution is under our control.   sets a delimiter,  , at the starting and at the ending points of each CER for bounding them",This delimiter identifies the CERs into the binary,"Three parameters control the generation of controlled entropy regions:  ,   and a Gaussian probability distribution sampled to select their placement position (described by its mean   and standard deviation  )",The   parameter is given as the percentage of the available size a region can use,The   parameter is a number between 0 and 1 used to calculate the percentage of bytes used in the CER from the 256 possible bytes,"The probability distribution is a Normal distribution over the interval [0,1].   samples this distribution and multiplies the value obtained by the size of the compressed file to define an insertion point","To construct a CER, we sample bytes from a uniform distribution until the number sampled over 255 reaches or exceeds the  ","We concatenate these bytes in sampling order into an initial string, then construct a new string by repeating that initial string until the desired length is reached, based on Shannon’s entropy definition ( ) so this method achieves the maximum entropy possible for the probability distribution on byte frequencies in the CER","For example, imagine three strings, 111111, 121212 and 123123","The entropy of the first is 0, as the probability of 1 is maximum","The entropy of the second is 1, as the probability for each value is 0.5",The entropy for the third is 1.58 because the probability for each value is   and so on,"The entropy increases as we introduce values until our string contains all 256 bytes, when the entropy is maximum,  , 8",A fixed number of CER descriptors limits the CER search space,Descriptors define a set of CERs sharing similar properties,"Descriptors are formed by a  , a  , and a pair of Normal distribution parameters","Formally, a CER descriptor   where   represents the number of CERs instantiated on this descriptor",The number of descriptors and their characteristics are parameters for the genetic search algorithm ( ),"For giving room to the CERs without significantly incrementing the file size, we start compressing the binary","After, we instantiate the CER descriptors creating   CERs per descriptor",Each CER has a memory position ranged from 0 to the size of the compressed area,"There is a low probability to generate out-of-range CERs, due to the Gaussian distribution","The size of the CER depends on the available area,  , the difference between the original binary size and the compressed size","We aim to not create a variant bigger than the original malware, after the CER injection (within a 1% of tolerance)",This allows EEE to work with compressed binaries,The CERs sizes are normalized according to the available size,"After setting their final size,   injects the CERs ( 
                         ) between two delimiters ( ). 5.2 Genetic CER creation and placement The genetic algorithm looks for the best combination of CER descriptors and delimiters ( )","They form the chromosome, where the CER parameters become the encoding or search space","The number of descriptors is fixed to limit the size of this space, improving the performance. 
                          encodes the CER descriptors parameters into a real valued vector, which serves as a chromosome during  ’s search.  
                          shows the components of a chromosome: delimiter ( ), and, for each region descriptor:  ,  ,   and number of regions ( )",The adversarial search process runs as given in  ,It starts compressing the original program   and creating a population of chromosomes ( ) that represents different parametrizations for the CERS (lines 1 and 2),These chromosomes are created by sampling from a uniform distribution,"Then for each chromosome, it creates the CERs, as explained in   (lines 4 to 10), and injects them into the compressed program   creating a program variant  ′ (line 11)","This variant remains executable, as explained in  ",The algorithm measures the variant’s quality to calculate the chromosome’s fitness (line 12),"If this fitness is better than the best variant found during the whole search so far, this record ( *) of the best variant is updated (line 13)","The fitness function, which guides the search, is the malicious class probability provided by the malware detector we seek to defeat",The search aims to minimize this value,"Once the algorithm ends with the fitness computation of each chromosome, it applies four genetic operations to the population (lines 14 to 16): reproduction (chooses chromosomes for crossover), elitism selection (chooses the best chromosomes for use in the next generation), crossover (swaps a random number of common parameters between two chromosomes) and mutation (sets new values for a chromosome)",These operations improve the population’s quality using the information learnt from the previous generation,"When there are no improvements in the population, or after a fixed number of generations, the search process stops, and the algorithm returns  *, the fittest individual (line 17). 5.3 Modifying UPX to produce  
                      
                          implementation is based on UPX ( ) packer","UPX reformats binaries, compressing their sections and creating a new binary with three sections: (1) UPX0 an empty section where the code is uncompressed; (2) UPX1 the compressed original binary and the uncompression code (named stub); and (3) UPX2 a section containing all imports to properly run the binary","When an UPX-packed binary is invoked, the stub in UPX1 executes and reconstructs the original binary by rebuilding the imports table using the imports in UPX2 and uncompressing the code in UPX1 into UPX0 ( )",UPX uses the UCL compressor ( ),"This compressor produces outputs with higher entropy, and consequently a n-gram distribution closer to uniformity than its input",The adaptation of UPX to create   requires the manipulation of both the packing and unpacking processes in a synchronized way,The manipulation of the packing process is performed after the compression step when new space is available ( ),"At this point,   reads the parameters for the CERs and creates new regions with different entropy densities",The positions of these regions in the binary depend on a Gaussian probability distribution ( ),The delimiters are set at the beginning and end of the regions,"The manipulation of the unpacking process employs the stub,   the assembly code injected into the packed file that will undo the packing process at runtime","Inside the stub, we include a step that identifies and eliminates the CERs before decompression",The identification process uses the inserted delimiters to find the CERs,"Following these steps, we create executable static variants that in execution run just as their original programs. 6 Evaluating  
                   We built   to learn the limits of EnTS","Here, we conduct experiments using   to find these limits",We establish baselines by evaluating   againts   and frequency-based techniques extracted from the literature,"Unfortunately, we find that   comprenhensively defeats all the frequency-based techniques, including EnTS",We discuss the prospects for EnTS in  ,"For   study, we use executable malware","Due to Kaggle malware has no PE headers, the binaries can not run",Therefore for   experiments we used directly the VirusShare dataset for training the detection algorithms ( ),"The variants generated by   require a base binary, hence we sample the malware from this dataset to choose it, in order to ensure that it is known malware for the detectors. 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   6.1 Algorithms and parameters for   study We have focused this part of the study on machine learning tools","In  
                          the different techniques are listed",For each technique we also list the feature space (n-gram vectors or entropy) and the machine learning technique that is used during the classification phase,"Kolter techniques ( ) use the same feature space (4-grams) and authors report 3 classifiers that generates top results: Boosting combined with J48 trees, Support Vector machines and boosting combined with Support Vector Machines","In the case of McBoost ( ) the authors use dynamic analysis to generate an unpack version of the malware, however, this work is not focused on dynamic analysis, therefore we take the two classifiers used for the authors to decide if a binary is malware or not (authors named these classifier as C1 and C2)","Structural entropy and EnTS are combined with Random Forest, as described in  ","For the algorithms we have set the specific parameters specified in each paper, for those parameters that are not specified, we left the default parameters of the implementation","For training, we have used   of the whole data and we leave   of fresh data for testing",The parameters for   are the following: the genetic algorithm has a population containing 50 individuals and evolves during 20 generations,"In each generation the chromosomes are chosen for reproduction using a tournament process, while 10 individuals are chosen to pass directly to the next generation by elitism",Those that are chosen for reproduction used a two-point crossover operator whit a probability of 0.8 and the elements mutate with a probability of 0.1,"When no changes are produced in the fitness value after 5 generations, the algorithm considers it as a convergence point and stops","For those parameters of UPX that are not controlled by the GA, we have set the default parameters","The search is also bounded for those parameters that have no maximum limit: the delimiter length is fixed to 8 bytes, the number of CER types is 10 and for each type the number of CERs goes from 0 to 20. 
                          focuses on entropy-based detectors, but it is more sensitive to some detectors than others, therefore   studies its effectiveness against different statistical detection techniques","After, we aim to understand its learning abilities",This is studied in  ,"We have trained the 7 techniques described in  , and we measured their test accuracy and FNs, shown in  . 6.2 
                          effectiveness against frequency-based detection","Initially, our interest is to understand how the detectors can achieve high level of accuracy.   shows the accuracy for the classifiers. 
                          aims to defeat detectors based on entropy features, therefore initially, we ask:   For this experiment, we train two detectors using our malware corpus: Structural Entropy ( ) ( ) and Entropy Time Series (EnTS)","After, we sample 250 packed malware and 250 non-packed malware from the training corpus using an uniform distribution.   repacks these instances, originally classified as malware, until they are no longer detected, increasing the false negatives rate of the detector. 
                         
                         shows how the false negative rate is increased by   for Structural Entropy ( ) and the Entropy Time Series (EnTS) detection techniques.   is more sensitive than EnTS to   growing from 7.4% false negatives to 95.9%, while EnTS grows from 9.4% to 90.8%","This supposes an increment of, at least, 80 points showing the effectiveness of   against these techniques. 
                          manipulates the original entropy signature used by both, EnTS and  ","It changes its shape to obtain the misclassification.   operates with the segmentation generated by the signature, and compares two signatures with the Levenshtein distance,  , the edition needed from the original signature to obtain the other, using the segments’ entropy and size as the edition units","Once   manipulates the signature, the edition from the original to the variant is higher, this makes the variant more different, guiding to the misclassification of  ","EnTS considers the whole signature as a point in a multi-dimensional space.   manipulations translate the point in several dimensions, changing its possition with respect to the discrimination boundary","This is enough to determine that the two signatures are far from each other, and, as a consequence, they are considered different","Once we have evaluated the effectiveness of   against entropy-based detection methods, we also want to ask about its effectiveness against other frequency-based detection techniques, therefore, we ask:  
                      In this case,   is not targeting these methods, however, both, the compression and the injection of controlled entropy regions, manipulate the n-gram frequency, therefore they affect the detection","For this experiment, we implemented 5 different n-gram vector based detection methods extracted from the literature",Their accuracy and false negative rates are in  ,In this case we have chosen three different classifiers sharing the same feature space and two different feature space sharing the same classifier,"This decision helps to understand the importance of the feature space and the chosen classifier. 
                         show the false negative rates for all the techniques","For the three Kolter’s technique, we can see that, even when the boosting based classifiers are more accurate, according to  , they are also more sensitive to   than SVM","According to the figure, boosting J48 and boosting SVM increase their false negative rates in 98.7 and 89.7, respectively, while SVM only increases its false negative rate 85.9 points","For McBoost’s techniques, the increment is stronger in the 2-gram feature space than in the 3-gram feature space (95.7 and 87.5 points, respectively)","These results show that both, the feature space and the classifier are sensitive places for attacking using techniques such as  , because, for Kolter’s classifiers, SVM shows better results than the others using the same feature space","MCBoost’s detectors show that the 3-gram feature space is stronger than the 2-gram feature space, using the same classification technique",These results show that   performs modifications that affect the n-gram counting process,"This changes the distribution, affecting specially the most frequent grams","These changes affect the classifiers, specially those sensitive to specific features, such as the most frequent one. 
                         
                      
                            
                            
                         Findings 
                                is effective against entropy-based detection, and surprisingly, it is also effective against n-gram vectors based detection, incrementing the false negative rates, at least, 80 points with respect to the original rate. 6.3 
                          learning process The previous section measures the abilities of   to evade n-gram vectors and entropy based detection techniques, however, we are also interested in measuring the effort of   to defeat these techniques",This effort can be measure in terms of the evolutionary process,"Therefore we ask:  
                      For this experiment we use the same setup of the previous section, and we increment the granularity to the number of generations","By design, all the classifiers detect malware when its malicious probability is higher than 0.5","Then, considering a population of   parameters, this experiment aims to measure when these parameters are properly set to generate variants that always evade the classifier","In terms of search, this is consider as a convergence point, therefore we want to find the convergence point of the evolutionary algorithm. 
                         
                         shows the evolution of the median detection probability of the whole population over a number of generations","The gray line in 0.5, represents the boundary between being detected as malware (over 0.5 probability) and not being detected as malware (under 0.5 probability)",The figure shows that during the first generation at least one technique goes under 0.5 probability (McBoots detector using 2-grams),"In the fourth generation, there is a strong decaying tendency for all the techniques","From the seventh generation, no technique is over the threshold","This shows how the learning process is reducing the detection abilities of all techniques, but it also shows that the behaviour of   is different for the different techniques","It is important to remark that some techniques, such as McBoost’s classifiers, have a stepped tendency","Analysing the classifier feedback during the evolutionary process, we discovered that it provides discreet values, generating a fixed set of detection levels",Kolter’s and entropy based detection techniques have a continuous tendency,"In this case, the classifier generates continues values","Continuity is better for the search process, due to it is easier to find a gradient by learning",It is also interesting to remark that several techniques have an asymptotic behaviour close to 0.2,"The search process focuses on defeating the classifier finding vulnerabilities on it, that is, areas where our modifications might generate a misclassification, but the variants used in this experiment come from the original training data of the classifier","Due to the classifier knows the original malware, the modifications are bounded by this knowledge","Trying to set this value from these variants is an open problem. 
                         
                      
                            
                            
                         Findings 
                                learns to defeat detectors reaching its convergence point in less than 8 generations of the search process","The most resisting technique against   is EnTS. 6.4 Prospects for EnTS 
                          comprehensively defeats EnTS and the other frequency-based detectors from the state of the art",The next step in the arms race is how to improve EnTS to defeat  ,There are potential improvements in different directions,"First, the packing process of   is not protected against sophisticated unpacking techniques",Using one of these technique will remove the CERs exposing the original malware,"Second,   constantly attacks the detection technique","Adding an extra protection to the classifier for detecting small variations on adversarial queries, might give it the ability of detecting an adversarial attack","However, a smart adversary could include some dummy variants to cheat this adversary detector","Finally, EnTS can specialize itself in detecting  , finding specific features of   variants that can detect an attack. 7 Related work This work examines the prospects for frequency-based malware detection by taking two steps in the malware arms race","First, it introduces EnTS to advance the state of the art in frequency-based detection and, then, immediately creates   to advance the state of the art in evasion by defeating EnTS",This section contextualizes both tools with respect to the literature,"First, we discuss malware detection in general, before turning prior specifically on frequency-based detection","We use frequency-based detection to explore the arms race, so we close with adversarial machine learning. 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   7.1 Dynamic and static analysis in the malware arms race Somayaji described the cybersecurity arms race as a coevolution between the black hats and the white hats ( )","He explained that this is a competition with both sides learning from each other, but he did not model this phenomena or study it",Guerra   did study this coevolution in the context of spam e-mails ( ),"They studied this from the perspective of white hats, focusing on a tool for spam filtering and a dataset of spam across 12 years","This tool, and similar tools, forced spammers to evolve","At the same time, the filters also improved their features to detect new spam","This study provided evidence of the arms race coevolution, and we based our work in a similar idea, focused on modern machine learning detectors","Our main detection scenery is disk resident malware, prevalent specially in Windows machines",Windows malware has become increasingly sophisticated at hiding itself and resisting analysis,"The literature contains seveal works mainly focused on static, dynamic or hybrid techniques aiming to detect it","Static analysis, whether based on abstractions of Control Flow Graphs and program semantics ( ) or on opcode analysis ( ), or focused on PE Headers and Static API Calls ( ) as features for machine learning, faces the increasing difficulty of initial reverse engineering","In addition, Moser et al. demonstrated hard limits to the ability of static anaysis to deal with obfuscation ( )",Dynamic analysis via virtual machines and sandboxes can avoid anti-disassembly measures but suffer from resistance via dynamic defence predicates and red pill environment detection techniques ( ),"Windows malware analysis aiming to integrate dynamic and static analysis, as  ,   and  , to produce features for data mining approaches suffer the same problems",Recent approaches to Android malware exploit the relative lack of sophistication of that type of malware,"These include Drebin ( ), CopperDroid ( ), which combine machine learning with behavioural models",Other tools as DroidSIFT ( ) are focused on anomaly detection and malware family classification,"Malware detection tools focused on network neighbourhoods, for example, Nazca ( ) and AESOP ( ) show real promise in terms of scale and accuracy but require ground truth as a seed, just as our similarity techniques do","Also, the work of Zhongqiang Chen et al. focuses on how malware is propagate on these networks ( )","Our work focuses on frequency-based detectors, that have the ability of detecting malware before any static or dynamic analysis. 7.2 Frequency-based detection In 1994 Kephart presented an n-gram approach for extracting signatures but reported no results ( )","In 2001, Schultz et al. used several data mining techniques on binaries to distinguish between benign and malicious executables in Windows or MS-DOS format",Memory consumption was a scalability bottleneck,They experimented on a dataset of 3265 malware and 1001 benign files but lacked fresh data for testing,Validation achieved 97.11% Accuracy with 3.8% FP rate ( ),"In 2006, Kolter and Maloof used Information Gain combined with byte level analysis of n-grams to classify and detect malware",Again they did not use fresh data for the test phase,"They experimented on two small datasets, one of 476 malware, 561 benign-ware (95% accuracy with 5% FP in validation); the second of 1971 benign-ware, 1651 malware (94% accuracy and 1% FP in validation) ( )","In 2007, Lyda and Hamrock used the average entropy of a whole file and the entropy of specific code sections (discovered only by using static analysis)",They showed that binary files with a higher entropy score tend to be correlated with the presence of encryption or compression,"They compared more than 20,000 malware to check whether they are able to detect these concealment methods but did not consider malware detection ( )","In the same year, Stolfo et al. used 1-gram and 2-gram byte distributions for a file to compare it with different filetype models for filetype identification ( ) in malware detection within DOC and PDF files",They reported on experiments with over 140 pdfs and 361 benign and 616 malware and results with between 3% to 20% of false positives but no accuracy information,"This work considered n-grams in a vector space, using their frequency and variation as features, but each dimension was a n-gram resulting in exponential increase in the number of dimensions ( )","Tabish et al. in 2009 divided files into blocks, and calculated frequency of n-gram histograms for each block, then extracted statistical and information-theoretic features from the histogram to define a feature vector per block",They used this to classify a feature vector as normal or potentially malicious,Pairwise comparison between blocks of different files reduces the scalability of this approach,They claimed an accuracy rate of 90% with a False Positive rate of around 10% ( ),Santos et al. in 2011 introduced a semi-supervised methodology to reduce the labelling process,"Their n-gram vector was the frequency of all possible n-grams, an important scalability limitation","After experiments on 1000 malware and benign-ware, they reported 89% of accuracy with 10% of false positives ( )","Finally, Sorokin presented   in 2011 ( )",The first evaluation of   was an use case comparison between two files,"After, Baysa   extended it to metamorphic malware in 2013 ( ), showing that this technique scales quadratically",This was a consequence of the implicit pairwise comparison of the metric,"Another relevant bottleneck, that the authors identified in the technique, was the definition of the segments that describe the files",This definition depends on three parameters whose setting depends on the analyst,"The Levenshtein distance, applied during the files comparison, depends directly on the parameters","They directly affect the number of segments that will pass to this metric, affecting to the performance","EnTS is free of this parametrization, leveraging directly the properties of the wavelet to speed up the comparison and scale linearly","EnTS has three advantages over previous work in detection via byte level content: (1) better accuracy combined with lowenr false positive rates, (2) better (linear) scalability in the detection phase, and (3) a more rigorous experimental approach","Nevertheless, EnTS is sensitive to adversarial machine learning, introduced in the next section. 7.3 Adversarial machine learning Adversarial machine learning inspired our step forward into the arms race","This field aims to exploit the vulnerabilities of a learning system, attacking the test data distribution and making it different to the training data ( )",The adversary introduces noise into the data or makes some other alteration to achieve a misclassification,"This sensitivity was originally noticed on spam detectors ( ), where the adversary studied different modifications to emails to enable the passing of machine learning based filters. 
                          were the first authors applying these models to malware","They created evademl, a genetic programming tool that modifies pdf malware to cheat two machine learning based detectors, extracted from the literature: Hidost ( ) and PDFrate ( )","In this work, the authors knew the features used by the machine learning algorithm, the classifiers, and the training data","In particular, they had access to the classification probabilities, providing them with a search gradient per classifier","In addition, they were effectively working off-line with no evolution on the part of the detectors","In our experiments,   did not use any information about the training data or the detector features it attempted to attack","Moreover,   creates variants for Windows binary executable malware that is protected against disassembly or reassemble, while evademl manipulates PDF malware","Furthermore, since UPX is compatible with several different architectures,   can potentially be adapted to several different platforms ( )","More recent works, as the one introduced by   apply adversarial machine learning to cheat the triage process of malware analysis",Adversarial machine learning has been also consolidated as an analytical process to measure the degree to which different machine learning algorithms can be exploited,A good example is the work of Biggio et al. who studied different vulnerabilities for Support Vector Machines,They also presented a methodology to improve the robustness of this classification technique ( ),"They extended this work to another classifier, where they also formalized the language for adversarial models ( )","While important, this work is tangential to this paper as we only used access to the classification output as a fitness function. 8 Conclusions We have demonstrated that EnTS outperform previous information theoretic similarity measures",Its level of abstraction makes it difficult to counter and it offers scalability advantages,We have demonstrated excellent precision and accuracy on a representative mixture of malware types drawn from the Kaggle malware data and VirusShare,"Indeed, EnTS outperforms existing AntiVirus engines (as represented in VirusTotal) for accuracy and precision",Its time complexity is bounded above by the number of files to be classified,"As an automated, execution agnostic, string-based similarity metric it offers wider scalability advantages beyond its time complexity class alone – reducing human effort and reducing the need for dynamic or static analysis",EEE also demonstrated its ability to increment false negatives on entropy and n-gram based detectors,"It learns from them, creating variants whose properties are unknown to the classifier or similar to benign-ware",It is the first packer with the ability to learn about its concealment strategy,"Acknowledgments This work has been supported by the next research projects: SeMaMatch EP/K032623/1, DAASE EP/J017515/1, LUCID EP/P005659/1 and InfoTestSS EP/P006116/1 from EPSRC."
s0888613x13002041," 1 Introduction Based on fuzzy time series concept, first forecasting model was introduced by Song and Chissom  ","They presented the fuzzy time series model by means of fuzzy relational equations involving max–min composition operation, and applied the model to forecast the enrollments in the University of Alabama","In 1996, Chen   used simplified arithmetic operations avoiding the complicated max–min operations, and their method produced better results","Later, many studies provided some improvements in Song and Chissom method in terms of the following issues: 
                   To enhance the accuracy in forecasted values, many researchers recently proposed various fuzzy time series models","For example, Wong et al.   proposed an adaptive time-variant model that automatically adapts the analysis window size of fuzzy time series based on the predictive accuracy in the training phase and uses heuristic rules to determine forecasting values in the testing phase","To extend the applicability of the univariate models to accommodate multiple variables and to improve forecasting result, Huarng et al.   proposed a multivariate heuristic function, which can be integrated with univariate fuzzy time series models to form a multivariate model",Bai et al.   proposed a heuristic time-invariant forecasting model in which there is a trend predictor for indicating trend of the increase or decrease in time series,"To resolve the problem associated with determination of length of intervals and data defuzzification, Singh and Borah   proposed two-factors high-order fuzzy time series model",The proposed model is based on the hybridization of ANN with fuzzy time series,Singh and Borah   presented a new model based on hybridization of fuzzy time series theory with ANN,"In this model, to defuzzify the fuzzified time series values and to obtain the forecasted results, an ANN based architecture is developed, and incorporated in the model",The application of fuzzy time series in financial forecasting has also attracted many researchers' attention in the recent years,"In recent years, many researchers focus on designing the models for TAIEX   and TIFEX   forecasting",Their applications are limited to deal with either one-factor or two-factors time series data sets,"For the stock index forecasting, Huarng and Yu   show that the forecasting accuracy can be improved by including more observations (e.g.,  , and  ) in the models","All the models proposed by researchers above are based on Type-1 fuzzy set concept except the model proposed by Huarng and Yu   in 2005, which is based on Type-2 fuzzy set concept","Researchers employ Type-2 fuzzy set concept (which is an extension of Type-1 fuzzy set) in various domains such as control system design and modeling  , because Type-2 fuzzy set systems are much more powerful than Type-1 fuzzy sets systems to represent highly nonlinear and/or uncertain systems  ","Nowadays, Type-2 fuzzy set concept is successfully applied in time series forecasting  ","In this study, we aim to propose an improved fuzzy time series model by employing M-factors time series data set","To deal with these factors together, we design a model based on Type-2 fuzzy time series concept, which is an improvement over the existing Type-2 model proposed by Huarng and Yu  ","Later, to enhance the forecasting accuracy, we hybridize the PSO algorithm with the proposed Type-2 model","The daily stock index price data set of SBI is employed for the experimental purpose, which consists of 4-factors, viz., “Open”, “High”, “Low” and “Close” factors/variables","After that, performance of the hybrid model is evaluated, which demonstrates its effectiveness over conventional fuzzy time series models and non-fuzzy time series models",The proposed model is also validated by forecasting the stock index price of Google,This paper is organized as follows,"In Section  , we present the problem definitions","In Section  , we review the theory of fuzzy set with an overview of fuzzy time series","In Section  , the particle swarm optimization is introduced","In Section  , we present the algorithm and defuzzification process for the Type-2 model","In Section  , we explain the proposed Type-2 model","In Section  , we provide the details of the new proposed hybrid forecasting model","In Section  , we discuss some statistical parameters, which are used to check the robustness the model",The performance of the model is assessed and presented in Section  ,"Finally, the concluding remarks and future works are discussed in Section  . 2 Problem definitions From review of literature, it is observed that there are still many unresolved issues that are associated with fuzzy time series models","Determination of effective length intervals and determination of importance of each interval in terms of events, are two most important issues among them","These two issues are explained next.  
                   
                      
                   3 Fuzzy sets and fuzzy time series In this section, we will discuss the basics of fuzzy sets and its application in time series forecasting.  
                       where   is the membership function of  ,  , and   is the degree of membership of the element   in the fuzzy set  ","Here, the symbol “+” indicates the operation of union and the symbol “/” indicates the separator rather than the commonly used summation and division in algebra, respectively","When   is continuous and infinite, then the fuzzy set   of   can be defined as:  where the integral sign stands for the union of the fuzzy singletons,  .  
                   With the help of the following two examples, the concept of fuzzy time series can be explained: 
                      
                   
                      
                   Above two examples are dynamic processes, and conventional time series models are not applicable to describe these processes  ","Therefore, Song and Chissom   first time use the fuzzy sets concept in time series forecasting","Later, their proposed method has gained in popularity in scientific community as a “Fuzzy time series forecasting model”. 
                       
                      
                      
                      
                      
                      
                   Chen   suggested that FLRs having the same fuzzy sets on the left-hand side can be grouped into a same fuzzy logical relationship group (FLRG)","So, based on Chen's model  , these FLRs can be grouped into the same FLRG as:  
                      
                   One-factor fuzzy time series model uses only one factor for forecasting, such as   price of the stock index","If some additional information, such as  ,   and   stock indices use with the   price, then it is referred to as M-factors fuzzy time series model","For example, the model proposed by Song and Chissom   is based on one-factor, because they simply use the enrollments data to solve the forecasting problems","On the other hand, the model proposed by Huarng and Yu   is based on M-factors, because they use   and   as the secondary-observations to forecast the   price of TAIEX.  
                   The concept of Type-2 fuzzy set is explained with an example as follows: 
                      
                   
                      
                   Following  , we define two operators, viz., union and intersection, for the set theoretic operations","These two operators are explained as follows  : Observation that is handled by Type-1 fuzzy time series model can be termed as “main-factor/Type-1 observation”, whereas observations that are handled by Type-2 fuzzy time series model can be termed as “secondary-factors/Type-2 observations”","Due to involvement of Type-2 observations with Type-1 observation, a massive FLRGs are generated in Type-2 model","To deal with these FLRGs together and establish the relationships among multiple FLRGs, the union (∪) and intersection (∩) operations are employed",Both these operations in terms of handling FLRGs are explained next,"Consider the following FLRGs for Type-1 and Type-2 observations as follows: 
                      
                   Based on these FLRGs, we define ∪ and ∩ operations as follows: 
                      
                   Following  , the FLRGs of Type-1 and Type-2 observations are combined as follows: 
                   
                      
                   Following  , the FLRGs of Type-1 and Type-2 observations are combined as follows: 
                   4 Particle swarm optimization (PSO) algorithm The PSO algorithm was first developed by Eberhart and Kennedy  ","It is a population-based evolutionary computation technique, which is inspired by the social behavior of animals such as bird flocking, fish schooling, and swarming theory  ",The PSO can be employed to solve many of the same kinds of problems as genetic algorithms  ,"The PSO algorithm is applied to a set of particles, where each particle has assigned a randomized velocity",Each particle is then allowed to move towards the problem space,"At each movement, each particle keeps track of its own best solution (fitness) and the best solution of its neighboring particles",The value of that fitness is called “ ”,"Then each particle is attracted towards the finding of global best value by keep tracking the overall best value of each particle, and its location  ",The particle which obtained the global fitness value is called “ ”,"At each step of optimization, velocity of each particle is dynamically adjusted according to its own experience and its neighboring particles, which is represented by the following equations: 
                   The position of a new particle can be determined by the following equation:  where   represents the  th particle and   represents the dimension of the problem space","In Eq.  ,   represents the inertia weight factor;   represents the current position of the particle   in iteration  ;   denotes the previous best position of the particle   that experiences the best fitness value so far ( );   represents the global best fitness value ( ) among all the particles;   gives the random value in the range of  ;   and   represent the self-confidence coefficient and the social coefficient, respectively; and   represents the velocity of the particle   in iteration  ","Here,   is limited to the range  , where   is a constant and defined by users","The steps for the standard PSO are presented in  
                      . 5 Algorithm and defuzzification for Type-2 model In this section, we will first present the algorithm for the proposed Type-2 fuzzy time series model","Then, defuzzification process for the proposed model will be presented in the subsequent subsection. 
                      
                      
                      
                      
                   
                      
                      
                      
                   5.1 Algorithm In this subsection, we have presented an algorithm for the Type-2 fuzzy time series model, which is based on Huarng and Yu   model","Therefore, we first present the algorithm for Type-2 model proposed by  ","This algorithm is presented as  
                         ","To improve the forecasting accuracy of Type-2 fuzzy time series model, we apply some changes in  ","This algorithm is presented as  
                         . 5.2 Defuzzification for Type-2 model After obtaining the fuzzified forecasting data from the ∪ and ∩ operations, they are defuzzified based on the “Frequency-Weighing Defuzzification Technique”, which is the modified version of the defuzzification technique proposed by  ","In the subsequent section, this technique is discussed","The defuzzified values obtained for the left-hand side and right-hand side of the FLRGs can be referred to as points “M” and “N” in  , respectively",The forecasted value of the proposed Type-2 model can be derived by taking the average of these defuzzified values,"The forecasted value for this Type-2 model can be referred to as point “D” in  . 6 Type-2 fuzzy time series forecasting model In this section, the proposed “Type-2 Fuzzy Time Series Forecasting Model” is presented","To verify the proposed model, the daily stock index price data set of SBI for the period   (format: mm-dd-yy), is collected from the website:  ","A sample of data set is listed in  
                      ",The model consists of ten phases,"The functionality of each phase is explained in step-wise as follows:   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   To defuzzify the fuzzified time series data set and to obtain the forecasted values, defuzzification technique proposed by Singh and Borah   is employed here","Based on the application of technique, it is slightly modified and categorized as:   1 and   2","The procedure for   1 is given as follows: 
                   
                      
                   If   1 is applicable, then forecasted value for day   can be computed as: 
                   If   2 is applicable, then forecasted value for day   can be computed as: 
                   In this way, we obtain the forecasted values for the ∪ and ∩ operations individually based on the proposed model","To measure the performance of the model, mean absolute percentage error (MAPE) is used as an evaluation criterion","The MAPE can be defined as follows: 
                   Here,   represents the forecasted value at time  ,   represents the actual value at time   and   represents the total number of days to be forecasted",The MAPE value of the forecasted stock index price is presented in  ,"Based on the proposed model, we present here an example to compute the forecasted value of daily stock index price of SBI using the ∪ operation as follows: 
                      
                   7 Improved hybridized forecasting model The main downside of fuzzy time series forecasting model is that increase in the number of intervals increases accuracy rate of forecasting, and decreases the fuzziness of time series data sets  ",Kuo et al.   show that appropriate selection of intervals also increases the forecasting accuracy of the model,"Therefore, in order to get the optimal intervals, they used PSO algorithm in their proposed model  ",Kuo et al.   signify that PSO algorithm is more efficient and powerful than genetic algorithm as applied by Chen and Chung   in selection of proper intervals,"Therefore, to improve the forecasting accuracy of the proposed Type-2 model, we have hybridized the PSO algorithm with  ","The main function of the PSO algorithm in   is to adjust the length of intervals and membership values simultaneously, without increasing the number of intervals in the model",We have entitled this model as “FTS-PSO”,The detailed description of the FTS-PSO model is presented next,"Let   be the number of intervals,   and   be the lower and upper bounds of the universe of discourse   on historical time series data set  , respectively","A particle is an array consisting of   elements such as   and  , where   and  ","Now based on these   elements, define the   intervals as  ,  , … ,  , … ,   and  , respectively","In case of movement of a particle from one position to another position, the elements of the corresponding new array always require to be adjusted in an ascending order such that  ","The graphical representation of particle is shown in  
                      ","In this process, the FTS-PSO model allows the particles to move other positions based on Eqs.   and  , and repeats the steps until the stopping criterion is satisfied or the optimal solution is found","If the stopping criterion is satisfied, then employ all the FLRs obtained by the global best position ( ) among all personal best positions ( ) of all particles","Here, the MAPE is used to evaluate the forecasted accuracy of a particle","The complete steps of the FTS-PSO model are presented in  
                      ",The main difference between the existing models   and the FTS-PSO model is the procedure for handling the intervals based on their importance,"The FTS-PSO model also incorporates more information in terms of observations, which are represented in terms of FLRs",These FLRs are later employed for defuzzification based on the technique discussed in Section  ,"In the following, an example is presented to demonstrate the whole process of the FTS-PSO model. 
                      
                   According to  , all particles move towards the second positions based on Eqs.   and  ","The second positions for all particles and their corresponding new MAPE values are presented in  
                      ","For example, in  , the second position of particle 1 (for the ∪ operation) is obtained by using Eqs.   and  , which are based on Eqs.   and  , respectively. 
                      
                      
                      
                      
                      
                      
                      
                   On comparison of the MAPE values between  , it is obvious that particle 1, particle 2 and particle 4 for the ∪ and ∩ operations attained their own   values so far in  ","Therefore, these three particles update their   values, which are shown in  
                      ","In  , the new   value is obtained by particle 4 for the ∪ operation and particle 1 for the ∩ operation, because their MAPE values are least among all the particles so far",The above steps are repeated by the FTS-PSO model until the stopping criterion is satisfied,"After execution, a new set of optimal FLRs are obtained by the   that the particle 4 (∪ operation) and particle 1 (∩ operation) attain so far, and are further employed for obtaining the final forecasting results. 8 Performance analysis parameters The robustness of the proposed model is evaluated with the help of mean and standard deviations (SD) of the observed and forecasted values, correlation coefficient ( ) and Theil's   statistic","All these parameters can be defined as follows: 
                   Here, each   and   is the forecasted and actual value of day   respectively,   is the total number of days to be forecasted","In Eqs.   and  ,   are the observed values of the actual time series data set and   is the mean value of these observations","Similarly, the mean and SD for the forecasted time series data set are computed","For a good forecasting, the observed means and SDs should be close to the forecasted means and SDs","In Eq.  , the value of   is such that  ","The “+” and “−” indicate the positive linear correlations and negative linear correlations between the forecasted and actual value of time series data set, respectively",A correlation coefficient ( ) greater than equal to 0.5 is generally considered the strong,"In Eq.  ,   is bound between 0 and 1, with values closer to 0 indicating good forecasting accuracy. 9 Empirical analysis To illustrate the forecasting performance of the proposed method, the daily stock index price of SBI and Google are used as data sets in verification and validation phases, respectively","The experimental results of the proposed model are compared with different existing models for various orders of FLRs and different intervals. 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   9.1 Stock index price forecasting of SBI In this subsection, we present the forecasting results of the FTS-PSO model","The FTS-PSO model is validated using the stock index data set of SBI, as mentioned in Section  ","For forecasting the stock index price, “Open”, “High” and “Low” variables are considered as the Type-2 observations, whereas “Close” variable is considered as the Type-1 observation",The “Actual Price” is chosen as the main forecasting objective,Further comparisons on the FTS-PSO model and the other existing models are discussed next,"The FTS-PSO model is trained simultaneously for the ∪ and ∩ operations, and the best results obtained by the particles are considered to forecast the stock index data",The necessary setting of all the parameters for the FTS-PSO model is discussed in Section  ,"The best forecasted accuracies (i.e., the least MAPE) are made by particle 4 (for the ∪ operation) and particle 1 (for the ∩ operation)","Therefore, results obtained by these particles are used for the empirical analysis","The forecasted results for the ∪ and ∩ operations are presented in  
                         ","In  , the forecasted results for the existing fuzzy time series models   are also presented",The considered fuzzy time series models including the FTS-PSO model use the first-order FLRs to forecast the stock index data,"From  , it is obvious that the FTS-PSO model is more advantageous than the conventional fuzzy time series models  ","To verify the superiority of the proposed model under various high-order conditions, existing forecasting model, viz., Chen   model, is selected for comparison","A comparison of the forecasted results is shown in  
                         ","During simulation, the number of intervals is kept fix (i.e., 9) for the existing model and the FTS-PSO model","At the same intervals, the MAPE values obtained for the existing model are 1.39%, 1.41%, 1.40%, 1.41%, 1.40% and 1.40% for third-order, fourth-order, fifth-order, sixth-order, seventh-order and eight-order FLRs, respectively","On the other hand, at the same intervals, the FTS-PSO model gets the least MAPE values which are 1.07% (for ∪ operation) and 1.21% (for ∩ operation)","However, the smallest MAPE value, which is 1.07%, is obtained from the proposed model for the ∪ operation",We can see that the FTS-PSO model outperforms than the existing model under various high-order FLRs at all,"To verify the performance of the proposed model under different number of intervals, the forecasted results are obtained with different intervals ranging from 9 to 15","The forecasted results are listed in  
                         , where the proposed model uses first-order FLRs under different number of intervals","The least MAPE values are 0.82% (for ∪ operation) and 0.99% (for ∩ operation) for the intervals 15 and 10, respectively","However, between intervals 9 and 15, the best forecasted result is obtained from the ∪ operation at interval 15 (MAPE = 0.82%)","To evaluate the performance of the proposed model, it is compared with the existing Type-2 fuzzy time series model  , under different number of intervals","A comparison of the forecasted results between the proposed model and the existing Type-2 model is shown in  
                         , where both these models use different intervals ranging from 16 to 20","For the existing model, the lowest forecasting error is 1.13%, which is obtained at intervals 19 and 20","For the proposed model, the least MAPE values are 0.72% (for ∪ operation) and 0.78% (for ∩ operation), which are obtained at intervals 20 and 19, respectively","From comparison, it is obvious that the proposed model produces more precise results than existing Type-2 model under different number of intervals","To verify the superiority of the proposed model in terms of forecasted accuracy, three existing models, viz., Grey model  , Back-propagation neural network model (BPNN)   (with one hidden layer and one output layer) and Hybridized model based on fuzzy time series and ANN  , are selected for comparison",These three competing models are simulated using MATLAB (version 7.2.0.232 (R2006a)),"During the learning process of the BPNN, a number of experiments were carried out to set additional parameters, viz., initial weight, learning rate, epochs, learning radius and activation function to obtain the optimal results, and we have chosen the ones that exhibit the best behavior in terms of accuracy","The determined optimal values of all these parameters are listed in  
                         ",The forecasted results for these three models are obtained with different number of input values ranging from 5 to 10,"A comparison of the forecasted results is presented in  
                         ","The least MAPE values for the Grey model, BPNN model and Hybridized model are 1.05% (for input 5), 1.25% (for input 10) and 1.17% (for input 10), respectively","In our proposed model, the selection of input depends on the establishment of FLRs","For the proposed model, the least MAPE values are 0.63% (for ∪ operation) and 0.64% (for ∩ operation), which are obtained using the first-order FLRs","From comparison, we can see that the proposed model gets a higher forecasting accuracy than the existing three models, viz., Grey model, BPNN model and Hybridized model","The empirical analysis shows that the proposed model is far better than the existing forecasting models for stock index data set of SBI. 9.2 Stock index price forecasting of Google In this subsection, the performance of the proposed model is evaluated with the stock index data set of Google","The data set of stock index price is covered from the period 6/1/2012–7/27/2012, which is shown in  
                         ","Here, “Open”, “High” and “Low” variables are selected as the Type-2 observations, whereas “Close” variable is selected as the Type-1 observation",The “Actual Price” is chosen as the main forecasting objective,The historical stock index data set of Google is collected from the website:  ,"The FTS-PSO model optimizes the forecasting results and obtains the best results (i.e., the least MAPE)","The forecasting results are shown in  
                         ","The universe of discourse   is considered as  , and is partitioned into 30 intervals","But, the historical data cover only 24 intervals, and the proposed model obtains the forecasted results using these 24 intervals","More detail results on partitions of the universe of the discourse and positions of the particle (i.e., best particle) among these 24 intervals are shown in  
                         ","For comparison studies, various statistical models listed in   are simulated using PASW Statistics 18 ( )","A comparison of the forecasted accuracy among the conventional statistical models and the proposed model is listed in  
                         ","The comparative analysis clearly shows that the proposed model outperforms the considered models. 
                          To check the robustness of the proposed model for forecasting the stock index price of Google, various statistical parameters values as mentioned in Section  , are obtained","The experimental results are shown in  
                         ",The values of parameters listed in   are based on the forecasted results presented in  ,"From  , it is clear that the mean of actual price is very close to the mean of forecasted price",The comparison of the SD values between actual price and forecasted price show that predictive skill of our proposed model is good for both the ∪ and ∩ operations,The   values between actual and forecasted values also indicate the efficiency of the proposed model,"The   values for both ∪ and ∩ operations are closer to 0, which indicate the effectiveness of the proposed model","Hence, the robustness of the proposed model is strongly convinced with the outstanding performance in case of daily stock index price data set of Google. 10 Conclusions and the way ahead This paper presents a novel approach combining Type-2 fuzzy time series with the particle swarm optimization (PSO) for building a time series forecasting expert system","The main contributions of this article are presented as follows: 
                   Still, there are scopes to apply the model in some other domains in a flexible way as follows: 
                  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0957417419302489, 1 Introduction A person’s palm skin presents a particular line pattern that is permanent and unique ( ),"Given these characteristics, an imprint of the palm’s line pattern, called palmprint, can be used to identify an individual uniquely ( )","Identification is possible even from palmprints obtained from the imprint left when the palm comes into contact with a surface, called latent palmprints ( )","Latent palmprint identification is paramount for law enforcement as approximately 30% of the prints found at a crime scene are from palms ( ), and palmprints contain more information than fingerprints ( )","Given its importance, in this study, we survey the current state of the art of latent palmprint identification, focusing on the feature representations used for identification","Palmprint identification involves the comparison of a given palmprint, called a query, against a collection of existing palmprints taken under controlled conditions, called templates, each of which has been associated with a known identity","The palmprints are compared by key features that are extracted from the palmprint, such as palmprint direction ( ), or the principal lines of the palm ( )",These features can be represented in different ways to include information from the palmprint to allow for increased discerning capabilities when making the identification between pairs of query and template palmprints,"For each pairwise comparison, a matching algorithm compares the feature extracted from both palmprints and emits a similarity score","Supposedly, the comparison that results in the highest similarity indicates the identity of the query that is under investigation ( )","Because the performance of the matching algorithm depends on the feature representation to be compared, there is an interest in determining which features allow to obtain better performance ( )",Latent palmprint identification is a difficult problem in palmprint identification,"As the latent palmprints are not obtained under controlled conditions, they could present distortion or degradation, depending on the surface where they were found ( )","Furthermore, latent palmprints commonly do not contain information from the whole palm region",Most methods for palmprint identification perform full-to-full palmprint comparisons and require to find an area of interest to align the palmprints,"However, these methods do not apply when carrying out a partial-to-full comparison, which is the common case in latent palmprint identification ( )","The methods that can be used for the latent case are those where the features are not from a specific area of the palm, such as line patterns of the hand, or the salient points of these lines, called minutiae","In this study, we survey the features used in the different methods for latent palmprint identification, the representations of the features, and the reported identification performance that each method has attained","In our review, we found that methods that use minutiae supposedly obtain better results","Besides, we found that there is no standard evaluation methodology and that the existing evaluations are conducted using different datasets","Furthermore, existing methodologies do not allow determining whether the differences in performance are due to the matching algorithms or the feature representations used","Given the lack of a standard evaluation methodology, the reported performance results are not comparable, and thus, we cannot determine which method and feature representation for latent palmprint identification, if any, is better","Forensic experts ascertain the identity of individuals involved in crime scenes by using features, such as minutia, extracted from palmprints","Given the number of prints that have to be compared to determine the possible identity based on a latent palmprint, it is impossible for an expert to perform all the comparisons manually","Thus, experts rely on Automated Fingerprint Identification Systems (AFIS) for comparing an unknown latent print against a dataset of template prints containing known identities, which return a ranked collection of possible suspects to be analyzed by experts manually","The combination of manual matching done by an expert and the matching done by an AFIS has shown that can outperform the expert and the system alone ( ), as happens in other areas such as face recognition ( )","Given the importance of AFIS to aid the decisions of experts in determining the identity of possible crime suspects, it is necessary to evaluate each part of the identification systems separately, with the aim of determining which of them could be improved for obtaining better identification results","In this paper, we address the lack of a standard methodology for fairly evaluating feature representations, which from our point of view should provide all the necessary materials for independent replication and evaluation of palmprint features publicly","In latent palmprint identification, minutiae are the most commonly used feature, which is also used by forensics experts, and the methods using them have the highest reported performance","To compare the matching performance when using different minutiae representations and to determine which one allows to obtain a better performance, in this paper, we propose a new methodology to fairly evaluate minutiae-based feature representations used in the context of latent palmprint identification","To achieve this, the proposed methodology measures the matching performance using the similarity scores obtained when comparing the minutiae representations in the query palmprints against those in the templates","Thus, our evaluation is performed independently of the global matching algorithms proposed in each latent palmprint identification method","By our evaluation methodology, we show that no minutiae representation allows to obtain a better performance than the others, by always obtaining the highest similarity scores for matching minutiae pairs","Because our results were obtained using a public latent palmprint dataset ( ), they serve to establish a baseline for future developments in this area",Our results show that the performance in latent palmprint identification can be enhanced further with the development of minutiae representations that better capture discriminative information about the palmprints,"Although we focus on evaluating the feature representations, further work involves evaluating different matching algorithms independently of the features and their representations or determining whether representations can be combined in a manner that improves the performance","The contributions of this paper are as follows: (i) an in-depth review of the features used in the context of latent palmprint identification, and their representations, (ii) the introduction of a methodology to evaluate palmprint feature representations based on minutiae, and (iii) experimental results that allow for a fair comparison of the performance obtained by using different minutiae-based representations used in latent palmprint identification. 2 Features used in latent palmprint identification Latent palmprint identification methods usually follow the same steps, which address the complexities of working with latent palmprints","The input is composed of two sets of images that contain palmprints, where the first set contains an image of the query latent palmprint and the second is a collection of images containing template palmprints, called the gallery","The first step is preprocessing, where the images are enhanced to reduce noise, such as motion blur or salt and pepper ( )","The second step is feature extraction, where the features are obtained depending on the quality of the images ( )","In this step, we include the calculations used to describe the features to allow the matching algorithms to use them","The third and final step is matching, where the feature representations are compared to calculate the similarity between palmprints","As some features do not describe the whole palmprint, but only parts of it, alignment is required before palmprint matching","For those methods, matching is divided into two parts: local and global",Local matching compares each feature on the query to each of the templates to determine which parts are more similar and to align the palmprints,Global matching then compares the two aligned palmprints ( ),The output of latent palmprint identification methods is the similarity score obtained after comparing the query with each of the templates in the gallery,"In this study, we will focus on the feature description, and the similarity score of the descriptors, without taking into account the global matching methods",Features for palmprint identification are generally extracted from the particular line pattern of the palms,"This pattern is mainly the result of two elements, namely friction ridges, which are formed by the fusion of multiple sweat glands, each with their corresponding sweat pores ( ), and flexion creases, which are the lines where the skin has a firmer attachment to the underlying structures to enable the flexion of the palm ( )","An example of these elements can be observed in  
                      ","In this study, we review the features used in the methods for latent palmprint identification","Multiple features have been used successfully in the context of palmprint recognition, which have been extracted using different methods, such as feature points ( ), Gabor Filters ( ), palmprint direction ( ), principal line orientation ( ), topothesy-fractal dimension ( ), and minutiae ( )","However, not all features are suitable to the latent palmprint identification case, as most of them require the detection of a Region-Of-Interest (ROI) that allows matching two palmprints","Since in latent palmprints, usually, there is no ROI to be found, we do not include in our review these methods","However, we have included those features used for palmprint identification that are not designed for the latent case, but they can be used in this context because they perform partial-to-full comparisons and do not require the ROI to be present, or an alignment of the palmprints, to be compared","To provide order to our discussion, we separate the features using a taxonomy based on the one proposed in   for fingerprint features","Maltoni et al. divided the features into the following levels: 
                   From our state-of-the-art review, we did not find any latent palmprint identification methods that use the level 3 features","Furthermore, some methods rely on computer vision techniques to match the palmprints, which use the pixels of the image that contains them, regardless of their content","Considering these factors, our taxonomy uses three categories: the features extracted from the image without considering the palmprint lines, level 1 features, and level 2 features","We will refer to these categories as image-based, line-based, and minutiae-based, respectively","We discuss the image-based, line-based, and minutiae-based features in  – , respectively",Our discussion focuses on how the features are constructed and on how the similarity is calculated,"To also discuss the relative merit of each feature, in  , we briefly explain the datasets and the performance measures used to evaluate the methods","A summary of the survey, along with the reported performance and the dataset used for testing, is shown in  
                      . 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   2.1 Evaluation of the performance of latent palmprint identification methods Each of the features and their representation, has been proposed in conjunction with matching algorithms and preprocessing techniques","Thus, the performance reported is not only that of the features used but also that of the complete method","However, these methods cannot be compared fairly as they are tested using different methodologies, which include the use of different datasets and the different manners of reporting the performance","Nevertheless, to understand the current state of the art of latent palmprint identification, we must discuss the relative merits obtained by each method, alongside the datasets and performance measures used","In   and   we discuss the datasets and performance measures used in the evaluation of latent palmprint identification methods. 
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      2.1.1 Datasets An essential element in latent palmprint identification experiments is the selection of the datasets to test a method","If the datasets include palmprints that cover different situations that can happen in a real identification scenario, then it is feasible that the performance in a real case is close to that in the experiments","The reported experiments used palmprints that came from one of three sources: public datasets, datasets collected in situ, and institutional datasets",There are two public datasets that have been used in latent palmprint identification experiments,"The first is THUPALMLAB ( ), which contains 1280 palmprint images from 80 subjects (eight images per palm for each subject)","Because THUPALMLAB does not contain real latent palmprints, the methods that use this dataset create simulated palmprints by cropping the template palmprints and adding salt and pepper noise, motion blur, or Gaussian noise ( )","The second is the Latent Palmprint Identification DataBase (LPIDB) ( ), which contains a gallery of 102 palmprint images from 51 subjects and 380 latent palmprints images taken from the imprints left by the subjects after touching different surfaces","There are two institutional datasets, which have been collected with the help of law enforcement agencies and private companies, and which contain latent palmprint images from real criminal cases","The first is provided by the Beijing Institute of Criminal Technology (ICT) ( ), and the second is provided by both the Michigan State Police (MSP) and private companies ( )","However, these are not publicly available to all researchers",Datasets collected in situ are those wherein the researchers themselves collect the palmprints to be used,These datasets are not publicly available and have the disadvantage of not containing any latent palmprint image,"The experiments are performed using simulated latent palmprints, obtained from cropping the template palmprint images and subjecting the cropped images to different types of noise","The experiments that use this type of datasets are not reproducible, and the methodology followed to obtain and modify the images might not be representative of real latent palmprint identification situations ( ). 2.1.2 Performance measures Latent palmprint identification experiments use multiple query palmprints, each of which is compared against all the templates contained in the gallery","If the query and template are matching palmprints, the pair being compared is considered genuine or impostor otherwise",Latent palmprint identification methods should calculate a higher similarity value for comparisons of genuine pairs than the values obtained for comparisons of impostor pairs,"Given the similarity values of each comparison, there are two approaches used to calculate the performance measures: ranking the similarity results or using thresholds to determine which palmprint pair matches ( )","Although a query palmprint may match with a template palmprint that has a higher similarity value, it is possible for a genuine pair to have the second, third, or lowest similarity value","Given this, the ranking of similarity values from highest to lowest is used by three performance measures to evaluate the identification methods","The first is called DIFFS ( ), which is the ratio of the difference between the similarity values at ranks 1 and 2 to the difference between the similarity values at rank 1","The second is the rank   identification rate, which is the ratio of the genuine pairs with similarity values at rank   or higher to the total numbers of genuine pairs","The most important rank   identification rate is rank 1, denoted as r1 henceforth, because it measures how many query palmprints are correctly matched when using the most similar palmprint according to the identification method used","The third is the cumulative matching curve (CMC) ( ), which plots the different identification rates of different ranks and allows determining visually how the identification method performs","Other methods for measuring the performance rely on comparing a similarity score with a threshold, where scores above this value are considered a match","Using a threshold, a researcher can choose from multiple performance measures, the most important being the following: false matching rate (FMR), which is the ratio of the impostor pairs that are considered a match to the number of matching comparisons; false non-matching rate (FNMR), which is the ratio of the genuine pairs considered non-matching to the number of non-matching comparisons; and the equal error rate (EER), which is the error at the threshold where FMR=FNMR","The FMR and FNMR are commonly plotted in detection error trade-off (DET) curves ( ) or in receiver operating characteristic (ROC) ( ) curves, which plot FMR vs. (1-FNMR) at different thresholds. 2.2 Image-based features As the palmprints are stored as images, there are palmprint identification methods that use techniques from the computer vision field in the area of feature extraction and matching algorithms ( )","For latent palmprint identification, the techniques used take as features those extracted using the scale invariant feature transform (SIFT) algorithm ( ) and the image’s Fourier transform ( ). 
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      2.2.1 Scale invariant feature transform In the first paper that covered latent palmprint identification ( ), the authors found that a standard commercial fingerprint matching algorithm, Neurotechnology VeriFinger v.4.2 ( ), could not perform correctly in 70% of the query palmprints in their experiments","Supposedly, this was due to the number of comparisons required and to the fact that the features were not extracted from the same area of the palm","Jain and Demirkus proposed to use the SIFT algorithm ( ), which allows finding keypoints in an image","An example of the keypoints found in a latent palmprint by the SIFT algorithm is shown in  
                            ",Jain and Demirkus measured the similarity between two palmprints depending on how many keypoints were matched in the two palmprint images,"Using the SIFT and VeriFinger algorithms on a dataset collected in situ, they obtained an 82% r1","However, Jain and Demirkus’ experiments did not reproduce faithfully the conditions of working with latent palmprints, as they manually indicated the area of interest to reduce the number of comparisons","In ( ), the authors proposed the partial palmprint keypoint matching degraded recognition (PP-KMDR) method, which reduces the noise in an image and then applies the SIFT algorithm to obtain keypoints",The similarity is measured depending on the number of keypoints matched,"PP-KMDR was tested on THUPALMLAB, where the latent palmprints were simulated using different types of noise, obtaining a 27.11% EER for undegraded partial palmprints, 38% with Gaussian noise degradation, 26.28% with salt and pepper noise, and 35.96% with motion blur","From these results, it is clear that PP-KMDR cannot deal correctly with motion blur","Furthermore, because the latent palmprints were simulated, it is not clear whether the degradations used were representative of those found in real latent palmprints. 2.2.2 Palmprint fourier transform In ( ), the authors proposed to use Fourier transform on palmprint images to calculate the similarity between a pair of palmprints","Laadjel et al.’s method is based on phase-only correlation (POC) ( ), which measures how similar two Fourier transforms are","When a POC is plotted, if the images are similar, a distinct sharp peak appears, as can be observed in  
                            ","However, Laadjel et al. determined that measuring the peak was not sufficient, and thus, proposed to measure the similarity using the modified phase-only correlation (MPOC) technique","Their new technique finds the highest peak and defines two areas: the area surrounding the highest peak, called the inside lobe, and the rest, called the outside lobe",The similarity between two images is given by the ratio of the magnitude of the highest peak in the inside lobe to the magnitude of the highest peak in the outside lobe,"MPOC was tested on a dataset collected in situ, and a 0.03% EER was reported for simulated latent palmprints with motion blur","However, for their experiments, Laadjel et al. did not consider rotation or scaling, which are factors that negatively impact MPOC ( )",Singh et al. proposed the partial palmprint rotation-invariant and degraded recognition (PP-RIDER) method as an enhancement to Laadjel et al.’s MPOC in ( ),"PP-RIDER uses a technique for aligning two images that could be translated, rotated, and scaled with respect to each other in the Fourier domain, called the Fourier-Mellin transform ( )","After this transform is used, MPOC is applied to measure the similarity between a query and a template","Using PP-RIDER on a subset of THUPALMLAB and on simulated latent palmprints with noise, Singh et al. reported a 38.5% EER and a 50% EER for MPOC alone","According to  , PP-RIDER is robust against motion blur","Thus, Carreira et al. proposed a hybrid method that uses PP-KDMR when low-motion blur is present and PP-RIDER otherwise",The hybrid method achieved a 22.7% EER on partial palmprints without noise,"However, the EER was 46% on cases with high values of motion blur, showing that neither PP-RIDER nor PP-KDMR, which are methods based on POC and SIFT, respectively, have good performance when identifying images with motion blur. 2.3 Line-based features An approach for feature extraction from palmprints is based on the friction ridges and flexion creases ( )","For latent palmprint identification, only three line-based features have been used","The first feature is the orientation map, which captures the orientation of the friction ridges in the palmprint","The second feature is the density map, which measures the distance between all the adjacent and the parallel ridges, showing how near or far the ridges are from each other in all the palmprints","The third feature used is the principal line map, which captures the most prominent lines in the hand, generally the biggest flexion creases","Although all the methods that use line-based features also use minutiae, the methods considered here use them as points that need to be aligned globally and do not exploit their distinctive characteristics, as the methods discussed in   do","Dai and Zhou explored the use of four features for latent palmprint identification: orientation map, density map, principal line map, and minutiae ( )","For the density and orientation maps, the palm area shown in the palmprint image is separated into subareas",The similarity is measured in terms of how many subareas have similar density and orientation,"For principal lines and minutiae, the Hough transform ( ) is used to align the features, and then the similarity is computed in terms of the number of matched lines or minutiae","Dai and Zhou tested their method using an extended version of THUPALMLAB, obtaining a 91.7% r1","Although they obtained a good performance, they achieved this result by determining manually the area of interest that would be matched",Dai et al. used density and orientation maps to determine the area of interest without any human intervention in  ,"Using the same features and matching algorithm as in   on THUPALMLAB, they obtained an 8.1% FNMR when the FMR was  ","One of their conclusions was that the minutiae alone could achieve 70% r1 when used for global matching, whereas the minutiae combined with the density map can achieve an 89% r1",This shows that the combination of the right features might increase the identification capabilities of the matching algorithms. 2.4 Minutiae-based features A common feature used for palmprint identification is the minutiae ( ),"For latent palmprint identification, the minutiae are widely used by forensic examiners because they can be extracted automatically ( ), and a set of minutiae can be used to uniquely identify persons even if only a subset of all the features contained in a palm is used, making minutiae useful in cases where there is no ROI to be found ( )","The most widely used are ridge endings, where a ridge continuity terminates, and ridge bifurcations, where one ridge forks into separate ridges","The minutiae that are automatically extracted from a latent palmprint is shown in  
                         ","Generally, minutiae features contain the   and   coordinates where they were found in the palmprint image, along with the type of the feature (ending or bifurcation) and the angle   of the associated ridge","However, methods that match only the minutiae coordinates, type, and angle are not robust against translation, rotation, or distortion ( )","Furthermore, spurious minutiae can appear because of distortion","To address these problems, researchers have proposed multiple descriptors to capture the information about the local structure of minutiae for latent palmprint identification, aiming to increase the discrimination capabilities of the feature","These descriptors fall into two categories: nearest-neighbor descriptors, which describe a minutia based on a fixed number of neighboring minutiae, and fixed-radius descriptors, which describe the minutia using elements from a fixed area around the minutia. 
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                      2.4.1 Nearest-neighbor descriptors Li et al. proposed a minutiae descriptor based on the polar coordinates of the nearest neighbors of the minutiae ( )","In Li et al.’s descriptor, for the minutia to be described, called central, the coordinates   and   and the angle   must be stored","For each neighbor   of the central minutia, a structure containing the polar radius, the polar angle, and the relative direction with respect to the angle   of the central minutia is recorded","The similarity between two descriptors is measured by the sum of the differences in radius, angle, and relative direction of the neighbors in the vicinity of the central minutia","Li et al.’s descriptor was tested on a dataset developed in situ, obtaining a DIFFS score of 47.19%","According to their results, the use of polar coordinates makes the descriptor robust against translation and rotation, but it is susceptible to spurious or missing minutiae",Two other works proposed similar minutiae descriptors to that proposed in  ,"First, in  , the similarity measure was changed to be an average of the differences in radius, angle, and relative direction of the most similar neighbors of each minutia","Testing it on a dataset developed in situ, they obtained a DIFFS score of 43.6%","Second, Rao et al. proposed to describe the central minutia using only the radius and angle difference of four nearest neighbors, called minutiae quadruplets ( )","Testing it on THUPALMLAB, they obtained a 0.12% EER","However, Rao et al. did not describe the method used for simulating the latent palmprint","Wang et al. proposed a minutiae descriptor based on triangles, as they are robust to rotation and translation, and allow small distortions ( )","They used radial triangulation because it finds sets of triangles with a unique centroid, allowing the alignment of the triangles to match them more efficiently","A minutia, called central, is described using its   and   coordinates and the angle   of the associated ridge","For each neighbor   of the central minutia, a structure containing the polar radius, the polar angle, the relative direction with respect to the angle   of the central minutia, the distance between neighbor   and neighbor   and the area of the triangle formed by the center minutia, neighbor  , and neighbor   is obtained",The similarity between two descriptors is the average distance of the Euclidean distance of the neighbors,"Using an institutional dataset, they obtained a 62% r1",Wang et al. determined that the previous version of their similarity measurement had an unstable behavior and that the matching required fine-tuning to be able to align two palmprints,"Thus, in  , they changed the similarity measurement such that only the pairs of neighbors with the best match are considered, obtaining for the same dataset a 69% r1","Radial triangulation deals correctly with rotation, translation, and small distortions, and using the best match helps to reduce the effect of spurious or missing minutiae",Medina-Pérez et al. proposed to use triangles that do not rely on a central minutia ( ),Triangles used in this manner are tolerant to distortions in the palmprint,"However, as there is not a single alignment point, they could be susceptible to rotation and translation","To address these problems, Medina-Pérez et al. proposed a feature called m-triplets, which order the minutiae on a triangle on a clockwise fashion and store the lengths of the sides of the triangles, the angles a minutia should be rotated starting from   to a point at another minutia in the triangle, and the angle required to rotate a minutia starting from   to align it with the   of another minutia",The similarity between two triangles is obtained by comparing all the angles and the length of the sides,"This comparison is made by rotating the triangle three times to add rotation invariance, translation invariance, and stability to the computation of similarity","Although the m-triplets feature was initially proposed for fingerprint identification, it was tested on LPIDB ( ), achieving an 82.1% r1","In  , a different feature based on triangles was proposed, which uses the concept of Delaunay triangulation ( ), which for a set of points, such as a minutia, finds all the triangles where a circle encompasses the three vertexes of each triangle, and no other point is inside that circle","Hernández-Palancar et al. claimed that Delaunay triangulation is useful for describing a set of minutiae, but that the triangulation is heavily affected by spurious or missing minutiae","To solve this problem, they proposed to calculate the Delaunay triangulation of all the minutiae and then remove one, calculate the resulting triangulation, add the minutia, and remove the next, until all the triangulations are obtained","Their triangle description contains the sign, the angle   of each minutia in relation to the opposite side, the length of each side of the triangle, and the ridge count between the minutiae","The similarity between two triangles depends on the difference in angle, sign, ridge count, and length","Tested on an institutional dataset, their method obtained a 77% r1","However, Hernández-Palancar et al. determined that the ridge count can introduce errors owing to the distortion in the latent palmprints","Thus, in  , they removed the ridge count from the descriptor, obtaining a 77.5% r1","Although their method is robust against distortion, rotation, translation, and missing or spurious minutiae, the number of triangles required for comparison makes this method slow. 2.4.2 Fixed-radius descriptors Jain and Feng proposed the MinutiaCode, a descriptor that uses the information from a circle around each minutia, separated into sectors ( )","For each sector, the mean ridge period; the mean ridge orientation, if the sector is in the background or foreground; and the number of minutiae separated into four types are stored","The minutia in each sector can be of the type that supports or forms an opposite pair, depending on whether the difference in the angle   of the minutia in the sector is less than  /2 with respect to the central minutia","Moreover, a minutia in a sector can be categorized as either reliable or unreliable, depending on whether the minutia forms an opposite pair with another in the close vicinity","The similarity between two MinutiaCode descriptors is the weighted average of the similarity of each sector, which depends on the difference in orientation, period, and number of minutiae","MinutiaCode was tested on an institutional dataset, achieving a 61% r1","It is robust against translation, rotation, and spurious minutiae","Furthermore, Jain and Feng showed that, for latent palmprints with good quality, the identification is better, as the r1 obtained was 91%","However, in Jain and Feng’s experiments, they manually selected the region of interest, which does not reproduce the conditions of latent palmprint identification faithfully","Similar to MinutiaCode, the descriptor proposed in   uses the information from a circle around each minutia, obtaining information from the orientation map, ridge periods, and minutiae",The descriptors for the orientation and period use a sampling of points around the minutiae to obtain the average orientation and ridge period,"The descriptor that uses the minutiae is calculated after the circle is separated into 32 sectors, and for each minutia in the sector, the polar radius, angle, and relative direction with respect to the angle   of the central minutia are stored","In the case of the orientation and period descriptors, the similarity depends on the difference in the measured value on each point","However, in the case of the minutiae, the similarity depends on the number of matching minutiae in each sector, along with the quality of each minutia","As the latent palmprints could present distortion and the sectors could not be matched correctly, the authors used sectors that overlapped",The total similarity is the sum of the similarity of the three descriptors,"Liu et al. tested their method on an institutional dataset, obtaining a 79.4% r1","Their results were better than those reported by Jain and Feng, and their descriptor is rotation and translation invariant, while also being robust against distortion and spurious minutiae owing to the inclusion of the quality measure","Laadjel et al. claimed that SIFT can be used for partial-to-full palmprint identification when the image has suffered from translation, rotation, and illumination changes, with the disadvantage that SIFT generates too many keypoints","To reduce the number of keypoints, Laadjel et al. proposed to use minutiae as the keypoints, in a feature that they call invariant local minutiae descriptor (ILMD) ( )","For each minutia, they obtain a SIFT descriptor, making it rotation invariant by using the angle   of the minutia as the reference orientation",The Euclidean distance is used for measuring the similarity between two ILMDs,"Laadjel et al. tested their proposal with a dataset developed in situ, obtaining a 0.8% EER","Their experiments showed that ILMD achieves a performance similar to those of methods using SIFT keypoints, but reduces the number of comparisons by two orders of magnitude","However, Laadjel et al. did not compare their method with other methods and did not use real latent palmprints or simulated latent palmprints with noise","In  , a minutiae descriptor based on the directional and spatial relationships between minutiae, called minutia cylinder code, was proposed",Their descriptor uses the information on a circle around each minutia but considers multiple orientations,"The descriptor for each orientation is stacked to produce a cylinder, with the minutiae at the center of the base","The cylinder is broken down into cubes, where the contribution of each minutiae near the one being described is measured","Then, the contribution is converted into a binary value, depending on a threshold",The similarity between two minutia cylinder codes is calculated depending on the contribution of each pair of cubes in the cylinder,"Cappelli et al.’s descriptor is rotation and translation invariant, robust against distortion, and tolerant against spurious or missing minutia","When tested on THUPALMLAB, the minutia cylinder code obtained an EER of less than 0.01% 2.5 Summary of the existing methods for latent palmprint identification To our knowledge extent, the first method for identification in latent palmprints was proposed by  , which uses two features: minutiae and characteristics of the image","In  , the minutiae are matched using a commercial matcher",The authors discussed the challenges of working with latent palmprints such as the increased surface when compared to fingerprints and the partial-to-full comparison problem,"However, in their evaluation, they did not address these problems","As a consequence, they manually indicated the area to match between latents and templates","MPOC was proposed to match palmprints in  , having the advantage of detecting the area where the partial palmprint matches, but is negatively affected by rotation",Their evaluation methodology includes generating simulated latent palmprints from the public dataset THUPALMLAB,"MPOC is enhanced in   and  , changing the dataset to a public one, but still working with simulated palmprints","Another method for working with the partial-to-full comparison problem was proposed in  , where the authors use SIFT for matching palmprints and use a similar methodology evaluation methodology as the one proposed by  ","In  , the authors proposed to combine MPOC and SIFT and evaluate them using the same methodology","One advantage of the evaluation performed in  ,  ,  , and   is that by following similar methodologies, using the same dataset and performance measurements, the features and matching methods in the articles mentioned above are comparable between them","However, given that some palmprints from the public dataset are not used, and they do not publish the simulated latent palmprints, their results cannot be compared against other methods, nor serve as a baseline","Furthermore, by evaluating all the palmprint identification method, the contribution of the extracted features and the matchers cannot be evaluated separately","In   and   authors proposed to use information from the friction ridges and flexion creases, such as orientation, direction, and principal palm lines",Their evaluation methodology includes an extended THUPALMLAB and has the advantage of considering the most significant number of simulated latent palmprints,"However, it has as a disadvantage that the simulated latent palmprints are not public, and they do not follow the same methodology in the two papers, differing in number of palmprints and performance measures, making these results non-comparable with others","For minutiae-based features,   proposed a nearest-neighbor descriptor, which was enhanced in  , and used in  ","The evaluation methodology in   and   uses simulated latent palmprints, and an undisclosed dataset","Their methodology has the disadvantages that the authors used a very small number of latents, and the performance measure DIFFS is not widely used, so their results cannot be compared with others","In  , the authors evaluated their descriptor using THUPALMLAB; however, it is not indicated how the simulated latents are obtained",In   and   authors proposed to build on the nearest-neighbor descriptor adding features to make them more robust,"The evaluation done by   of their descriptor has the advantage that it uses real latents against a significant number of templates, and all the palmprints come from the Beijing Institute of Criminal Technology","However, their dataset is not public, and the number of latents is small","In  , a descriptor that is robust against missing minutiae is proposed, which is evaluated using the same dataset and methodology as Wang et al., making their results comparable","In  , the method proposed in   is enhanced and evaluated with the same dataset, plus more real latent palmprints","While increasing the number of latent palmprints is an advantage that allows evaluating their method on more types of latent palmprints, it makes the results non-comparable with the other ones","Furthermore, both ( ) and ( ) are enhancements of previous works that modify the descriptor and the matching algorithm","Since the evaluation methodology only considers the final result, it is impossible to determine if both changes have a beneficial impact, and in what quantity","The methods proposed in  , and   are related, as they both use fixed-radius minutiae-descriptors, and combine information from the friction ridges orientation and lines","These methods are evaluated with similar datasets provided by industry and the Michigan State Police and have the advantage of comparing a high number of latent palmprints and templates, obtained in similar conditions","However, their method has the disadvantage that the datasets are not public, which does not allow to evaluate other descriptors and methods with the same methodology, which also prevents to compare the results fairly",Two evaluation methodologies have been applied only on a single latent palmprint identification method,"First, in  , a minutia is described using SIFT on the image around the minutiae",Laadjel et al. evaluate their method using an undisclosed dataset with simulated latent palmprints,"Second, in   a fixed-radius descriptor is proposed, which is robust against rotation","Cappelli et al. evaluate their descriptor using THUPALMLAB, but do not disclose the methodology for obtaining simulated latent palmprints","In  , the descriptors proposed in   and   are evaluated using the only public dataset that contains real latent palmprints",The evaluation methodology has the disadvantage that it does not evaluate the feature representations and matching algorithms separately,"In  , we show a summary of the performances reported by each aforementioned method, along with the measures used and the reported characteristics of the datasets","If the dataset is publicly available or institutional, it is indicated in parentheses","Otherwise, the dataset was collected in situ","From  , we can observe that only the minutiae-based methods were tested on real latent palmprints","Moreover, these methods are the most popular and the ones that achieved better results","Line-based methods were competitive, like those proposed in   and  ","However, these methods are also used with minutiae-matching algorithms","The highest-reported results were those of  , with a 91.7% r1 when combining minutiae with orientation, density, and principal line maps, and of  , with an EER of less than 0.01%","When the methods were tested on real latent palmprints, there were three methods with good results",Liu et al.’s method ( ) achieved an r1 of 80%,"Morales et al. reported that the methods with higher performance, when tested on LPIDB ( ), were those in   and  , with an 82.1% and an 85.7% r1, respectively","It is evident from   that, even when using the same datasets, such as THUPALMLAB, the different methods used the palmprints differently, evaluate the methods by creating more simulated palmprints or removing template palmprints","Furthermore, the performances reported with different measures could not be compared as there was no equivalence between different measures","As these results evaluated the complete method and not just the features, the results do not indicate whether the improvements needed to increase the performance had something to do with the preprocessing step, the features, or the matching algorithm","Thus, there is a need for a standard methodology to evaluate the performance obtained by each step of the latent palmprint identification methods. 3 Proposed methodology In our review of latent palmprint identification, we found that the focus on most studies was centered on the creation of new matching algorithms",These algorithms are proposed in conjunction with new features extracted from the palmprints,"However, given the unfair comparison between methods, there is no consensus as to which of the existing features allows to obtain a better performance or whether new research on latent palmprint identification should focus on the features or the matching algorithms","To address these problems, in this section, we propose a methodology to evaluate the performance obtained when using the different minutiae-based feature representations on latent palmprint identification","From our review of the existing methods for latent palmprint identification, where an ROI is not required, and partial-to-full comparisons can be made, we can observe that minutiae-based features are the most used","Furthermore, minutiae have the highest-reported performance of these methods and are accepted as evidence when presented in courts of law by forensic experts to ascertain the identity of a suspect","Given the advantages, usefulness, and ubiquity of minutiae-based feature descriptors, our methodology is focused on evaluating this type of descriptors",We propose to evaluate the descriptors in terms of the performance achieved at the local matching stage of the identification step,"Our rationale is that, if a representation can extract enough discriminative information to allow a successful local matching, this should increase the performance of the identification when global matching is done, as the representations with the highest similarity are commonly used to align the palmprints before global matching","To the best of our knowledge, there is not a methodology to evaluate solely the minutiae-based feature representations used for latent palmprint identification, without taking into account the complete matching algorithms, and the preprocessing of the images","Our methodology is based on that of  , who proposed to evaluate fingerprint features","Our methodology key differences with Feng and Zhou’s one, is that we use a different performance measure that allows us to extract more information about the matches, and our ground truth set of matching minutiae is obtained manually by an expert instead of using automatic minutiae matching algorithm as Feng and Zhou do","While the manual matching is a time-consuming task, which prevents using datasets with thousands of minutiae, the feature representation with the best performance will be the one that allows performing a minutiae matching closer to the one that an expert does, while preventing the bias of evaluating as a better representation the one that allows producing similar results to the automatic matching algorithm","Our evaluation methodology ( 
                      ) takes as an input the sets of palmprints   and  , consisting of query and template palmprints, respectively; a function that returns whether a minutia in a query palmprint matches with that in a template palmprint based on the expert’s evaluation; a function to calculate the minutiae descriptor; and a similarity function for that descriptor","First, it calculates the descriptor for each minutia in all the palmprints in   and  ","Second, using the similarity function, it calculates the similarity between all the minutiae in all palmprints in   and  ","If the two minutiae being compared form a matching pair according to the ground truth, then the method saves the similarity value in  , which contains the similarity of genuine comparisons; if not, it saves it in  , which contains the similarity values of impostor comparisons (The whole process can be appreciated in  
                      )","Given   and  , the performance can be determined at a threshold or by varying the threshold to obtain DET curves or ROC curves","A different element in our methodology to that proposed in   is that we decided to use DET curves, as these allow plotting the FMR and FNMR, and are commonly used in biometrics and identification problems","Moreover, these curves allow for a quick visual comparison of the methods and for the extraction of the EER, which gives us a measure of the performance when balancing false matches and non-false matches, which can serve to indicate the limits of each feature","Because our methodology is centered on local matching, we do not calculate the similarity of the whole palmprint, which does not allow obtaining a ranking of the most similar templates to a query","While the focus of our methodology is only the descriptors, we cannot compare against the results published in terms of r1 and CMC curves, as these measures evaluate the whole palmprint matching. 4 Experimental setup We implemented and evaluated, using our methodology, the most prominent minutiae-based features for latent palmprint identification","The features we evaluated were Li et al.’s nearest-neighbor descriptor (LNN) ( ), Tan et al.’s nearest-neighbor descriptor (TNN) ( ), minutia quadruplets (MQ) ( ), radial triangulation (RT) ( ), m-triplets (mT) ( ), MinutiaCode (MC) ( ), Liu et al.’s fixed-radius descriptor (LFR) ( ), invariant local minutiae descriptor (ILMD) ( ), and minutia cylinder code (MCC) ( )","Originally, we also considered the triangle descriptor proposed in  ","However, in our experiments, Hernández-Palancar et al.’s descriptor, when implemented following the description in  , required several months to evaluate, whereas the other descriptors were evaluated in no more than 48 h","Given the time complexity, we decided not to include Hernández-Palancar et al.’s descriptor, as its requirements make it nonviable for use in latent palmprint identification","In our experiments, we used the publicly available dataset LPIDB ( ), which contains 102 template palmprints and 380 real latent palmprints, taken from 51 different subjects","All the palmprints contained in LPIDB already have the minutiae marked, where the template palmprints have, on average, 1612 minutiae and the latent palmprints have 88 minutiae, on average","As our methodology takes as an input a ground truth set, for each latent palmprint, we found manually, with the help of an expert, which minutiae match in the query and template palmprints","Given this ground truth set, 19,386 minutiae comparisons were considered genuine, and 5,447,617,566 comparisons were considered impostors",Since the descriptors MC and LFR used not only the minutiae but also the orientation map and the density map of each palmprint,"For the orientation map, we marked manually, with the help of an expert, the average orientation of the ridges in areas of 16  ×  16 pixels of each palmprint contained in LPIDB","For the density map, we used a state-of-the-art algorithm to extract it automatically",From   we can observe that LPIDB is the only public dataset with latent palmprints,"However, this dataset does not contain the markings that indicate the matching minutiae in latent and template palmprints","Since the fairness of the methodology is contingent on this manual marking, and to allow other researchers to replicate our results and evaluate new features, we provide the ground truth, orientation map, and ridge maps publicly","While the use of LPIDB alongside the ground truth obtained by an expert allows for a fair evaluation of the performance when using a feature representation, the dataset contains only 102 template images","Since latent palmprint identification usually compares a latent against thousands of template palmprints, we use the 1280 templates in THUPALMLAB dataset to increase the number of comparisons in an order of magnitude","Since THUPALMLAB does not include the minutiae as LPIDB, nor the orientation and density maps, we used a state-of-the-art algorithm to extract them automatically","We did not include simulated latents from the templates in THUPALMLAB, as these would not have the minutiae, and matches verified by an expert and could compromise the fairness of the experiments","By including the minutiae in the THUPALMLAB template palmprints, each experiment performs 58,048,644,234 comparisons between minutiae descriptors","We implemented the minutiae descriptors we evaluated, along with their similarity measures, according to the specifications given in each paper","These descriptors and the evaluation methodology were implemented using the C# programming language and are publicly available for other researchers to evaluate them, or they can use our methodology to evaluate their features 
                      ","The parameters of each feature were set using the specifications given in each paper, except for the quality values in LFR, which were determined experimentally in a similar fashion as done in  . 5 Results We used the DET curves to present the results of our experiments","To aid the visual analysis, we plotted separately the minutiae descriptors based on the nearest-neighbor descriptors from those based on the fixed-radius descriptors","As a measure of the performance, we also obtained the EER from the DET curves","Furthermore, we measured the efficiency in terms of the average storage requirements of each descriptor in kilobytes, the time in hours and minutes used to match all the minutiae in the dataset, and the average time required to match a latent palmprint to a template","We present the results when using LPIDB and the combination of LPIDB and THUPALMLAB separately. 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   5.1 Results with LPDIB In  
                         , we show the DET curve resulting from the evaluation of the minutiae descriptors based on the nearest-neighbor descriptors","We can see that Li et al.’s LNN was better than the other descriptors that use polar coordinates to describe the nearest neighbors, namely, MQ, RT, and TNN","Furthermore, it had a lower FMR when the FNMR was less than 0.2 when compared to mT, which is the nearest-neighbors descriptor that does not use a central minutia","However, for values of FMR above 0.2, Medina et al.’s mT achieved lower values of FNMR","Given these results, we can see that, from the nearest-neighbor minutiae descriptors, LNN was better when the system required lower FMR values, whereas mT was better for obtaining lower FNMR values, and that the other descriptors that proposed an alternative to LNN, such as MQ or TNN, did not obtain better results","In  
                         , we show the results of the DET curve obtained after evaluating the fixed-radius minutiae descriptors","From this figure, we can see that the highest-performing fixed-radius minutiae descriptor was MCC, as it had the leftmost DET curve","Whereas MCC is based only on the minutiae on a fixed-radius descriptor, ILMD is based on SIFT features, and the other two fixed-radius descriptors use the orientation and period of the ridges","On the basis of these results, we observed that features that use the image or the friction ridges result in a lower performance in latent palmprint identification, which could be due to the distortions in the latent palmprints modifying the structure of the friction ridges heavily, making the descriptors that used them unreliable, as noticed in  ","In  
                         , we show the performance obtained by evaluating each descriptor using the EER measure and the efficiency based on storage and comparison time","The highest-performance was obtained when using MCC, with the lowest EER of 14.84%, followed by LNN with 18.56% and mT with 18.74%","Regarding the matching time, the best results were obtained by MQ, MCC, and LNN, with 1:24, 1:39, and 3:16 h, respectively","Moreover, in terms of storage, the descriptors with the lowest requirements were MQ, MCC, and mT","Although MQ has lower storage and time requirements, because it is the descriptor that collects less information about the minutiae, its performance when used was not competitive with the highest-performing ones, namely, mT, LNN, and MCC","By not considering MQ, we can see that MCC was the most efficient descriptor, allowing to obtain the lowest EER","In second place were mT and LNN, which had similar performance and storage efficiency, but with LNN being 15 min faster","From   we can also see that descriptors that relied on the friction ridges or on the palmprint image to extract the feature, such as MC, LFR, and ILMD, show performances that were more than 10% lower than those of the highest-performing methods, with much higher time and storage requirements","In  
                         , we show the comparison of the two highest-performing fixed-radius and nearest-neighbor minutiae-descriptors based on their DET curves","As shown in  , ILMD had the lowest performance of the four descriptors","The descriptor that achieved the lowest values of FMR was MCC, which also achieved lower values of FNMR when FMR was less than 0.3","After this value, the descriptor that achieved a lower FNMR was mT, followed by LNN","Given these results, we can conclude that, if lower values of FMR are required, then MCC is the recommended descriptor","However, to obtain lower values of FNMR, we find mT to be a better descriptor, with the disadvantage of taking more time to perform all minutiae comparisons than MCC. 5.2 Results with LPIDB and THUPALMLAB In  
                         , we show the DET curve resulting from the evaluation of the minutiae descriptors based on the nearest-neighbor descriptors using the combination of LPIDB and THUPALMLAB","Using a larger background dataset, we can corroborate the results obtained when using only LPIDB, where Li et al.’s LNN was better than the other descriptors that use polar coordinates",The most significant difference is that Medina et al.’s mT allows to obtain a lower FNMR when the FMR is less than 0.2 than LNN,"However, the RT has a lower FNMR when an FMR of 0.6 or higher is used","In  
                         , we show the results of the DET curve obtained after evaluating the fixed-radius minutiae descriptors when using LPIDB and THUPALMLAB","When compared with  , we do not see much changes in the performance obtained when using the different descriptors","Both LFR and MC, which use the density and orientation images as part of the minutiae descriptor result in the lowest discerning between matching and non-matching minutiae of the representations compared","ILMD, while it uses the image around the minutiae as LFR and MC, performs better than the other methods","Lastly, MCC continues to be the minutiae descriptor that allows to obtain the best performance of the fixed-radius descriptors","In  
                         , we show the DET curves of the four minutiae descriptors which allow to obtain the best performance when evaluated using LPIDB and THUPALMLAB","Compared with  , which shows the highest-performing descriptors evaluated only with LPIDB, we can observe a similar graph, where the descriptors are the same, and ILMD is still the rightmost descriptor","On the other hand, mT and LNN show a slightly better result than when only evaluated using LPIDB","Given this increase in performance, there is not a minutiae descriptor that outperforms the others. mT has the lowest FNMR when the FMR is less than 0.3","Between 0.3 and 0.4 FMR, LNN obtains the lowest FNMR","Moreover, from 0.4 FMR and above, MCC has the lowest FNMR","Given that LNN has a lower FNMR than mT and MCC in a smaller range, the better performing minutiae descriptors are MCC and mT, even when tested using a bigger template dataset, such as it happens in the experiments with THUPALMLAB. 5.3 Results discussion From the results obtained, the most important insight is that using information other than minutiae, such as the friction ridges and their orientation, has a negative impact in establishing the similarity of minutiae","Given the distortion that latent palmprints can suffer, and the differences in angles resulting from the different orientation between query and template, this result is expected","This also serves to confirm the findings in  , where using the line count, a feature obtained from the friction ridge image, affected the result of the matching negatively","The other method that uses information from the image, namely ILMD, also results in a lower performance when compared with other descriptors that do not use the image","Based on these results, we discourage the use of image-based features to describe the area around a minutia in latent palmprint identification","However, it is important to mention that our methodology does not take into account special preprocessing of the images and that images are not aligned","These details may enhance the performances when using minutiae-descriptors that rely on image-based features, and make them applicable in certain cases where an expert can align the palmprints","Regarding the highest-performing descriptors, namely MCC and mT, when compared to other minutiae-based descriptors, we can observe the inclusion of elements that make the descriptors robust against rotations and differences between the template and the query","On the side of MCC, it takes the minutiae in a fixed-radius not as fixed points regarding the central minutiae, but as areas of information in different orientations","On the other side, mT takes information from the nearest-neighbors but adding the angles and distances when taking the minutiae in a different order",These elements make them more robust against rotations and distortions in the latent palmprints,"Since other fixed-radius and nearest-neighbor minutiae descriptor does not have this level of handcrafted features by experts, it could indicate that new descriptors have to look for relationships on minutiae that can help to describe them robustly. 6 Conclusions In this study, we surveyed the features used for latent palmprint identification",We found that methods that rely on minutiae-based features reported better performance,"However, currently, the evaluation methodologies proposed do not allow determining whether the performance is due to the merit of the features or the matching algorithms","Moreover, owing to the differences between these methodologies, the performance were not compared fairly","To solve this, in this paper, we proposed a new evaluation methodology for minutiae-based features, which evaluates the features in terms of the performance obtained in the local matching step","Using our methodology, we evaluated the features used in latent palmprint identification","Our results showed that using Cappelli et al.’s descriptor, called minutia cylinder code ( ), allowed to obtain a higher performance than other descriptors, while also being the aptest descriptor in terms of the time required to match the minutiae and in the storage requirements","However, the m-triplet descriptor proposed in   and the nearest-neighbor descriptor proposed in   can find more genuine matching minutiae than those of the minutia cylinder code","Furthermore, there is still room for improvement, as the descriptor with the lowest equal error rate, achieved a value of 14%",We aim to provide the researchers on latent palmprint identification the means to evaluate new features fairly,"To achieve this, we have released the implementation of our methodology and the features publicly","Moreover, the LPIDB dataset with the matching minutiae, the orientation map, and the density map are publicly available","From this review and the results of evaluating the minutiae-based descriptors for latent palmprint identification, we have identified several future research lines","The main areas are an evaluation of other parts of the identification process, descriptor enhancement, and automatically created descriptors",We discuss these areas in the following paragraphs,"In this paper, we only evaluated descriptors, as we believe they are crucial for the latent palmprint identification process","However, we left out two other critical areas: preprocessing and matching","Preprocessing is crucial because it enhances and makes clearer the latent palmprints, allowing the feature extractors to work better and obtain meaningful information","However, almost all of the descriptors we reviewed use different preprocessing methods","To determine the influence of each one, and if there is a method that enhances a descriptor in particular, an evaluation should be done using each of the preprocessing methods and then evaluating each of the descriptors",The same process could be done to evaluate the different matching algorithms that are proposed with each descriptor,"Obtaining an evaluation of the three parts independently, and then the relation between each one could produce a guide for researchers, manufacturers, and experts to decide which preprocessing, descriptor, and matching algorithm combination works better","Furthermore, experiments could include different types of latent palmprints that the experts find in the field, to create a meta-heuristic about which method to use in different scenarios, as it is done in fingerprints, where datasets contain fingerprints of different qualities","For the enhancement of minutiae-based descriptors, we believe that handcrafted features that include relationships between minutiae that make the descriptor more robust against missing minutiae, rotation, and distortion, such as the ones shown by m-Triplets and Minutia Cylinder Code, are a direction that has to be better explored for palmprint","Furthermore, these descriptors could include information from the flexion creases, as this information is not used in the latent palmprint identification case, and could serve to make guesses in the alignment or section of the palmprint","Descriptors could also be combined to allow obtaining more discriminative information from the palmprint, as done in   for palmprint recognition, which resulted in a better performance",Again having a dataset with different types of latent palmprints could result in generating a meta-heuristic for determining which descriptors to combine in each specific case,"Lastly, there is a trend in fingerprint recognition of automatically extracting the minutiae descriptor based on applying a deep convolutional neural network to an area around the minutia, to extract a descriptor ( )","Currently, there is no evidence showing that this type of features allow for a better performance than traditional methods","However, the use of convolution neural networks to learn and extract discriminative information could enhance existing descriptors, or even surpass them in the context of latent palmprint identification, as the images are bigger and with more information than the ones used in fingerprint identification","Furthermore, multiple architectures have worked in other contexts, such as siamese neural networks for image recognition ( ), which can help to enhance further the performance of systems that automatically extract minutiae-descriptors from the image around the minutia","To evaluate the performance obtained when using these new descriptors, our methodology, experiments, datasets, and result can serve a the baseline","CRediT authorship contribution statement 
                       Conceptualization, Data curation, Investigation, Software, Visualization, Writing - original draft, Writing - review & editing.   Conceptualization, Data curation, Funding acquisition, Investigation, Methodology, Resources, Writing - review & editing.   Conceptualization, Funding acquisition, Project administration, Supervision, Writing - review & editing.   Conceptualization, Investigation, Writing - review & editing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s107158191630074x," 1 Introduction Business processes such as in manufacturing and logistics or in administration, but also technical processes like in robotics, are becoming increasingly complex while they are at the same time more and more automated, computerized and monitored in real-time ( )","This is true for processes in many domains, but especially for industrial productions, where a delayed delivery of raw materials can lead to a standstill in production and thus high loss of profit",On the one hand the increasing amount of data offer an enormous potential to better monitor and control processes,On the other hand it puts increasing pressure on monitoring personnel who need to observe processes,"The status quo in large-scale process monitoring is heavily focused on control centers where users observe production on multiple screens, using both video features as well as schematic overviews of process and machines/facilities, charts/graphs, textual descriptions and alerts ( )","Especially in smaller- and medium-sized production companies, there are often no dedicated personnel charged with full-time monitoring, but instead engineers and supervisors need to primarily perform other tasks, yet monitor the process’ status at the same time","However, especially in such peripheral or   scenarios where the attention is focused on a primary task and other information is monitored indirectly at the same time, visual means are not well suited, as pointed out by  ","Meanwhile, maintenance experts have been using the auditory sense to identify or anticipate possible machine problems, a technique referred to as vibration analysis, for a long time","Crucial vibration properties are amplitude, frequency, phase and modulation ( )","Therefore, traditional production monitoring is still considered to be a holistic approach, covering the visual, auditory and even olfactory sense, even though automation has enhanced   vibration analysis in the recent years ( )","In modern production settings, sound is typically only used as a means to convey warnings and alerts, e.g., to convey an alarm situation when a machine broke down or a predefined threshold had been exceeded ( )","In a production scenario, this could for instance be the case when the stock level of a resource has dropped below a critical level, or when a temperature sensor of a machine measures a critical temperature, indicating imminent machine failure ( )","However, this type of auditory display has several drawbacks: on the one hand, if rules that define alert triggering thresholds are defined too  , i.e., requiring strong evidence before issuing positive classifications, potentially critical situations such as machine failures might occur without issuing an alert","On the other hand, if the values are defined too  , i.e. risking high false positive rates, the resulting flood of (in many cases unnecessary) alerts and alarms might lead to an information overload of the user, or to the situation that the user stops to take the alerts as serious as they are","Furthermore, in many scenarios engineers are not able to define all states and values that might lead to a critical situation beforehand","Levels and values that might constitute a critical state are often complex to decide, as e.g., the question if a specific parameter value constitutes a critical situation or not often depends on the context, given by various other parameters","But even if all possibly critical situations are covered by alerts and alarms, in most cases operators might prefer to be informed even   a situation might become critical, thus enabling them to  ,   and   the problem",A constant awareness of states and values through an   might enable such an anticipation of critical situations,"Thus, we suggest to use the mentioned tradition of auditory monitoring as a leverage effect by supplementing state-of-the-art visual process monitoring with techniques from sonification","Sonification is the systematic, reproducible and thus scientific method for representing data as (mostly non-speech) sound in an auditory display ( )","Well-known examples of sonifications are the Geiger counter for displaying radioactive radiation, or the auditory parking aid which conveys the distance to the vehicle or obstacle behind as pulse rate of a beep sound","Beyond these very basic and simple types, sonification researchers have developed a plethora of approaches to represent more complex data such as multivariate time series (e.g","EEG and ECG), or spatio-temporal data (e.g. images and well logs), and also general high-dimensional data distributions","Sonification has several key advantages that makes it suitable especially for the application area of real-time process monitoring, like our ability to process audio faster than visuals or the fact that we easily habituate to static sound sources, yet that we are at the same time very sensitive to changes ( )",For these reasons sonification promises a solution to the aforementioned challenges of state-of-the art process monitoring,"However, there are several open questions when it comes to supporting users in monitoring as a secondary task that concern the sonification design and as well as how different types of sound-enhanced process monitoring affect attention and concentration in main- and secondary task, which we tackled with this paper","Our main research goals were (a) to find out if a continuous, soundscape-based sonification of individual production steps can support users better in monitoring as a secondary task than a purely-visual solution, or one that is based on auditory alerts","Other open research questions were (b) to what extent the three different conditions distract users from their main task, (c) how users rate the three different conditions concerning relevant aspects such as pleasingness, helpfulnesses, intrusiveness or exhaustiveness","Answering those research questions poses several challenges, such as simulating the potential users' main- and secondary task in such a way, that they are both cognitively demanding and thus binding the undivided attention, while at the same time allowing for an easy and reliable measurement of task performance in a fine-grained manner","As there are no standardized environments that fit these requirements, we have developed the SoProMon system (Sonification for Process monitoring), that is a hard-/software system for reproducible research in sonification for peripheral monitoring, particularly for the investigation of attention allocation in dual-task-settings","The system has already been presented in  , and consists primarily of a main task console to bind the user's attention by presenting simple arithmetic problems and a simulated production process that requires different types of user interactions (see  )","Based on the SoProMon system, we conducted an extensive experiment in a within-subject design (n=18), whose results contribute to answering the mentioned open research questions, and thus to advancing research in this area (see  ) in the following ways: 
                   As industrial production is an area, in which it is especially crucial to monitor processes in real-time and that can probably be intuitively understood also by domain novices, the secondary task of this experiment is based on a simulated production processes","However, as the experiment design aimed at quite fundamental questions of attention allocation, the results should be generic enough to be transferable to monitoring scenarios in other domains as well. 
                       The details on the current state-of-the-art concerning research in sonification for (peripheral) process monitoring as well as on the open research issues that we tried to tackle with the experiment can be found in  ","The hypotheses derived from the literature which we tried to tackle with the experiment are described in  , followed by an introduction into the SoProMon system ( ) and the methodology of our experiment ( )","Experimental results will be presented in   and discussed in  , followed by overall conclusive considerations. 2 Related work There is a substantial amount of research concerning applications of auditory process monitoring, spanning various areas such as industrial production processes, program execution or web server behavior",A good summary is given in  ,"In the following we focus on the most relevant works here with respect to the SoProMon system. 
                       explore with their   the production processes of a bottling plant in a multi-modal representation that combines visual and auditory means ( )","In  , sonification has been applied for the direct monitoring of an assembly line",The authors concluded that participants of a study who had visual as well as event-based auditory feedback felt more self-assured and socially accepted than in the visual condition,"A popular application area of auditory process monitoring is computer program debugging, as e.g. investigated in  ","This so-called program auralization is relevant as it assumes a similar monitoring mode as in process monitoring; in debugging, however, the monitoring becomes typically the main task","An area where monitoring is more in the periphery is auditory monitoring of web servers and computer networks, such as the concepts implemented in the systems   ( ) or   ( )",Sonification is also used frequently for computer security and -intrusion detection (e.g. see   and  ),"As in both fields the focus of attention is often elsewhere, they offers relevant ideas and approaches for our task at hand",First steps towards sonification in business process monitoring have also been taken in our preliminary work ( ),"As process monitoring is in many cases a peripheral task, a very important aspect in designing process monitoring systems for peripheral monitoring is attention allocation","Auditory monitoring systems should ideally, during normal operation, hardly be perceived actively at all","In cases that require the user's attention, such as exceptional or even potentially dangerous situations, the sonification should nonetheless be able to attract the user's full attention",This leads to a trade-off between   and  ,"Generally, the more information a sonification conveys, the greater the risk of disturbance","This trade-off has been researched by  , among others","In  , the authors suggest to use a soundscape that is designed to achieve unobtrusiveness by relying on nature recordings","There is a wide selection of research that investigates how sonifications can guide the user's attention, such as by   or ( ), and on how to design sonifications for peripheral monitoring ( )","In summary, there is a substantial body of research on sonification for process monitoring in general","There exists furthermore research dealing specifically with peripheral process monitoring, often in dual-task scenarios, although there are some research gap in this area","Only for a few of those approaches, studies have been conducted to test their effectiveness","Those studies that compare a visual-only condition to a multi-modal condition that conveys sporadic auditory alerts or alarms conclude, that the performances in both tasks seem to be in most cases not significantly affected by the auditory signals ( )","In a few cases, both tasks are negatively affected (e.g.  )","Typically there are less head movements and attention switches measured in the multi-modal condition ( ), something that has also been observed for continuous sonifications ( )","When comparing the performances in the main task – which in many studies is simulated by presenting arithmetic problems – in experiments that include continuous sonifications, the results are mixed: in some experiments less mistakes were made in the multi-modal condition compared to the visual-only condition ( ), while in other experiments the best main task performance was observed in the visual condition ( )","In tendency, the main task performance seems to be slightly negatively affected by sound (more so in multi-modal conditions than in auditory-only conditions), although these differences between the conditions are in the majority of studies not statistically significant","Even so, especially the results in the conditions that include sound are typically better when the respective condition is not the subject's first, but second or third condition of the experiment (e.g.   or  )","Thus, the observed distraction by sound seems to be smaller when the participants are already used to and familiar with the two tasks themselves","The performance in the secondary task (monitoring) is typically significantly higher in multi-modal conditions that feature continuous sonifications, compared to visual-only conditions ( ), although a few studies report the opposite result (e.g.  )","Like for the main task, there seems to be a strong familiarization effect that especially benefits the multi-modal conditions ( ), which may be an explanation for why the advantage of auditory conditions over the visual-only condition seems to be greater for domain experts than for amateurs ( )","In general, participants and especially domain experts, when asked for their opinion, state that they preferred the multi-modal conditions including continuous sonifications, as – among other reasons – they made them feel more in control ( ). 3 Hypotheses Based on the related research in  , we define several hypotheses","As a baseline, we take the two most common modes of monitoring in current production scenarios (and of many other domains as well): 
                   These two are compared to 
                   In general, we expect differences between   and  , but especially between   and the other two, basically in favor of the auditory information types",We expect the differences to manifest both in the users' perceptions (measured by their questionnaire responses and their comments) and the quantitative performance measured in the monitoring task,"We expect, in accordance with the literature, that the three mentioned conditions will have no significant effect on the performance of the main task (H1.1)","Concerning process monitoring performance and behavior, we expect that additional auditory cues have no effect (H2.1)","We do however expect continuous sonification to have a significantly positive effect on monitoring performance, compared to the other two conditions (H2.2), as this is what the sonification design mainly aims at","Concerning the questionnaire responses, we expect items associated with helpfulness, attention switching and performance increase of the respective mode of operation to be more favorably rated in   compared to   and   (H3.1)",We furthermore believe that users feel more self-assured and in control with   (H3.2),"By contrast, we expect additional information conveyed aurally to increase exhaustion, and therefore   and   to be more exhaustive than   (H3.3)","For each of these hypotheses, the null hypothesis ( ) says that there are no significant differences between the groups, while the alternative hypotheses ( ) assumes the opposite. 4 The SoProMon system The Sonification for Process Monitoring (SoProMon) system 
                       is a hard-/software system for reproducible research in sonification for peripheral monitoring, particularly for the investigation of attention allocation in dual-task-settings","The core software components are (a) a process simulation, (b) a visual monitoring system including graphical user interface elements (buttons) to intervene, (c) a sonification system that allows to plug-in different sonification types for multi-modal variants and (d) a main task console to bind the user's attention to a (different) focus","This is complemented by a set of service modules for logging all relevant data, including a video camera mounted atop the user to store head orientations, and calibration modules for user-adjustment of the sound levels","The system is highly modular and flexible, and individual modules can be replaced by other customized code if required","For our first practical implementation we decided to create a setting where a user is seated in front of two monitors, oriented perpendicular to each other, one for the main task and one for the monitoring console, in order to be able to stimulate and observe attention shifts","Furthermore, the keyboard and the mouse were fixed to the table, so that they could not be moved (see  
                      )","Yet other implementations, such as letting users move freely in the room and solve practical problems, (as opposed to computer tasks) are certainly also conceivable","Concerning (a), the process simulation, we chose a rather lifelike scenario in the realm of manufacturing: we represent the process as a graph of 6 production steps that partially run in parallel and require input of one or more previous production steps at times","The number 6 is arbitrary, yet chosen here to have enough complexity to be not trivial and low enough to first learn about processes of limited complexity","Even though, as already motivated before, the current implementation of SoProMon is based on the simulation of a production process, the system can be adapted to simulate different types of processes as well","We designed the simulation so that it requires several user interactions, in order to measure the performance of auditory monitoring in attention allocation and in interrupting the users during their main task","The required interactions are:  
                   After a click, the machines idle a given short time, to discourage users from performing unnecessary actions","This is in so far realistic, as real machines often require a short downtime when they are being refilled, and a longer one if they are being maintained or repaired","As to (b) the visual monitoring, we depict a graph of the machine setup and flow of goods","While  
                       shows the   state of simulation, in which all machines are working,  
                       shows a critical state, in which several machines are out of order","The visualization can be assumed to be checked very quickly, leaving any time for the interpretation and reaction to be accounted for the interpretation of sounds","As our main concern is the assessment of sonifications that complement an existing visual monitoring console, the visual part remains invariant in all experimental conditions and thus does not require a dedicated motivation or testing","Concerning (d), the main task, we chose a task that is both cognitively demanding and thus binds the undivided attention, while at the same time allows the easy and reliable measurement of task performance","For this purpose, it is best if the task consists of a series of repeated smaller elementary tasks whose correctness can be computed",Ideally the main task can be interrupted to attend to the monitoring,"Typical tasks in real-world scenarios are, depending on the user group, processing documents such as emails, planning/scheduling, or repairing machines","For the sake of easier evaluation we selected the   task, which is a mental arithmetic task of summing up two numbers (each smaller than 50)",The result is to be entered into a text field using the computer keyboard,"On hitting the return key the task, the result, and the timestamp are logged, and the next pair of random numbers is drawn and presented","The window is displayed in low font size on the screen perpendicular to the monitoring screen. 5 Methods As already mentioned, the main goal of the experiment was to answer the previously discussed research challenges","Three conditions, as explained in  , are being compared to each other:  ,   (combining   with auditory alarms which were conveyed when a machine stopped), and   (combining the other two conditions with a continuous soundscape sonification introduced in detail in  ). 
                       is true to the event-like nature of individual machine executions, meaning that every process step in the production yields a tiny sonic counterpart so that their superimposition creates a soundscape that reflects the overall activity",Assigning different sounds/timbres to different machines results in 6 voices that play simultaneously,"The reason to not only sonify those machines that require interactions is, that the sonification of irregularities in the production for machines that precede machines for which interactions are required can help to anticipate problems that might occur for those machines at a later stage","We chose sounds of the forest theme, namely small bird, woodpecker, water drop, bee, river splash and cracking twig",The sonification is designed to form a soundscape by selecting sounds that constitute the perception of a coherent setting (e.g. forest),"While soundscape ecology would suggest an optimization process so that the bandwidth allocation reduces the risk of masking, we here just select sounds based on subjective choices to fit into the theme","When a machine has reached a critical level, its sound is repeated at a fast rate and high volume, until the problem is solved","However, beyond a mere display of individual executions we add information by using a mapping of machine-specific data to the acoustic shape of the sound events: For more details please refer to  ","Video examples for the SoProMon sonification (for both   and  ) used within the study are available on our website 
                       and as supplementary material at the online version ( ) of this paper",Spectrograms of the different machines and states are available as well,"In order to answer our research questions, different types of data were recorded during the experiment: 
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   5.1 Study population The study population had an age median of   years.  
                          shows the experiences of the participants in various relevant fields, as well as their visual and hearing impairments. 
                         
                          shows the participants' opinions and estimations concerning the foundations of the experiment, such as the graphical user interface",Two participants criticized that they had to press a key in order to switch the focus of attention to the main task window,In recognition of this possible error source we corrected the calculation mistakes that were (supposedly) due to the first digit after an action being ignored in post-processing,"Furthermore it was mentioned, that the mouse positioning favored left-handed users. 5.2 Experiment plan Two pre-tests have been performed before the experiment to ensure the understandability of the system, the questionnaires, the two tasks and the provided instructions, the validity of the experiment, and the technical functionality of all components of the SoProMon system, especially the logging mechanisms",Several adjustments have been made after the pre-tests,"In order to have three complete sets of condition sequences in this within-subjects design, 18 participants were recruited for the experiment itself",All 6 possible permutations of the three conditions were realized among the runs,"Before signing an informed consent, the participants were give a written introduction into the goals and aims of the study, its duration and experiment procedure, as well as written instructions for the system","The experiment goal, namely to solve as many arithmetic problems as possible during the three experiment parts of 10 min each, while at the same time trying to avoid critical process states as well as possible was, shortly before the experiment, repeated verbally as well","Before starting the process simulation with one of the three conditions  ,  ,  , the participants were informed regarding how the process status and criticality was conveyed (e.g. no sound, sound in the case or errors, and permanent sounds)","If the respective part of the experiment was the participant's first one, he or she had time to familiarize with the process simulation beforehand, and ask questions","If the participants' first condition was  , there was additionally a sound level calibration phase, during which the individual machine's volumes were adjusted until they were just loud enough to identify","Questionnaires were handed out before the experiment, after the 3 conditions, as well as after the experiment","In total, the experiment lasted around 65±5 min for each participant. 5.3 Main task measurement Different measures can be used to compare the performances","The number of calculations that the participants were able to execute in the given experiment time of 10 min, would be the most obvious choice","On the other hand, not only the number of calculations is relevant, but also their correctness","Therefore, the conditions are also compared concerning their mean  , which is calculated by averaging the deviation of each entered result from the correct result by  where   is the participant number,  
                          is the number of questions that participant   has replied, and  
                          (resp.  ) are the given (resp. correct) results for the  th question","Furthermore, in order to compare the overall   of the participants between the three conditions, we introduce the ‘main task score’, a variable that encompasses both the number of solved calculations and their correctness as  with   referring to the sample mean and   being the standard deviation of its variable. 5.4 Process simulation measurement An obvious metric to measure the monitoring performance between the different conditions would be to compare how many clicks the users made in average for each condition","Furthermore of interest are the buffer values of the respective buffers at the time of the user's interaction with the simulation (e.g., the input buffer of a certain machine at the time of refilling it)","A relatively high average buffer value can e.g. signify that the users do not trust that the respective mode of process monitoring conveys the need for interaction in time, leading the users to switching their attention to the process simulation in regular intervals, and performing interactions  ","A low average buffer can, on the other hand, signify that the users rely on the respective conditions’ ability to signal interaction needs","On the other hand, if e.g. an input buffer had already been completely depleted at the time of intervention, this may signify that the respective condition has failed to inform the users in time","In many cases, participants used double clicks for their interactions, while a single click would have been sufficient, a fact that was perhaps not communicated clearly enough to the participants","Therefore, if several clicks were performed directly one after another, only the first click was taken into account. 
                         
                         
                         
                         
                      5.4.1 Anticipation optimal rationality The concept of   tries to be true to a real-life production scenario: interactions that come  , and were therefore  , are here punished","This is realistic, as in most real-life production scenarios it is a goal to maximize production and to minimize downtimes as much as possible","As each interaction in the simulation entails a downtime (e.g. for maintaining a machine), it is logical to minimize such interactions","However, the anticipation optimality not only punishes interactions that are too early but also such that are   in this case means, that a critical situation had, at the time of interaction, already occurred, or that it was so close, that it could not have been prevented assuming average reaction times",The rationale behind this is that downtimes due to critical states (such as a machine that has broken down) are to be avoided even more than ‘planned downtimes’ due to maintenance,"Thus, in anticipation optimal rationality  
                            , we calculate the mean time that participants needed to shift the attention from the main task to the process simulation","This time span included finishing the calculation that they were working on, turning around to the process simulation, and performing the necessary action","If an interaction was performed when the respective machine had already stopped due to a critical state, or when e.g. the buffer value was already so low, that an interaction was likely to be performed too late, this interaction was evaluated with a score of 0","Interactions that have been performed exactly at a time at which it would have been, given average reaction times, possible to intervene just before a critical state would have been reached, were rated optimal (1.0)","All interactions that occurred at a later point were mapped linearly between 1.0 and 0 from the optimal interaction point (1.0) to the point at which an interaction has been completely   (0), e.g. when an input buffer was still completely full (see  
                            ). 5.5 Preprocessing of questionnaire data In order to test the subjects' accuracy in answering and in order to be able to detect random answering, several items of the questionnaire dealt with the same subject, often in an opposing scale","If the given answers were too contradictory, the respective item pair for the specific participant has been marked as outlier/inconsistent data point and removed for the analysis","This way, out of the 1908 individual answers, 
                          13 of such pairs have been removed (e.g. “I was always in full control of the process simulation” vs. “I was overstrained by the process simulation”)","As several analyses required complete sets of answers (such as the combination of different items), the removed answers have for these purposes been filled with the mean of the respective item","In order to allow a more powerful analysis and more representative results, corresponding items have been combined into Likert scales (composite scores)",The factors have been grouped by average,"Subsequently, their consistency has been tested with Cronbach's alpha: only those factors with a reliable consistency outcome (i.e.  ) have been used for data analysis","If not stated otherwise, the scales range from 10 (fully applies) to 0 (does not apply at all)","The complete questionnaire (translated from German) as well as tables with raw data and detailed analyses can be found as supplementary material at the online version of this paper. 5.6 Statistics All results concerning the main task performance have the following format: mean±standard deviation, as they are interval-scaled and normally distributed","For the other results (process monitoring performances and questionnaire results), the standard way of communicating results will be median±IQR (interquartile range), due to the fact that those results have been tested to not be normally distributed, which is why mean values would have little significance","An exception is the discussion of the Likert scales toward the end of the questionnaire results, which have been aggregated out of different Likert items and tested for normality distribution","For scores for which several data points for one experiment part existed (e.g. the buffer values when refilling a machine), for each of the 54 experiment parts (18 subjects × 3 conditions), a mean value has been calculated and compared between the three modes using Friedman tests for repeated measures, respectively repeated measures ANOVA (for the main task results)","The reason aggregating using the mean, instead of the median, even though for some variables only a handful of data points may exist, was to avoid loosing the information of   and variance","This is due to the fact, that in process monitoring, outliers (e.g. standstills of machines) are crucial and to be avoided whenever possible, wherefore this information is important","Individual comparisons between two conditions were performed using Wilcoxon signed-rank tests for dependent samples, respectively dependent t-tests (main task results)",For all modes the false discovery rate has been adjusted with a Benjamini–Hochberg correction,"All  -values have been divided by two, if a directed hypothesis has been defined beforehand",The same is true for presented correlations,"The results of correlation tests, if not stated otherwise, combine the r value of Spearman's Rho with the  -value of Kendall's Tau, as it is more accurate for smaller samples. 6 Results This section describes the most interesting and most significant experiment results","Further results are presented as supplementary material, structured into detailed main task results, non-significant process monitoring results and sound perception and mapping comprehension. 
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   6.1 Main-task results For the main task performance, no significant differences between the three conditions have been observed, neither in terms of the number of performed calculations, nor their correctness, nor the main task score (the combination of both). 6.2 Process monitoring results In total, 1367 interactions (supply, empty, maintain) were performed, out of these 483 during the   conditions (median number of interactions per participant in  :  ), 448 during   ( ) and 436 in   ( ), see  
                         . 
                         
                         
                         
                      
                         
                         
                         
                         
                      6.2.1 Analysis of buffer values Although there were no statistically significant differences concerning the mean buffer values of the respective machines when the participants performed the supply-action, the number of supplies that were performed when the respective input buffer was already completely depleted was substantially higher under   (29 out of 216 supplies=13.4%) and   (34/199=17.1%) than under   (2/197=1%)",Significant differences between the conditions were observed for one of the two machines that required maintenance ( ),The aggregated median machine condition when maintaining it was  %,"The median condition at maintenance was   under  ,   under   and   under  ","However, there have been no significant differences between   and   ( ),   and   ( ) or   and   ( ) observed","In  , 7 out of 83 maintenances were initiated when the machine had already stopped producing, compared to 6 out of 96 in  , and 0 in  . 6.2.2 Anticipation optimal rationality The aggregated median anticipation optimal rationality score over all conditions was  ","There are highly significant differences   between   
                            ,   
                             and   
                            ","The results under   deviate less than under the other conditions (see  
                            ), something that has been observed for many other metrics in this experiment as well","The difference between   and   is not significant  , but between   and   
                             as well as between   and   
                             significant differences have been observed","As can be seen from  
                            , the difference between   and   and   has been higher, when these conditions have not been the participants' respective first condition. 6.3 Questionnaire results There seems to be a clear preference for   concerning the responses to those items that compare the three conditions and for which significant differences between the conditions have been observed: 
                      It is interesting that the mean of the items that are related to the intrusiveness of   (see  
                         ) is slightly higher (5.04±2.1) than of those associated with the sound design being pleasing (4.18±1.9)","However, the feedback related to information aspects was in average quite positive (6.73±1.9). 7 Discussion 
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   7.1 Main task results As there have been no statistically differences between the three conditions observed, neither in terms of the number of solved calculations, their correctness, nor concerning the overall main task score, the null hypothesis of H1.1 can be accepted. 7.2 Process monitoring results The participants' performance in the process monitoring task was significantly higher under   than under   and  , while the performance under   was in average not significantly different than that under  ","Thus, the null hypothesis of H2.1 can be accepted, while it can be rejected for H2.2","In tendency, substantially fewer interactions where performed under   compared to   and  , when a machine had already reached a critical state","In general, the results of   seem to posses a lower variability than those of   and   concerning almost all aspects, as the results typically deviate less and contain less outliers","An explanation for this might be, that participants in   interacted less often  , thus avoiding critical states more often, but also less often  . 
                         
                          reinforces these observations by showing the average  
                          values for the three conditions within 10 time slots","What can be seen is that the participants achieved higher  
                          values in   and   compared to   in the beginning","After a short while, the  
                          values for   increase significantly and are on average higher than those of   for the rest of the experiment","The highest scores throughout the experiment, except for a short period during the middle of the experiment, can be observed for  ","In general, all three conditions show the same ups and downs throughout the experiment","However, towards the end of the experiment, a sudden drop in the  
                          values of   has been observed","Independent of these ups and downs, in tendency the  
                          values of   and   seem to be steadily improving throughout the experiment until the end, while the  
                          values for   remain more or less constant. 
                         
                          shows all user interactions that have been performed by all participants for the conditions  ,   and  ",The different event types are highlighted by different symbols and at the same time different colors as explained in the caption,"The plots for   and   look pretty similar, with events clustered more or less equally along the  -axes, however with larger clusterings at the lower end of the chart, indicating interactions that have been evaluated with an  
                          of zero","In   on the other hand, there are almost no symbols at the bottom, but more towards the higher part, indicating higher  
                          values","Furthermore, the   chart looks more organized and  ","This is likely because of two factors: (a), in   there have been fewer interactions made in general, and (b) the events of the different types seem more grouped together, indicating that an interaction of the same type has more often been executed at around the same time in the experiment, and with a similar  
                          value","This is partly due to the fact that as the different interactions are required at around the same intervals within the experiment (apart from a small random factor), interactions have more often been performed at the ‘optimal time’. 7.3 Questionnaire results 
                          was considered significantly more helpful than  , as was  , thus in this regard the null hypothesis of H3.1 can be rejected, although   was not considered to be significantly more helpful than  ",There were no significant differences in terms of attention switching,"Although there was also no significant difference in terms of estimated process monitoring performance increase between   and  ,   scored significantly higher than both","Concerning H3.2, the null hypothesis cannot be rejected, as participants did not feel significantly more in control during   (8.0±5.5), than during   (5.5±5.0) or   (4.0±4.75)","As there could be no statistically significant differences in terms of perceived exhaustion observed, the null hypothesis of H3.3 cannot be rejected as well","However, several participants stated that it would be better not to use sounds that become continuously stronger, but to combine the problem-sounds with additional warning sounds (e.g. when the input stack is at only 10% etc.)","All participants that commented negatively on   further indicated in their questionnaire responses that they found   to be more intrusive, less pleasing and less euphonious than the median participant (one subject even stated that he would   listening to our sound design for a longer period of time.)","On the other hand, one participant noted after  , that due to the fear of the appalling sounds, one automatically tries to observe the graphical representation more, thus it feels like the sounds distract more from the actual task","Another participant mentioned, that this mode   was very shocking","In the mode with all sounds   it was a bit problematic to differentiate all sounds, but in principle this mode is the most pleasing one, as there is a smooth transition from what you are actually doing, to the monitoring",Verbal comments of two participants further suggest that their performance in   would most likely increase over time and the intrusiveness of the sounds would decrease,Almost all participants stated that they believe that sounds can in general be helpful for process monitoring (9.0±2.0),"The experiment data further suggests, that there is room for improvement concerning the sound selection and mapping strategies (see supplementary material)","Such improvements might further increase both monitoring performance, as well as the participants’ acceptance","Furthermore, there is a very strong correlation (Pearson: 0.900,  ) between the understanding of the mappings from data to sound applied in  , and the believe that the sonification of   is helpful","Not surprisingly, there is also a very strong correlation between the understanding of the   mappings, and the perceived informativeness of the sounds of   
                         ,  . 
                         
                          provides an overview over the hypotheses described in  , and their answers. 8 Conclusion We wanted to find out how well   can direct attention in comparison to   sonification for process monitoring as a secondary task, something that to our best knowledge has not been investigated experimentally before","Furthermore, there already exist approaches that are based on continuous sonification for peripheral monitoring, but most employed sonification techniques left room to believe that they would not be considered pleasing (as they e.g. are based on synthesized sounds that are not very complex), and could lead to fatigue if listened to over a long period of time, e.g. a complete workday",Our approach therefore featured an event-based forest soundscape,We have developed a system that allows its users to compare the effectiveness of different sonifications for process monitoring in a fine-grained manner that extends beyond the typically used reaction times and binary correctness measures,"The main task is simulated by means of simple arithmetic problems that have to be solved, whereas for process monitoring a simplified production process has been simulated which requires several user interactions","An experiment with 18 subjects has been conducted that compared three conditions in a within-subjects design:   (visual only),   (visuals + auditory alerts after reaching a critical state) and   (combining the two former with a continuous, event-based sonification that applies a forest soundscape)","Each of the three experiment parts was conducted for 10 min. 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   8.1 Results The main results are: 
                      In general, as  
                          shows, it seems that a trade off between main task and process monitoring task has to be made","If the main task is of the highest importance while process monitoring can be neglected,   seems to be the mode of choice, as it shows the highest main task scores (but the lowest monitoring performance)","On the other hand, if is not absolutely crucial, that the employees are not to be disturbed during their main task, while process monitoring is also important,   seems to be the best suited mode:   leads to slightly lower main task scores than  , but by far the highest process monitoring results.   on the other hand seems to be unsuitable for most cases, as it has a lower main task performance and at the same time a significantly lower process monitoring effectiveness than  . 8.2 Limitations and future work In general, the process simulation of this experiment was designed to simulate a real-life production process","Such processes are typically ‘bigger’ and more complex, e.g. concerning the number of machines and the number of values that can and should be observed (e.g. temperature measures)","Therefore, in real production scenarios there can exist a significantly higher number of potentially critical states and situations, making it often difficult or nearly impossible to define them all beforehand","Such a more complex scenario would potentially be less feasible with  , as the situations and states that would issue an auditory cue would have to be defined beforehand","A continuous sonification, that does not rely on pre-defined values and states, but instead conveys all interesting events and values might be able to better handle such a scenario","However on the one hand, the sound design of   would have to be adjusted to account for the fact that, when not all critical states and situations can be known beforehand, one can also not map the approaching of such situations to volume","Implications of this might e.g. be that a continuous sonification would have to be designed so aesthetically pleasing, that it is also acceptable when played at ”normal” volumes over a complete work day","On the other hand, in a scenario with dozens of different machines, it would be difficult to distinguish and assign a unique sound to each machine","In such a scenario, the sonifications would either have to base on aggregated, process-level data (such as e.g. so called KPIs – Key Performance Indicators), or the individual machines/data points to be sonified would have to be interactively selectable by the user","With such techniques however, both a mapping of approaching critical states to volume in a continuous sonification, and an auditory-cue based sonification might be suitable again","Thus, for both approaches different mappings would have to be tested and compared for more complex scenarios, that better represent real-life working conditions as well","This could for example be achieved by modeling a more complex process simulation that has more machines, more data attributes requiring attendance, and also more different interaction possibilities","However, an even better way would probably be to install a sonification system in a real-world monitoring context (e.g. in the control room of a factory), and let users actually use the system for a longer period of time (e.g. for several work days or weeks)","With a mix of questionnaires and semi-structured interviews, specifically aspects of long-term usability and intrusiveness could be answered in more detail than it was possible in this study","Furthermore, as requested by two users, further experiments will be conducted that include a condition with more fine-grained warnings that are more pleasing in the beginning, but gradually become more intrusive","At the same time, different – potentially more pleasing –   designs will be tested to find out (a) if they are considered more pleasing and participants could imagine using them for a longer period of time and (b) if they would still enable the same level of effectiveness as   of this study","Such sonification designs could e.g. base on continuous soundscapes that are not based on short, repetitive events (such as in this experiment), but on longer, looped samples, or even   concepts (see  )","Furthermore, the literature suggests that the results of domain experts could differ from those of process monitoring novices, which would have to be evaluated as well","During this study, head tracking data has been collected",The data will be analyzed and presented elsewhere,One hypothesis is that the participants in   had to shift their focus of attention significantly less frequent than under the other conditions,"Although   would have alerted the users in case of critical situations, it can be expected that not in all cases the participants   the system to convey an alarm in time, therefore users possibly checked the visual display to be safe","To conclude, continuous sonifications, like our forest-soundscape sonification, enhance the adequacy of interactions in peripheral process monitoring better than displays based on auditory cues and systems that rely solely on visual means, while they do not significantly affect the main task performance",Appendix A Supplementary data Supplementary data associated with this article can be found in the online version at  ,"Appendix A Supplementary data 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0888613x17302335," 1 Introduction 
                       usually occurs in survey data, and it refers to respondents that under-report the answer to a question, for example due to a perceived social stigma  ","A famous example is maternal smoking during pregnancy, which is a key risk factor for adverse offspring outcomes including preterm birth and low birth weight (LBW)","Like many health behaviours, accurate measurement of smoking habits can be difficult and expensive during pregnancy","For that reason, many studies use   data, e.g",Wright et al.  ,"Given that most smokers know their habit to be harmful, both to themselves and their unborn child, there are strong motivations for women to under-report or deny their smoking status  ","As such, the frequency of smokers in a sample is expected to be significantly lower than would be expected according to expert knowledge, derived for example from blood test result",Gorber et al.   presented a comprehensive analysis of the literature and compared the prevalence estimates of smoking based on self-reported data against the prevalence estimates based on directly measured smoking biomarkers,"According to this analysis, self-reported smoking is generally under-reported in such a way that the true smoking figures may be underestimated by up to 47%",Estimating the association between an   (UR) variable and another will be biased in a manner that is specific to the degree/pattern of UR,"Thus, any policy decisions made on the basis of such a biased result will be questionable","For example, government policies on tobacco control, e.g.  , maybe ill-formed if they do not take into account UR","Maternal smoking and alcohol consumption are our focus for this paper, but there are many important health applications where   for UR are needed – e.g","One method to correct UR bias is to spend time and resources to manually identify individuals that are likely to have misreported, and ignore/correct their testimony, i.e. identify smokers by performing cotinine blood tests","Unfortunately, in many applications it is impossible to completely correct the misreported cases","For example, Gorber et al.   present some possible flaws of the biochemical markers that identify smoking","As an alternative to this, authors in medical statistics treat it as a problem of  , and combine data with a prior belief of the pattern of misclassification","They use this prior knowledge to derive corrected estimators for the log-odds ratio  , and the relative-risk  , or to suggest ways for performing tests of independence  ","These solutions suffer from a number of weaknesses, which are addressed in this paper","For testing independence, they do not control both types of error (false positive/false negative)","For estimating effect sizes, the suggested solutions are naturally only applied to estimate the correlation between a binary UR variable and a binary target variable – multi-class target variables with more than two categories are handled via a one-vs-one or one-vs-all strategy","Furthermore, ranking of variables in relation to a target – a common need in feature selection and other machine learning tasks – is not straightforward","Our strategy to overcome these limitations is to provide an information theoretic insight for the under-reporting problem, by disentangling three intimately related activities:  ,  , and   of features","Our main goal is to derive corrected estimators for the   (MI), a measure of effect size widely used in machine learning applications with several interesting properties  ","To achieve our goal we reinterpret the challenge not as dealing with misclassification and biased data, but as a problem of  , and particularly learning from positive and unlabelled data  ","By this interpretation, we present solutions using a graphical representation called   or  graph  , which is a tool to naturally incorporate a prior belief over the misreporting at the population (or appropriate sub-demographic) level","Furthermore, with our work, we show how to correct MI for under-reporting by examining independence properties observable via the  graph representation","In this paper, we present the following novel contributions 
                       in relation to UR variables:  
                       
                       presents two different methods for information theoretic feature ranking in the presence of UR variables, by using our suggested estimators","Section   presents Corrected-MIM, a univariate approach that provides rankings and captures only the relevance, while Section   presents Corrected-mRMR, a multivariate method that captures both the relevance and redundancy between the features","All the above contributions are novel, with the exceptions of Sections   and  , which have been published in a conference paper  ","Furthermore, we provide further experimental results in two applications","Firstly,   
                       derives rankings of risk factors related to low birth weight infants using a case study of 13,776 births in northern England, where we demonstrate some significant false conclusions that might be drawn when ranking variables without the correction factors","Finally,   
                       presents a machine learning application where we derive rankings of features when training/test distributions differ. 2 Background material To the best of our knowledge, our work is the first that tackles the problem of estimating MI in under-reporting scenarios","In classic statistics there are some works that estimate other types of effect sizes (i.e. odds/risk ratios, limited to binary data) and we review them in Section  ",Section   shows how the under-reported can be phrased as a missing data problem,"Finally, Sections  ,   and   give the background on testing, estimation and ranking using information theoretic measures. 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   2.1 Under-reporting as a misclassification bias problem We assume that we have two random variables   and  , representing a scenario where   is likely to be UR","In this case, we cannot observe the true value of  , but instead receive observations from a proxy variable  ",In the notation below we use lower case letters ( ) to denote a realisation from these variables,"In our example of smoking during pregnancy,  , is a binary indicator of LBW,   is whether the mother smoked during pregnancy (1 for smoking and 0 for not smoking), and   is whether the mother   that she smoked in pregnancy (1 for reported smoking and 0 for not smoking)","While in our running example   is binary, the techniques presented in this work are also applied to categorical data with more than two levels  ","A classical solution to the under-reporting problem is to consider it as a   
                         ","Following Greenland   terminology, for an under-reported variable, the   is  , while the   is  ","Here, the specificity is the probability that a non-smoker would tell the truth (equal to 1 in this setting) and the sensitivity is the probability that a smoker would tell the truth (in our setting strictly <1, if it is equal to 1 the variable is not UR)","As presented, this is the simplest scenario – referred to as   – that is, the probabilities do not vary with respect to  ","The more complex case is when the sensitivity depends on  , that is  , known as   
                         ","In this work, we will focus on the non-differential UR scenario, and leave the differential as a future work, outlined in Section  ","The non-differential assumption is reasonable to cohort studies (such as the Born in Bradford project   presented in Section  ), since as Greenland   states: “... studies that collect exposure data before the outcome occurs (such as most cohort studies) provide settings for reasonable employment of the non-differential assumption.” Estimating the strength of association between variables, using this misclassification approach, is a well explored challenge in epidemiology","For example, Chu et al.   derive corrected estimators for the log-odds ratio, while Rahardja and Young   did it for the relative-risk","To derive these corrections, knowledge of the specificities/sensitivities, or in other words knowledge of the misclassification rates, is needed","This can be derived in different ways, such as   or domain  ","A different way of estimating these effect sizes is to use a model to impute the values of the possibly misclassified examples, for example Edwards et al.   present a way of using multiple imputations to estimate log-odds ratios","With our work we derive corrections for the MI, by incorporating simple forms of prior knowledge",A further challenge other than estimation is to conduct a valid independence test,"Testing independence under misclassification is a very old problem: Mote and Anderson   showed that the usual  -test of independence is valid when the misclassification is non-differential, but statistical power (i.e. true positive rate) is reduced","We found only one study  , which suggested a correction factor that captures the amount of power loss in the  -test","In that work a binomial difference-of-proportions test is analysed and used to suggest properties of the  -test, using as an argument the equivalence of these two tests under the null hypothesis","With our work we derive a correction factor, which estimates the power loss more accurately than the correction factor suggested by Bross   for the same quality of prior knowledge. 2.2 Under-reporting as a missing data problem A different way to phrase the under-reporting problem is by connecting it with the equivalent problem from the missing data literature",The first step is to consider the under-reporting bias as a   (PU) problem  ,"That is, a semi-supervised binary classification problem where we have a set of positive examples and a separate set of unlabelled examples, which can be either positive or negative","The positive examples can be seen as the reported “smoking” cases ( ), while the unlabelled can be seen as the reported “non-smoking” cases ( )","Furthermore, from the missing data literature we borrow a graphical representation which will help us to make apparent the assumptions behind the under-reporting mechanism","Mohan et al.   introduced a formalism for graphical modelling in the presence of missing data, known as   or  -graphs","While in the literature of misclassification bias there is a different graphical representation  , our modification of the  -graphs provides more useful information, by capturing both the data generation model and the causal mechanisms responsible for the misclassification process. 
                         
                         (a) shows the simplest case of   UR. 
                          A solid node indicates a fully observed variable, whilst dashed nodes represent unobserved variables","Associated with every unobservable variable   there are two additional nodes: firstly  , which controls whether a value from   is correctly reported ( ) or not ( ), and secondly, the proxy variable   which is fully observed","The major difference between missingness graphs used by Mohan et al.   and those here is that the mechanism   is not observed, and for that reason we must incorporate prior knowledge over the sensitivity   and specificity  ",The  -graph representation allows us to read off independence properties such as:   — which corresponds to the   assumption in the positive and unlabelled literature  .  (b) shows a more complex situation where we have two UR variables,The current paper shows (in its simplest case) how to recover the value   from   by deriving a correction based on prior belief over the mechanism  ,"And how to test the independence between   and  , by using the observed variables   and  . 2.3 Background on testing independence The most usual way to decide independence between categorical variables is through the  -test, calculated from sample data,  ",We note that this is related to the   (SMI)   as so:  where   is the maximum likelihood estimate of the squared-loss mutual information,"Under the null hypothesis that   and   are statistically independent, the  -statistic is asymptotically  -distributed, with   degrees of freedom  ","To decide independence between   and  , for a given sample of data, we calculate the statistic and check whether it exceeds the critical value defined by  , where   is the user-specified significance level of the test and   is the inverse cumulative distribution function of the  -distribution with   degrees of freedom","If the critical value is not exceeded, we   to reject the null hypothesis of independence","The user specified significance level defines the probability of  , which is the probability that the test will falsely reject the null hypothesis","To calculate the probability that the test will falsely reject the null hypothesis, or probability of  
                         
                         , we should perform a power analysis  ",To do this we need to know the sampling distribution of the test statistic under the alternative hypothesis,"The  -statistic asymptotically follows a non-central   distribution under the alternative hypothesis, with the same degrees of freedom   and with non-centrality parameter   
                         ","Thus, the probability of a type II error depends also on the sample size   and on the population value of the SMI  ","Given this context, a very important tool of a priori power analysis is   
                         ","In this prospective procedure we specify the significance level of the test (e.g.  ), the desired power (e.g.   or the probability of false negative to be 0.01) and the population value of the desired effect size described in terms of   — from this we determine the minimum number of examples required to detect that effect with the given probabilities of error",Section   presents how we can use the above methodology when we have UR features. 2.4 Background on estimating mutual information In practical applications we want to explore relationships between random variables,"Just giving a yes/no answer through a hypothesis test may not be of much interest, and estimating the size of the effect gives more useful information, for example, how strongly smoking is correlated with low birth weight",In machine learning one of the main ways of measuring the strength of this association is by estimating Shannon's mutual information (MI)  ,"The maximum likelihood (ML) estimate of the MI is:  Firstly, it is a non-negative quantity which takes its minimum zero value when the random variables are independent","Furthermore, it can be associated with both upper and lower bounds on the Bayes error  ","Brown et al.   present an extensive discussion of this in the context of feature selection, including various heuristics which provide approximations for high dimensional data, resulting in a unifying theoretical framework derived from a simple probabilistic model","Together with point estimates, it is a good practice to give an   estimate, a range of possible values that the mutual information can take","Asymptotic distribution theory has a set of tools to derive the sample distribution of the ML-MI estimator and the following theorem presents this known result  . 
                      Proof sketch: This result can be proved by using delta methods  ","While the asymptotic variance here depends on the population values  , in practice, for interval estimation we replace them by their sample values  ",This standard procedure   is followed for all the sampling distributions that we present in this work,"Section   presents how we can estimate mutual information in UR scenarios. 2.5 Background on information theoretic feature ranking and selection In most real world problems we have more than one feature, i.e. we observe a sample dataset  , where the feature vector   is a  -dimensional realisation of the joint random variable  ","With a slight abuse of notation, in the rest of this section, we interchange the symbol for a set of variables and for their joint random variable","In this scenario, it is also useful to order the features according to their relationship with the target variable, a procedure known as  ","Feature rankings provide very useful information, and applications of this principle range from model selection to decision tree construction","There are two main categories of feature rankings  : univariate methods, which consider only the individual relevance of each feature and multivariate methods, which also take into account dependencies between features","In information theoretic feature selection, firstly we rank the features according to a score measure and then select the ones that contain most of the useful information (i.e. higher score)","By ranking the features with respect to their mutual information with the target variable, we derive a ranking that takes into account only the   with the target",Choosing the features according to this ranking corresponds to the univariate   (MIM) criterion  ; where the score of each feature   is given by:  This approach does not take into account the   between the features,"More advanced multivariate techniques take into account both relevance and redundancy,   having to compute very high dimensional distributions","For example, a popular multivariate criterion is the minimal Redundancy Maximal Relevance (mRMR), which ranks the features according to the score  :  where   is the set of the features already selected",The first term of the   captures the relevance and the second the redundancy,"Section   derives extensions of MIM and mRMR that handle UR features. 3 Testing independence in under-reported scenarios To answer the question of whether two variables are independent or not we need a hypothesis testing procedure, where we can have a control over the two probabilities of error: false positive (Type I error) and false negative (Type II error)",Testing independence in under-reported scenarios is not straightforward and this section explores the dynamics of this test,"In non-differential under-reporting, it is   to test independence by using the under-reported variable   
                      ",This can be easily proved since we have:  ,This can be also read directly from the  -graph in  (a): when there is no direct arc between   and   there is no path that connects   with   and vice versa,Quantifying the loss of power in the  -test is more challenging,Now we will show how to derive a correction factor that captures this loss effectively,"We first need to write the non-centrality parameter of the under-reported test   in terms of the same parameter of the unobservable test  .  
                   Proof can be found in Appendix  ",The proof builds upon the fact that the following relationship holds between the population values of the MI terms:  ,"Since  , the under-reported test will be always less powerful than the unobserved correctly reported test","Furthermore, by having a correction factor, we can quantify the amount of the power loss by incorporating knowledge over  ","As a result, a  -test between   and   with   examples will have the same power as a  -test between the under-reported   and   with   examples, referred to as  ","Bross   suggested a different approach to derive correction factor in non-differential misclassification, using as a starting point the equivalence, under the null hypothesis, between  -test and the difference-of-proportions-test","This correction factor under our notation in the UR scenario is written as (the analytical derivation can be found in Appendix  ): 
                   By comparing the equations for the two correction factors,   and  , we can derive the following proposition.  
                   The proof of this proposition is straightforward",In the UR scenarios it holds  ,"Thus the relationship between the effect sample sizes is:  , which means that using our correction factor   results to higher sample size, and as a result more powerful test","To verify experimentally our theoretical results we generate synthetic random variables   and   with different degrees of dependency and we explore the false positive/negative rates through the graphs presented in  
                      ","In the  -axis we have different effect sizes in terms of the squared loss mutual information between   and  , while in the  -axis we have the acceptance rate of the null hypothesis   (over 2,000 repeats)","The  -intercept represents  , and should be close to   in order for the tests to be valid, while elsewhere the plots indicate the  . 
                       verifies experimentally the correctness of our correction factor   (verification of  ) and it shows the superiority against   (verification of  )","Having a known correction factor is very useful for power analysis activities, such as sample size determination  ","Furthermore, it shows that testing using   instead of   is a valid approach, since all lines have the same intercept at  , and thus the tests have the same false positive rate","Section   shows how our results on UR test of independence can be useful in analysing a clinical dataset. 4 Estimating mutual information in under-reported scenarios In many applications, just giving an informed answer through a hypothesis test may not be of much interest, while estimating the size of the effect gives more useful information","In this section we will present different ways to estimate mutual information, despite under-reporting","The   method to completely correct UR is to spend resources to identify the individuals that have misreported, and correct their testimony","For example, it could be done by performing cotinine blood tests to all women that reported non-smoking ( )","This approach is expensive, and still it may not be possible to identify the individuals that have misreported  ","On the other hand, the simplest way to estimate mutual information in under-reported scenarios is to follow a   approach and just use the observed data","Unfortunately, this estimator,  , is asymptotically biased for estimating  ","This can be easily proved, since under the model of  (a) the following strict inequality holds 
                      :  ",Another way to estimate mutual information is by trying to “predict” the real values of the misclassified examples using some prediction model,"Then, impute new values for these examples, and finally, estimate MI using the imputed data","This is similar to solving the missing data problem by   
                      ",In our running example this means imputing the actual values of the women who reported not smoking ( ),"To do so we need to build a model to derive the Bayesian posterior distribution 
                       
                      , and we use this model to impute the values for the examples with  ","Then, we can use these imputed values to derive point and interval estimates of the MI using the expressions presented in Section  ",One limitation of single-imputation is that estimating standard error using conventional methods – such as eq.   – does not take into account the fact that some of the data were imputed  ,"One solution to this problem is to perform multiple-imputations and use improved ways of estimating the standard errors, such as Rubin's rule presented in  ","Multiple-imputation has some limitations; for example, it is computationally expensive, while, in the case of estimating 
                       MI, there are no guarantees that the confidence intervals derived by Rubin's rule will have the coverage defined by the nominal (user specified) level",For more details on the strengths and weaknesses of multiple-imputation we refer to Rubin  ,"In the next section we present a corrected estimator for the mutual information that takes into account the under-reporting and overcomes the above limitations: (1) it is consistent, unlike the naive approach, (2) it produces valid interval estimates, unlike the simple-imputation, and (3) it is computational-efficient, unlike multiple-imputations. 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   4.1 Correcting for under-reporting the mutual information that captures relevance To estimate mutual information in the under-reported scenario, we need to come up with a way to estimate marginal and joint/conditional probabilities, despite the restrictions of the problem","While we can estimate the marginal   from all data, the conditionals are more challenging","For example, the conditional   is inaccessible, as we do not have access to the full set of the examples with  , i.e. we do not know the identities of all smokers, but only those that self-reported it ( )","Because of the event based independence assumption   it holds that  
                          To find the other conditional   we use a simple trick first introduced by Denis et al.   in the context of positive and unlabelled data","By using   we can write the marginal as   and solving for  :  Finally, since we do not have access to the marginal distribution  , and since it cannot be estimated without modelling assumptions, we incorporate prior knowledge 
                          as a parameter  , provided by a user's belief over the true prevalence  ",Incorporating prior knowledge over the true prevalence is a widely used approach in the positive and unlabelled literature  ,"By assuming perfect knowledge over the prevalence   and using only the observed variables   and   we can estimate   using the following corrected estimator.  
                      The following Lemma proves the consistency of the estimator.  
                      To prove that the estimator is consistent is straightforward, since when we have perfect prior knowledge  , by using   and   it holds that  ","Only proving the consistency of the corrected estimator is not useful, and we need to capture also the variance that it has in the finite sample size","We do so through the following theorem. 
                      Proof can be found in Appendix  . 
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      4.1.1 Experiments with synthetic data and perfect prior knowledge As a “sanity check” for our theoretical results we generated synthetic random variables   and   with different degrees of dependency","To create the data, firstly we generate the values of  , by taking   samples from a Bernoulli distribution with parameter  ","Then, we randomly choose the parameters   that guarantee the desired degree of dependency, expressed in terms of  , and we use these parameters to sample the values of  ",To create the under-reported variable   we sample with   the examples with  ,"We estimate mutual information using five different methods:  For the imputation-based approaches, we imputed the potentially misclassified examples through the following posterior, which can be naturally derived by the model of  (a):  As we mentioned, we use perfect prior knowledge over  , while the rest of the parameters are estimated through ML from the observed data","To get a fair comparison between the last three methods, we used the same modelling assumptions and   is assumed to be known and equal to  . 
                            
                             compares the five methods in terms of their mean squared error","The three methods that take into account the under-reporting (single/multiple imputation and our corrected estimator) outperform the naive estimator, which is not consistent","As the sample size/sensitivity increases, all of these three approaches tend to behave in a similar way to the ideal estimator","Our corrected estimator outperforms the imputation-based approaches, especially in small sample sizes and small levels of sensitivity – which are the most challenging situations","Interestingly, our method clearly outperforms methods with the same complexity (no correction and simple imputation). 
                            
                             verifies that the suggested standard error in   is correct, and that our method is a valid way to derive interval estimates, similar to those derived using the ideal estimator","In this figure we estimate the proportion of times that the 90% confidence intervals, derived by using different standard errors for the different methods, contain the true value of the mutual information  ","Since the estimated coverage probability for the ideal and our proposed method are at the nominal (user specified) level of 90%, we can conclude that only these methods produce accurate interval estimates. 4.1.2 Experiments with synthetic data and uncertain prior knowledge Perfect prior knowledge, i.e.  , will not always be available",Therefore it is important to explore ways to deal with uncertain knowledge and examine the behaviour with incorrect priors – results are presented below for an artificial scenario where we can exert control over the “quality” of prior knowledge,"Let us assume that non-smoker births are drawn from a normal distribution with   g and   g, while weight of smoker births are drawn from a normal distribution with   g and   g","Birthweight was considered to be “low”,  , if the weight was <2500 g  ","We assume that in a cohort of   pregnant mothers, 30% are smokers, so  ","However, only half of the mothers on average would admit to this, so  ","In a typical draw from this simulation, the mutual information is estimated with an under-reported variable","However, after using our corrected estimator and by incorporating the prior knowledge that the   variable is non-differential under-reported, the estimated mutual information increases by a factor of three ( 
                            (a))",One way to handle uncertain prior knowledge is by performing a   as  (a) shows,"To do so we plot the interval estimates for the corrected MI, calculated by eq.  , for different values of our belief over the probability of smoking ( )",As we observe the point estimate for   (perfect knowledge) is the same with the true (ideal) value of the MI,"A different way to handle uncertainty is through a  , where we represent uncertainty over   as a probability distribution, sample from this distribution many times, and estimate the corrected MI for each value","For example, in  (b) we model   as a generalised Beta distribution (bounded between a minimum and a maximum value) and we explore the resultant uncertainty in the estimate of the corrected mutual information","As we observe, the true value of the MI is very close to the average over the simulations. 4.2 Correcting for under-reporting the mutual information that captures redundancy Using the results of the previous section, we can measure redundancy terms when only one of the features is under-reported, i.e.  , but we cannot measure terms when both of the features are under-reported, i.e.   in  (b)","To do so we will use the following estimator.  
                      The following lemma proves that with perfect prior knowledge this estimator is consistent.  
                      Proof can be found in Appendix  ",As a “sanity check” for our theoretical results we generated synthetic random variables   and   with different degrees of dependency,To create the under-reported variables   and   we sample with   the examples with   and with   the examples with  ,"We estimate mutual information using three different methods:  We will not compare against imputation based methods, since, in this scenario, we do not have any correct reported variable to build the imputation model. 
                         
                          compares the three methods in terms of their mean squared error","As the sample size/sensitivity increases, our suggested method tends to behave in a similar way to the ideal estimator","Our corrected estimator clearly outperforms the naive method (no correction). 5 Ranking features in under-reported scenarios By using our theoretical results on estimating mutual information terms in under-reported scenarios, we can derive two different algorithms for producing feature rankings",Before that we will introduce some extra notation,We can assume that the feature vector   consists of two parts:  ,"Where   contains features that are correctly reported, while   the ones that are under-reported. 
                      
                      
                      
                   
                      
                      
                      
                   5.1 Rankings that capture only relevance Using our findings from Section   we suggest  , which is an extension of MIM (Section  ), and it is suitable when we have UR features","The score of each feature   is estimated by:  where 
                      5.2 Rankings that capture both relevance and redundancy To derive feature rankings that capture   relevance and redundancy, we need to combine our findings from Sections   and  , and the mRMR paradigm presented in Section  ","Our suggested method,  , ranks the features according to the score:  where   is the set of the features already selected, the term that captures relevance   is given by eq.   and redundancy   by:  In the following two sections we present two applications of our theoretical results. 6 Application in ranking risk factors for low birth weight infants In this section we present a real-world application of our results — ranking the risks factors that may lead to adverse birth outcomes derived from a large real-world dataset","To describe the usefulness of our theoretical findings we will use data from a prospective birth cohort, the Born in Bradford (BiB) study","BiB is a longitudinal multi-ethnic birth cohort study aiming to examine the impact of environmental, psychological and genetic factors on maternal and child health and well-being  ",Bradford is a city in northern England with high levels of socio-economic deprivation and ethnic diversity,"The full BiB cohort recruited 12,453 women with 13,776 pregnancies between 2007 and 2010, and the cohort is broadly characteristic of the city's maternal population in terms of age, deprivation and ethnicity",Ethics approval for the study was granted by Bradford Research Ethics Committee (Ref. 07/H1302/112),"In our analyses we focus on term births only, and we excluded ethnic groups (such as Pakistani mothers) that are much less likely to smoke and drink alcohol than the rest of the cohort  ","As a result, the number of suitable pregnancies reduced to 5,457",We show how to rank several risk factors according to their association with LBW,"The risk factors that we focus on are the correctly-reported   variables: ethnicity   (3 levels), age   (3 levels), Body Mass Index (BMI)   (4 levels), index of multiple deprivation   (5 levels), gestational diabetes   (binary), taken vitamins   (binary), and the following binary UR variables: any smoking  , passive smoking   and alcohol   consumption during pregnancy","Let us assume that 1/3 of the overall women under-report these three variables, and we assume non-differential UR","In  
                      (a) we observe the MIM ranking by using the MI of the observed covariates and the target variable (LBW)","Then, to correct the three UR variables, we use prior knowledge and our corrected estimators presented in Sect.  , and we derive the Corrected-MIM ranking of  (b)","Finally, to take into account the correlations between the risk-factors we derive the Corrected-mRMR ranking of  (c)","From these three rankings we can arrive to the following interesting conclusions: 
                   The differences between the three rankings illustrate the importance of having techniques that are able to produce estimates that correct under-reporting and take into account the correlation between the risk factors",We have demonstrated that failure to correct for potential under-reporting of exposure will lead to biased estimates of the ranking of the relative effect between variables and outcomes,"To appropriately validate our statistical findings access to the true values of  ,   and   will be necessary, i.e. through a blood test","Unfortunately, this information was not available","For that reason, in the following section we present the merits of our analysis in a machine learning application using datasets for which we have access to the ground truth. 7 Application in feature ranking in under-reported scenarios In this section we present a machine learning application of our findings: producing feature rankings when the training/test distributions differ, and particularly, when the features are non-differentially under-reported. 
                       We used four categorical UCI 
                       datasets (Splice, Chess, Mushroom and Congress) and two artificial lung cancer 
                       datasets (LUCAS0 and LUCAP0) from the Causation and Prediction Challenge  ","Splice is a 3-class classification problem, while the rest are binary","Categorical attributes are expanded into several binary attributes.  
                       shows the characteristics of each dataset","We assumed that the features of these datasets are correctly reported, and to generate UR datasets we randomly under-reported the original features. 
                      
                      
                      
                      
                   
                      
                      
                      
                   7.1 Deriving MIM ranking In this section, we will compare the rankings derived by using different MIM methods.  
                          compares the similarities between the MIM rankings derived by UR methods and the ideal ranking – the MIM ranking that we would have if we had access to the actual values of the features",To check the similarity between the rankings we use Spearman's   correlation coefficient  ,"The range of values that this coefficient takes is  , where 1 means that the two rankings are identical, 0 means that there is no correlation between them, while −1 means that the two rankings are inverse","We compare the three UR methods with the same complexity: no correction, simple imputation and our correction (for a fair comparison we used perfect prior knowledge for the last two approaches).   shows that our suggested approach,  , outperforms the other approaches in all the settings",Now we will compare the different methods in terms of their misclassification error,"As a classification model we will use a  -nearest neighbour ( ) classifier, which makes few assumptions about the data and treats all features equally, a desirable property when we compare different feature selection/ranking approaches  .  
                          compares the different UR approaches in terms of their misclassification error",In the Splice and Chess datasets our approach outperforms the others and achieves similar performance as using the ideal estimator,"In the rest of the datasets, all methods have similar performance, and always our method performs similarly with the ideal. 7.2 Deriving mRMR ranking Now we will compare the rankings derived by using different mRMR methods.  
                          compares the similarities between the mRMR rankings derived by UR methods and the ideal ranking","Our suggested approach,  , outperforms the naive not-corrected approach in all the settings.  
                          compares the different approaches in terms of their misclassification error",In the Mushroom and Chess dataset our approach outperforms the naive method and achieves similar performance as using the ideal estimator,"In the rest of the datasets, all methods have similar performance, and always our method performs similarly with the ideal. 8 Conclusions and future work In this work we have provided an information theoretic solution to the problem of under-reported variables","Initially, by reinterpreting under-reporting as a missing data problem, we presented how we can use the tool of   
                       for providing graphical representations of the different under-reported scenarios","Then, by using these representations, we explored valid ways to test independence, and we derived a correction factor that quantifies the power loss of the  -test","Furthermore, by incorporating simple prior knowledge, we derived ways for estimating mutual information quantities that capture both relevance and redundancy","Our suggested estimators are computationally efficient, while they have similar error with more complex imputation based approaches","Additionally, for the mutual information that captures the relevancy, we derived confidence intervals that achieve the ideal coverage","Using our suggested estimators we proposed two methods for feature ranking: Corrected-MIM, which captures only relevance, and Corrected-mRMR, which captures both relevance and redundancy",Our theoretical results are supported through experiments with synthetic data,"Finally, we showed how we can use our findings in a real-world health care application (ranking the risk factors that may lead to low birth weight) and in a machine learning application (feature ranking when training/testing distributions differ)","In many practical applications we have misreporting mechanisms that are correlated, for example by having a latent variable which is a parent of both missingness mechanisms   and   in  (b)",One limitation of our work is that it assumes independent misreporting mechanisms,Thus an interesting future direction is to explore ways of estimating redundancy terms without making any independence assumption,"Furthermore, providing ways for testing, estimation and ranking in   under-reporting (i.e. when there is a direct arc between the missingness mechanism   and the variable   in  (a)) seems challenging","Lastly, finding ways to consistently estimate   mutual information terms will provide us with algorithms for structure learning or Markov blanket discovery in UR scenarios  ","In our earlier work   we suggested a way for correcting conditional mutual information when we condition on correctly reported variables, while a promising future direction is to derive a corrected estimator, when we condition on under-reported variables","We believe our results are highly applicable in a wide variety of machine learning applications, when we face the problem of under-reporting","Estimating mutual information, testing independence, ranking sets of features according to their relevance/redundancy, learning Bayesian network structures and sample size determination for experimental design are some – but not all – of the possible applications","Appendix A Proofs of the theorems 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                   A.1 Proof of  
                      The non-centrality parameter of the under-reported   test is equal to  , while the non-centrality parameter of the unobserved test is equal to  ",In order to derive a relationship between the parameters we should derive a relationship between the two squared loss mutual information terms,"We will start by re-expressing   as follows 
                      Following exactly the same procedure for the   we get 
                      Under the non-differential assumption because of the event based independence assumption   it holds that  , so from   and   we derive that:  And by multiplying both sides with 2  we can derive the relationship between the non-centrality parameters:  with   □ A.2 Derivation of the correction suggested by Bross  
                      Bross   suggests that in order to calculate the power of the  -test we should use an effective sample size of   times the actual sample size, where the correction factor   is given in eq. (1.02) of  ",Bross derived this correction factor by starting from the equivalence under the null hypothesis between the  -test and the different-of-proportions-test,"Using our notation, this correction factor can be written as:  In our UR setting we have  ",By substituting   in eq.   we get:  The conditional probability can be written as:  ,"Because of the UR constraint, details in Section  , whenever an example has   then it also holds that  ","This means that:  , and as a result the conditional probability takes the following form:  ","By substituting this expression in eq.   we get:  The last expression for   is the one presented in Section  .  □ A.3 Proof of  
                      To derive the asymptotic distribution of   we will use the delta method  , which we formally present in the following lemma.  
                      The expression of the corrected estimator is:  Since in the expression of   we have the maximum likelihood estimates for the probabilities  ,   the first step is to calculate the partial derivatives of these quantities with respect to the parameters of this model   and  :  where   is the Kronecker delta, which takes the value of 1 if   and 0 otherwise",By using the above partial derivatives we have the following results:  So by using delta method the asymptotic variance of the estimator equals  where   and   are calculated earlier and are functions of  ,"Furthermore, when   it holds that   and so the estimator   is asymptotically normally distributed around  .  □ A.4 Proof of  
                      Using   we can re-write the mutual information   as:  When both the random variables are non-differentially under-reported it holds  and  By substituting we get:   □ Appendix B Supplementary material Supplementary material related to this article can be found online at  ","Appendix B Supplementary material The following is the Supplementary material related to this article. 
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s1071581916000021, 1 Introduction The ability to accurately link a physical person to their activities conducted in the digital realm is an increasing challenge for our intelligence and law enforcement agencies,"The present work was inspired through collaboration with the large multidisciplinary project Super Identity 
                       whose overarching aim is to model links between a wide range of physical/digital identity measures in order to build new identity attribution techniques",The theme of this paper was inspired by a need to address a specific real-world problem: how to identify criminals who conduct their activities using temporary burner mobile phones that are unregistered to them by name,Our main objective is to establish whether touch interaction dynamics can be leveraged to provide usable evidence as to the physical characteristics of their creator,"In this paper, we present initial findings from an exploration into whether it is feasible to infer a single specific physical characteristic of a person – specifically the length of their thumb – from the way in which they perform a common smartphone interaction gesture, the ‘swipe’","If such a link could be made, we suggest that by following a route through known proportional relationships between that digit length and other human measurements, we would conceivably be able to infer various other physical characteristics of the person who created the gestures","To provide an example, to possess a long thumb suggests a longer hand length, a longer hand length suggests a longer forearm length and a longer forearm length suggests a taller standing height. 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   1.1 The use of anthropometrics as a predictive route between segment lengths in the human body As classically illustrated by Da Vinci in ‘Vitruvian Man’ ( 
                         ), studies of anthropometrics (e.g.  ) have shown that numerous segment lengths of the human body follow a common proportionality, approximating the ‘golden ratio’ of 1:1.618","This ability to predict the length of one body segment length (e.g. a persons height) from another (e.g. the length of their forearm) has found utility in multiple domains, most notably forensic science and ergonomics","Such is the importance of access to accurate anthropometrics data for the purposes of human tool and clothing design, large corpura of detailed body measurement data have been collected over the last century",Larger datasets include works conducted by the U.S,Military (e.g.  ) and NASA (e.g.  ),The anthropometric data provided by ‘Kodak׳s Ergonomic Design for People at Work’ ( ) is a frequently cited classic resource within the field of ergonomics and product design,"Analysis of the proportional relationships between various body segment lengths continues to produce the support to forensic and medical science, often to assess their suitability as a usable predictive tool","The ability to predict the length of a missing body segment (e.g. a femur bone) is for example useful in forensic anthropology, where the need to perform reconstructions of bodies from incomplete skeletal remains is a frequently encountered task",Results of a number of studies that have specifically considered the hand offer promising evidence that a predictive route from thumb length to larger body segment lengths is available,"The close relationship between hand length measurements and standing height has been reported by  ,  ,   among others, while similar analyses examining the relationship among hand length, foot length and standing height can be found in  . 1.2 Capturing thumb length to serve as a route to hand length and beyond When held one-handed, interacting with the touchscreen of a smartphone is naturally restricted to the use of the thumb only","In this paper, we hypothesise that when a smartphone is used in this way, evidence as to the likely length of its user׳s thumb is revealed by dynamics of their gestural touch interactions (such as their length and drawing speed)","If this were indeed the case, it would provide a useful link between what would be considered a behavioural biometric, to a soft physical biometric for that individual","In the present study, we obtained an accurate measure of the length of each of our participants’ thumb, before then capturing a number of their touchscreen   gestures using an instrumented smartphone",Swipe gestures were drawn and captured in multiple directions,"To maximise any potential correspondence between thumb length and the resultant characteristics of swipes created using that thumb, we restricting our participants’ interaction with the device to one-handed use only, with the device held in portrait orientation",The swipe gesture (sometimes referred to as   or  ) involves placing a finger or thumb on the screen and – while maintaining contact with the screen – quickly swiping it in the desired direction,"Akin to flicking through the pages of a book, the swipe gesture is frequently used to support content navigation, such as navigating a photo album or paging through a list","The swipe gesture was selected for this investigation over other touch gestures (e.g.  , or multitouch gestures such as  ) primarily because of its high frequency of use, but also for its high range of extractable features across a number of dimensions, including length, drawing speed and touch pressure. 1.3 Related work Biometrics are measures of human characteristics and traits",Biometrics can be physical or behavioural based,"Physical biometrics relate to  , and can be subclassed into either   or   depending on the degree to which they are unique to a particular group of individuals or a specific individual respectively",An example of a hard physical biometric would be the unique pattern associated with a person׳s fingerprint or their DNA profile,An example of a soft biometric would be the colour of a person׳s hair or iris,A behavioural biometric relates to  ; the identification of an individual through some unique aspect of their behaviour,"Like physical biometrics, behavioural biometrics can also take hard and soft forms","Human–computer interaction research has explored potential sources of behavioural biometric material from a variety of angles, including the way in which individuals type on a keyboard (  e.g.  ), how they use their mouse ( ) and even the way that they walk ( )","In recent years, the interactive touchscreen has become the dominant input mechanism for a range of mobile computing devices, most notably smartphones and tablets","The ubiquity of these devices, along with rapid advances in the sensitivity and accuracy of touchscreen technology has attracted renewed interest from researchers for their potential as a source of new behavioural biometrics",Of particular interest is whether the dynamics of a given person׳s touch interaction style are unique enough to provide new user authentication mechanism that are both secure and usable ( ),"This effort is a response to the increasing concerns of both security researchers including   and   and end-users ( ) that the security/usability tradeoff of existing user authentication schemes for these new mobile devices is not optimal, particularly when taking into account the broad ways in which these devices are used","For example, a common setup of these devices is to rely upon a single active authentication scheme (such as a pin-lock or password) that is entered every time the device is activated for use","A 4-digit pin lock – while easier to enter quickly – offers little actual security value compared to most passwords ( ), yet passwords – and particularly ‘strong’ passwords of sufficient length and complexity – are hamstrung by the difficulty of their swift and accurate entry on small ‘virtual’ keyboards ( )","The need for the development of effective user authentication mechanisms that offer actual security value while also addressing the needs of users and how these mobile devices are used is an issue of increasing significance, particularly given the value of personal data that is stored or accessible via these devices","However, and as a number of researchers have noted (e.g.  ) this effort must take a two-pronged but much more user-centric approach: users must be made aware of the importance of protecting their devices, but they must also be provided with the means of doing so that is appropriate for the ways in which these devices are used","By way of example, as these devices tend to be used frequently on the move and/or in short bursts, lengthy authentication procedures that require concentration or dexterity (e.g. one-handed virtual keyboard entry) are inappropriate",A consequence of authentication procedure design that fails to take into account the end user׳s needs is an entirely understandable but worrisome tendency for users to turn user authentication measures off ( ),"Given the increased interactive affordances of touchscreens relative to physical keypads, many novel graphically based active authentication systems have been proposed ( ) as usable and secure alternatives to pin-locks and passwords","Oftentimes however, instead of reducing the potential for unauthorised access, many of these methods serve instead only to increase the range of attack vectors available to criminals",Notable recent examples include the ‘smudge’ attack ( ) and the observation of key entry via ‘shoulder surfing’ ( ),"Even the use of dedicated fingerprint scanners has been demonstrated as being easily defeated (e.g.  , often by exploiting the same skin-oil deposits upon which the smudge attack relies","An alternative solution is to include features of touch interaction dynamics as a   biometric, for the principle reason that it is assumed that such behaviours would be more difficult for an attacker to consistently mimic","Some researchers, including  ,   and   have explored this in depth, reporting high levels of success using touch-behaviour based material to augment existing active authentication mechanisms",Others have presented entirely unique systems that are based on single or multi-touch gesture dynamic profiles (e.g.  ) or through fusion with other sensory data such as accelerometers (e.g.  ) with similarly positive findings,"Finally, researchers, including  ,  ,   and   have gone further still by developing touch-behavioural authentication systems that are entirely passive in nature, constantly monitoring touch interaction behaviour as a background process, invoking active authentication requests only when sufficient evidence has mounted that the active device user has changed",Our work here however takes the use of touchscreen behavioural dynamics in a slightly different direction,"While other work has considered the degree to which touchscreen gestural dynamics might offer value as a behavioural biometric for user authentication, relatively little attention has been given to what touchscreen gestures are able to reveal about the physical characteristics of their creator","Here then, instead of using touchscreen dynamics as a means of directly identifying an individual, we seek to establish whether links exist between these behavioural characteristics and other physical characteristics of the person behind the behaviours",Such a technique would be considered a ‘soft’ biometric in that we are seeking to identify a feature that is related to a group of individuals as opposed to a feature that is unique to an individual,"While the potential for such techniques have been demonstrated using physical keystroke dynamics to predict a user׳s gender (e.g.  ), to our knowledge, there currently exists no equivalent research on touchscreen gestural dynamics for this purpose. 1.4 Research questions In this initial exploration, our overarching research question was to identify the degree to which a relationship exists between the length of a person׳s thumb and the resultant characteristics of touchscreen swipe gestures made with that thumb","To investigate this question, we first assumed that there would exist variance in swipe gesture characteristics","Further, we assumed that this variance would have two main sources: (1) the physical physiology of the user and (2) their individual level of touchscreen interaction skill/experience. 
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      1.4.1 Physiology-based variance The swipe gesture can be completed in one of four directions (left-right, right-left, down-up, up-down) 
                            ","For one-handed use, the performance of a swipe gesture requires various manoeuvres of the thumb, dependent upon the direction of the swipe and the hand in which the device is held","Specifically, two types of thumb movement are required for one-handed swipe gestures","These are flexion (bending)/extension (straightening) at the interphalangeal joint (the first joint from the thumb-tip), and palmar abduction (towards palm)/adduction (away from palm) at the carpometacarpal joint – the joint closest to the wrist","Again, assuming one-handed use, to swipe horizontally primarily demands palmar abduction/adduction, but will also involve some degree of supporting movement at the interphalangeal joint for precision i.e. to maintain a reasonably straight horizontal line","Conversely, vertical swiping demands more involvement of the interphalangeal joint",It is somewhat difficult (though not impossible) to perform vertical swipes using palmar abduction/adduction in isolation,"We expected that these differences in manoeuvre would be detectable in the characteristics of swipe gestures on aggregate, hence: 
                             1: Different manoeuvres required for the thumb to swipe in different directions will result in differences in the characteristics of swipe gestures made in each direction",Differences in the length of thumb also afford more or less ability to manoeuvre across the screen,"Most obviously, a longer thumb can naturally and more easily reach further across the screen both horizontally and vertically","This we hypothesised would also be detectable, hence: 
                             2: People with longer thumbs will create swipes with different characteristics to people with shorter thumbs","Supplementing hypothesis two, we also consider the fact that males have – on average – hands that are larger and thumbs that are longer than females","Given this known difference in thumb length across the biological genders, we hypothesised that males and females would generate swipes that are detectably different, hence: 
                             3: The characteristics of swipe gestures made by males will be different to those made by females","Finally, and following our previous hypothesised differences in thumb manoeuvre, the hand in which the device is held is also potentially a factor in the eventual characteristics of a swipe gesture made in a given direction","Using the right hand, swipes in the D–U and R–L direction demand an initial flexion of the thumb at the interphalangeal joint prior to a ‘pushing away’ motion towards extension, whereas U–D and L–R swipes demand an initial extension of the thumb before ‘pulling’ the thumb across the screen","Using the left hand, these ‘pushing’ and ‘pulling’ manoeuvres for horizontal strokes will of course be reversed, hence: 
                             4: There will be differences in the characteristics of horizontal swipe gestures between left-handed device users and right-handed device users. 1.4.2 Previous experience with touchscreens In the last ten years, touchscreen-based smartphones have rapidly become a ubiquitous feature of modern urban life","Interacting with a touchscreen efficiently is however a skill, and one that takes some time to develop fully","In light of this, and in combination with our main investigation into physiological differences of device users, we also considered the effect of user experience with current touchscreen technologies","For finger-based operation of screens with ever increasing pixel densities, almost all smartphones now use capacitive touchscreen technology to provide a much higher sensitivity and accuracy than previous stylus-based resistive screen technologies",We expected that people with less experience with capacitive touchscreens (or indeed touchscreens in general) would exhibit a higher range of variance in their swipe patterns,"Conversely, we expected experienced users to display a more habituated/stable gesturing pattern with smaller, more efficient movements and lighter pressures: 
                             5: People with higher self-reported experience with touchscreens will produce more consistent swipe patterns, with lower levels of variance across their features on aggregate. 2 Method A small software application was created to facilitate the systematic and controlled capture of swipe gesture samples in four directions","This application was deployed on a single reference touchscreen-based smartphone running the   operating system. 
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   2.1 Participants 178 participants ( =87,  =91) were recruited from the Universities of Bath and Southampton","The age range of our participant pool was between 18 and 59 years old, with 71% aged between 18 and 24 years old","At Bath ( =61), an opportunity-based sampling method was used and no reward was given for participation","At Southampton ( =117), participants volunteered their data as part of a larger data collection exercise for which they were paid",The collection methodology was identical in both cases,"All participants were over 18 years of age, but no other participation eligibility criteria were applied","Of our participants, 161 used their right hand to complete the study and 17 used their left",Ethical approval for the study was sought and approved independently by the experimental ethics committees of the Universities of Bath and Southampton. 2.2 Apparatus and materials All swipe gestures for this study were captured using a single reference smartphone device,"The device used was a Samsung GT-I9100 ‘Galaxy S2’ model smartphone, chosen for its high-resolution screen and high processor speed","The GT-I9100 has a   capacitive touchscreen with a pixel density of 219 pixels-per-inch, providing an overall resolution of 480 × 800 px (W × H)",All screen settings were set to the default values for the version of Android OS installed (2.3),"No screen protectors were used. 
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      2.2.1 The development of a touchscreen gesture recording tool The requirements of our study demanded that we develop a systematic method for eliciting and capturing touchscreen swipe gestures made in four directions from a reference smartphone","While it is possible to log all touch interaction events on an Android-based smartphone as a background process (i.e. regardless of the application currently in use), to do this requires maximum privilege, or ‘root’ level access to be made available on the device","As the process of obtaining root level access (‘rooting’) invalidates the device warranty, this method was not available to us","Without root level access, the recording of touchscreen interaction events is – for reasons of security – limited by the Android OS to a single application that must be custom built for this purpose","For the purposes of our study, we required an application that would utilise multi-directional swiping as its primary interaction dynamic (so as to maximise our data yield)","Common tasks, such as text messaging and internet browsing, do not utilise the swipe gesture to any great extent and were thus not considered appropriate options for further investigation","The potential of modifying an existing application such as the Android homescreen, photo browser or calendar systems was explored as they all use swiping for the purposes of navigation, but these options were later abandoned – primarily due to their very limited use of vertical swiping","The final system that we developed for our data capture was a standalone Android application that used a tile sliding paradigm similar to that used to control the popular puzzle game 2048. 
                            
                         Our chosen method presents a trade-off between ecological validity and experimental control that we recognise","However, we felt that this was the most appropriate method of collecting a controlled number of genuine swipe gestures from a large number of people without contaminating the data with other gestures (e.g. tap/double-tap), or by inadvertently directing participants to swipe from a particular point on the screen","Further, as all measures collected used standard methods of the Android SDK with no transformation or filtering applied (including details of the screen size, orientation and resolution of the reference device used), we are confident that the data collected here can be repeated, reproduced and built upon by other researchers who wish to use devices with more advanced screens than the Samsung i9100 used in this study","To elicit swipe gestures in as unconstrained a manner as possible while still forcing the participant to swipe in several directions (specifically L–R, R–L, U–D, D–U), our gesture capture system was augmented with a simple reading task that was designed to produce a controlled number of swipe requests over all four directions",The task set to participants was to use the application to read a series of short jokes,Each joke (e.g.  ) and its punch line ( ) were presented separately as simple slides,"To progress through the slides, participants were required to perform a swipe gesture in the direction indicated on the screen","For the purposes of user feedback, upon completion of each slide the content of the slide would exit the screen in the direction that the swipe was made ( 
                            ). 2.2.2 Defining and capturing valid swipe gestures For the operating system of our reference device (Google Android), a swipe gesture interaction is triggered (and subsequently recorded by our application) upon satisfaction of two parameters that must be specified at the application level",These are (1) a minimum value for length (measured in pixels) and (2) a minimum value for velocity (measured in milliseconds),"As screen sizes and pixel density vary widely across smartphones, there currently exists no standard value for these parameters","For the purposes of the study, the minimum length was set at 120 pixels and the minimum velocity was set at 200 ms","These values were suggested by the Android software development community as being commonly used values, and are based upon those used on the default Android OS home-screen","A series of pilot studies of gesture captures conducted with six users indicated that, in practice, the distance travelled during a typical swipe gesture is typically several times this minimum figure, and had no identifiable impact on the way that participants completed their gestures","Upon capture of each gesture sample, all raw data that could be captured about it was recorded and sent immediately to a central database via Wi-Fi",All measure were captured using the standard touchEvent methods provided by the Android software development toolkit,"Custom built web-based software automatically analysed the incoming gesture data, producing a set of calculated metrics along with the ability to immediately reconstruct each gesture to scale as a vector graphic ( 
                            )",The ability to immediately and visually inspect each swipe gesture proved to be extremely useful in accurately identifying software recording errors and other potential data contaminants. 2.2.3 Swipe gesture feature extractions Each swipe gesture generates a series of time-stamped points that together trace the path of the gesture as it travels across the screen,"From each recorded gesture, six features were extracted and/or calculated","These feature were felt to adequately cover the main dimensional features of the swipe gesture and are described thus: 
                         For the purposes of reproducibility, all data relating to touch interaction events, including timing information, were collected via methods supplied by the Android OS   class. 
                             
                             and   co-ordinates were collected using the   and   methods respectively","During data collection, all user controllable applications on the device were shut down prior to launching our custom gesture recording application. 2.3 Procedure Each participant was first provided with written instructions detailing the requirements of the study, after which their signed consent was obtained","Participants were then asked to provide details of their biological gender, phone handedness 
                          and the length of their thumb (mm)","The length of the thumb was measured from the carpometacarpal joint (closest the wrist) to the thumb tip, with the thumb at full extension","Finally, we asked all participants to self-report their prior experience with touchscreen-based smartphones, using a 6 point Likert scale (1: no prior experience to 6: highly experienced)",The participant was then presented with the instrumented smartphone and the researcher demonstrated the swipe gesture,"In order to maximise consistency in our sample set and to provide the best possible conditions for linking our known physical measurement of thumb length to subsequent touch screen interactions, for this study the orientation of the screen was fixed to portrait mode and interactions with the screen were restricted to single-handed use only (participant׳s preference), using only the thumb of that hand to interact with the screen",Future work will seek to establish the degree to which two handed operation impacts on our findings here,"Prior to completing the study, each participant was allowed a few moments to familiarise themselves with the device and the generation of swipe gestures by navigating menus on the home-screen and browsing the Internet","These applications were then shut down, and the participant was then instructed to launch the gesture recording application, following the instructions provided on the screen",Participants were instructed to complete the task using only the thumb of one hand (their choice) to generate each swipe gesture,The order of slide presentation (and by extension the direction of swipe required to progress through the slides) was randomised for each participant,"In total, each participant submitted 120 swipe gestures (30 samples in each direction)","The total duration of the study was between 10 and 15 min, after which all participants were fully debriefed. 3 Results A total of 21,360 individual swipe gestures were collected from 178 participants, with each participant providing 30 swipe gestures for each of four directions (U–D, D–U, L–R and R–L)","Prior to analysis, 2207 samples were identified as containing extreme outliers on at least one measure and were removed, leaving a final dataset of 19,153 samples",Exploration of the data revealed a non-normal distribution for all six of our feature extractions,Normalisation of the data for each feature was attempted using logarithmic and square transforms to reduce skew where appropriate,"However, subsequent examination of the Kolmogorov–Smirnov and Shapiro–Wilk tests of normality indicated that both remained highly significant, and Levene׳s test for homogeneity of variance also failed for all features, regardless of transform","Consequently we chose to employ non-parametric data supportive statistical tests in all subsequent analyses of swipe gesture characteristics. 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   3.1 Previous experience with smartphones and the relationship between experience and intra-participant gesture feature consistency On a scale of 1 (no experience) to 6 (highly experienced), 72% of our participants self-rated their level of experience with touchscreen-based smartphones at 5 or above, while only 2.8% reported as having no prior experience with this technology","A statistically significant difference was observed in self-reports of touch screen experience for males and females, with males (M=5.02, SD=1.2) rating themselves slightly higher on average than females (M=4.51, SD=1.42), one-way ANOVA:  (1,176)=6.88,  ",A question arises as the whether this difference in self-rated scores has a direct impact on actual performance at completing swipe gestures,"From observations of our participants as they completed the study, we found no evidence of difficulty interacting with the screen",We instead suspect that the more conservative scores reported by our female participants is related to a perceived technical ‘confidence gap’ across the genders that is unfounded,"In summary, and particularly given that both male and female scores for experience were very high, we consider the impact of this gender difference upon our broader research questions as minimal","Spearman rank order correlation revealed no significant relationship between participant age and touch screen experience ( 
                         (178)=−0.06,  =0.40 n.s.), though this result needs to be considered with caution given that the majority of our participants were less than 25 years of age","To examine the relationship between self-reported touchscreen technology experience and swipe gesture consistency, we first calculated the total variance for each of the six features across all of the gesture samples provided by each participant",These values were then examined against their self-reported levels of touchscreen-based smartphone experience using the Spearman RHO correlation coefficient procedure,"Our   5,  , was partially supported","Spearman RHO revealed weak but statistically significant relationships between self-reported levels of touchscreen-based smartphone experience and the variance of three of our six features, providing some evidence that swipe gestures do become more internally consistent as experience with touchscreen operation increases","Intra-participant variance in gesture length ( 
                         (178)=−0.163,  =0.03) and completion time ( 
                         (178)=−0.189,  =0.01) were observed to reduce as self-reported experience increased, while max. accelerations achieved were observed to increase with experience ( 
                         (178)=−0.208,  )","No significant relationship was observed between self-reported levels of touchscreen-based smartphone experience and intra-participant variances in max. speed, gesture thickness or applied touch pressure. 3.2 The effect of direction upon swipe gesture characteristics A Kruskal–Wallis test was conducted to determine if there were significant differences in our six measures across the four directions of swipe captured","Post hoc pairwise comparisons were performed using   procedure, with a Bonferroni correction for multiple comparisons applied","Our   1,  , was partially supported",The Kruskall–Wallis test revealed a significant omnibus effect of direction on all six of our gesture feature extractions,Post hoc analysis further revealed statistically significant differences at the 0.05 level for all features between almost all direction pairs,"These differences suggest that the direction in which a swipe is drawn is an important factor in its resultant characteristics, and that any further analysis of swipe gestures should take these directional differences into account","However, there were instances where particular characteristics did not differ across specific direction pairs that are important to note: gesture lengths for swipes drawn in the vertical directions (U–D and D–U) were not significantly different from one another, and completion times for swipes drawn in the horizontal directions (R–L and L–R) were also statistically indistinguishable from one another in our sample. 3.3 Differences in horizontal swipe gesture characteristics between left and right handed device users Median values for all six measures were compared between participants who held the device in their left hand and participants who used their right hand using the Mann–Whitney   Test procedure",All pairwise comparisons were again performed using   procedure with a Bonferroni correction for multiple comparisons applied,"Our   4, there will be differences in the characteristics of horizontal swipe gestures between left-handed device users and right-handed device users was supported","Mann–Whitney revealed that swipes created by left-handed device users exhibited lower completion times and higher speeds than right-handed device users for swipes made in the R–L direction (where completing the gesture demanded that the thumb be extended first before being pulled across the screen), whereas right-handed device users outperformed left-handers on the same measures with swipes made in the opposite L–R direction",These differences were observed as being statistically significant at the 0.05 level,"Median completion times and maximum speeds for left and right handers across the two directions of horizontal swipe are presented in  
                         
                          respectively",Post hoc analysis also revealed several other statistically significant differences in gesture characteristics that were related to handedness but limited to specific directions of swipe,"Right handers were observed to create longer L–R ‘pulling’ swipes than left-handed device users (Mdn[left]=250.00, Mdn[right]=252.00,  =860823.5,  =−2.095  =0.04), and right handers created thicker R–L ‘pushing’ swipes than did left handers (Mdn[left]=43.4, Mdn[right]=44.4,  =819043,  =−2.823,  =0.005)","Conversely, left handers applied more pressure on L–R ‘pushing’ swipes (Mdn[left]=0.225, Mdn[right]=0.214,  =999993.5,  =2.960,  =0.003)","No differences were observed for maximum accelerations achieved. 3.4 The relationship between thumb length and resultant swipe characteristics 
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      3.4.1 Linking thumb length and swipe characteristics A revised and more accurate method for collecting thumb lengths was used in the second data collection at Southampton ( =116)",Our revised thumb measurement protocol was developed in collaboration with forensic anthropologist Prof,"Sue Black (University of Dundee, U.K.)","Under her advisement, in the second (Southampton) data collection, our measurement of thumb length was taken from the carpometacarpal joint (closest the wrist) to the joint tip","For each participant in the Southampton data collection, high resolution scaled photographs of both hands were collected under controlled conditions for reference, and all measurements were completed by the same person","In the first collection (Bath) we took only a manual measurement from the adjacent metacarpophalangeal thumb joint to the tip, without collecting a photographic reference",This change in procedure rendered the two measurement sets incompatible with one another,"As result of this, thumb length measures from the Bath dataset ( =61) have been excluded from this analysis",Participant thumb lengths ( =116) ranged from 9.8 cm to 14.1 cm ( =11.8 cm),"Following population trends, male thumbs ( =12.4 cm, SD=0.62) were on average longer than females ( =11.2 cm, SD=0.53) in our sample","This difference was found to be statistically significant at the 0.05 level (one way ANOVA:  (1,114)=39.75,  )","To examine the relationship between thumb length and swipe gesture characteristics, we compared each of our six features with thumb length using the Spearman RHO correlation coefficient procedure","Our   2,  , was supported by three of our measures","Spearman׳s RHO revealed a statistically significant relationship between thumb length and the following features of swipe gesture: Completion times were observed as being related to thumb length ( 
                            (115)=−0.305,   two-tailed), showing that as thumb lengths increased, the completion time for swipe gestures decreased ( 
                            )","Maximum speeds were observed as being related to thumb length ( 
                            (115)=0.268,   two-tailed), showing that as thumb lengths increased, the maximum speed achieved increased ( 
                            )","Finally, maximum accelerations were observed as being related to thumb length ( 
                            (115)=0.265,   two-tailed), showing that as thumb lengths increased, maximum acceleration increased ( 
                            )","No significant relationship was observed between thumb length and gesture thickness, avg. pressure or gesture length. 3.4.2 Linking swipe characteristics to gender A comparison of the median values for our six measures between our male and female participants was performed using the Mann–Whitney   Test",Pairwise comparisons were again performed using   procedure with a Bonferroni correction for multiple comparisons applied,"Our   3,  , was partially supported","Building upon the correlations that we observed for thumb lengths, the Males in our sample completed swipe gestures faster than the females in all directions, and this was again observed in terms of shorter completion times ( 
                            ), higher speeds ( 
                            ) and higher accelerations ( 
                            )","An additional difference between the genders was also observed in that males were found to apply more pressure to the screen than did females, across all directions of swipe ( 
                            )","No statistically significant differences were observed for gesture lengths or gesture thicknesses between males and females in any direction of gesture. 4 Discussion In this paper, we present an initial investigation of the relationship between six touchscreen swipe gesture characteristics and a specific physical characteristic of their creator: the length of their thumb","Should such a relationship exist, we suggest that this could – via following the known proportional relationships between human segment lengths – offer a route to inferring other measures of a given user׳s physical characteristics, such as their standing height or foot size",This work was inspired by a known problem in contemporary criminal investigations: how to identify criminals who conduct their activities using temporary ‘burner’ mobile phones that are unregistered to them by name,"Within this scenario, we suggest that the ability to link physical characteristics to behavioural metrics captured from the way in which a person interacts with a touchscreen device could prove useful to law enforcement analysts as they begin eliminating groups of potential suspects towards the identification of a specific individual","From an analysis of approximately 19,000 swipe gestures supplied by 178 participants, we show that there is a relationship between thumb length and three features of swipe gestures","Our analysis thus far has shown that users with longer thumbs complete swipe gestures with shorter completion times, higher speeds and higher accelerations than users with shorter thumbs","Further supporting these findings, these differences in swipe composition were also observed to exist between our male and female participants, where there is a known difference in thumb length within the general population (males on average having longer thumbs than females)","Finally, while no relationship was found between thumb length and touch pressures applied during swipe gesture composition, we did observe a statistically significant difference between males and females (males apply more touch pressure than females, regardless of swiping direction), suggesting that other physiological factors relating to larger hands – such as variance in grip strength – might also be detectable",That our two remaining features (swipe gesture length and thickness) were not found to be useful indicators of the physiological characteristics of their creator׳s thumb was unexpected,Gesture length in particular was surprisingly unfruitful given the reasonable assumption that a longer thumb affords an increased range of motion that we expected would result in longer swipes,"However, from what we have seen of the swipe gesture in practice, the length of a swipe gesture is actually too short for longer thumbs to be of any detectable benefit","Also contrary to our initial expectations, participants in our sample did not create swipe gestures as efficiently or as consistently as we assumed they would","However, we did find some evidence to suggest that increased experience with touchscreens reduced intra-participant variance in some the features we examined, namely that higher self-reported experience was weakly correlated with swipe gestures of a higher efficiency (shorter lengths, faster completion times and higher accelerations)","However, we must currently treat this finding with some caution given that our participant pool was heavily skewed toward younger smartphone users. 
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   4.1 Lessons learned Through the course of the collection and analysis of the large dataset collected during the study, there were a number of lessons learned from our experience that will inform our future work and would be of benefit to other researchers and practitioners working in this area","Firstly, both intra- and inter-participant swipe gesture data is somewhat noisy, with all of our measures following a non-normal distribution","The non-normal nature of the data placed limitations upon the statistical tools that could be appropriately used to perform our analysis, particularly with regard to predictive modelling techniques such as regression","While machine learning algorithms potentially offer an alternative predictive route (and one we are actively investigating), there exists significant risk of both type I and II error in datasets of this type if inappropriate statistical tools are applied",Researchers are encouraged to be mindful of this,"Further, researchers are encouraged to be mindful to pay particularly close attention to the impact of extreme outliers in touchscreen interaction data at the level of gesture capture","Though rank-order based statistical analysis that focus upon median values rather than means are better suited to mitigating the effect of outliers, examination of our data prior to analysis indicated that we needed to discard some 10% of our results due to the presence of extreme outliers on at least one of our measures",Identifying reasons and developing countermeasures for these extreme values to maximise data yields is the focus of continuing investigation,Some are certainly due to the intermittent software level recording error that we mentioned in our results,These are relatively easy to identify and discard as they occur as they all have clearly identifiable breaks in their internal path sequence identifiers,"However, others appear to be valid swipes in all respects other than that they have characteristics that fall far outside the ‘normal’ range of their creator","A full appreciation of the reasoning behind these intermittent ‘extreme’ swipe patterns is far from complete, but we suspect that at least some are the result of an over-compensation immediately after a recording error (i.e. as the participant received no feedback that the swipe gesture was recognised, they immediately re-doubled their efforts in the next swipe attempt leading to a far longer/higher pressure swipe gesture)",More work is required both to finesse the gesture collection technique we have developed and to evaluate the impact that these errors have,"Secondly, our analysis thus far indicates clearly that the direction in which the swipe is drawn does affect its characteristics, thus swipe gesture patterns should be analysed separately on a per-direction basis","Similarly, left and right handed device users exhibit differences in swipes made horizontally",Both of these factors should be controlled for when analysing swipe data characteristics at the aggregate level,"Thirdly, and though care was taken to clean the screen of our reference device regularly between collection sessions, a rapid build up of skin oils on the screen was still observed","Indeed, we suspect that a source of some of the noise encountered in our data was due to variances in skin/screen friction as a result of the build up of skin oils","Further, and though we ourselves did not use any aftermarket screen protectors, we recognise that many device users do fit such devices",Further investigation is required to understand the degree to which these factors affect the characteristics of swipe gesture composition and – if necessary – how these effects be detected and controlled for,"Finally, we must accept that our initial methodology for collecting measurements of thumb length lacked sufficient rigour, resulting in the loss of a significant amount of data","It is apparent that techniques for collecting anthropometric data vary widely, and a discussion of the relative accuracy of various techniques (e.g. manual calliper vs. imaging, the identification/selection of external joint/skinfold landmarks) is ongoing","Researchers are strongly encouraged to seek advice from the medical and forensic research communities before gathering data of this type. 4.2 Limitations of the research In discussing the conclusions of our research thus far, we must of course be mindful of the potential future applications of any system that relies upon indirect measures in order to draw inferential conclusions about people",This is of particular importance here given that there is obvious potential for gesture data collection to be conducted covertly without knowledge or consent,"While a full discussion of the privacy implications involved in developing such techniques is outside the scope of this paper, it is important to be very clear that this work – currently in its infancy – has significant privacy issues attached to it and is some considerable distance from deployment in a real-world scenario","Therefore, while we feel that our results presented here offer an encouraging first step towards our ultimate goal, we stress the limits of our findings thus far, particularly given the limited choice of hardware examined and the restricted interaction mode imposed upon our participants","To restate our main objective, the aim of the present work was to explore whether it is feasible to link touchscreen gesture characteristics to the thumb length of their creator, as a precursor to the detailed and significant amount of further work that would be required to operationalise such links for use in a forensic context","So far, we have found evidence to support the presence of relationship between several features of the swipe gesture and thumb length, but note that this relationship is currently only useful as an indicator when considered relative to population averages (as opposed to revealing the actual length of a particular user׳s thumb)","Further, the data collection method was conducted under controlled laboratory conditions","Much more research is required to establish a means of inferring actual thumb length to any degree of accuracy, particularly when taking into account the myriad devices available and the many ways in which people choose to interact with these devices","To this end, we highlight below several limitations of our research that will be the focus of attention in our future work","Firstly, we accept that our findings are currently limited to one specific mobile device that was held and operated in a very specific way (our participants were forced to hold our instrumented smartphone in one hand, using only their thumb to interact with the device)",This we argue was necessary at this early stage to maximise any potential correspondence between thumb length and touch gesture characteristics,"However, we do of course recognise that these devices can and often are operated with both hands and with other digits than the thumb",Further work will seek to address this by allowing participants to choose the way in which they hold the device and the digit that they use to interact with the screen,"Secondly, there are limitations to our sampling method","While the effect of both physiological differences and performance factors related to past experience with touchscreens was considered in our analysis, as a result of using an opportunity-based sampling method, the majority of our participants were young adults",Most of our participants were highly experienced touchscreen operators,"Despite this however, we did still observe a weak but statistically significant signal in our less confident participants, indicating that lower touchscreen technology experience may well effect the stability and consistency of swipe gestures over time – particularly as novices become more adept at using the technology",The development of our future recruitment strategy will address this issue by substantially increasing the range of age and experience in our population sampling method,"Finally, in this study we collected only 120 gesture samples from each participant within a single session","Given that we now know that direction is important, future work would certainly seek to increase the number of samples that we collect in each direction, perhaps by instrumenting a game (such as the previous puzzle game 2048 mentioned previously) that uses multi-directional swipe as its main control",Future work will seek to capture multiple gesture samples over a number of sessions in order to develop a better understanding of individual gesture consistency over time. 5 Conclusions and future work Our results thus far offer positive signals that a route between a behavioural biometric and a physical biometric is potentially available via features of a common touchscreen interactional gesture,Our work so far has uncovered some evidence to support the notion that a physical characteristic of a person – namely their thumb length relative to population trends – could be detectable from characteristics of their behavioural touchscreen interactions,"Future work will build upon the relationships we have observed here, and will seek to examine other feature extractions along with assessing the predictive capability of the features we have examined thus far",Future studies will also seek to increase our population sample and to examine free input (using finger instead of thumbs) along with devices of different form factors.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0925231219301961," 1 Introduction Segmentation of medical images is an essential task for many applications such as anatomical structure modeling, tumor growth measurement, surgical planing and treatment assessment  ","Despite the breadth and depth of current research, it is very challenging to achieve accurate and reliable segmentation results for many targets  ","This is often due to poor image quality, inhomogeneous appearances brought by pathology, various imaging protocols and large variations of the segmentation target among patients","Therefore, uncertainty estimation of segmentation results is critical for understanding how reliable the segmentations are","For example, for many images, the segmentation results of pixels near the boundary are likely to be uncertain because of the low contrast between the segmentation target and surrounding tissues, where uncertainty information of the segmentation can be used to indicate potential mis-segmented regions or guide user interactions for refinement  ","In recent years, deep learning with convolutional neural networks (CNN) has achieved the state-of-the-art performance for many medical image segmentation tasks  ","Despite their impressive performance and the ability of automatic feature learning, these approaches do not by default provide uncertainty estimation for their segmentation results","In addition, having access to a large training set plays an important role for deep CNNs to achieve human-level performance  ","However, for medical image segmentation tasks, collecting a very large dataset with pixel-wise annotations for training is usually difficult and time-consuming","As a result, current medical image segmentation methods based on deep CNNs use relatively small datasets compared with those for natural image recognition  ","This is likely to introduce more uncertain predictions for the segmentation results, and also leads to uncertainty of downstream analysis, such as volumetric measurement of the target","Therefore, uncertainty estimation is highly desired for deep CNN-based medical image segmentation methods",Several works have investigated uncertainty estimation for deep neural networks  ,"They focused mainly on image classification or regression tasks, where the prediction outputs are high-level image labels or bounding box parameters, therefore uncertainty estimation is usually only given for the high-level predictions","In contrast, pixel-wise predictions are involved in segmentation tasks, therefore pixel-wise uncertainty estimation is highly desirable","In addition, in most interactive segmentation cases, pixel-wise uncertainty information is more helpful for intelligently guiding the user to give interactions","However, previous works have rarely demonstrated uncertainty estimation for deep CNN-based medical image segmentation","As suggested by Kendall and Gal  , there are two major types of predictive uncertainties for deep CNNs:   uncertainty and   uncertainty.   uncertainty is also known as model uncertainty that can be explained away given enough training data, while   uncertainty depends on noise or randomness in the input testing image","In contrast to previous works focusing mainly on classification or regression-related uncertainty estimation, and recent works of Nair et al.   and Roy et al.   investigating only test-time dropout-based ( ) uncertainty for segmentation, we extensively investigate different kinds of uncertainties for CNN-based medical image segmentation, including not only   but also   uncertainties for this task","We also propose a more general estimation of   uncertainty that is related to not only image noise but also spatial transformations of the input, considering different possible poses of the object during image acquisition","To obtain the transformation-related uncertainty, we augment the input image at test time, and obtain an estimation of the distribution of the prediction based on test-time augmentation","Test-time augmentation (e.g., rotation, scaling, flipping) has been recently used to improve performance of image classification   and nodule detection  .  Ayhan and Berens   also showed its utility for uncertainty estimation in a fundus image classification task","However, the previous works have not provided a mathematical or theoretical formulation for this","Motivated by these observations, we propose a mathematical formulation for test-time augmentation, and analyze its performance for the general    uncertainty estimation in medical image segmentation tasks","In the proposed formulation, we represent an image as a result of an acquisition process which involves geometric transformations and image noise","We model the hidden parameters of the image acquisition process with prior distributions, and predict the distribution of the output segmentation for a test image with a Monte Carlo sampling process","With the samples from the distribution of the predictive output based on the same pre-trained CNN, the variance/entropy can be calculated for these samples, which provides an estimation of the   uncertainty for the segmentation",The contribution of this work is three-fold,"First, we propose a theoretical formulation of test-time augmentation for deep learning","Test-time augmentation has not been mathematically formulated by existing works, and our proposed mathematical formulation is general for image recognition tasks","Second, with the proposed formulation of test-time augmentation, we propose a general   uncertainty estimation for medical image segmentation, where the uncertainty comes from not only image noise but also spatial transformations","Third, we analyze different types of uncertainty estimation for the deep CNN-based segmentation, and validate the superiority of the proposed general   uncertainty with both 2D and 3D segmentation tasks. 2 Related works 
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                   2.1 Segmentation uncertainty Uncertainty estimation has been widely investigated for many existing medical image segmentation tasks","As way of examples, Saad et al.   used shape and appearance prior information to estimate the uncertainty for probabilistic segmentation of medical imaging","Shi et al.   estimated the uncertainty of graph cut-based cardiac image segmentation, which was used to improve the robustness of the system",Sankaran et al.   estimated lumen segmentation uncertainty for realistic patient-specific blood flow modeling,Parisot et al.   used segmentation uncertainty to guide content-driven adaptive sampling for concurrent brain tumor segmentation and registration,Prassni et al.   visualized the uncertainty of a random walker-based segmentation to guide volume segmentation of brain Magnetic Resonance Images (MRI) and abdominal Computed Tomography (CT) images,"Top et al.   combined uncertainty estimation with active learning to reduce user time for interactive 3D image segmentation. 2.2 Uncertainty estimation for deep CNNs For deep CNNs, both   and   uncertainties have been investigated in recent years","For model ( ) uncertainty, exact Bayesian networks offer a mathematically grounded method, but they are hard to implement and computationally expensive","Alternatively, it has been shown that dropout at test time can be cast as a Bayesian approximation to represent model uncertainty  ",Zhu and Zabaras   used Stochastic Variational Gradient Descent (SVGD) to perform approximate Bayesian inference on uncertain CNN parameters,"A variety of other approximation methods such as Markov chain Monte Carlo (MCMC)  , Monte Carlo Batch Normalization (MCBN)    and variational Bayesian methods   have also been developed","Lakshminarayanan et al.   proposed ensembles of multiple models for uncertainty estimation, which was simple and scalable to implement","For test image-based ( ) uncertainty, Kendall and Gal   proposed a unified Bayesian deep learning framework to learn mappings from input data to   uncertainty and composed them with   uncertainty, where the   uncertainty was modeled as learned loss attenuation and further categorized into   uncertainty and   uncertainty","Ayhan and Berens   used test-time augmentation for   uncertainty estimation, which was an efficient and effective way to explore the locality of a testing sample","However, its utility for medical image segmentation has not been demonstrated. 2.3 Test-time augmentation Data augmentation was originally proposed for the training of deep neural networks",It was employed to enlarge a relatively small dataset by applying transformations to its samples to create new ones for training  ,"The transformations for augmentation typically include flipping, cropping, rotating, and scaling training images",Abdulkadir et al.   and Ronneberger et al.   also used elastic deformations for biomedical image segmentation,Several studies have empirically found that combining predictions of multiple transformed versions of a test image helps to improve the performance,"For example, Matsunaga et al.   geometrically transformed test images for skin lesion classification.   used a single model to predict multiple transformed copies of unlabeled images for data distillation",Jin et al.   tested on samples extended by rotation and translation for pulmonary nodule detection,"However, all these works used test-time augmentation as an ad hoc method, without detailed formulation or theoretical explanation, and did not apply it to uncertainty estimation for segmentation tasks. 3 Method The proposed general   uncertainty estimation is formulated in a consistent mathematical framework including two parts",The first part is a mathematical representation of ensembles of predictions of multiple transformed versions of the input,We represent an image as a result of an image acquisition model with hidden parameters in  ,Then we formulate test-time augmentation as inference with hidden parameters following given prior distributions in  ,"The second part calculates the diversity of the prediction results of an augmented test image, and it is used to estimate the   uncertainty related to image transformations and noise",This is detailed in  ,"Our proposed   uncertainty is compared and combined with   uncertainty, which is described in  ","Finally, we apply our proposed method to structure-wise uncertainty estimation in  . 
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                   3.1 Image acquisition model The image acquisition model describes the process by which the observed images have been obtained","This process is confronted with a lot of factors that can be related or unrelated to the imaged object, such as blurring, down-sampling, spatial transformation, and system noise","While blurring and down-sampling are commonly considered for image super-resolution  , in the context of image recognition they have a relatively lower impact","Therefore, we focus on the spatial transformation and noise, and highlight that adding more complex intensity changes or other forms of data augmentation such as elastic deformations is a straightforward extension","The image acquisition model is: where  
                          is an underlying image in a certain position and orientation, i.e., a hidden variable.   is a transformation operator that is applied to  
                         .   is the set of parameters of the transformation, and   represents the noise that is added to the transformed image.   denotes the observed image that is used for inference at test time","Though the transformations can be in spatial, intensity or feature space, in this work we only study the impact of reversible spatial transformations (e.g., flipping, scaling, rotation and translation), which are the most common types of transformations occurring during image acquisition and used for data augmentation purposes","Let   denote the inverse transformation of   then we have: Similarly to data augmentation at training time, we assume that the distribution of   covers the distribution of  
                         ","In a given application, this assumption leads to some prior distributions of the transformation parameters and noise","For example, in a 2D slice of fetal brain MRI, the orientation of the fetal brain can span all possible directions in a 2D plane, therefore the rotation angle   can be modeled with a uniform prior distribution   ∼  (0, 2 )","The image noise is commonly modeled as a Gaussian distribution, i.e.,   where   and   are the mean and standard deviation respectively","Let  ( ) and  ( ) represent the prior distribution of   and   respectively, therefore we have   ∼  ( ) and   ∼  ( )","Let   and  
                          be the labels related to   and  
                          respectively","For image classification,   and  
                          are categorical variables, and they should be invariant with regard to transformations and noise, therefore  ","For image segmentation,   and  
                          are discretized label maps, and they are equivariant with the spatial transformation, i.e.,  . 3.2 Inference with hidden variables In the context of deep learning, let  ( · ) be the function represented by a neural network, and   represent the parameters learned from a set of training images with their corresponding annotations","In a standard formulation, the prediction   of a test image   is inferred by: For regression problems,   refers to continuous values","For segmentation or classification problems,   refers to discretized labels obtained by   operation in the last layer of the network","Since   is only one of many possible observations of the underlying image  
                         , direct inference with   may lead to a biased result affected by the specific transformation and noise associated with  ","To address this problem, we aim at inferring it with the help of the latent  
                          instead: where the exact values of   and   for   are unknown","Instead of finding a deterministic prediction of  , we alternatively consider the distribution of   for a robust inference given the distributions of   and  ","For regression problems, we obtain the final prediction for   by calculating the expectation of   using the distribution  ( | )","Calculating  ( | ) with   is computationally expensive, as   and   may take continuous values and  ( ) is a complex joint distribution of different types of transformations","Alternatively, we estimate  ( | ) by using Monte Carlo simulation",Let   represent the total number of simulation runs,"In the  th simulation run, the prediction is: where  
                          ∼  ( ),  
                          ∼  ( )","To obtain  , we first randomly sample  
                          and  
                          from the prior distributions  ( ) and  ( ), respectively","Then we obtain one possible hidden image with  
                          and  
                          based on  , and feed it into the trained network to get its prediction, which is transformed with  
                          to obtain   according to  ","With the set   sampled from  ( | ),  ( | ) is estimated as the average of   and we use it as the final prediction   for  : For classification or segmentation problems,  ( | ) is a discretized distribution",We obtain the final prediction for   by maximum likelihood estimation: where Mode( ) is the most frequent element in  ,This corresponds to majority voting of multiple predictions. 3.3 Aleatoric uncertainty estimation with test-time augmentation The uncertainty is estimated by measuring how diverse the predictions for a given image are,Both the variance and entropy of the distribution  ( | ) can be used to estimate uncertainty,"However, variance is not sufficiently representative in the context of multi-modal distributions","In this paper we use entropy for uncertainty estimation: With the Monte Carlo simulation in  , we can approximate  ( | ) from the simulation results  ",Suppose there are   unique values in  ,"For classification tasks, this typically refers to   labels","Assume the frequency of the  th unique value is   then  ( | ) is approximated as: For segmentation tasks, pixel-wise uncertainty estimation is desirable",Let   denote the predicted label for the  th pixel,"With the Monte Carlo simulation, a set of values for   are obtained  ","The entropy of the distribution of   is therefore approximated as: where   is the frequency of the  th unique value in  . 3.4 Epistemic uncertainty estimation To obtain model ( ) uncertainty estimation, we follow the test-time dropout method proposed by  ","In this method, let  ( ) be an approximating distribution over the set of network parameters   with its elements randomly set to zero according to Bernoulli random variables.  ( ) can be achieved by minimizing the Kullback–Leibler divergence between  ( ) and the posterior distribution of   given a training set","After training, the predictive distribution of a test image   can be expressed as: The distribution of the prediction can be sampled based on Monte Carlo samples of the trained network (i.e, MC dropout):   where  
                          is a Monte Carlo sample from  ( )","Assume the number of samples is  , and the sampled set of the distribution of   is  ",The final prediction for   can be estimated by   for regression problems or   for classification/segmentation problems,The   uncertainty estimation can therefore be calculated based on variance or entropy of the sampled   predictions,"To keep consistent with our   uncertainty, we use entropy for this purpose, which is similar to  ",Test-time dropout may be interpreted as a way of ensembles of networks for testing,"In the work of Lakshminarayanan et al.  , ensembles of neural networks was explicitly proposed as an alternative solution of test-time dropout for estimating   uncertainty. 3.5 Structure-wise uncertainty estimation Nair et al.   and Roy et al.   used Monte Carlo samples generated by test-time dropout for structure/lesion-wise uncertainty estimation","Following these works, we extend the structure-wise uncertainty estimation method by using Monte Carlo samples generated by not only test-time dropout, but also test-time augmentation described in Section   ","For   samples from the Monte Carlo simulation, let   denote the set of volumes of the segmented structure, where   is the volume of the segmented structure in the  th simulation",Let   and   denote the mean value and standard deviation of   respectively,We use the volume variation coefficient (VVC) to estimate the structure-wise uncertainty: where VVC is agnostic to the size of the segmented structure. 4 Experiments and results We validated our proposed testing and uncertainty estimation method with two segmentation tasks: 2D fetal brain segmentation from MRI slices and 3D brain tumor segmentation from multi-modal MRI volumes,The implementation details for 2D and 3D segmentation are described in   and   respectively,"In both tasks, we compared different types of uncertainties for the segmentation results: 1) the proposed   uncertainty based on our formulated test-time augmentation (TTA), 2) the   uncertainty based on test-time dropout (TTD) described in  , and 3) hybrid uncertainty that combines the   and   uncertainties based on TTA + TTD","For each of these three methods, the uncertainty was obtained by   with   predictions","For TTD and TTA + TTD, the dropout probability was set as a typical value of 0.5  ","We also evaluated the segmentation accuracy of these different prediction methods: TTA, TTD, TTA + TTD and the baseline that uses a single prediction without TTA and TTD","For a given training set, all these methods used the same model that was trained with data augmentation and dropout at training time",The augmentation during training followed the same formulation in  ,We investigated the relationship between each type of uncertainty and segmentation error in order to know which uncertainty has a better ability to indicate potential mis-segmentations,"Quantitative evaluations of segmentation accuracy are based on Dice score and Average Symmetric Surface Distance (ASSD). where   and   are true positive, false positive and false negative respectively","The definition of ASSD is: where   and   denote the set of surface points of a segmentation result and the ground truth respectively.  ( ) is the shortest Euclidean distance between a point   ∈   and all the points in  . 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   4.1 2D fetal brain segmentation from MRI Fetal MRI has been increasingly used for study of the developing fetus as it provides a better soft tissue contrast than the widely used prenatal sonography","The most commonly used imaging protocol for fetal MRI is Single-Shot Fast Spin Echo (SSFSE) that acquires images in a fast speed and mitigates the effect of fetal motion, leading to stacks of thick 2D slices","Segmentation is a fundamental step for fetal brain study, e.g., it plays an important role in inter-slice motion correction and high-resolution volume reconstruction  ","Recently, CNNs have achieved the state-of-the-art performance for 2D fetal brain segmentation  ","In this experiment, we segment the 2D fetal brain using deep CNNs with uncertainty estimation. 
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      4.1.1 Data and implementation We collected clinical T2-weighted MRI scans of 60 fetuses in the second trimester with SSFSE on a 1.5 Tesla MR system (Aera, Siemens, Erlangen, Germany)","The data for each fetus contained three stacks of 2D slices acquired in axial, sagittal and coronal views respectively, with pixel size 0.63–1.58 mm and slice thickness 3–6 mm",The gestational age ranged from 19 weeks to 33 weeks,"We used 2640 slices from 120 stacks of 40 patients for training, 278 slices from 12 stacks of 4 patients for validation and 1180 slices from 48 stacks of 16 patients for testing","Two radiologists manually segmented the brain region for all the stacks slice-by-slice, where one radiologist gave a segmentation first, and then the second senior radiologist refined the segmentation if disagreement existed, the output of which were used as the ground truth",We used this dataset for two reasons,"First, our dataset fits with a typical medical image segmentation application where the number of annotated images is limited",This leads the uncertainty information to be of high interest for robust prediction and our downstream tasks such as fetal brain reconstruction and volume measurement,"Second, the position and orientation of fetal brain have large variations, which is suitable for investigating the effect of data augmentation","For preprocessing, we normalized each stack by its intensity mean and standard deviation, and resampled each slice with pixel size 1.0 mm","We used 2D networks of Fully Convolutional Network (FCN)  , U-Net   and P-Net  ","The networks were implemented in TensorFlow 
                             
                             using NiftyNet 
                             
                            ","During training, we used Adaptive Moment Estimation (Adam) to adjust the learning rate that was initialized as   with batch size 5, weight decay   and iteration number 10k","We represented the transformation parameter   in the proposed augmentation framework as a combination of   and  , where   is a random variable for flipping along each 2D axis,   is the rotation angle in 2D, and   is a scaling factor","The prior distributions of these transformation parameters and random intensity noise were modeled as   ∼  ( ),   ∼  ( 
                            ,  
                            ),   ∼  ( 
                            ,  
                            ) and   ∼  ( )","The hyper-parameters for our fetal brain segmentation task were set as   
                             
                             
                             and  ","For the random noise, we set   and   as a median-filter smoothed version of a normalized image in our dataset has a standard deviation around 0.95","We augmented the training data with this formulation, and during test time, TTA used the same prior distributions of augmentation parameters as used for training. 4.1.2 Segmentation results with uncertainty 
                            
                             shows a visual comparison of different types of uncertainties for segmentation of three fetal brain images in coronal, sagittal and axial view respectively","The results were based on the same trained model of U-Net with train-time augmentation, and the Monte Carlo simulation number   was 20 for TTD, TTA, and TTA + TTD to obtain   and hybrid uncertainties respectively","In each subfigure, the first row presents the input and the segmentation obtained by the single-prediction baseline",The other rows show these three types of uncertainties and their corresponding segmentation results respectively,The uncertainty maps in odd columns are represented by pixel-wise entropy of   predictions and encoded by the color bar in the left top corner,"In the uncertainty maps, purple pixels have low uncertainty values and yellow pixels have high uncertainty values.  (a) shows a fetal brain in coronal view","In this case, the baseline prediction method achieved a good segmentation result","It can be observed that for   uncertainty calculated by TTD, most of the uncertain segmentations are located near the border of the segmented foreground, while the pixels with a larger distance to the border have a very high confidence (i.e., low uncertainty)","In addition, the   uncertainty map contains some random noise in the brain region","In contrast, the   uncertainty obtained by TTA contains less random noise and it shows uncertain segmentations not only on the border but also in some challenging areas in the lower right corner, as highlighted by the white arrows","In that region, the result obtained by TTA has an over-segmentation, and this is corresponding to the high values in the same region of the   uncertainty map",The hybrid uncertainty calculated by TTA + TTD is a mixture of   and   uncertainty,"As shown in the last row of  (a), it looks similar to the   uncertainty map except for some random noise. 
                            (b) and (c) show two other cases where the single-prediction baseline obtained an over-segmentation and an under-segmentation respectively",It can be observed that the   uncertainty map shows a high confidence (low uncertainty) in these mis-segmented regions,"This leads to a lot of overconfident incorrect segmentations, as highlighted by the white arrows in  (b) and (c)","In comparison, the   uncertainty map obtained by TTA shows a larger uncertain area that is mainly corresponding to mis-segmented regions of the baseline","In these two cases, The hybrid uncertainty also looks similar to the   uncertainty map",The comparison indicates that the   uncertainty has a better ability than the   uncertainty to indicate mis-segmentations of non-border pixels,"For these pixels, the segmentation output is more affected by different transformations of the input ( ) rather than variations of model parameters ( ). 
                            (b) and (c) also show that TTD using different model parameters seemed to obtain very little improvement from the baseline","In comparison, TTA using different input transformations corrected the large mis-segmentations and achieved a more noticeable improvement from the baseline","It can also be observed that the results obtained by TTA + TTD are very similar to those obtained by TTA, which shows TTA is more suitable to improving the segmentation than TTD. 4.1.3 Quantitative evaluation To quantitatively evaluate the segmentation results, we measured Dice score and ASSD of predictions by different testing methods with three network structures: FCN  , U-Net   and P-Net  ","For all of these CNNs, we used data augmentation at training time to enlarge the training set","At inference time, we compared the baseline testing method (without Monte Carlo simulation) with TTD, TTA and TTA + TTD",We first investigated how the segmentation accuracy changes with the increase of the number of Monte Carlo simulation runs  ,"The results measured with all the testing images are shown in  
                            ","We found that for all of these three networks, the segmentation accuracy of TTD remains close to that of the single-prediction baseline","For TTA and TTA + TTD, an improvement of segmentation accuracy can be observed when   increases from 1 to 10","When   is larger than 20, the segmentation accuracy for these two methods reaches a plateau","In addition to the previous scenario using augmentation at both training and test time, we also evaluated the performance of TTD and TTA when data augmentation was not used for training","The quantitative evaluations of combinations of different training methods and testing methods ( 20) are shown in  
                            ","It can be observed that for both training with and without data augmentation, TTA has a better ability to improve the segmentation accuracy than TTD","Combining TTA and TTD can further improve the segmentation accuracy, but it does not significantly outperform TTA ( -value  >  0.05). 
                            
                             shows Dice distributions of five example stacks of fetal brain MRI",The results were based on the same trained model of U-Net with train-time augmentation,"Note that the baseline had only one prediction for each image, and the Monte Carlo simulation number   was 20 for TTD, TTA and TTA + TTD","It can be observed that for each case, the Dice of TTD is distributed closely around that of the baseline","In comparison, the Dice distribution of TTA has a higher average than that of TTD, indicating TTA’s better ability of improving segmentation accuracy","The results of TTA also have a larger variance than that of TTD, which shows TTA can provide more structure-wise uncertainty information.   also shows that the performance of TTA + TTD is close to that of TTA. 4.1.4 Correlation between uncertainty and segmentation error To investigate how our uncertainty estimation methods can indicate incorrect segmentation, we measured the uncertainty and segmentation error at both pixel-level and structure-level","For pixel-level evaluation, we measured the joint histogram of pixel-wise uncertainty and error rate for TTD, TTA, and TTA + TTD respectively",The histogram was obtained by statistically calculating the error rate of pixels at different pixel-wise uncertainty levels in each slice,"The results based on U-Net with   are shown in  
                            , where the joint histograms have been normalized by the number of total pixels in the testing images for visualization","For each type of pixel-wise uncertainty, we calculated the average error rate at each pixel-wise uncertainty level, leading to a curve of error rate as a function of pixel-wise uncertainty, i.e., the red curves in  ",This figure shows that the majority of pixels have a low uncertainty with a small error rate,"When the uncertainty increases, the error rate also becomes higher gradually.  (a) shows the TTD-based uncertainty ( )","It can be observed that when the prediction uncertainty is low, the result has a steep increase of error rate","In contrast, for the TTA-based uncertainty ( ), the increase of error rate is slower, shown in  (b)",This demonstrates that TTA has fewer overconfident incorrect predictions than TTD,The dashed ellipses in   also show the different levels of overconfident incorrect predictions for different testing methods,"For structure-wise evaluation, we used VVC to represent structure-wise uncertainty and  Dice to represent structure-wise segmentation error.  
                             shows the joint distribution of VVC and  Dice for different testing methods using U-Net trained with data augmentation and   = 20 for inference","The results of TTD, TTA, and TTA + TTD are shown in  (a)–(c) respectively","It can be observed that for all the three testing methods, the VVC value tends to become larger when  Dice grows","However, the slope in  (a) is smaller than those in  (b) and (c)","The comparison shows that TTA-based structure-wise uncertainty estimation is highly related to segmentation error, and TTA leads to a larger scale of VVC than TTD",Combining TTA and TTD leads to similar results to that of TTA. 4.2 3D brain tumor segmentation from multi-modal MRI MRI has become the most commonly used imaging methods for brain tumors,"Different MR sequences such as T1-weighted (T1w), contrast enhanced T1-weighted (T1wce), T2-weighted (T2w) and Fluid Attenuation Inversion Recovery (FLAIR) images can provide complementary information for analyzing multiple subregions of brain tumors","Automatic brain tumor segmentation from multi-modal MRI has a potential for better diagnosis, surgical planning and treatment assessment  ",Deep neural networks have achieved the state-of-the-art performance on this task  ,"In this experiment, we analyze the uncertainty of deep CNN-based brain tumor segmentation and show the effect of our proposed test-time augmentation. 
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                         
                      4.2.1 Data and implementation We used the BraTS 2017 
                             
                             training dataset that consisted of volumetric images from 285 studies, with ground truth provided by the organizers","We randomly selected 20 studies for validation and 50 studies for testing, and used the remaining for training","For each study, there were four scans of T1w, T1wce, T2w and FLAIR images, and they had been co-registered",All the images were skull-stripped and re-sampled to an isotropic 1 mm  resolution,"As a first demonstration of uncertainty estimation for deep learning-based brain tumor segmentation, we investigate segmentation of the whole tumor from these multi-modal images ( 
                            )","We used 3D U-Net  , V-Net   and W-Net   implemented with NiftyNet  , and employed Adam during training with initial learning rate   batch size 2, weight decay   and iteration number 20k","W-Net is a 2.5D network, and we compared using W-Net only in axial view and a fusion of axial, sagittal and coronal views",These two implementations are referred to as W-Net(A) and W-Net(ASC) respectively,"The transformation parameter   in the proposed augmentation framework consisted of   and  , where   is a random variable for flipping along each 3D axis,   is the rotation angle along each 3D axis,   is a scaling factor and   is intensity noise","The prior distributions were:   ∼  (0.5),   ∼  (0, 2 ),   ∼  (0.8, 1.2) and   ∼  (0, 0.05) according to the reduced standard deviation of a median-filtered version of a normalized image","We used this formulated augmentation during training, and also employed it to obtain TTA-based results at test time. 4.2.2 Segmentation results with uncertainty 
                             demonstrates three examples of uncertainty estimation of brain tumor segmentation by different testing methods",The results were based on the same trained model of 3D U-Net  ,"The Monte Carlo simulation number   was 40 for TTD, TTA, and TTA + TTD to obtain   and hybrid uncertainties respectively.  (a) shows a case of high grade glioma (HGG)",The baseline of single prediction obtained an over-segmentation at the upper part of the image,The   uncertainty obtained by TTD highlights some uncertain predictions at the border of the segmentation and a small part of the over-segmented region,"In contrast, the   uncertainty obtained by TTA better highlights the whole over-segmented region, and the hybrid uncertainty map obtained by TTA + TTD is similar to the   uncertainty map",The second column of  (a) shows the corresponding segmentations of these uncertainties,"It can be observed that the TTD-based result looks similar to the baseline, while TTA and TTA + TTD based results achieve a larger improvement from the baseline.  (b) demonstrates another case of HGG brain tumor, and it shows that the over-segmented region in the baseline prediction is better highlighted by TTA-based   uncertainty than TTD-based   uncertainty.  (c) shows a case of low grade glioma (LGG)",The baseline of single prediction obtained an under-segmentation in the middle part of the tumor,"The   uncertainty obtained by TTD only highlights pixels on the border of the prediction, with a low uncertainty (high confidence) for the under-segmented region","In contrast, the   uncertainty obtained by TTA has a better ability to indicate the under-segmentation","The results also show that TTA outperforms TTD for better segmentation. 4.2.3 Quantitative evaluation For quantitative evaluations, we calculated the Dice score and ASSDe for the segmentation results obtained by the different testing methods that were combined with 3D U-Net  , V-Net   and W-Net   respectively","We also compared TTD and TTA with and without train-time data augmentation, respectively","We found that for these networks, the performance of the multi-prediction testing methods reaches a plateau when   is larger than 40.  
                             shows the evaluation results with   = 40","It can be observed that for each network and each training method, multi-prediction methods lead to better performance than the baseline with a single prediction, and TTA outperforms TTD with higher Dice scores and lower ASSD values","Combining TTA and TTD has a slight improvement from using TTA, but the improvement is not significant ( -value  <  0.05). 4.2.4 Correlation between uncertainty and segmentation error To study the relationship between prediction uncertainty and segmentation error at voxel-level, we measured voxel-wise uncertainty and voxel-wise error rate at different uncertainty levels","For each of TTD-based ( ), TTA-based ( ) and TTA + TTD-based (hybrid) voxel-wise uncertainty, we obtained the normalized joint histogram of voxel-wise uncertainty and voxel-wise error rate.  
                             shows the results based on 3D U-Net trained with data augmentation and using   for inference",The red curve shows the average voxel-wise error rate as a function of voxel-wise uncertainty,"In  (a), the average prediction error rate has a slight change when the TTD-based   uncertainty is larger than 0.2","In contrast,  (b) and (c) show that the average prediction error rate has a smoother increase with the growth of   and hybrid uncertainties",The comparison demonstrates that the TTA-based   uncertainty leads to fewer over-confident mis-segmentations than the TTD-based   uncertainty,"For structure-level evaluation, we also studied the relationship between structure-level uncertainty represented by VVC and structure-level error represented by  Dice.  
                             shows their joint distributions with three different testing methods using 3D U-Net","The network was trained with data augmentation, and   was set as 40 for inference.   shows that TTA-based VVC increases when   Dice grows, and the slope is larger than that of TTD-based VVC","The results of TTA and TTA + TTD are similar, as shown in  (b) and (c)","The comparison shows that TTA-based structure-wise uncertainty can better indicate segmentation error than TTD-based structure-wise uncertainty. 5 Discussion and conclusion In our experiments, the number of training images was relatively small compared with many datasets of natural images such as PASCAL VOC, COCO and ImageNet","For medical images, it is typically very difficult to collect a very large dataset for segmentation, as pixel-wise annotations are not only time-consuming to collect but also require expertise of radiologists","Therefore, for most existing medical image segmentation datasets, such as those in Grand challenge 
                      , the image numbers are also quite small","Therefore, investigating the segmentation performance of CNNs with limited training data is of high interest for medical image computing community","In addition, our dataset is not very large so that it is suitable for data augmentation, which fits well with our motivation of using data augmentation at training and test time",The need for uncertainty estimation is also stronger in cases where datasets are smaller,"In our mathematical formulation of test-time augmentation based on an image acquisition model, we explicitly modeled spatial transformations and image noise","However, it can be easily extended to include more general transformations such as elastic deformations   or add a simulated bias field for MRI","In addition to the variation of possible values of model parameters, the prediction result is also dependent on the input data, e.g., image noise and transformations related to the object","Therefore, a good uncertainty estimation should take these factors into consideration.   and   show that model uncertainty alone is likely to obtain overconfident incorrect predictions, and TTA plays an important role in reducing such predictions","In   we show five example cases, where each subfigure shows the results for one patient.   shows the statistical results based on all the testing images","We found that for few testing images TTA + TTD failed to obtain higher Dice scores than TTA, but for the overall testing images, the average Dice of TTA + TTD is slightly larger than that of TTA","Therefore, this leads to the conclusion that TTA + TTD does not always perform better than TTA, and the average performance of TTA + TTD is close to that of TTA, which is also demonstrated in   and  ","We have demonstrated TTA based on the image acquisition model for image segmentation tasks, but it is general for different image recognition tasks, such as image classification, object detection, and regression","For regression tasks where the outputs are not discretized category labels, the variation of the output distribution might be more suitable than entropy for uncertainty estimation.   shows the superiority of test-time augmentation for better segmentation accuracy, and it also demonstrates the combination of W-Net in different views helps to improve the performance","This is an ensemble of three networks, and such an ensemble may be used as an alternative for   uncertainty estimation, as demonstrated by  ","We found that for our tested CNNs and applications, the proper value of Monte Carlo sample   that leads the segmentation accuracy to a plateau was around 20–40",Using an empirical value   is large enough for our datasets,"However, the optimal setting of hyper-parameter   may change for different datasets",Fixing   for new applications where the optimal value of   is smaller would lead to unnecessary computation and reduce efficiency,"In some applications where the object has more spatial variations, the optimal   value may be larger than 40","Therefore, in a new application, we suggest that the optimal   should be determined by the performance plateau on the validation set","In conclusion, we analyzed different types of uncertainties for CNN-based medical image segmentation by comparing and combining model ( ) and input-based ( ) uncertainties",We formulated a test-time augmentation-based   uncertainty estimation for medical images that considers the effect of both image noise and spatial transformations,"We also proposed a theoretical and mathematical formulation of test-time augmentation, where we obtain a distribution of the prediction by using Monte Carlo simulation and modeling prior distributions of parameters in an image acquisition model",Experiments with 2D and 3D medical image segmentation tasks showed that uncertainty estimation with our formulated TTA helps to reduce overconfident incorrect predictions encountered by model-based uncertainty estimation and TTA leads to higher segmentation accuracy than a single-prediction baseline and multiple predictions using test-time dropout,"Supplementary material Supplementary material associated with this article can be found, in the online version, at  ","Appendix A Supplementary materials 
                      
                  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0957417417300751," 1 Introduction The growth of user-generated content in web sites and social networks, such as Twitter, Amazon, and Trip Advisor, has led to an increasing power of social networks for expressing opinions about services, products or events, among others","This tendency, combined with the fast spreading nature of content online, has turned online opinions into a very valuable asset","In this context, many Natural Language Processing (NLP) tasks are being used in order to analyze this massive information","In particular, Sentiment Analysis (SA) is an increasingly growing task ( ), whose goal is the classification of opinions and sentiments expressed in text, generated by a human party","The dominant approaches in sentiment analysis are based on   techniques (  
                      )","Traditional approaches frequently use the Bag Of Words (BOW) model, where a document is mapped to a feature vector, and then classified by machine learning techniques","Although the BOW approach is simple and quite efficient, a great deal of the information from the original natural language is lost ( ), e.g., word order is disrupted and syntactic structures are broken","Therefore, various types of features have been exploited, such as higher order  -grams ( )","Another kind of feature that can be used is Part Of Speech (POS) tagging, which is commonly used during a syntactic analysis process, as described in  ","Some authors refer to this kind of features as   forms, as they consist in lexical and syntactical information that relies on the pattern of the text, rather than on its semantic aspect",Some prior information about sentiment can also be used in the analysis,"For instance, by adding individual word polarity to the previously described features ( )","This prior knowledge usually takes the form of  , which have to be gathered","Sentiment lexicons are used as a source of subjective sentiment knowledge, where this knowledge is added to the previously described features ( )",The use of lexicon-based techniques has a number of advantages ( ),"First, the linguistic content can be taken into account through mechanisms such as sentiment valence shifting ( ) considering both intensifiers (e.g. very bad) and negations (e.g. not happy)","In addition, sentiment orientation of lexical entities can be differentiated based on their characteristics","Moreover, language-dependent characteristics can be included in these approaches","Nevertheless, lexicon-based approaches have several drawbacks: the need of a lexicon that is consistent and reliable ( ), as well as the variability of opinion words across domains ( ), contexts ( ) and languages ( )",These dependencies make it hard to maintain domain independent lexicons ( ),"In general, extracting complex features from text, figuring out which features are relevant, and selecting a classification algorithm are fundamental questions in the machine learning driven methods ( )","Traditional approaches rely on manual feature engineering, which is time consuming","On the other hand, deep learning is a promising alternative to traditional methods","It has shown excellent performance in NLP tasks, including Sentiment Analysis ( )",The main idea of deep learning techniques is to learn complex features extracted from data with minimum external contribution ( ) using deep neural networks ( ),These algorithms do not need to be passed manually crafted features: they automatically learn new complex features,"Nevertheless, a characteristic feature of deep learning approaches is that they need large amounts of data to perform well ( )",Both automatic feature extraction and availability of resources are very important when comparing the traditional machine learning approach and deep learning techniques,"However, it is not clear whether the domain specialization capacity of traditional approaches can be surpassed with the generalization capacity of deep learning based models in all NLP tasks, or if it is possible to successfully combine these two techniques in a wide range of applications","In this paper, we propose a combination of these two main sentiment analysis approaches through several ensemble models in which the information provided by many kinds of features is aggregated","In particular, this work considers an ensemble of classifiers, where several sentiment classifiers trained with different kinds of features are combined, and an ensemble of features, where the combination is made at the feature level","In order to study the complementarity of the proposed models, we use six public test datasets from two different domains: Twitter and movie reviews","Moreover, we performed a statistical study on the results of these ensemble models in comparison to a deep learning baseline we have also developed",We also present the complexity of the proposed ensemble models,"Besides, we present a taxonomy that classifies the models found in the literature and the ones proposed in this work","With our proposal we seek answers to the following questions, using the empirical results we have obtained as basis:
 
                   The rest of the paper is organized as follows.   shows previous work on both ensemble techniques and deep learning approaches.   describes the proposed taxonomy for classifying ensemble methods that merge surface and deep features, whereas   addresses the proposed classifier and ensemble models","In  , we describe the designed experimental setup",Experimental results are presented and analyzed in  ,"Finally,   draws conclusions from previous results and outlines the future work. 2 Related work In this section we offer a brief summary of the previous work in the context of ensemble methods and deep learning algorithms for Sentiment Analysis. 
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   2.1 Ensemble methods for sentiment analysis In the field of ensemble methods, the main idea is to combine a set of models (base classifiers) in order to obtain a more accurate and reliable model in comparison with what a single model can achieve","The methods used for building upon an ensemble approach are many, and a categorization is presented in  ","This classification is based on two main dimensions: how predictions are combined (rule based and meta learning), and how the learning process is done (concurrent and sequential)","Regarding the first dimension, on the one hand, in   approaches predictions from the base classifiers are treated by a rule, with the aim of averaging their predictive performance","Examples of rule based ensembles are the majority voting, where the output prediction per sample is the most common class; and the weighted combination, which linearly aggregates the base classifiers predictions","On the other hand,   techniques use predictions from component classifiers as features for a meta-learning model","As explained in  , weighted combinations of feature sets can be quite effective in the task of sentiment classification, since the weights of the ensemble represent the relevance of the different feature sets (e.g. n-grams, POS, etc.) to sentiment classification, instead of assigning relevance to each feature individually","The benefits of rule based ensembles were shown also in  , where several variants of voting rules are exhaustively studied in a variety of datasets, with an emphasis on the complexity that results from the use of these approaches","In a different work,   have compared the majority voting rule with other approaches, using three types of subjective signals: adjectives, emoticons, emphatic expressions and expressive elongations","They report that adjectives are more impacting that the other considered signals, and that the average rule is able to ensure better performance than other types of rules","Also, in   a meta-classifier ensemble model is evaluated, obtaining performance improvements as well","An adaptive meta-learning model is described in  , which offers a relatively low adaptation effort to new domains","Besides, both rule based and meta-learning ensemble models can be enriched with extra knowledge, as illustrated in  ","These authors propose the use of a number of rule based ensemble models, namely a sum rule and two weighted combination approaches trained with different loss functions",The base classifiers are trained with n-grams and POS features,These models obtain significant results for cross-domain sentiment classification,"As for the second dimension,   models divide the original dataset into several subsets from which multiple classifiers learn in a parallel fashion, creating a classifier composite",The most popular technique that processes the sample concurrently is bagging ( ),Bagging intends to improve the classification by combining the predictions of classifiers built on random subsets of the original data,"On the contrary,   approaches do not divide the dataset but there is an interaction between the learning steps, taking advantage from previous iterations of the learning process to improve the quality of the global classifier","An interesting sequential approach is boosting, which consists in repeatedly training low-performance classifiers on different training data",The classifiers trained in this manner are then combined into a single classifier that can achieve better performance than the component classifiers,"An example of bagging performance in the sentiment analysis task can be found in  , where bagging and other classification algorithms are used to show that the sentiment evolution and the stock value trend are closely related.   also show several experimental results in relation to the bagging techniques, attending also to the associated model complexity","Moreover, some authors have shown that bagging techniques are fairly robust to noisy data, while boosting techniques are quite sensitive ( )",The suitability of bagging and boosting ensembles is also experimentally confirmed by  ,"This work also includes the study of a different ensemble technique, random subspace, that consists in modifying the training dataset in the feature space, rather than on the instance space","The authors stand out the better performance of random subspace in comparison with similar approaches, such as bagging and boosting",Another study ( ) shows a comparison between bagging and boosting on a standard opinion mining task,"Besides,   proposes a three phase framework of multiple classifiers, where an optimal subset of classifiers is automatically chosen and trained",This framework is tested in several real-world datasets for sentiment classification,"Nevertheless, these works also show that ensemble techniques not always improve the performance in the sentiment analysis task, and that there is not a global criteria to select a certain ensemble technique. 2.2 Deep learning approaches In the realm of Natural Language Processing much of the work in deep learning has been oriented towards methods involving learning word vector representations using neural language models ( )","Continuous representations of words as vectors has proven to be an effective technique in many NLP tasks, including sentiment analysis ( )","In this sense,   is one of the most popular approaches that allows modeling words as vectors ( )",Word2vec is based on the Skip-gram and CBOW models to perform the computation of the distributed representations,"While CBOW aims to predict a word given its context, Skip-gram predicts the context given a word",Word2vec computes continuous vector representations of words form very large datasets,"The computed word vectors retain a huge amount of syntactic and semantic regularities present in the language ( ), expressed as relation offsets in the resulting vector space","These word-level embeddings are encoded by column vectors in an embedding matrix   ∈ IR , where | | is the size of the vocabulary",Each column   ∈ IR  corresponds to the word embeddings vector of the  -th word in the vocabulary,"The transformation of a word   into its word embedding vector   is made by using the matrix-vector product:
 where   is an one-hot vector of size | | which has value index at   and zero in the rest","The matrix   components are parameters to be learned, and the dimension of the word vectors   is a hyper-parameter to be chosen","The vector representations computed by these techniques can result very effective when used with a traditional classifier (e.g. logistic regression) for sentiment classification, as shown by  ","An approach based in word2vec is   ( ), that models entire sentences or documents as vectors","An additional method in representation learning is the auto-encoder, which is a type of artificial neural network applied to unsupervised learning","Auto-encoders have been used for learning new representations on a wide range of machine learning tasks, such as learning representations from distorted data, as illustrated in  ","In deep learning for SA, an interesting approach is to augment the knowledge contained in the embedding vectors with other sources of information","This added information can be sentiment specific word embedding as in  , or as in a similar work, a concatenation of manually crafted features and these sentiment specific word embeddings ( )","In the work presented by   the feature set extracted from word embeddings is enriched with latent topic features, combining them in an ensemble scheme",They also experimentally demonstrate that these enriched representations are effective for improving the performance of polarity classification,"Another approach that incorporates new information to the embeddings is described in  , in which deep learning is used to extract sentiment features in conjunction with semantic features.   describe an approach where distant supervised data is used to refine the parameters of the neural network from the unsupervised neural language model","Also, a collaborative filtering algorithm can be used, as is detailed in  , where the authors add sentiment information from a small fraction of the data","In the line of adding sentiment information, in   is portrayed how a sentiment Recursive Neural Network (RNN) can be used in parallel to another neural network architecture","In general, there is a growing tendency which tries to incorporate additional information to the word embeddings created by deep learning networks","An interesting work is the one described in  , where both sentiment-driven and standard embeddings are used in conjunction with a variety of pooling functions, in order to extract the target-oriented sentiment of Twitter comments",Enriching the information contained in word embeddings is not the only trend in deep learning for SA,"The study of the compositionality in the sentiment classification task has proven to be relevant, as shown by  ","This work proposes the Recursive Neural Tensor Network (RNTN) model, and it also illustrates that RNTN outperforms previous models on both binary and fine-grained sentiment analysis","The RNTN model represents a phrase using word vectors and a parse tree, computing vectors for higher nodes in the tree using a tensor-based composition function","In relation to the ensemble schemes showed in  , some authors ( ) have used a geometric mean rule to combine three sentiment models: a language model approach, continuous representations of sentences and a weighted BOW","That ensemble exhibits a high performance on sentiment estimation of movie reviews, and better performance that its component classifiers","To the best of our knowledge, a hybrid approach in which deep learning algorithms, classic feature engineering and ensemble techniques for sentiment analysis are used has not been thoroughly studied. 3 Ensemble taxonomyy This section presents the proposed taxonomy for ensemble techniques applied to Sentiment Analysis in both surface and deep domains",This classification intends to summarize the work found in the literature as well as to compare these models with the ones we propose,"Also, with this, we address the first question raised in   regarding how combination techniques can be classified",The taxonomy can be expressed as combination of two different dimensions,Each dimension represents a characteristic of the studied approaches,"On the one hand, one dimension considers which features are used in the model","Those features can be either surface features (which stands for  ), generic automatic word vectors ( ), or affect word vectors specifically trained for the sentiment analysis task ( )","On the other hand, the other dimension attends to how the different model resources are combined","These combinations can be: using no ensemble method at all, through a ensemble of classifiers, or taking advantage of a feature ensemble.  
                       shows a representation of this taxonomy, where the two dimensions appear as rows for the first dimension, and columns for the second dimension","We have classified all the reviewed work in this paper using the proposed taxonomy, obtaining a visual layout of the techniques that are used in each approach in relation with both ensemble methods and the combination of surface and deep features","Regarding the dimension that tackles the ensemble techniques, in the   category we find the classifiers that do not make use of an ensemble technique","Under the   category we classify the approaches that are based on ensemble techniques ( ), such as the voting rule or a meta-learning technique, to name a few","In the same manner, the   category contains the approaches that make use of feature combination techniques",The feature ensemble consists in combining different set of features into an unified set that is then fed to a learning algorithm,"As for the dimension that represents which features are used, several possibilities are represented: only surface features, generic or affect words vectors (  and   respectively), where only one type of feature is used","Besides, this dimension also takes into account the combination of different types of features:  +  (surface features combined with generic word vectors),  +  (generics word vectors with affect embeddings),  +  (surface features combined with affect word embeddings), and  + +  (all three types of features combined in the same model)","These two dimensions are combined, creating a grid where the different approaches can be classified","The blank spaces in the taxonomy represent techniques that, to the extent of our knowledge, have not been studied","As such, they represent work that can be addressed in the future","In conclusion, the introduced taxonomy provides a framework for characterizing and comparing ensemble approaches in sentiment analysis",This framework provides us with the opportunity to characterize and compare existing research works in sentiment analysis using ensemble techniques,"Moreover, the framework can help us to provide guidelines to choose the most efficient and appropriate ensemble method for a specific application. 4 Sentiment analysis models This section presents the sentiment analysis models proposed in our work",These models have been validated in the Twitter and movie reviews domains ( ),"First we describe the developed deep learning based analyzer used as baseline for the rest of the paper, and after this we detail the proposed ensemble models",These models are: ensemble of classifiers and ensemble of features,"Regarding the ensemble of classifiers, we tackle two main approaches in further experiments: fixed rule and meta-learning models. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   4.1 Deep learning classifier (M ) Generic word vectors, also denoted as pre-trained word vectors, can be captured by word embeddings techniques such as   ( ) and   ( )","Generic vectors are extracted in an unsupervised manner i.e., they are not trained for a specific task","These word vectors contain semantic and syntactic information, but do not enclose any specific sentiment information","Nevertheless, with the intention of exploiting the information contained in these generic word vectors, we have developed a sentiment analyzer model based on deep word embedding techniques for feature extraction, in order to compare it to other approaches in the task of Sentiment Analysis",The computed word vectors are combined into a unique vector of fixed dimension that represents the whole message,"Then, this vector is fed to a logistic regression algorithm","The computation of the combined vector can be made using a set of convolutional functions, or using a embedding that transforms documents into a vector","In this way, the proposed baseline model codification is input dependent, as can be seen in  ","In this paper, we propose the use of word2vec for short texts, where the word vectors of the text are combined with convolutional functions; and doc2vec for long ones, representing each document with a vector",The combination of word vectors from short texts are obtained through the   and   convolutional functions,These functions may be combined through the concatenation of its resulting vectors,"The combination of   of these functions produces a vector of   dimensions, where   is the original dimension of the word vectors","A diagram of this model is shown in  
                         ","We refer to this model as  
                         , with the G standing for generic word vectors. 4.2 Ensemble of classifiers (CEM) Our objective is to combine the information from surface and deep features",The most straightforward method is to combine them at the classification level,"In this way, we propose an ensemble model which combines classifiers trained with deep and surface features","Thus, knowledge from the two sets of features is combined, and this composition has more information than its base components",This model combines several base classifiers which make predictions from the same input data,These predictions can be subsequently used as new data for extracting a single prediction of sentiment polarity,"This ensemble model aims to improve the sentiment classification performance that each base classifier can achieve individually, obtaining better performance",There are many possibilities for the combination of the base classifiers predictions that outputs a final sentiment polarity (e.g. a fixed rule or a meta learning technique),"Also, any number of base classifiers can be combined into this ensemble model","A schematic diagram of this proposal is illustrated in  
                         ","We denote this model as  , which stands for Classifier Ensemble Model","The subscript indicates the types of features its base classifiers have been trained with, like in  
                         , where the ensemble combines classifiers trained with surface features and generic word vectors","Next, the two ensemble techniques used in this ensemble model in the experimentation section are further described. 
                         
                         
                         
                      
                         
                         
                         
                      4.2.1 Fixed rule model This model seeks to combine the predictions from different classifiers using a simple fixed rule","Consequently, this ensemble does not need to learn from examples",The rules used in this approach can be any fixed rule used in ensemble models,In this work the rule used for the ensemble is the voting rule by majority,This rule counts the predictions of component classifiers and assigns the input to the class with most component predictions,"In case of a match, a fixed class can be selected as the predicted by the model. 4.2.2 Meta classifier model In the meta-classifier technique, the outputs of the component classifiers are treated as features for a meta-learning model","One advantage of this approach is that it can learn, i.e. adapt to different situations","As for the selection of the learning algorithm of this approach, there is no indication as of which one should be used","In this work, we select the Random Forest algorithm, as it can achieve high performance metrics in sentiment analysis ( ). 4.3 Ensemble of features (M  and M ) This model is proposed with the aim of combining several types of features into a unified feature set and, consequently, combine the information these features give","In this way, a learning model that learns from this unified set could achieve better performance scores that one that learns from a feature subset","In this sense, we can distinguish two main types of ensembles of features",The first type is the ensemble of features that combines both surface and deep learning features,"We address to this first model type as M , as it combines surface features and generic word vectors",The second type consists on an ensemble of features that were completely extracted using deep learning techniques,"This second type is referred as M , combining both generic and affect word vectors","We refer to affect vectors as the result from training a set of pre-trained word vectors for a specific task, which in this case it would be SA","Additionally, we also propose a third feature ensemble model, where all the three types of features are combined","This model, where surface features, generic word vectors and affect word vectors are combined is denoted by M ",A diagram representing two instances of the model is shown in  . 5 Experimental study This section describes the experiments conducted in order to answer the questions formulated in the introduction ( ),"Each performance experiment is made with six different datasets, widely used by the community of Sentiment Analysis",The metric used in this work is the macro averaged F1-Score,"Accuracy, Precision and Recall are also computed for all the experiments, and the interested reader can find these results in the web 
                      ",We also publish the computed vectors that have been used in the deep models,These experiments ( ) are aimed to compare the performance between the deep learning baseline we have developed (M ) and the proposed ensemble models,"Also, some experiments ( ) are also aimed to characterize the sentiment analysis performance for each individual classifier of the CEM models","For the last purpose, we have collected several sentiment analyzers for composing a classifier ensemble","As for the sentiment analysis of natural language, it is conducted at the message level, so it is not necessary to split the input data into sentences","The classifiers label each comment as either positive or negative. 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   5.1 Datasets The datasets used for testing are   ( ),   ( ),   ( ),   ( ) and   ( )","Also, we use the   ( ) and IMDB datasets for training and developing our deep learning baseline, M ","These datasets are described next, and some statistics are summarized in  
                         ","The   test corpus is composed of English comments extracted from Twitter on a range of topics: several entities, products and events","Similarly, we have also use the   test dataset","In both   datasets, the data is not public but must be downloaded from the source first","As some users have already deleted their comments online, we have not been able to recover the original datasets, but subsets of it","Besides, since the development dataset contains only binary targets (positive and negative), we have made an alignment processing of the   datasets, filtering other polarity values",The obtained sizes are detailed in  ,"The   dataset contains 4200 tweet-like messages, originally inspired by real Twitter comments",A subset of these messages is specifically designed to test some syntactical and grammatical features that appear in the natural language,"The   dataset for Twitter, which has been collected as a complement for Twitter sentiment analysis evaluations processes ( )","As for the training data of our Twitter baseline model, the selected dataset is the   dataset, containing 1,600,000 Twitter messages extracted using a distant supervision approach ( )","The abundance of data in this dataset is very beneficial to our deep learning approach, as it requires large quantities of data to extract a fairly good model, as pointed out by  ","Regarding the movie reviews domain,   contains 50,000 polarized messages, using the score of each review as a guide for the polarity value","Besides, this dataset contains 50,000 unlabeled messages that have been used for training the movie reviews baseline model",We use this dataset for the training of the movie reviews baseline model,The   dataset is a well-known dataset in this domain,"For the results in this dataset, we report the 10-fold cross validation metrics using the authors’ public folds, in order to make our results comparable with the ones found in the literature. 5.2 Baseline training Due to the different characteristics of the two studied domains (Twitter and movie reviews), the vector computing process for the baseline model has been made differently","In the Twitter domain, the word vectors computed by word2vec are combined using convolutional functions","For the movie reviews domain, doc2vec is used for the combination of the word vectors","For the implementation of this model, we use the gensim library ( )",We found that the use of the convolutional functions in large text documents does not yield better performance than doc2vec combinations,"Hence, we performed an evaluation of these two approaches on the two development datasets","While the convolutional combinations yield a F1 score of 77.53% in the Sentiment140 dataset, they also achieve 73.66& in the IMDB dataset",When using doc2vec the F1 scores are 75.00% and 89.45% in the Sentiment140 and IMDB datasets respectively,"Considering the average number of words presented in   and the difference on the performance of each approach, we use the convolutional functions for the twitter domain, where short texts are analyzed and the doc2vec technique for the movie reviews domain, which contains large documents","Regarding the training process for the short text word embeddings, we empirically fixed the dimension of the word vectors generated to 500","We use 1,280,000 tweets randomly selected from the Sentiment 140 dataset","Once this model is extracted, we feed a logistic regression model (implementation from scikit-learn) with the vectors of each tweet and the labels from the original dataset","The movie reviews baseline has been similarly trained, with the 50,000 unsupervised documents of the IMDB dataset, setting the dimension of the document vectors to 100",The same linear model is use for the classification of the document vectors,"All the performance metrics have been obtained using K-fold cross validation, with folds of 10","With respect to the convolutional functions, we have conducted an effectiveness test of the   and   functions on the Sentiment140 development set","The results are shown in  
                         ","As can be seen, the   function is very close to the performance of the complete set of functions   and  ","Consequently, we select the   function as the one used for further experiments, as it provides very good results compared to the rest, and it also reduces the computational complexity of the experimentation","No pooling functions are used in the movie reviews, as there is no need to combine different word vectors","Lastly, the preprocessing of natural language, we tokenized the input data and removed punctuation, excepting the most common (‘.,!?’)","We also transformed URLs, numbers and usernames (@username) into especial characters to normalize the data","The preprocessing is applied to all the texts before generating the word vectors. 5.3 Ensemble of classifiers In order to improve the performance of the deep learning baseline, we have built an ensemble composed of this analyzer and six different sentiment classifiers","Following, a list and a brief description of each of these classifiers is shown:
 
                      We have built ensemble classifiers using two combining techniques in the CEM model: a rule based method and a meta learning approach, both using the predictions of the classifiers composing the ensemble as features for the next step","For the meta-learning approach, we use the implementation of scikit-learn of the Random Forest algorithm",For this algorithm we have used 100 as the default number of estimators,"As is shown in  
                         , the value of this parameter does not affect to the classification performance in the range from 50 to 1000","Additionally, two versions of the CEM model have been implemented for the experiments","While the CEM  combines the six aforementioned classifiers and the M  model; the CEM  version combines the base classifiers from CEM  with the M , M  and M  models. 5.4 Ensemble of features Based on the work by  , we have selected the following surface features: SentiWordnet ( ) lexicon values for each word, as well as total number of positive, neutral and negative words extracted with this lexicon; number of exclamation, interrogation and hashtags marks ‘!?#’; number of words that are all in caps and number of words that have been elongated ‘gooooood’","This feature set has been cross validated on the development sets, with the objective of obtaining the smaller surface feature set that yields the best classification performance","With the aim of complementing the surface features, we have also explored the role of n-grams","More specifically, bigrams are used, as the introduction of unigrams and trigrams did not improve the classification performance","The ensemble of features model that includes generic word vectors, the described surface features and bigrams is represented as M ","As for the M  model, we use the word vectors obtained by  ","More specifically, we use the vectors extracted using the SSWE  neural model","These vectors have been extracted for learning sentiment-specific word vectors but not general semantic information, so we use them as affect word vectors","Also, for the composition of a tweet vector, we have used the   function on the word vectors, as the combination of the other convolutional functions did not improve the results of the model. 6 Results The conducted experiments show the sentiment classification performance of each base classifier separately (including our deep learning baseline) on each of the six test datasets, as well as the metrics for the ensemble of classifiers and ensembles of features","In this section, the experimental results are shown and discussed","The experimental results for the proposed models are gathered in  . 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   6.1 Base classifiers performance As it can be seen in  
                         
                         , the better F-score performance is achieved by TextBlob in SemEval2013, by the WSD classifier with an important difference over TextBlob in SemEval2014; while pattern.en has a slightly better performance than the rest in the Vader dataset and has also the better performance in the STS-Gold dataset by far",The classifier with the lower performance is CoreNLP in all Twitter test sets,"In contrast, in the IMDB and PL04 datasets (movie reviews), CoreNLP achieves the better performance in IMDB and the second best in PL04","As expected, the nature of the domains has a strong impact on the performance of the base classifiers, since they have been trained in a specific domain","WSD, pattern and TextBlob) are more suitable to the Twitter domain, while others (e.g",CoreNLP and vivekn) are better adapted to the review domain,"In general, none of the base classifiers exhibits a high performance in all the baseline datasets","Finally, the average F1 score performance for the base classifiers is 73.02, 63.79, 75.32, 71.81, 81.20 and 77.41% in the SemEval2013, SemEval2014, Vader, STS-Gold, IMDB and PL04 datasets respectively. 6.2 Classifiers and features classifiers performance CEM models gather the predictions from M  baseline and the other six base classifiers whose classification performance has been analyzed",The voting and meta-learning techniques are used as ensemble techniques,"It can be seen in the   that nearly all the ensemble models surpass the baseline, as well as all the other base classifiers","In fact, the best performance is achieved in 4 out of 6 datasets by these CEM classifiers","As for the feature ensemble models, they also push the performance further than the baseline",The M  ensemble is very close to the best metrics in almost all the test datasets,"Also, it seems that M  is suffering overfitting, as the combination of all three types of features decreases the performance when comparing to M  model",This could be due to the increase in the number of features used to train the model,"Moreover, as an additional observation, it can be seen that the better improvements are achieved by CEM  and CEM  models, with 3.65 and 2.53% of performance gain in STS-Gold and SemeEval2013 datasets respectively, and by CEM  model in the IMDB and PL04 datasets, with improvements of 1.48% and 5.77% respectively","Although the biggest improvements have been achieved with CEM models, the feature ensemble models also improve the baseline, sometimes by a large margin","Considering this type of models, the better results are achieved by the M  model","This fact could be explained attending to fact that bigrams can successfully capture modified verbs and nouns ( ), such as the negation","Also, the M  model results are comparable, generally, to the best performances in the Twitter domain","Nevertheless, this model does not yield such results in the movie reviews domain",This result indicates that combining word vectors through convolutional functions in long texts does not lead to high sentiment classification performances,"We can conclude that the transformation of the convolutional functions on the sentiment signals that are contained in the affect word vectors is retained when applied to short texts, but lost in long texts","Attending to the difference of performance between the M  and the CEM  and CEM  models, we see that the same types of features do not yield the same result","We make the assumption that dividing the features into smaller sets, as it is done in the ensemble models, benefits the classification performance","The division is made at the classifier level, since these ensemble models combine the predictions of classifiers trained with features (e.g. surface, generic and affect)","Considering that the whole set SGA (including bigrams)is a complex collection of features, and based on the experimental results, the assumption is that this division of features prevents overfitting",These results are in line with those by   and  ,"This shows that when dividing a complex set of features into simpler subsets, an ensemble can yield better performance. 6.3 Statistical analysis In order to compare the different proposed models in this work, a statistical test has been applied on the experimental results","Concretely, the Friedman test with the corresponding Bonferroni-Dun post-hoc test, that are described by  ",These tests are specially oriented to the comparison of several classifiers on multiple data sets,"Friedman’s test is based on the rank of each algorithm in each dataset, where the best performing algorithm gets the rank of 1, the second best gets rank 2, etc",Ties are resolved by averaging their ranks.   is the rank of the  th of the   algorithms and on the  -th of   datasets,"Friedman test uses the comparison of average ranks   and states that under the null-hypothesis (all the algorithms are equal so their ranks   are equal) the Friedman statistic, with   degrees of freedom, is:
 
                      Nevertheless,   shows that there is a better statistic that is distributed according to the F-distribution, and has   and   degrees of freedom:
 
                      If the null-hypothesis of the Friedman test is rejected, post-hoc tests can be conducted","In this work we employ the Bonferroni-Dunn test, as it allows to compare the results of several algorithms to a baseline","In this case, all the proposed models are compared against M ","This test can be computed through comparing the critical difference (CD) with a series of critical values ( ), which   summarizes","The critical difference can be computed as:
 
                      For the computation of both tests, the ranks have been obtained",The average ranks ( ) are showed in  ,The   values is set to 0.05 for the following calculations,"With those averages,  
                          and the critical value  ","As   >  (8, 40), the null-hypothesis is rejected and the post-hoc test can be conducted","According with this, the critical difference is 4.31","Following, the difference between the average ranks of the baseline and the  th model is compared to the CD and, if greater, we can conclude that the  th algorithm performs significantly better than the baseline","Friedman’s test has pointed the CEM  and the M  models as the two best classification models, followed by the CEM  and M  models","Besides, the Bonferroni-Dunn test points out that CEM  and M  models perform significantly better that the baseline","These results indicate that the hypothesis suggested in question 2 is supported, since the combination of different sources of information improves the performance of sentiment analysis","As for the rest of the models, it is not possible to reach a conclusion with the current data","On top of this, an interesting result of these experiments is that the performance of the meta learning approach is higher than that of the fixed rule scheme","While the meta learning ensemble with all types of features (SGA + bigrams) is significantly better than the baseline, the voting model is not","This could be caused by the learning capabilities of the meta-classifier technique, feature that the fixed ensemble methods like voting rule do not have. 6.4 Computational complexity One possible drawback of ensemble approaches is their higher cost in terms of computational resources","With the aim of analyzing the efficiency of the proposed models, the computational cost is presented","The results for this cost analysis are studied at train time, since the costs in the test phase do not result relevant for this analysis",All these measurements were made in a Intel Xeon with 12 cores available and a memory friendly environment,"In relation to the training and computation of the word embeddings approaches,  
                          presents the associated computational cost",It can be seen that word2vec is the lighter model at train time by a large margin,"Also, the implementation of SSWE largely increases the computational complexity",We believe that implementing this model for a GPU environment can have a great impact on the time performance of the SSWE training,"Please note that the SSWE trained model for Twitter is available for research, and so the training using the Sentiment140 dataset has not been performed","Besides, we can see that computing the pooling functions on the word vectors increases the complexity, as can be seen by comparing with the doc2vec approach","Combining different sets of features increases the computational complexity, as  
                          shows","The largest increment can be found in the M  and M  models, which use bigrams in the learning process","In this way, it can be seen that using feature ensemble with bigrams and other sets of features leads to the addition of complexity to the model","The difference of training times between the Sentiment140 and IMDB datasets is due to their number of instances, being larger in the first","Finally, the CEM models do not introduce a relevant complexity to the model at training time","The ensemble of classifiers based on the voting scheme do hardly introduce a cost to the computation, as there is no learning process in this case","For the meta-learning scheme, the maximum time taken in the meta learning process is 1.5 s, with no significant difference between the training data. 7 Conclusions and future work This paper proposes several models where classic hand-crafted features are combined with automatically extracted embedding features, as well as the ensemble of analyzers that learn from these varied features","In order to classify these different approaches, we propose a taxonomy of ensembles of classifiers and features that is based on two dimensions","Furthermore, with the aim of evaluating the proposed models, a deep learning baseline is defined, and the classification performances of several ensembles are compared to the performance of the baseline","With the intention of conducting a comparative experimental study, six public datasets are used for the evaluation of all the models, as well as six public sentiment classifiers","Finally, we conduct an statistical analysis in order to empirically verify that combining information from varied features and/or analyzers is an adequate way to surpass the sentiment classification performance",There were three main research questions that drove this work,The first question was whether there is a framework to characterize existing approaches in relation to the ensemble of traditional and deep techniques in sentiment analysis,"To the best of our knowledge, our proposal of a taxonomy and the resulting implementations is the first work to tackle this problem for sentiment analysis",The second question was whether the sentiment analysis performance of a deep classifier can be leveraged when using the proposed ensemble of classifiers and features models,"Observing the scores table and the Friedman ranks ( ), we see that the proposed models generally improve the performance of the baseline","This indicates that the combination of information from diverse sources such as surface features, generic and affect word vectors effectively improves the classifier’s results in sentiment analysis tasks","Lastly, we raised the concern of which of the proposed models stand out in the improvement of the deep sentiment analysis performance","In this regard, the statistical results point out the CEM  and M  models as the best performing alternatives","As expected, these models effectively combine different sources of sentiment information, resulting in a significant improvement with respect to the baseline","We remark the M  model, as it does not involve an ensemble of many classifiers, but only a classifier that is trained with an ensemble of deep and surface features","To summarize, this work takes advantage of the ensemble of existing traditional sentiment classifiers, as well as the combination of generic, sentiment-trained word embeddings and manually crafted features","Nevertheless, Considering the results of this work, we believe that a possible line of work would be applying these models to the task of aspect based sentiment analysis, with the hope of improving the classification performance","Furthermore, we intend to extend the domain of the proposed models to other languages and even paradigms, like Emotion analysis.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0963868717301798, 1 Introduction The challenge of socializing newcomers has become an ever more pressing issue for organizations as the nature of work has increasingly shifted from long term employment within a single employer marked by slow but steady upward progression to more short-term positions and lateral movements across a variety of different organizations ( ),"With organizational affiliation waning, occupational affiliation has been on the rise","Whereas in the 1970s, workers were more likely to change their occupation than their employer, by the early 1990s, changing employers had become more common than changing occupations ( )","Information technology (henceforth, IT) workers are among those who demonstrate greater occupational than organizational loyalty ( )",The problem of employee flight is substantial: the cost of losing an employee is up to 3 times the employee’s salary ( ),"According to an IT staffing company, the direct and indirect costs incurred by organizations in replacing a single employee who makes $60,000 per year reach approximately $150,000 ( )","The lack of organizational loyalty is important not just in terms of the costs an organization faces in hiring and training replacements, but also in the productivity losses incurred when well-trained IT workers leave a project before completion and the team must either redistribute the work or integrate a new member","So significant is the problem of IT talent and retention, that the issue has been rated by CIOs as the 2nd or 3rd most important issue facing IT leaders for five consecutive years in the SIM survey on IT issues and Trends ( )",One way that organizations may increase employee loyalty to the organization is through socialization programs ( ),"Facing large numbers of new IT workers entering the workforce ( ) as well as the challenge of integrating experienced workers, IT departments are showing increased interest in socialization programs designed not just to train new employees in task-related skills, but also to instill a sense of loyalty to the organization in hopes of increasing the organizational affiliation of its IT workforce","Given the costs associated with hiring and training new IT employees as well as the loss in productivity incurred when valuable employees leave, the issue of effectively socializing new IT employees is of strategic importance to IT departments","Socialization is the process whereby newly hired employees learn the beliefs, values, orientations, behaviors, social knowledge, and work place skills necessary to successfully fulfill their new organizational roles and responsibilities ( )","Socialization leads to positive outcomes such as better job performance, less stress, higher job satisfaction, and reduction in intent to leave ( )","While the benefits of socialization are clear, the means of achieving effective socialization are complex with many tools and techniques available","Historically, socialization programs have relied upon formal onsite orientation sessions, offsite training sessions, buddy systems, mentoring programs, and business trips with co-workers ( )","Recently, organizations have begun implementing enterprise social media (ESM) as an informal organizational socialization tool","Social media allows users to create, edit and exchange web-based content ( ), thereby enabling organizations and employees to foster relationships, share knowledge and collaborate ( )","ESM have a role to play in organizational innovation, operations, and human relations ( )",Considering the potential role of ESM in an organization’s IS strategy is important for organizations that wish to realize business value from ESM ( ),Academic and practitioner research has encouraged IS managers to develop a social media strategy based on the capabilities of social media platforms to manage interpersonal networks and share content,These capabilities are well-suited for socialization programs ( ),"Organizations have begun using ESM systems to help new employees learn about their jobs, their colleagues, and the organization ( )",ESM enables fast and extensive knowledge sharing and facilitates open conversations ( ) both of which can foster new hire socialization,"Moreover, ESM provide various opportunities such as self-marketing and relationship building that extend beyond the embedded functions and features of the technology and that may hold important ramifications for new hire socialization and, in essence, make the socialization process an “open” one","Much as ESM has been shown to enable open strategizing with a resultant sense of community and stronger organizational commitment ( ), ESM may enable open socialization wherein active participation may result in a strong sense of community and commitment","However, the multivocality enabled through ESM in which more voices are heard and more messages are generated ( ) may shift the control of organizational communication away from central, largely senior, sources to employees who have access to, and choose to engage with, the ESM","While such participation changes the rhetorical practice of organizations, in a sense democratizing the practice ( ), it may also create conflicts and tensions ( )","For example, in the context of open strategy, ESM has been shown to create tensions between the participatory practices of the technology and the existing managerial practices ( )",Such tensions might also be created in the application of ESM to organizational socialization practices,"Formal socialization programs have been carefully scripted by senior management to convey a desired organizational message, culture, and mission",The introduction of ESM as informal socialization tools has the potential to threaten this careful scripting and disrupt the cultural norms of the organization,"ESM thus have both the potential to foster a greater sense of community and organizational commitment, but also the potential to create tensions","Given the strategic importance of socialization in the current organizational context of decreasing organizational commitment marked by frequent job changes, ESM for socialization are strategically important systems and must be mindfully implemented in order to produce effective results","Despite the strategic importance of ESM systems in organizations (Gartner  ) and the strategic importance of attracting, training and retaining a skilled IT workforce ( ), few studies to date have investigated the use of ESM for employee socialization ( )","In order to contribute to our understanding of how ESM affects employee socialization, this paper invokes a case study of an organization that had recently incorporated ESM into its IT new hire program","Drawing upon technology affordance theory as our lens, we address the following research question: how do ESM affordances influence the socialization of IT new hires? This paper is organized as follows",We first provide the theoretical foundation,"We then present the method, a case description and the analysis followed by the implications, limitations, and conclusion. 2 Theoretical foundation Our investigation draws from organizational socialization research ( ) and from the technology affordance perspective of information systems ( )",The research on organizational socialization informs our understanding of the socialization process,"We then apply the technology affordance perspective as the theoretical foundation for understanding how and why ESM may alter socialization processes and outcomes. 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   2.1 Organizational socialization Organizational socialization is a learning process wherein newly hired employees acquire the requisite knowledge, skills, values, and norms to enable them to perform their roles in their organization ( )","Four elements comprise the socialization process: task mastery (learning how to perform one’s job), role clarification (gaining an understanding of one’s job), acculturation (adjusting to the organization’s culture), and social integration (developing relationships with others in the organization, especially peers and superiors) ( )","Effective socialization practices are those that enable newly hired individuals (henceforth, new hires) to achieve proximal outcomes of self-efficacy, role clarity, knowledge of organizational culture, and a sense of belongingness ( )",The socialization process can take place formally via institutionalized socialization and training programs as well as informally through interactions among employees and observation,"Indeed, how one is socialized is as important as the content of socialization ( ) and the initial socialization experience has implications for perceptions, behaviors and attitudes that remain throughout an individual’s employment in the organization ( )","As the importance of informal socialization practices became recognized and as the organizations into which individuals were being socialized became increasingly characterized by distributed teams and virtual communities, the potential importance of information technology in socialization processes began to receive research attention","IT has been shown to play an important role in not only the initial socialization into a group, but also in the ongoing socialization that is particularly important in distributed and/or virtual settings ( )","According to  , difficulties in sharing norms, attitudes, and behaviors can be alleviated by the use of electronic communication and collaboration tools before, during and after face-to-face meetings","For example, video-conferences can be used to introduce new team members to each other, which may serve as an important socialization tool prior to a face-to-face meeting of the team whereas email may be used to clarify key points both before and after face-to-face encounters",Some research indicates that the formation of virtual communities can assist in the socialization of employees,"In this case, the role of IT is to enable communication and knowledge sharing which facilitate learning, identity formation, and relationship development ( ), all of which are considered essential components of socialization","To date, the research on IT in socialization has largely focused on traditional communication and collaboration tools (e.g., email, video conferencing, intranets, on-line chats) ( ) and on knowledge-sharing platforms ( )","IT use in these studies has focused on individuals’ behaviors (e.g., information seeking vs. contribution) and the content of information exchange (such as questions, internal documents, and clarifications)","To further advance our understanding of the role of IT in socialization, we investigate a new technology being used for socialization (ESM)",Our research seeks to uncover the mechanisms through which ESM influences the socialization processes and socialization outcomes of organizational new hires,"In order to delve deeply into the question of how ESM influences organizational socialization, we draw from the theory of technology affordance. 2.2 Technology affordances perspective In the IS literature (e.g.,  ), affordances refer to possibilities for action offered to an individual by an object ( )","An affordance is a property of the relationship between an actor and an object ( ) and thus, represents an opportunity for action ( )",One affordance arising from the relation between a user and a technology can provide multiple affordances and produce multiple outcomes ( ),"In the same manner, the interaction between the user and the technology can afford actions that create hindrances","In spite of its growing prevalence in IS studies, IS researchers have yet to agree on how to distinguish technology affordance from technology use","The term affordances has been described in manifold ways such as: “what is offered, provided, or furnished to someone or something by an object,” “a property of the relationship between an object and an actor which is defined as an opportunity for action,” “the potential for behaviors associated with achieving an immediate concrete outcome,” (all the above from  ), “the action potential that can be taken given a technology” ( ), “a relational construct linking the capabilities afforded by technology artifacts to the actors’ purposes” ( ), “the possibilities of using select features or combinations of features in a way meaningful to the user’s goals, abilities, and line of action” ( ), and as something “constituted in relationships between people and the materiality of the things with which they come in contact” ( )",These views of affordance emphasize that affordance is an action (or potential for action until it has been actualized) and that it is a fundamentally different perspective than merely looking at technology use or technology feature use,"However, in spite of this conceptual separation of use and affordances in word, in practice, much of the IS affordance research does not sufficiently distinguish between features, use, and action.  , for example, describe four affordances of social media: persistence, visibility, editability, and association","However, these are not actions","Rather, these are attributes of the technology",Other affordances research mingles the concepts of feature use and affordance,"For example,   assign the affordance label “metavoicing” to the action of “reacting online to others’ presence, profiles, content and activities.” Yet reacting online via voting or commenting or other social media features is a direct use of the features of social media","Similarly, when describing electronic health record (EHR) affordances,   label as an affordance the “capturing and archiving digital data about patients”, yet capturing and archiving data are using EHR features to capture and record data","Likewise, they label “accessing and using patient information anytime from anywhere” as an affordance whereas these are again direct uses of system features, as is “monitoring organizational operations.” Moreover, the literature on affordance has been inconsistent in carefully distinguishing the outcome of affordance actualization from the affordance itself","For example,   identify “capturing and archiving digital data” and “standardizing data, processes, and roles” as EHR affordances","They then identify as outcomes the fact that “digital data about patients are captured and archived” and that “data, processes, or roles are standardized”",The outcomes are the same as the affordances,"Because of the failure to meticulously distinguish use from affordances and affordances from outcomes, the result is that the distinction between use, affordances, and outcomes becomes muddled","To clarify our position on these conceptual distinctions, we provide an example of commuting to work",One might choose to ride a train to commute to work,Riding the train is the equivalent of using the technology,"In this case, the technology in question (e.g., the train) is an object that moves","By definition, to “ride” is to be carried by an object that moves","As one uses the technology (e.g., rides the train), one might engage in various affordances, such as working, sleeping, meditating, or conversing with another passenger","These affordances are possible by virtue of the fact that the individual chose to ride the train (e.g., use the technology)","One could achieve these same affordances via other means, such as if one took a bus or a taxi to work and one could also achieve these affordances without going to work at all","However, if the goal is to get to work and one takes advantage of a moving object to get to work (e.g., one rides the moving train), then as one uses the object to achieve a particular goal (getting to work), one may benefit from other affordances along the way","Riding the train is the direct use of the object whereas working, sleeping, meditating, or conversing are not uses of the train itself, but affordances made possible by the train ride","One might be tempted to say that the outcome is that the individual arrives at work, but this is the outcome of riding the train, not the outcome of the affordances produced by riding the train","An outcome of affording the ride on the train to work, for example, may be that the individual completes more work in a given day than he would if he drove to work",Or perhaps the outcome of the individual who afforded the ride to meditate is that this individual arrives at work in a relaxed state of mind,The affordance lens is a powerful tool for helping IS researchers understand the choices made regarding a technology and the consequences of these choices,"We suggest that to move forward in affordance research, it is important to carefully and intentionally separate technology use from technology affordance, and technology affordance from outcomes of the affordance, in order to understand how the use of technology features provide affordances to individuals and how these affordances produce outcomes",The affordance perspective has both theoretical and methodological implications,"Theoretically, it helps provide an explanation of how and why technology produces affordances and outcomes","Methodologically, it requires researchers to carefully distinguish between use, affordance, and outcomes in their analysis","Applying this to our context of ESM and new hire socialization, an affordance perspective will allow us to investigate the interactions between new hires and the ESM in ways that go beyond the use of the ESM’s features in order to explain how the affordances actualized by new hires affects their socialization into the organization. 3 Method Because studies of ESM within organizational socialization programs are scarce, we chose to study one case in depth","In   study of IS positivist case research, 60% of all studies they found were of a single case with 40% being multi-case studies","Since Dubé and Paré’s analysis, single case studies continue to be well represented in top IS journals (e.g.,   to name but a few) because of their potential to discover new insights through unique, extreme, or revelatory cases","According to  , a single case study is appropriate in three situations when the case: (1) represents the critical situation in testing a well-formulated theory, (2) represents an extreme or unique instance, or (3) is a revelatory inquiry","In this latter case, a researcher has an opportunity to observe and analyze a phenomenon previously inaccessible to scientific investigation",Our case fits the second and third situations,"At the time we began collecting data, the case was very unique",Organizations were just beginning to adopt ESM and IT departments were not widely using ESM and certainly not as part of a new hire program,"Moreover, it was not previously possible to study how social media influences the socialization of new hires, because as a phenomenon, it had only begun to exist. 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   3.1 Data collection We refer to our case organization as Financial Services Plus (FSP), a pseudonym",Our data collection spanned the course of 8 years,Our continuous involvement over a long period of time allowed us to acquire a deep contextual understanding of the IT department and its new hire program and rich insights into the interactions of new hires with the ESM,"Data collection consisted of face-to-face interviews with new hires, middle managers and executives","We collected additional data by attending events, meeting with employees during off time (i.e., dinner and or breaks), observations, and reading weekly journals maintained by new hire interns","Since 2007, we have conducted over 100 interviews and 8 focus groups with 50 of FSP’s professional IT and human resource employees. 
                         
                          lists the demographics of the employees who participated in our interviews.   in the appendix details the focus groups conducted","During the focus groups, we had round-table discussions with multiple participants",These discussions were very important to our understanding of the ESM and the organizational context,We recognize the possibility that focus group settings might constrain a participant’s answers,"We therefore rely on the focus group observations as helpful in understanding the context surrounding the introduction and use of the ESM, but base our detailed analysis on the interview data","Culturally, our new hire interviewees were similar",Most participants were United States citizens who had recently graduated from a 4-year degree program in management information systems or computer science,"Employees in a variety of roles (i.e., new hires, managers, and human resource professionals) took part in our interviews and focus groups","Questions centered around what the ESM allowed the new hires to do, what ESM activities they participated in, what happened once they started using the ESM system, and challenges the system created",The interviews lasted between 30 and 60 min,Most interviews were recorded and transcribed,"In cases where it was not possible to record (for example, a few interviews with managers took place over lunch and background noise interfered with recording), detailed notes were taken. 3.2 Data analysis Because our aim was to understand how the affordances actualized from the ESM created outcomes for new hires, we adopted a critical realist data analysis approach focused on uncovering the generative mechanisms that explain empirical outcomes","As explained by  , the identification of affordances helps researchers specify mechanisms that explain the outcomes of the introduction of new technology in organizations",Generative mechanisms are the causal structures that explain empirical outcomes ( ),"Here, we do not expound on the principles of critical realism because these are well addressed in the IS literature (see, for example,  )",Less well explained are the specific data analysis procedures one should follow in seeking to identify generative mechanisms,"Authors describe various procedures.   describe four steps: identifying the critical events; explicating the structure and context from the event analysis; identifying generative mechanisms through retroduction; and confirming the generative mechanisms through empirical corroboration.   describe a four step process of using open coding to identify key events: identifying the objects of the case, identifying key mechanisms through retroduction, and analyzing contextual conditions and outcomes of the mechanisms.   also describe four steps in the DREI methodology: describe the events, retroduce explanatory mechanisms, eliminate false hypotheses, and identify correct mechanism.   provide a six step framework: description of events and issues; identification of key entities; theoretical re-description; retroduction (identification of immediate concrete outcomes, analysis of the interplay of human and technical entities, identification of candidate affordances, and identification of stimulating and releasing conditions); analysis of the set of affordances and associated mechanisms; and assessment of explanatory power","Taking inspiration from these various suggestions on how to analyze data from a critical realist perspective, we undertook a five step process",The first step was an open coding of the transcripts and notes with a view towards identifying key events in the new hire program leading to and following the introduction of the ESM and identifying the features and functionalities of the ESM,The second step involved an analysis of the perceived outcomes of the ESM,This step entailed another round of data coding wherein we looked specifically for references to the impact of the ESM on the new hires,"This process was iterative in that we began with a long list of stated outcomes, but then developed general categories within which to group similar outcomes",The third step entailed coding for affordances,This step involved carefully reading the interview transcripts and field notes to look for statements about how the ESM was used,It was critical in this stage to separate direct use of system features from the affordances such use provided,"For example, creating user profiles is a use whereas building relationships with peers is an affordance","This step was very iterative with the three researchers working independently to identify candidate affordances, discuss them, refine them, and return to the data to corroborate them with examples","As previously mentioned, prior research discussing social media affordances has tended to confound use of features with affordances","We therefore began our analysis of affordances with a clean slate, allowing the affordances to emerge from the data",The fourth step was our retroduction in which we linked affordances into strands of affordances and associated these affordance strands with particular outcomes,"Through this process of linking affordances into strands (or patterns of actualized affordances) and affordance strands to outcomes, we were able to abstract the generative mechanisms","Our final step was to establish the context of the affordances, outcomes, and mechanisms","In this step, we looked for insights into why some new hires experienced socialization and others did not","This analysis revealed three types of users who actualized different affordance strands and experienced different socialization outcomes. 3.3 The FSP case description FSP is one of the largest providers of financial planning, investments, insurance, and banking in the United States",FSP’s IT department houses 2500 employees among which roughly 10% are new hires,"FSP’s IT department has historically suffered from a 60–70% new hire turnover rate, reflecting the IT department’s struggle to acclimate and socialize new hires into its IT workforce","To improve the organizational socialization of new hires, FSP charged an IT director with revamping FSP’s new hire program","The IT director leveraged social media technologies and implemented an ESM tool, called OnBoard, a pseudonym","OnBoard is a Web 2.0 technology that consists of features inclusive of, but not restricted to, social networks, discussion forums, micro blogs, and profile pages",OnBoard consists of a technical and social system,The social system consists of face-to-face events and meetings,The technical system consists of the social media platform,OnBoard complemented the standard 6-week orientation period by providing a platform for new hires to get to know one another and stay in touch during their 3-year new hire program,Leadership of the ESM comes from a core team of six IT new hires responsible for creating and maintaining the OnBoard technical and social system,The IT new hire program director chooses these leaders from a pool of new hires who have been identified by existing core team members and who have expressed interest in leading OnBoard,"Core team members can serve a maximum of two years, but most serve one year","Within the second year of OnBoard’s implementation, the human resources (HR) director recognized IT new hires’ involvement with OnBoard’s socializing activities and officially integrated OnBoard as part of the HR’s organizational recruiting and onboarding efforts",HR gives new hires access to OnBoard as soon as they accept a position so that they can start connecting with other new hires,"Then, after going through FSP’s new employee training program, new hires use OnBoard to continue their socialization into the organization","To date, executives credit Onboard with reducing turnover, increasing employee engagement, and improving morale","Middle managers who had previously been involved in mentoring new hires report spending less time as a mentor, something they viewed as a personal benefit of OnBoard","In addition, executives began using OnBoard to solicit input from new hires on new products and services FSP was considering","Whereas the organizational perspective of OnBoard’s outcome was overwhelmingly positive, the new hires’ perspectives of OnBoard were more nuanced","New hires reported a wide range of outcomes from OnBoard, including such positive outcomes as productivity enhancement, attractive job assignments, comfort around superiors, and a sense of support as well as negative outcomes such as additional work, stress, and social struggle  ","In the appendix lists the outcomes with supporting quotes and examples. 4 Case analysis We begin our analysis by describing the three types of users that emerged from our analysis ( 
                      )","We label the three types as go-getters, work-players, and just-doers ( )",The go-getters were the most active OnBoard users,They used many features on a daily basis and viewed their engagement with OnBoard as an opportunity to grow professionally,"The work players were active OnBoard users and engaged in both social and work-related uses of OnBoard, but tended to not take leadership roles that demanded time and energy","The just-doers were the least active users of OnBoard, consuming, but not contributing, information and avoiding activities that were not directly work-related","Of the 31 new hires and interns interviewed, 12 were go-getters, 11 were work-players, and 8 were just-doers","Following  , we will highlight the differences across these three groups as we describe the affordances, generative mechanisms and outcomes. 
                      
                       shows the system features, use of the features, and the associated affordances","As noted, we carefully distinguish between use of the features and the affordances provided by such use","Because we are using affordances to identify the generative mechanisms connecting the OnBoard system to outcomes, we will only briefly describe the affordances. 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   4.1 Affordances 
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      4.1.1 Networking affordances OnBoard affords users the ability to build relationships, interact with peers, socialize both during and after working hours and take a break during the workday","New hires’ first OnBoard exposure precedes their first workday, when they use OnBoard to connect with FSP new hires that graduated from their university (Affordance 1,  )","A new hire described his pre-first day experience using OnBoard as “the best type of networking you can do because it allows you to have a connection with someone prior to your first day at work.” During orientation, OnBoard provides a way for new hires to get to know one another by facilitating open communication (Affordance 2,  )","When formal orientation concludes and new hires enter their various work groups, OnBoard allows them to maintain connections from their hiring class","By promoting interactions, informal online communication, and socializing, OnBoard helps new hires become friends with co-workers","New hires can plan informal meet-up events that occur outside of work hours (Affordance 3,  )","Meet-up events may include playing sports, picnicking or other forms of entertainment",OnBoard’s search feature enables new hires to find others with similar interests,"Then new hires reach out to those with similar interests to chat online and take a break (Affordance 4,  )","As a result of regular interactions, new hires meet after working hours to socialize and decompress from the rigor of the workday",It is through this type of interaction that new hires establish relationships that reach beyond their departmental boundaries,"Although go-getters, just-doers, and work-players all actualize the affordance of establishing relationships and interacting with peers to some extent, the just-doers did not actualize the affordances of socializing or taking a break",Their tendency is to only actualize affordances that directly apply to their work responsibility,"Consequently, just-doers develop a smaller and work-focused network in comparison to the go-getters and work-players. 4.1.2 Organizational visibility affordances OnBoard affords organizational visibility to IT new hires by providing opportunities for them to participate in OnBoard sponsored events, build peer relationships, develop and demonstrate leadership skills, and interact with superiors",All new hires who attend an OnBoard event have the possibility to interact with executives,"Events have included executive luncheons, casino nights, coding competitions, and cross-fit workouts",All events must have an executive who has agreed to sponsor and attend the event,This rule serves as an important enabler of the visibility affordances,"However, those new hires who lead events, (e.g., the go-getters) work much more closely with executives than do the work-players and just-doers who, at most, attend the event and briefly meet the executives","A go-getter comments: “OnBoard has helped me develop some leadership at an early stage in my career; it made me aware of how to get things done.” Another go-getter discussed how OnBoard allowed leaders to “promote the event, seek volunteers, connect with the next lead, and give event updates.” This type of exposure gives new hires a chance to make a name for themselves in front of management and peers","In the words of one go-getter, “I know so many more executives outside of my department than most of my teammates do","There’s no telling ten years down the road what promotional opportunities I’ll have and what these connections will do for me.” As the go-getters actualize the affordance of demonstrating leadership skills (Affordance 7,  ), they create an affordance of interacting with superiors (Affordance 8,  ) for the just-doers and the work-players","While the work-players will take advantage of such of an opportunity, the just-doers are less likely to participate in such events and pass on this affordance","By participating in OnBoard events (Affordance 5,  ), the work-players informally meet senior management and executives","Informal interaction with executives through participation in OnBoard events (e.g., Wounded Warrior, paintball, American Idol, and others) made new hires feel comfortable around superiors","Benefits of this include helping new hires approach superiors with less hesitation, relieving pressure in formal meetings, making new hires feel that management is interested in their well-being, and that they matter. 4.1.3 Information gathering/sharing affordances OnBoard affords new hires the ability to find resources and help peers as they settle into their new community","OnBoard provides various web pages (e.g., apartment lists, roommate lists, carpools, and recommended restaurants) with information to aid the new hires in their search for housing, transportation and shopping",All new hires who use OnBoard have the possibility to actualize the affordance that helps them gain or share information,A go-getter who is “not from this area” discussed how OnBoard made her aware of local businesses that give discounts to FSP employees and helped her find housing and a roommate,The information gathering affordance was especially helpful early on when new hires were embarrassed to admit what they did not know,"As explained by a work-player: 
                         This information gathering/sharing affordance was particularly helpful when new hires were struggling with assignments in that it linked them to information that they needed to complete their tasks more efficiently","For example, a work-player talked about how OnBoard introduced him to a tool that would automatically tell him everything about the databases his programming affected, including the owners","This tool automated the slow, time consuming process he was following",All three groups of users actualized the finding resources affordance,"By contrast, the helping peers affordance was only actualized by the work-players and go-getters",The information gathering/sharing affordance in OnBoard requires action from new hires to contribute the resources that helps others,"In one example, a go-getter created a “Navigating FSP: The Series” where he wrote a weekly report addressing the things he wished he would have known when he started",This included all the acronyms employees use and how to find one’s car in the parking lot,"OnBoard users who provide such information are actualizing the affordance of helping peers (Affordance 10,  ) that allows other new hires to actualize the finding resources affordance (Affordance 9,  )",Work-players and go-getters derived satisfaction from helping peers,"In the words of a work-player, “OnBoard allows me to mentor other new hires because I can relate to the kind of things they are going through; helping makes me feel good.” 4.1.4 Innovation affordances Innovation affordances include two affordances for new hires – broadening perspective and acquiring new technology skills – and one for senior management – acquiring insight on new products and services","The latter was not an originally envisioned function of OnBoard, but as executives began to see the variety of ways in which new hires used OnBoard, they realized OnBoard’s potential for igniting organizational innovation","The new hire affordance of acquiring new technology skills (Affordance 12,  ) first emerged after a technology vice president expressed displeasure about OnBoard’s social events during the workday","A concerned go-getter took this to heart and decided to organize an event with work, rather than social, purposes in mind",The go-getter initiated a coding competition,The competition challenged new hires to develop an application of their choice on a mobile platform with which FSP was experimenting,"All participants – go-getters, work-players and just-doers – expanded their technical skills by working nights and weekends to learn the mobile development language and build the application","In this way, the go-getters affording OnBoard to create a work-related outcome of benefit to FSP resulted in work-players as well as just-doers acquiring new technical skills","In another example, a go-getter discussed how OnBoard facilitates what were referred to as house calls","Through house calls, new hires can visit other work areas that interest them","This allows the new hires who wish to transition to another area to learn about the work area (Affordance 11,  ) before formally committing","A go-getter described this broadening perspective affordance as one that helps him with his career development. “I never feel trapped, because I know I can always transfer to a new area.” He further explained that visiting areas lets him know how his work impacts other areas and vice versa","Thus far, our analysis has focused on the primary users – the new hires – for whom FSP developed the OnBoard system","However, senior executives, who were not engaged with OnBoard outside of sponsoring and attending events organized by the go-getters, soon recognized that the platform itself could be of value to them as well and began to request feedback from new hires on new product offerings and software development (Affordance 13,  )","An executive stated: 
                         Executives began tasking go-getters with identifying groups of new hires with 0–5 years of work experience to provide feedback and future perspective on various tools","One such effort resulted in the creation of an app that allowed “customers who are being deployed to hit a button on their mobile phone and initiate a flow of events they want to happen.” As further explained by the executive: 
                         So pleased were executives with the newfound innovation potential of OnBoard that they further extended OnBoard to reach other users","One such extension of OnBoard is iInnovate, a SharePoint site that serves as an innovation lab where anyone with an idea to improve organizational processes, products or services can submit their suggestion","Another extension of OnBoard is Dev.Ask, an internal website that allows developers to post questions for the entire development community about coding or processes","In this way, the initial affordances of OnBoard that were actualized by the new hires, namely the networking and visibility affordances, triggered an interest in senior executives to enable other affordances through OnBoard that led to outcomes that were far removed from the initial desire to socialize new hires with OnBoard. 4.1.5 The interacting effects of affordances Identifying the single affordances of OnBoard serves as the first step in understanding how the actualization of affordances in OnBoard affects new hires’ socialization into the organization","Considering that multiple affordances are present at the same time, it is important to understand the nature of their relationships ( )","Consistent with  , certain affordances, later termed “higher-level” by  , can only be actualized after basic affordances","We refer to these as first-order and second-order affordances to highlight that the second-order affordances cannot be actualized until the first order affordances have been actualized and to avoid implying that “basic” affordances are somehow easier or simpler to actualize than “higher-order” ones because in our case, this is not found to be true",The first-order affordances were no easier or simpler to activate than the second-order nor did the second-order affordances demand any higher level of thinking or perception to activate,"In our case, the interacting with peers, demonstrating leadership, and participating in OnBoard events acted as first-order affordances","The actualization of these first-order affordances then allowed new hires to actualize second-order affordances, which collectively resulted in outcomes","As described in  
                             (and   in the Appendix) and explained in the following section, strands of first and second order affordances abstract into generative mechanisms that explain how the affordances lead to the outcomes (see  
                             for a summary of the outcomes)","We next explain these strands of interacting affordances and the generative mechanisms they form as well as the outcomes that the generative mechanisms explain. 4.2 Generative mechanisms 
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                      4.2.1 Bureaucracy circumvention Interacting with peers is a first-order affordance that makes several other affordances possible, including building relationships, finding resources, and helping peers","Together, these affordances explain the outcome of productivity enhancement through the generative mechanism we refer to as “bureaucracy circumvention” (see  )","Many large companies face a similar bureaucratic structure with rigid policies, procedures and hierarchies to follow","The bureaucracy circumvention mechanism is not about violating policies, but rather accelerating the response time by knowing who in the company is able to help","In the examples below, we explain how the four affordances comprising this strand of affordances leads to the outcome of productivity enhancement via the bureaucracy circumvention mechanism",New hires gave several examples of productivity enhancement made possible through their affordance of OnBoard,"On one occasion, a go-getter who was trying to meet a deadline for a database modification (e.g., table structure, permissions, and other) circumvented the standard process by reaching out to someone he knew personally through OnBoard This simplified the process because “they are more likely to take you seriously when they know who you are instead of just some random person coming with a problem.” This then enabled him to check the status of his needed database change","This information, from his fellow new hire, assured him that the database group was working on the needed modification and that he’d be able to deliver the project on time",The new hire was able to get the necessary information because he had a close relationship with someone in the database group that he had formed through his affording of OnBoard to establish relationships with peers,"In another example, management charged a work-player new hire with producing a recruitment video","Said the new hire, FSP is “bureaucratic with a strong chain of command and complex processes and procedures.” To accomplish their work, new hires were frequently left waiting on access, permission or someone to do something",This new hire in charge of the recruitment video was met head on with FSP’s bureaucracy,"He could not use video or camera equipment in the building without permission from security, which “often took weeks because security is thorough.” By contacting a peer whom the new hire had met through OnBoard and who had connections to the security group, the new hire was able to bypass the waiting process and accelerate the approval of his video request",The peer knew exactly with whom he needed to speak and within days his video request was approved,"In general, new hires report that the relationships they form through OnBoard and their ability to find resources through the peers they meet enable them to get things done more efficiently, as summarized by one new hire: 
                         An executive described the complexity of FSP as one that makes it difficult to “learn who to go to with different issues and the OnBoard alumni group does worlds of good in shortcutting some of that and helping these kids (i.e., IT new hires inclusive of go-getters, work-players, and just-doers) get up to speed in learning who, what, when, where, why, and how.” The new hires who had developed the most extensive networks and had the strongest ties with their superiors – the go-getters – were not surprisingly the ones able to achieve this outcome","The bureaucracy circumvention mechanism involves not just the actors themselves availing themselves of an affordance, but other actors (e.g. peers) must also actualize the affordance of helping peers",An important goal of socialization programs is to equip new hires with a level of confidence in the skills they need to do their jobs and fulfill their responsibilities,This is referred to as “self-efficacy” ( ),Experiencing productivity and being able to circumvent bureaucracy in order to get a job done arguably facilitates new hires’ confidence in their ability to perform their job tasks (e.g. self-efficacy). 4.2.2 Executive perspective Interacting with peers and participating in OnBoard events are first-order affordances that make possible the affordances of building relationships with peers and superiors and helping peers,"Jointly, these affordances explain the outcome of organizational culture understanding via the generative mechanism that we label “executive perspective” (see  ) by which we mean the new hires’ ability to see things through the perspective of executives",Learning about organizational culture and learning how to fit into the organizational culture is an important part of socialization ( ),New hires provided various examples of how their assimilation into the culture of FSP was an outcome of their OnBoard involvement,"As explained by a just-doer: 
                         New hires claimed to be “learning about FSP through superiors’ eyes.” In another example, a work-player explained how volunteering side by side with executives at events like Wounded Warriors helped him understand FSP’s mission and its customers","Reflecting on his Wounded Warrior volunteer experience, a work-player said, “now more than ever I understand why I need to build the video system that will allow our customers to interact with loan officers from conveniently located branches.” Though invisible and intangible, the executives’ perspective is much different than “what you get down in the weeds.” This executive perspective mechanism of executive perspective explains how the affordance strand of interacting with peers, participating with OnBoard events, building relationships with peers and superiors, and helping peers lead to the outcome of cultural understanding","Gaining knowledge about the organizational culture allows new hires to develop a sense of belonging, which makes them feel accepted by their peers and superiors and helps new hires understand how to complete their work tasks within the organization standards",This is referred to as social acceptance and role clarity respectively ( ). 4.2.3 Personal development Demonstrating leadership is a unique first-order affordance because the outcomes of this affordance also depend on other actors being willing to participate in the events that were developed by the actor taking a leadership role,"Therefore, the first-order affordance of participating in OnBoard events becomes available for other new hires","These two first-order affordances are actualized by different groups of actors and make several other affordances possible, including building relationships with peers and superiors, finding resources, helping peers, and acquiring insights on new products/services","These first-order and second-order affordances explain the outcomes of productivity enhancement, attractive job assignments, and comfort around superiors via the generative mechanism we label as “personal development” (see  )",The personal development mechanism is about new hires experiencing professional growth,"While go-getters organize most events, and in so doing demonstrate leadership, work-players and just-doers attend these events",As a go-getter comments: “OnBoard has helped me develop some leadership at an early stage in my career; it made me aware of how to handle myself more professionally.” One just-doer described his participation in OnBoard planning meetings,"As an example of how OnBoard helped him achieve productivity enhancement, the just-doer stated: 
                         In another example, the following quote from a work-player illustrates how OnBoard helped him enhance his productivity: 
                         An important outcome of this strand of affordances was attractive job assignments","For example, the winners of the coding competition described earlier received new job assignments in FSP’s prestigious mobile development division",Establishing relationships with superiors facilitated a sense of new hire comfort around superiors,"The following quote illustrates how a go-getter was able to interact with the Chief Information Officer (CIO) in an informal setting: “I met the CIO at a casino night event organized by OnBoard and I was able to chat with the CIO and get to know him on a personal basis.” Another go-getter described OnBoard usage as one that has helped him “make connections with executives” and mentioned that “executives came out to our paint ball event, which shows that they are part of the team and our interactions at such events gives a new meaning into the open door policy” at FSP","In addition, work-players became comfortable sharing opportunities, problems and insights with management","In another example, a work-player talked to an executive about a defect he had found in FSP’s infrastructure","A manager explains: 
                         While all three types of users benefited from some level of personal development, the go-getters and the work-players were the ones to achieve the most benefit because of their involvement in planning and organizing OnBoard events and higher participation in attending such events. 4.2.4 Name recognition Demonstrating leadership and participating in OnBoard events are two first-order affordances that make possible the affordances of building relationships with peers and superiors, socializing, helping peers, and acquiring new job skills","Collectively, these affordances lead to a beneficial outcome of the new hires feeling comfortable around superiors (as opposed to intimidated or nervous), but also to several negative outcomes, including additional work, stress, and social struggle","The mechanism that links the affordances of demonstrating leadership, participating in OnBoard events, building relationships with peers and superiors, socializing, helping peers, and acquiring new job skills to the outcomes of productivity enhancement, comfort around superiors, additional work, stress, and social struggle is what we refer to as “name recognition” (see  )",The name recognition mechanism is about establishing a reputation within the organization,Many large organizations tend to have hierarchal structures that make it difficult to meet executives,"Yet, OnBoard affords new hires the opportunities to establish relationships with peers and superiors while socializing","For example, when participating in the executive luncheons, new hires experience an intimate setting that allows them to build trust and personal relationships with executives","A go-getter stated: “having lunch with executives has helped us with our career growth because we get to know them personally.” And as stated by a work-player: “I get to know executives on a personal level that makes it easier to present in front of them during formal meetings; I learn how to better communicate with them.” The following quote from an executive reinforces the sentiment: “the COO of FSP knows twelve members of OnBoard because he works with OnBoard on a regular basis; he is on a first name basis with them.” Since go-getters lead events and manage the OnBoard ESM system, go-getters tap into the affordance that helps them expand their skills beyond their current job assignment","The skills include leadership, communication, marketing, salesmanship, project management, budget management, creativity, and SharePoint administration",These new skills often led to additional work,"On one occasion, a go-getter with experience in website development was assigned the task of working on all the images displayed in OnBoard","This led to the creation of a Geocaching site, where she spent time creating rollover graphics","She stated: “this is all done outside of my regular working hours.” A top manager stated that he “has now given OnBoard members (e.g., go-getters) new tasks, which includes creating videos that help the new hires know things they need to do at the organization as part of an employee development plan.” In another example, a go-getter described his experience of meeting an executive at an OnBoard event as one that not only provided him with “getting to know the executive on a personal level,” but one that led to the executive asking him to run the United Way campaign",These additional opportunities were extra tasks that superiors asked the recognized new hires to execute in addition to their assigned job responsibilities,"A go-getter comments about how assuming additional responsibilities created additional stress and led him to transition away from OnBoard: 
                         Even though the go-getters followed management’s mandate, superiors viewed OnBoard participation as discretionary and time for which they could not charge FSP","Superiors recognized OnBoard's benefits, even asking OnBoard go-getters to promote OnBoard to college recruitees, and yet new hires still had to confine their OnBoard use to non-working hours such as breaks, lunches, and evenings",This created a sense of inequity among new hires and made it difficult at times for OnBoard’s leaders to recruit their replacements,"For example, the less active new hires experienced some resentment and alienation, as the quote from a just-doer below illustrates: 
                         Given the link between networking and promotion, new hires that did not participate in OnBoard events resented the opportunities afforded to those who did (i.e., go-getters and work-players)","Yet, just-doers prioritized family, work tasks, and off-time over committing to OnBoard events or increasing their involvement",They viewed OnBoard as simply “more work to be done” or “a waste of time” and limited their level of usage,This perception blinded just-doers to the value in OnBoard’s outcomes,"A work-player who later agreed to lead OnBoard explains how OnBoard’s core team initially alienated the new hire community: 
                         The quote alludes to the social struggle that some new hires perceived as a result of the go-getters’ relationships with superiors","Recognizing that superiors provided OnBoard’s leaders additional opportunities and at times favorable work assignments, some work-players, just-doers, and even middle management experienced jealousy",The new hires resented that they were seemingly penalized for not fully participating in something that was outside their job scope and that superiors wanted them to relegate to after hours,"In addition, middle managers resented that they didn’t get the same opportunity to build their name by participating in social events that exposed them to top management","One middle manager comments: 
                         Since go-getters knew that their reputation depended on OnBoard’s success, they had a personal stake in making OnBoard prosper","Go-getters depended on their fellow new hires to participate in OnBoard, attend events, add content, and volunteer, but they had no control over the level of participation of their peers","Rather, go-getters involved in this process felt pressured to cajole their peers into participation in order for OnBoard events to succeed","A go-getter comments: 
                         As the quote above illustrates, go-getters recognized that a bad event reflects negatively on their leadership and may create a negative reputation with peers and management","The following quote eludes to a go-getter’s frustration: “it is hard to satisfy everyone; they complain about events or voice how we could have done something better.” Therefore, go-getters experienced a social struggle in that their reputation depended on the participation of work-players and just-doers both of which felt that the go-getters benefitted more from their participation than they did. 4.2.5 Morale booster Interacting with peers, demonstrating leadership, and participating in OnBoard events are first-order affordances that makes possible the affordances of building relationships with peers and superiors, finding resources, helping peers, socializing, and taking a social break","Together, these affordances explain the outcomes of cultural understanding and sense of social support via the generative mechanism we label “morale booster” (see  )","As the examples below illustrate, the morale booster mechanism raises the spirits of the new hires and provides them positive energy",New hires provided several examples of the outcomes of cultural understanding and sense of social support,"The following quote from a go-getter illustrates how OnBoard helps new hires learn about the organizational culture: 
                         In another example, a just-doer explains: 
                         He continued to describe this experience as one where he felt that OnBoard provided him a “support system that would help guide him and help him instead of just being thrown into the workplace.” The following quote from a just-doer illustrates how OnBoard provides a sense of social support: 
                         Most new hires want to feel welcomed and important when entering an organization","When new hires are treated special and given opportunities to get to know others, socialize, and take breaks from their work tasks to build and reinforce relationships, they experience a boost in their emotion and confidence","At times, the peer and superior relationships and interactions turned into mentoring","As the quote below shows, go-getters experience a sense of satisfaction from helping their peers: 
                         Thus, mentoring peers and being the go-to-person for other new hires gives go-getters a certain feeling of satisfaction in providing social support to others","Both the morale of those helping and the morale of those receiving help is boosted and through this boosting, important outcomes from the OnBoard affordances result. 4.3 Summary Our findings illustrate how OnBoard’s affordances led to various outcomes for different actor groups via the five generative mechanisms of bureaucracy circumvention, executive perspective, personal development, name recognition, and morale booster","The outcomes experienced were both positive and negative, consistent with the power of social media to unleash forces for both ( )","We next discuss the important theoretical and managerial implications. 5 Implications To date, ESM research has examined such issues as managing employee relations, balancing social and work life, managing knowledge, changing organizational culture and promoting innovation ( )",Our research extends the work on ESM to the important domain of organizational socialization,The objective of this study was to understand how ESM influences the organizational socialization of new hires,"Our study has implications for research in the areas of organizational socialization and technology affordances. 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   5.1 Socialization research Our research offers three implications for socialization research","First, given that social media is an important tool in the development of social capital ( ) and that social capital can be helpful as well as burdensome ( ), one might expect both positive and negative socialization consequences for employees that use the ESM",Our research helps shed light on these consequences of ESM use,"Individuals who are more inclined to participate in a social media system, or who have more time to do so, reap higher rewards",Yet they are not being rewarded for job performance so much as for system participation,"First, their use might very well distract them from their work, or, as experienced by several of our informants, lead to additional work outside of their primary responsibility",This can lead to role confusion and lower productivity,"Second, because the system use is divorced from the actual work tasks facing the new hires, it is not yet known whether the new hires who are gaining visibility and reaping the visibility benefits that provide them with more attractive job assignments are actually the new hires with the greatest aptitude for the work tasks and roles","Instead, it is possible that those who have the highest ESM performance (e.g., organize the most and best events and provide the most information) are not actually those who have the highest job acumen",Research into top performers in organizations has found that top performers are many times more valuable in generating business value than lower performing peers ( ),Star employees – those who demonstrate superior performance and who are highly visible in the labor market ( ) experience a “cumulative advantage” whereby their productive resources increase at a considerably greater rate than their less visible and valuable peers ( ),"Because of their importance, star employees are well rewarded and highly influential","For new hires, the relevant labor market is the internal one wherein they vie for attractive job assignments after they have become fully entrenched in the organization",The go-getter users of the ESM at FSP display the characteristics of “stars” – they demonstrate superior performance in the ESM and they become highly visible in the internal labor market of their organization,"Yet because ESM performance is not necessarily predictive of work task performance, the organization runs the risk that the use of ESM as a socialization mechanism inadvertently creates stars who will not be able to shine outside of the ESM","Future research is needed to understand the ways in which ESM performance is, and is not, tied to actual work performance so that organizations can design incentive mechanisms to encourage those uses that improve work performance and discourage those uses that do not","A second important implication of our study for organizational socialization research is that even as social bonding may emerge through ESM use for socialization, so too do social struggles","Management may intend for social media to serve as an inclusive mechanism whereby all new hires may establish relationships, but because relationships help develop social capital ( ) and social capital results in social power ( ), the implications extend well beyond a new hire socialization program",Recent research emphasizes that the socialization process of “becoming” includes “becoming unequal” meaning that occupational socialization creates inequality ( ),"Although the work emphasizes segregation across occupations within an industry (for example, women tend to be more represented as nurses and men, as doctors), our research suggests that this process of becoming unequal through socialization may also occur within an occupational group (in this case, a group of IT new hires)","In our case, the go-getters accrued greater connections to people and resources than the work-players and just-doers and, consequently, greater power","In such a situation, power struggles will ensue; in this case, social power struggles and inequalities form",This then results in divisiveness from a very system intended to promote inclusiveness,A stream of research is developing in the area of individual and group marginality and how marginality is tied to innovative behavior and performance,"Marginality is a condition of disadvantage facing individuals or groups resulting from vulnerabilities that arise from unfavorable environmental, cultural, social, political and economic factors ( )","Some of the negative consequences of marginality include limited career choices, poor performance, isolation, and exclusion ( )","Through socialization, segregation of members in an occupational group becomes naturalized","Given the potential of ESM to both promote belongingness and yet create marginality, future research should probe more deeply into how to avoid marginalization as a side effect of ESM use","Our study offers a third important implication for organizational socialization research, shedding light on how changes to the organization itself occur via the socialization process",Socialization research focuses on how new employees can learn about the organization and how to do their jobs ( ),"It largely assumes a static, and single, organizational culture into which successive groups of new hires are socialized and views new hires as the target of socialization programs ( )",Our findings challenge these assumptions,"First, our study suggests that even as new hires were learning the norms and culture of FSP, they were simultaneously altering the culture and norms through their engagement with the OnBoard system","What was before an 8–5 highly hierarchical environment where work-private boundaries were strong is becoming a much more organic, less hierarchical environment where boundaries between work and private life are more porous","Consequently, future new hires will learn norms that are quite different from the norms that the previous new hires were learning","By virtue of the previous new hires using the system to learn FSP’s norms, they were actually simultaneously changing the norms","Hence, introducing a change to the socialization practices resulted in a change to the organization’s culture into which socialization takes place",This resulted in a dual culture facing the new hires,"Some new hires embraced an emerging flexible culture built around OnBoard and based on the reputation economy with blurred work-life boundaries whereas others new hires maintained the traditional bureaucratic culture allowing for work-life separation and valuing hourly productivity. 
                          It may be that, in the future, an important work skill will be the ability to cope with seemingly inconsistent cultural norms embedded in various technology-based work practices","Second, our study suggests that the new hires shifted from a state of being socialized into the organization into a state of socializing each other into the organization",The very role of the new hire socialization process changed as the HR department began to observe the direct benefits of the ESM on new hire socialization,"As HR began to incorporate the system into its own human resources’ processes, new hires experienced a shift in perspective from being the target of socialization efforts to being the means of socialization efforts","Future research is needed to investigate how role flipping – making new hires both the leaders of and recipients of socialization initiatives – facilitates or impedes assimilation into the organization as well as group and organizational cohesiveness and identity. 5.2 Technology affordance research In terms of technology affordance, our study also offers important implications",The affordance lens compels scholars to contemplate the relationship between the potential action to be taken and technology capabilities ( ) as well as the relationship between affordances and outcomes ( ).   suggest that it is important to study the affordances themselves in order to gain a deeper understanding of how change occurs following the introduction of a new IT,The technology affordance research suggests that affordances when actualized by different actors even for similar objectives have differing outcomes for themselves and for others ( ),Our findings extend this research by demonstrating (1) how affordances of different groups of actors intertwine to produce outcomes not just for the actors themselves but also for non-actors and (2) how outcomes for one group of actors produces affordances for another group of actors,"Concerning the first, our findings provide insights into a phenomenon that we will refer to as the second-hand effects of technology","With their use of the OnBoard system, the new hires impacted middle managers, non-users of the OnBoard system","In the case of middle managers, the second-hand effects were the reduced time they had to invest in mentoring new hires, a positive effect, but also the feeling of resentment at new hires getting to meet senior managers that they had not even met",This feeling of resentment underscored a deeper concern that they might be disadvantaged by the visibility accruing to some of the new hires,IS research has long focused on use and users as important components in an information system,Our findings suggest that non-users are also affected by an IS in important ways,Future research should delve more deeply into this issue of the second-hand effects of technology,"In terms of the second, our research shows that outcomes do not just reinforce the actualization of affordances, as prior research has demonstrated ( ), but that outcomes create new affordances for different sets of actors","In our case, new hires meeting senior managers as a result of their participation in OnBoard events not only made the new hires more comfortable around their superiors, but also led to new affordances for senior managers, who recognized the potential insights new hires could provide into new product and service ideas and who therefore began soliciting feedback from new hires",This eventually led to entirely new outcomes – the Dev.Ask and iInnovate solutions,"Thus, affordances, actors, and outcomes intertwine with each other and create new affordances and outcomes for new sets of actors","Moreover, our findings suggest that outcomes stemming from the actualization of an affordance depend not only on how one user group uses the affordance, but are also contingent on how another group does, or does not, make use of the same or new affordance","In our case, this is vividly illustrated by the go-getters receiving benefits that were contingent upon how the other two groups actualized affordances","Without the work-players and just-doers actualizing the affordances of participating in OnBoard, the go-getters group would not have obtained the advantageous socialization benefits like superior recognition and positioning themselves for promotion",Future research can pay closer attention to the co-dependency of non-actualization of affordances by one group of actors with the actualization of affordances by another group of actors. 6 Limitations and conclusions This study’s implications need to be considered in light of the limitations,"First, the results relied on data collection from a single organization","Given that organizations use various socialization programs, our study raises questions of generalizability",It is possible that new hires may experience different outcomes in other organizations,"While our study does achieve within-case generalizability ( ), our insights may be seen as untested hypotheses ( )",Future research might empirically test the relationship between the various mechanisms and outcomes,"For example, researchers could compare the relative effectiveness of productivity enhancements to new hires via the two mechanisms of bureaucracy circumvention and personal development or researchers could examine other technology that create affordances that enable these same mechanisms","In like fashion, researchers could examine the relationship of the executive perspective mechanism and cultural understanding, comparing the effectiveness of this mechanism toward the achievement of shared cultural understanding to other mechanisms used to engender cultural understanding, such as company policy manuals and online courses","One might go even further to consider how these mechanisms might be useful in other contexts, such as how the executive perspective mechanism might be useful in achieving social alignment","Second, we rely on the new hires’ perception of ESM use, not a quantitative measure of use time or frequency",New hires might have over- or under-estimated their interaction with the system,"Nevertheless, this does not undermine the importance of the relationships uncovered","The new hires perceived the affordances we uncovered and based upon their self-reported level of engagement with the system, three distinct categories of users were identifiable","Future research might extend this by examining how users manage their usage level, increasing or decreasing their usage to fit what they feel is the “right” or “ideal” usage level","Furthermore, future research might examine whether users, once they have positioned their usage level relative to others, feel capable of becoming more engaged or feel trapped in a certain pattern of usage","In spite of the above limitations, our study offers an important extension to ESM research","Previous research on social media in organizations has focused on such important issues as how organizations can use social media to manage public perception ( ), how organizations must learn communicational ambidexterity to fully manage social media as a strategic capability ( ), how internal social media systems form a symbolic capital that employees seek to govern ( ), and how ESM influences employee performance ( )",Our research examines a previously unaddressed phenomenon of how ESM influences the socialization of new hires,"Given the importance of new hire socialization in ensuring a productive and committed workforce, the incorporation of ESM into the organizational socialization process is of strategic importance to organizations and the IS organization responsible for designing such systems","Our study uncovers five important mechanisms through which ESM influences organizational socialization: bureaucracy circumvention, executive perspective, personal development, name recognition, and morale booster",That ESM are capable of producing such important mechanisms is noteworthy in itself,"That these mechanisms enable IT new hires to be more productive and more comfortable in their new organization is of keen importance to organizations challenged with recruiting, training, and maintaining a skilled IT workforce","While our study indicates that ESM usage facilitates the acclimation of new hires into a large organization and facilitates their productivity, it also shows that ESM can create social struggle, isolation, and resentment among new hires","For this reason, managers should think carefully about their ESM strategy and consider how to encourage uses that create positive socialization benefits as well as positive productivity benefits without inadvertently fostering social divisiveness","Appendix A (See  
                       and  
                      ).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0736584518303831," 1 Introduction In the past 30 years, Additive Manufacturing (AM) has gradually evolved from prototype applications to parts production by improving manufacturability and reducing lead time  ","Even though AM is already used in many commercial processes, its full potential might appear in the near future, bringing a significant societal impact  ","Among numerous AM technologies, Wire + Arc Additive Manufacturing (WAAM) stands out, especially in the field of medium to large metallic deposition","Indeed, by combining arc welding tools with standard robotic manipulators, WAAM provides a potentially unlimited build volume and a high-rate deposition of various metals, such as steel, aluminium alloys or titanium alloys  ","Post-processing consolidation treatments like Hot Isostatic Pressing (HIP), which reduces porosity and lack of fusion, can be difficult to apply to large components due to the absence of sufficiently-big HIPing facilities","For this reason, defect-free deposition is essential to build primary structures that require high-structural integrity","Ding et al.   have shown that, in WAAM, the quality of deposition is fundamentally linked to the tool path strategy used","Therefore, the WAAM technology requires a dedicated software approach to generate optimised paths, thus guaranteeing uniform deposition and ultimately enabling a complete commercial solution","In fact, many studies have focused on this particular topic from which two mains approaches can be distinguished","The first approach is to slice a geometry and to generate a path using the same path planning strategy, for each resulting layer","Although this solution has been successfully used on other AM process such as FDM  , it is not directly applicable to WAAM, which has specific requirements inherent to arc welding deposition","Indeed as Ding et al. describe in their research  , several path characteristics such as discontinuities, sharp turns and overlaps contribute to an unstable deposition that, layer after layer, can lead to a catastrophic failure","These limits have been understood for a long time, in fact, early studies   have designed path planning strategies for WAAM that generate continuous paths","Unfortunately, removing discontinuities increases other factors like sharp turns","For these reasons, Ding et al. introduced several path planning strategies   limiting simultaneously all the faulty factors in a path to improve deposition","Nevertheless, in this approach, all the proposed solutions apply the same path planning strategy regardless of the layer shape","Yet, the higher the topological complexity of a geometry, the more discontinuities and sharp turns are likely to appear","Thus, the resulting quality can vary substantially according to the geometry",The alternative approach is the feature-based design introduced by Kazanas et al.  ,"In their research, they demonstrated WAAM’s ability to build complex parts like enclosed structures by designing a path strategy that fits the requirement of this particular targeted shape","This solution has been then followed by the development of cross structures  , T-crossing features   and more recently, multi-directional pipe joints   ( 
                      )","Thus, this approach has shown that designing a path strategy ad hoc for a given topology guarantees the deposition quality; however, this solution requires a time-consuming path design research for each new part, which is incompatible with the purpose of AM","Furthermore, one must bear in mind the fundamental differences between powder-bed AM and directed-energy deposition AM","In the former, the layer height is fixed by the downward movement of the build platform and the consistency between thickness of the sliced layers in pre-processing, and thickness of the layer built is somehow always ensured","The latter, instead, is closer to micro-casting, and numerous factors can influence the shape of the deposited bead (width and height)",One of these factors is the local variation in the part geometry,"This means that even if the same set of parameters is used, the resulting bead geometry can vary","Imagine a linear deposit; in such a case there is a balance between energy introduced, energy conducted away, energy used to melt the wire, and energy used to melt the underlying material","In this steady state, the resulting geometry does not change","However, when that linear structure changes into an intersection, the energy balance is disturbed; more heat is conducted away; the melt pool would shrink resulting in thinner wall width, and larger layer height, if no compensation is applied to the process parameters","Therefore it is absolutely essential that parameters are changed ad hoc to compensate for such variation and to ensure that the geometry obtained is the same as that expected per sliced CAD file, and no errors are accumulated throughout the build","This is also why simple reverse-machining strategies, which fill the sliced layers, cannot be applied","To tackle these challenges, this paper introduces a new approach to generate paths for WAAM of complex 3D geometries","The proposed solution, called Modular Path Planning (MPP), integrates the adaptability of the feature-based design into a more efficient layer-by-layer path planning solution","Thus, it will be shown how this solution guarantees a uniform layer deposition, leading to high-quality part building, and with limited effort in the pre-processing stage",The following Section presents the MPP concept and defines the rules and the decomposition process to guarantee the uniform deposition of a layer,"Then,   describes the MPP implementation that reduces user inputs to basic CAD modelling operations","To describe the entire solution, an application example is presented in  .   compares the MPP to a traditional path solution and shows its ability to build complex parts for industry","Finally, in  , the benefits and limits of the presented solution are discussed, followed, in   by the conclusion and the presentation of future work. 2 Theoretical approach 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   2.1 Slicing As introduced previously, the MPP aims to integrate modularity into the popular layer-by-layer deposition strategy","Therefore, like the traditional approach, the first step consists of slicing the 3D Computer Aided Design (CAD) model into layers","However, it should be highlighted that the deposition thickness is not necessarily the same for each layer","For instance, the heat dissipation variation within the first layers has a critical impact on the layer height  ","The slicing interval could, thus, compensate for this issue",The result of this slicing operation is a set of layers represented as 2D geometries,"As it can be seen in  
                         , these layers extracted from a single 3D CAD model can have substantial topology variation","Therefore, applying the same path planning solution to these diverse topologies will lead to disparate results","For this reason, the MPP allows the definition of a path planning strategy for each layer. 2.2 Segmentation Segmentation is the fundamental idea of the MPP to integrate modularity into the path design process","Indeed, where a traditional approach would apply a single path planning on the entire layer, the MPP requires users to segment the given layer into sub-parts called sections ( 
                         ) to then generate individual paths","The purpose of this segmentation step is to create a set of sections shaped into basic geometries, usually narrow rectangular shapes, to facilitate their deposition","However, the optimum segmentation is determined based on experience and is highly dependent on the part geometry","Nevertheless, some basic rules need to be followed","For example, if curved trajectories can be deposited, sharp turns should be avoided, and instead replaced by corner intersections ( 
                         )","Similarly, if a slight width variation does not alter the deposition ( 
                         a), an abrupt width variation can create irregular paths ( b) leading, layer after layer, to significant defects","Therefore, to avoid those irregularities, it is preferred to divide this part in multiple sections ( c)","The sections shape is fundamental to provide a controlled deposition but, to assure uniform deposition of the entire layer, it is also crucial to provide particular attention to the topology of the intersections since poor junctions can create critical defects in the final part","As it can be seen in  
                         , many junction configurations are possible when using only parallel and oscillated paths",Some of these intersections can be more complicated to deposit than others as they are more likely to produce defects,"In any case, an appropriate research study should be conducted on each intersection type to determine deposition parameters that will assure a defect-free junction. 2.3 Path planning Once the layer has been segmented, a path can be generated in each section","An advantage of the segmentation is that, compared to the traditional approach, multiple path planning strategies can be used across a single layer to best fit the requirement of each section","Any path planning strategies can be used to build those individual paths, however, the oscillated path ( a) is the most recommended since it can handle width variation and slight curve very well, and therefore its deposition is easier to control","Nevertheless, the parallel path ( b) can also be an adequate alternative, especially when used to build narrow shapes since it produces smoother surface waviness  ","Still, interconnections can be more problematic when using parallel paths. 2.4 Zoning As mentioned previously, although path design improves the deposition uniformity significantly, appropriate deposition parameters are essential to control the deposition","Deposition parameters depend on the geometry; on the location within a part; on the material being deposited and on the chosen WAAM sub-process (MIG, TIG, plasma, etc)","For those reasons, the MPP adopts a concept of zones: where a zone, identified by a colour, contains a particular set of parameters ( 
                         a) to be specified by the user after the path planning phase","Thus, as it can be seen in  a, a simple straight wall contains three zones to accommodate the different thermal conditions in the stages of deposition start, steady state, and end","This must be done whichever path is used: single bead, oscillated or parallel","Additionally, if a section contains a notable width variation requiring specific deposition parameters, a zone can be defined to account for that change in width, and to manually adapt the parameters locally ( b)","However, this situation could also be solved by using an algorithm that would calculate automatically the parameters needed to produce the desired layer width and height","Finally, because the heat dissipation is drastically different at the intersections, it is crucial to create zones at those locations ( c), as explained in the Introduction Section. 2.5 Layer path Once a path has been generated for all sections, they are combined into a single layer path","However, it is important to highlight that the deposition is not continuous along the entire layer","Instead, when reaching the end of a section, the deposition is stopped and the torch moves to the starting point of the following section with the arc off and without feeding any material ( 
                         )",Sorting algorithms   can be used to reduce downtime by defining a better order of deposition,"Yet, a particular deposition order can benefit some intersections","Indeed, in the case of the perpendicular intersection of oscillated paths ( b), depositing section 2 after section 1 helps to melt the waving border at the junction reducing risk of voids","Moreover, the deposition sequence has a significant impact on distortion   and should, then, be taken into consideration to minimise buckling risk","Finally, once the path of the first layer is made, the same methodology can be applied to each following layer, generating a set of layers that can be combined to build the entire part ( 
                         )","As shown, building complex geometries of various topologies can be achieved thanks to the presented MPP","However, applying the proposed solution can be challenging in practice","Indeed, the path planning of a single layer can already be a complex and time-consuming process: partitioning on its own involves many highly-technical CAD modelling operations","Therefore, repeating this operation for each layer of a standard-size part, which can contain hundreds of layers, multiplies the effort required to build the entire part by as much","The next Section proposes an implementation of the MPP that reduces the operational complexity to basic CAD inputs and really minimises user’s interventions. 3 Practical approach 
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   3.1 Slicing The central operation in the slicing stage is to extract the boundaries of the geometry at a given height","Actually, most 3D CAD frameworks contain a function that is able to compute the intersections between geometry and a plane","Therefore, to build the layers, a list of planes is first generated following the deposition direction from bottom to top","As explained previously, the gap between each plane is not necessarily constant but instead defined by user input","Then, by using the intersection function, a layer is extracted for each plane, resulting in a stack of layers. 3.2 Building Strategy (BS) The MPP solution aims to build a part by individually generating the path of each layer","However, as explained previously, the path planning of a layer can be laborious since it consists of partitioning the layer into simple sections (Segmentation); generating the appropriate paths for each section (Path planning) and integrating zones into each section (Zoning)","Therefore, to avoid complex CAD modelling operations, the following Sections introduce a three-step process called Building Strategy (BS) ( 
                         )","This process offers users the ability to outline the desired layer path configuration with basic CAD inputs while, in the background, the application processes the technical CAD operations to generate the actual path. 
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                      3.2.1 Segmentation For their first intervention, users are asked to identify each section of an extracted layer by following the rules defined in the theoretical approach ( )","Firstly, the active layer is shown in the background ( 
                            a)","Secondly, the user overlays planar closed-curves on the targeted sections ( b)","Thirdly, following the user’s input, the software extracts automatically and instantaneously the sections ( c) by applying a boolean intersection function ( 
                            
                            )",The result of this operation is multiple empty sections represented by their boundaries as planar closed curves ( ),"However, no path can be generated yet since it requires an additional user intervention as described in the following Section. 3.2.2 Path planning Once the layer is divided into sections, a path planning strategy needs to be applied to each section to generate paths","As mentioned previously, any path planning solution could potentially be implemented, however, in this paper only the oscillated and parallel deposition strategies are presented ( )","In general, at this point, users pick the best deposition strategies between those available, to best meet the requirements of the targeted geometry","However, given that sections are mere planar closed surfaces, other information is needed","Firstly, the user must specify the direction of travel by drawing a guide","As can be seen in  , a guide is a planar curve that specifies the deposition direction","If the oscillated path has been selected, it will be produced by generating an oscillation perpendicular to the guide-line, and with constant step-advancement","If the parallel path has been selected, a series of equidistant paths parallel to the guide will be produced","Moreover, intersections between the guide and the section boundary will represent the start and stop of the deposition","In fact, the guide must intersect the section’s boundary exactly twice",The result of this operation is an automatically generated path for each section ( ),It is essential to understand that these paths are not interconnected; meaning that during the deposition stage the manipulator will go from a path to another by stopping the deposition and retracting the end-effector,"However, as stated previously, using a single set of deposition parameters within a section will most likely lead to a poor deposition quality","Therefore, it is crucial to give the path the ability to change its deposition parameter along the path thanks to the zoning step described in the next Section. 3.2.3 Zoning In the theorical approach ( ), a concept of zones has been presented to facilitate the integration of various deposition parameters across sections assuring a uniform deposition","The zoning method, presented here, allows users to define zones intuitively within a section","By default, the path generated in a section is automatically associated with a zone ( 
                            a), meaning that all movements in this newly generated path are sharing the same deposition parameters","From this state, the user can split the main zone in two by simply locating a point on the path ( b)","Thus, knowing the location of the point, the software regenerates a new path and changes the parameters dataset reference whenever it passes over a splitting point",This process can then be repeated to generate the necessary number of zones ( c),"Alternatively, users can zone the path by defining a length at the beginning and/or at the end of this path ( d)",The benefits of this solution are detailed in   but are mainly related to the fact that arc-based deposition requires particular parameters at the ignition and termination stages for a limited length,"Finally, it is important to notice that both of these alternatives can be used simultaneously ( f), giving substantially more flexibility to the user throughout the process","At this stage, all required inputs for building a layer are completed, and the software can, therefore, combine all the generated section paths into a single layer path ( )","However, although this process is fast and straightforward to produce a single layer, repeating it over hundreds of layers can still be tedious. 3.3 Mask and 3D zoning All the inputs needed to generate the path of a layer can be grouped into one entity, the mask ( 
                         )",The advantage of this approach is that the same mask can be used over multiple layers,"In fact, as can be seen in  
                         , even when each extracted layer is slightly different ( a), applying a unique mask to all layers ( b) results in a path accommodating layer boundaries and users’ instructions, for each layer ( c)","In fact, this mask property is at the core of the MPP to reduce user’s interventions","Indeed, once users have defined the first layer mask, the software solution can automatically apply this mask to the following layers","However, if the input geometry contains various layer topologies as can be seen in  
                         , the program may fail to generate a path: for instance, when a single segmentation curve would produce two independent closed sections","In this situation, the software raises an exception, stops the path generation and asks users to create a new Building Strategy (BS) mask for the failed layer",This new mask is then used to generate automatically the current and following layers until a new exception is raised or the last layer is reached,Please note that users have the opportunity to integrate a new BS mask at any layer,"Indeed, in some situations, although the software correctly generates a path using the previous mask, users can consider having a better alternative for the current and following layers",The mask concept also enables 3D zoning,"Indeed, the zoning process described in   provides two alternatives to define a zone using zoning points or zoning lengths","If this can seem redundant in 2D, this combination gives the user better control over the zoning definition of a 3D CAD model","Indeed, having the ability to mix points and lengths enables the user to define which zones can vary when the boundaries are changing across layers","To clarify the 3D zoning control, a simple example is shown in  
                         ","In this example, zoning lengths are applied to the section to accommodate the arc welding behaviour at the ignition and termination of the deposition (Green and Red)","Additionally, zoning points are located in the middle of the section to define a particular zone (Yellow zone) as an intersection","The result of this combination is that the green, yellow and red zones keep a constant length over the different layers, while the blue and purple zones adapt their length","In such a way, users can easily control the zones configuration across multiple layers. 3.4 Deposition parameters Deposition parameters are deliberately omitted throughout the MPP process, so the user can focus entirely on the path architecture","Indeed, users are only asked to describe when those parameters need to be changed using the zoning method ( )","To facilitate their implementation, in parallel to the path generation, the software generates an empty XML file that is structured to reflect the path architecture","As shown in  
                         , the XML file is structured consistently to the MPP process","It contains a node for each layer; within each node, there are sections; within each section, there are the different zones, also identified by their colour; and inside the zones, the user then inputs the various deposition parameters (f.i","Current, Wire Feed Speed (WFS), Travel Speed (TS), etc)","Using an XML file enables users to fill parameters directly in the file, making it a simple and fast interface for experimental purposes","However, using the XML solution also facilitates the development of graphical interfaces enabling a commercial product, potentially","Moreover, having structured data storage will allow, in future, to automatically fill parameters by developing dedicated algorithms. 4 Application In this Section, a complete step-by-step example of the MPP solution is presented using the geometry seen in  ","To generate this example, the MPP method has been implemented into the Rhinoceros 3D software and its extension Grasshopper",This extension facilitates the development of innovative solutions thanks to its intuitive and powerful interface,"The first step is to slice the input geometry into layers: to achieve it, users define the various layer heights ( ) and the slicing orientation","The resulting layers are then automatically aligned on the top view ( 
                      ), waiting for the user to start the next step","From this stage, users are asked to define the mask of the first layer by following the three-step BS process described previously ( )","By drawing segmentation curves, guides and zoning points over the layer, the software generates the first path automatically ( )",Users can then verify the result and modify their inputs if required,The first mask is applied automatically to the following layers until an exception is raised ( ),"In this example, the program fails to generate the layer 25 since this layer topology is drastically different from layer 24","Therefore, users are asked to draw a second mask (BS 2) that fits the requirement of layer 25","Using the second mask, the program resumes the path generation from layer 25 until the last one","When all layers are successfully processed, all the paths are automatically grouped into a single path as seen previously in   ( )","At this stage, users can inspect the resulting path of the entire geometry and, if needed, can modify an input mask",Any modification would then be applied to all the layers impacted by this mask,"Before starting the actual deposition process, users have to define the deposition parameters by filling the XML file generated automatically with the path ( )","Once all the parameters are set, the path can be processed by a robotic software solution to generate the appropriate machine code, which will be used to finally start manufacturing. 5 Validation A test-piece, shown in  
                      , was designed to validate the MPP approach","For comparison, the test-piece was also built using a path planning strategy available in the academic literature","The deposition parameters for the Ti-6Al-4V alloy were chosen based on the target baseline bead width and height of 6 mm and 1.5 mm, respectively","Regardless of the approach, eight layers were deposited to attempt reaching the desired height of 12 mm",Four different tests were performed,"The first test used the adaptive path planning method described by Ding et al.  , which can be seen as a contour method when applied to this cross shape example ( 
                      a)",The process parameters were kept constant throughout the deposition.  d shows the resulting component,Extensive presence of keyhole defects can be appreciated throughout.  c shows a side view of the same component; the irregular height of the deposit can be seen,"The second test used the same method as the first attempt ( a), although parameters were different from the baseline ones, to try and avoid the defects seen previously.  d shows the resulting component","Keyhole defects could still be found, although the height of the deposit is certainly more stable ( e)","The third test used the MPP approach, albeit with segmentation only, and no zoning ( f).  g shows the resulting component","A small keyhole defect could still be found, but the height of the deposit was very stable ( h)","However, please note the lower height at the ends of the part","Finally, the fourth test used the MPP approach with both segmentation and zoning applied ( i).  j shows the resulting component","No defects can be seen, and the height of the deposit is stable ( k); the part ends are less steep as well","Taking the validation one step further, an Airbus A320 aft pylon bracket mount was built","The tool path plan is shown in  
                      a, while the resulting component is shown in  b","Please note this part was also in-process cold-worked, as described by Martina et al.  ; the tool-path-planning for the in-process cold-work was performed with the same MPP software used for the deposition","Unfortunately, the finish-machined component cannot be shown due to confidentiality issues",The machined component showed no defects. 6 Discussion The proposed MPP solution has been shown to be highly flexible as it can integrate a variety of parameters to fit material and deposition technology requirements,It can also integrate new path planning solutions to increase its ability to build new topologies,"Moreover, because the MPP solution is a layer-by-layer deposition strategy, it can integrate and plan the path of post-deposition-treatments such as rolling  , peening   or even machining  ","Therefore, this presented solution has a strong expansion potential as it can easily be adapted to new materials and processes","However, to successfully build a part, it is also essential that the part design complies to the rules explained by Lockett et al.  ","Moreover, to build parts containing overhang components, subdivision solutions   should be used beforehand to divide the geometry into buildable sub-features","Finally, in some cases, especially regarding simple building like cones, it can be more appropriate to use path strategies that take advantages of 5 axis depositions to follow the curve of the part, as shown by Hascoet et al.  ",It should be noted that the definition of process parameters is beyond the scope of this paper,"Instead, the software provides dedicated inputs so users can define those parameters","Indeed, such parameters depend on the process and the material used and, as such, would require extensive studies on their own  ","Similarly, parameters related to the path construction, such as stepover or bead-overlap, should be determined through experiments that define the deposition profile  ","Finally, as previously stated, the MPP approach differs substantially from the other tool-path-planning approaches published so far within the world of AM; however, the MPP is actually quite similar to what is done, in general, when planning the machining paths of a component in its entirety","In traditional CAM software, a part is divided into a number of manufacturing features each of which may have different process parameters, tools, etc",The CAM software then creates a toolpath for each of these manufacturing features and then stitches the different paths together into longer larger path that is encoded into the NC program,"Previously-published tool-path-planning approaches treat a sliced layer as if it were a single manufacturing feature, which they try to fill with a path, according to a certain desirability criterion and do not consider the need for local changes to process parameters depending on the feature geometry","This approach works well for powder-based additive manufacturing and FDM using polymers, but is too limited for complex WAAM deposition","Our approach proposes that the ”traditional” feature-based machining tool-path planning approach should be taken also in the case of AM, and a layer should be subdivided into simpler building blocks whose paths are then merged in an overall piece of code","This enables the definition of feature-specific tool paths, which on the one hand requires a certain amount of manual work, but on the other hand it ensures the level of control needed to program whatever geometry with the right focus on structural integrity. 7 Conclusion and future work This paper introduces a new path planning solution for WAAM called Modular Path Planning (MPP) that can be used to build a large variety of complex topologies","Because, in WAAM, the quality of deposition is fundamentally linked to the tool path strategy used, this proposed solution guarantees a uniform deposition by dividing a layer into a basic set of geometry that simplifies deposition prediction","Thus, by combining the efficiency of the layer-by-layer deposition strategy to the adaptability of the feature-based approach, this path generator offers the ability to use a diversity of material and deposition processes, and assures that the MPP solution can evolve and therefore become a standard path generator in a commercial WAAM solution","Moreover, the presented implementation of MPP allows users to build the path of a full part with limited and basic interventions","The method has been used to manufacture a test-piece, shaped as a cross, and demonstrate the ability of the MPP solution to provide a more uniform deposition than traditional path planning solutions, which apply a single path strategy regardless of the geometry shape, such as the adaptive path planning strategy proposed by Ding  ","Moreover, the production of a pylon bracket mount shows that the MPP solution can be applied to complex geometries while maintaining its deposition quality","In the proposed solution, users are invited to intervene during the path generation process to adapt the path to the topology","Even though this step increases the path planning time, it is believed to be highly beneficial in terms of result quality; indeed, having a framework that enables the local change of process parameters is absolutely fundamental","Nevertheless, to achieve greater efficiency, future works will focus on making this step automatic, by integrating deep learning solutions, which will learn from user’s interventions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0921889016306285," 1 Introduction In the early days of using robots, German technology assessment of robotics and automation was in particular characterized by studies on the impact on the labor market","Back then, the basic principle of automation was strictly applied, i.e., the work processes were divided into individual action sequences in order to examine which of these sequences could be automated","As a result, a manufacturing process was established in which automatable action sequences were performed by machines while non-automatable tasks continued to be performed by humans",The main objective was the substitution of human labor in order to achieve an increase in efficiency through labor cost savings,"Assembly lines were developed in which automated operations could be optimally coordinated, particularly in the automotive industry",The non-automatable actions to be performed by human workers constituted the so-called residual activities which were partly integrated in these assembly lines or remained as upstream or downstream tasks  ,"Today, due to major progress made in programming as well as further technological advances in engineering, robotic systems can increasingly take over non-standardized tasks previously reserved for humans—and at economically feasible costs","As a consequence, automation is no longer restricted to the production of standardized products in industry but increasingly becomes part of value creation processes in the service sector","Hence, the former paradigms of automated manufacturing have been put into perspective again",The focus has – at least partially – shifted from substitution to cooperation between human and machine,"Often the aggregate of tasks performed by humans is designed in such a way that they no longer merely represent “residual activities” of automation. “Exaggerated” automation strategies were modified as to create meaningful tasks for humans, which can often be done in teamwork","Again, the economic cost–benefitanalysis also plays a key role  ","Today, the robot is increasingly able to perform not only manual and routine cognitive tasks but also non-routine manual and cognitive tasks","As a result, the general areas of application for robots broaden and they can be used both to substitute even more job profiles than before and help mitigate shortages within the labor market","Due to their capabilities, robots also increasingly act as collaborators of human labor","Overall, this means that robots do not necessarily substitute human labor, but complement it and, in specific areas, make it even more productive","In the following, this paper will elaborate on these considerations and demonstrate how technical progress can enable not only a transition from industrial to service robotics but also a shift in the relationship between human and machine from a formerly substitutional to a complementary one",These considerations will be linked to the so-called capital-skill complementarity hypothesis which addresses the relationship between physical capital and different types of skills (cf.  ),This perspective addresses both challenges and opportunities for human labor resulting from technological change,The inherent complexities are highly relevant for any technology assessment of robotic systems,"On the one hand, the use of robots is aimed at optimal implementation of technical possibilities in order to achieve economic gains","On the other hand, it must be investigated from a work science perspective what exactly the tasks are that humans are supposed to perform in cooperation with machines","This paper addresses some key issues related to service robotics, which in this connection represents the “natural” further development of industrial robotics","Already in 1994, the Fraunhofer Institute for Manufacturing Engineering and Automation (Fraunhofer IPA) – one of the key players in the field of robotics – phrased the following definition of service robots, which is still valid today  : 
                      
                   In service robotics, the circumstances differ from those of industrial robotics because the environment can only very rarely be completely redesigned with regard to the use of robots (for example, an entire stable for milking robots)",That means that the robot system must be able to react flexibly to different environments in which services can be rendered,"In that respect, cooperation with humans becomes more multi-faceted","When the service is rendered to people, e.g. hair-washing, the challenge for the robot system is even greater","This paper aims to describe main lines of argumentation of two disciplinary approaches, namely (labor) economics and work science",The goal is to derive some general observations about the assessment of service robots and to outline first findings on technology assessment of specific service robot systems,"In this respect, this paper is an important preparation for the contextual case-by-case analysis of service robots. 2 Framing the economic argument: from substitution to complementarity 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   2.1 Human labor and automated production as perfect substitutes The introductory discussion is based on a stylized production process in which labor and automatic production are assumed to be perfect substitutes",This implies that a certain output can be produced in identical quality by either input alone or any convex combination of both inputs,"For given cost curves, it is thus easily possible to derive the partitioning of tasks being performed either by humans or machines or, put differently, the cost-minimizing degree of automation. 
                         
                          plots on the horizontal axis the cost-minimizing degree of automation, which lies between 0% (complete human production) and 100% (complete automation)","Total production costs (solid line) result from the utilization of labor (dashed line) and robots (dotted line).  
                         It is natural to assume that the latter are increasing and the former are decreasing with the degree of automation","Given this specification, any change in any cost component also affects the optimal degree of automation: e.g., it increases whenever process innovations reduce the cost of machine production (panel [b]) or when human production becomes more expensive (panel [c])","Total costs, however, are minimized for intermediate degrees of automation","Due to the assumption of perfect substitutability, co-working or any complementary relationship in the sense that a robot assists a human being and the corresponding allocation between labor and machine may not be analyzed within this simple framework","The presentation, however, is useful to better understand the impact of robots in those fields where human labor is threatened by machines","Up to now, this has mainly been the case in the field of industrial robotics where machines took over routine and mostly manual tasks with a strong repetitive character","Today, however, great potential is seen in the utilization of flexible co-working robots and service robots which – also in industrial robotics – increasingly take over non-routine tasks as well. 2.2 From industrial to service robotics—broadening the fields of application The following thoughts relate to the long-standing discussion of the impact of industrialization on labor",A common way to address such questions may be summarized under the label of the so-called capital-skill complementarity hypothesis,"It draws back on  , who first stated that physical capital is more complementary to skilled than to unskilled labor",An immediate consequence is thus that there is natural substitution pressure on unskilled labor performing repetitive tasks that may easily be codified,The argument also highlights that the skill level is a significant determinant driving the relationship between various inputs of production,"More recently, similar arguments have been picked up, e.g., by  ,  ,  ;  ,  ,   or  , who discuss the skill content of recent technological change and especially the role digitization plays herein","Especially Frey and Osborne  argue that, given the recent employment structure, a large share of employment is at high risk as computerization increasingly also penetrates both cognitive and non-routine tasks",It is not fallacious to expect that existing skill requirements and job profiles will also come under pressure as more and more value creation processes in the service sector become automated,"Thus, the diffusion of service robots is expected to have a major labor market impact","As a consequence of technological change, human–machine relationships become increasingly complex and may not be reduced to mere substitution of human labor by machines","Instead, we already observe co-working in a sense of collaborative activity between humans and machines on assembly lines where both inputs are complementary; a co-working machine enhances productivity of human labor","In any case, the utilization of the machine still mostly takes place in a self-contained factory","Altogether, the impact of robotics technology is driven by numerous interacting effects",Increasing machine intelligence allows for interaction not only between humans and machines but also between machines only (M2M),"As a consequence, the possible application fields for robots continuously expand and increasingly leave well-defined and protected environments like factories","In that sense, the aforementioned substitution pressure is no longer restricted to routine manual tasks","Even though the overall implications of robotic activity are not clear, it is obvious that in the future not only repetitive routine tasks but also duties with ambitious skill requirements may be resolved by robots that increasingly provide solutions for non-routine or cognitive functions. 2.3 A formal representation of substitution and complementarities The discussion may also be linked to a specific aggregate production function that characterizes the relationship between various inputs by their respective elasticity of substitution (a so-called CES production function).  
                         Roughly speaking, this elasticity specifies to which extent a given ratio of factor demand – e.g., the demand for human labor over the utilization of a robot – reacts to changes in the respective factor price ratio.  
                         If the percentage increase in the input ratio exceeds the percentage increase in the respective factor price ratio, the inputs are substitutes",The underlying production function enables a differentiated analysis of possible relationships (complement/substitute) between various production inputs,"As a matter of principle, these relationships can also change over time","Given aggregate production as the output of, e.g., the three inputs physical capital, skilled and unskilled labor, capital-skill complementarity as stated before holds if the elasticity of substitution between capital and unskilled labor exceeds the elasticity of substitution between capital and skilled labor","Skilled labor thus reacts less sensitive to changes that are induced, e.g., by technological change","An even more general approach which allows for different elasticities of substitution between   two inputs might be formalized by a two-level CES production function.  
                         We illustrate this for the case of an aggregate production function with four inputs and apply the argumentation to the interaction between different categories of human labor and a robot","The inputs considered are highly skilled human capital ( ), medium- and low-skilled labor (  and  ) as well as a robot ( )","The corresponding production function might have the following representation  
                         :  Within the equation,   represents a factor-neutral productivity parameter;   and   are distribution parameters.  
                         Concerning the input factors, the specified production function has the following implications: Skilled labor,  , is complementary to any of the other inputs","The level of the substitution parameters,   and  , allows both for substitutive ( ,  ) and complementary ( ,  ) relationships between   and  ","At an aggregate level, a robot might at the same time complement certain types of skills (highly qualified human capital) while substituting less skilled labor. 2.4 Skills and tasks and the transformation of robotics Concerning robotics, substitution potentials arise from the possibility of providing certain tasks automatically","In industrial robotics, this mostly holds for repetitive manual tasks that are codifiable",Programmed work sequences may easily be executed by machines,"In structured environments, it is mostly highly skilled labor that will be less exposed to substitution pressure","More generally speaking, if one interprets the emergence and evolution of service robots as the natural continuation of industrial robotics, one might follow the categorization first provided by  , who analyze the skill content of recent technological change","They distinguish routine/non-routine tasks on the one hand and cognitive vs. manual tasks on the other hand.  
                         
                          gives an overview","Applying this   matrix to better understand the emergence of service robotics out of industrial robotics, one might roughly argue that industrial robots are broadly used for manual routine tasks, thereby threatening unskilled workers","Due to digitization and M2M collaboration, the other constellations within the   matrix increasingly get penetrated by ever more sophisticated machines which then are able to perform complex tasks","As a consequence, the relationship between human and machine changes over time: both are substitutes in early phases of technological development and become complements as the technology advances","At the same time, also skill levels evolve.  
                         
                      
                         It becomes obvious that even at the same level of value creation the overall labor market consequences, however, are quite unclear",Broadening possible fields of application thus does not necessarily imply that increasingly manual and cognitive tasks will be overtaken by machines,Automation in the service sector frequently implies taking over non-routine tasks,"Due to complementarity effects, especially high-skilled workers may become more productive as they become co-workers of machines and both complement each other even at the same level of value creation. 3 Substitution of human labor by service robots—the work science perspective Strictly speaking, the question whether human labor is substituted by service robots can only be answered in a non-speculative way if the benefit of human labor output is identical to that provided by a robot, presuming that in a capitalist society the cost–benefit analysis then made is always the sum of individual business decisions","This presumption is, however, undermined by numerous variables of which predominantly those of a work science perspective will be discussed in the following","First of all, it must be pointed out that a robot does not necessarily represent a mere copy or imitation of human skills, but can include an expansion of human skills to an extent that the question of a cost–benefit analysis does not even arise",The skills of humans to land on Mars or to dive in the deep ocean are limited compared to those of a robot,"In addition, also services that could in principle be performed by humans or robots alike may reveal substantial differences in quality","To the advantage of robots, this may relate to the accuracy of today’s medical operations supported by robots, or to the advantage of humans, this may pertain to the emotional quality of services that robots cannot deliver (or can only simulate, with the simulation being obvious)","In some areas, the differences in quality are clear; in others they require a more detailed analysis",Especially in the service sector there are a number of tasks for which the above-mentioned cost–benefit analysis is at least made,These calculations are stimulated by new possibilities of human–robot cooperation which enable the joint action of humans and robots within one workspace,"The following perspectives are relevant for the possible sharing of functions between humans and robots in service work systems: 
                      
                   These are organizational perspectives of the allocation of functions between humans and robots who are in an interdependent relationship and hence cannot simply be read from top to bottom or vice versa","For example, phenomena in the field of social work organization – like demographic change or the shortage of nursing staff – can serve as an impetus for the use of robots","On the other hand, new technological possibilities like immediate cooperation between humans and robots can also stimulate the increased use of robots. 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   3.1 Information technology perspective Indeed, at least the marketable development of robots has been stimulated by the new possibilities of human–robot collaboration (HRC)","Due to their size and speed, industrial robots can pose a considerable safety hazard to staff in a work system","Therefore, such robots operate within a protective cage with no one being allowed in there during operation","This involves substantial costs (space requirements, protective devices) and limits the flexibility of the robot system—in general, the production workflow cannot be changed while the system is in operation","In recent years, robot systems have been developed that in principle allow the robots to leave their cages","Specific opportunities for function sharing between humans and robots arise from optical, acoustic and haptic signal processing systems robots are increasingly equipped with","On the one hand, this makes it possible to reliably stop the robot’s movements also below the emergency stop level (e.g., by the user simply touching the robot’s arm)","On the other hand, this results in additional options for programming the robot over and above using a computer or a manual programming device (e.g., through manual movement of the robotic arm)","This can enable users to program the robot via natural speech or specific teaching methods, such as demonstrating/imitating  ",All of this promotes the flexible use of service robots also by technically untrained staff and makes it possible that their work process knowledge  gains importance in the human–robot collaboration,"Both the operation of the robot without a protective cage and simple methods of robot control are critical requirements and stimuli for the use of robots in the service sector because, unlike in an industrial context, there are hardly any work environments in the service sector that are standardized or standardizable for the use of robots, and equally no highly-skilled specialists for controlling them. 3.2 Task-oriented perspective In manufacturing, work manifests itself in the product",It is past labor in a materialized form,"Given a certain quality, you cannot tell whether a car was manufactured in Germany or in China, or predominantly by qualified or non-qualified staff or by robots","In the service sector, however, work manifests itself in the very service activity",This means that the customer/client normally comes in contact with people or bodies that perform service tasks or respective operations,It also implies that the benefit of a service rendered by robots or by human labor in the service sector is much more difficult to compare than in a manufacturing environment,"Does friendliness or sympathy, for example, belong to a service or is it an insignificant accessory? In the meantime, the “quality of the relationship” between human and robot has been analyzed as part of several scientific approaches (cf., for example,  )","At this point, the idea of a contrastive task analysis (cf.  ) is to identify different qualities of human and machine performance","In this sense, a useful technical system is not simply the most realistic illustration of what the users themselves know, do and are able to do",It is more a matter of a quality of the information technology which is complementary to the skills of the user,"The robot technology’s strengths lie in the precise execution of operations – also under adverse conditions of space and time – which allow a certain degree of standardization, but also include a certain range of variation","If tasks were predominantly invariant, the question would be what the flexibility of a robot is needed for: for doing laundry, a washing machine is probably more suitable than a robot","On the other hand, if there is no way to standardize the execution of the task, the robot will soon reach its limits","Looking after small children, for example, which does not only entail pure supervision but also empathetic reactions to the child’s behavior exceeds the current and foreseeable capability of service robots","Within a certain spectrum, however, service robots could possibly take over a range of tasks which require a lot of effort from humans (with negative health effects), do not correspond to the human diurnal rhythm (night work) or exceed the capabilities of old, sick and disabled people",This goes for task sharing with clients as well as with service personnel (still) in the work system,"With regard to maintaining and developing the skills of service staff, it is important to ensure that tasks of decision-making are not substituted by the robot but are controlled by service personnel","Otherwise the so-called automation paradox occurs, which means that, during normal operations, human workers have to carry out cognitively draining activities, and in case of malfunctions or unexpected situations they suddenly have to make highly sophisticated decisions which they do not master anymore","Here, too, the principle of complementarity in the sharing of tasks between humans and robots applies from a work science perspective, i.e., robots take on tasks where humans are not or hardly able to do so",This principle is particularly relevant when it comes to the decision-making authority between service personnel/client on the one hand and the robot on the other,"The question arises because modern robots have a system of artificial intelligence (AI) with the help of which they can take decisions (to a limited extent, i.e., semi-autonomously)","It is a matter of interaction between human and artificial intelligence (cf. in more detail  ): When is the robot allowed or expected to render a service autonomously based on a situational analysis without being given the express order to do so? When is it allowed to correct human mistakes even without explicit instructions? 3.3 Perspective of social work organization Presuming that robots can render services to the same level of quality as human service personnel, this raises questions regarding the availability and remuneration of suitably qualified staff as well as the legal regulations both for their and the robots’ deployment","To illustrate the points mentioned above, the care sector is used as an example, as it is an area where the use of service robots is sometimes considered: As long as the remuneration for care services is poor and workforce needs are met by foreign workers, there is little reason for a widespread use of service robots to substitute human services","Furthermore, it is a question of social acceptance, which has already triggered a controversy in connection with the robot seal “Paro” used in the treatment of dementia patients  : Whilst care services in Germany are already very clearly defined as and limited to the instrumental provision of a desired function, it is in no way agreed that people in need of care, relatives, carers, health insurances, etc. will accept as “care” what a robot is able to render","Even in case of a possible labor shortage in the care sector, the use of robots will be a limited option—if all technical problems are solved. “Limited option” means that service robots will presumably cooperate with human care personnel within a work system and will interact with people (care personnel as well as clients/customers) who are not trained in the operation of robots",What was very roughly outlined here for the care sector is also applicable to other service areas,"The aspects that should always be considered include: 
                         
                      In addition, when service robots are used in a private environment, the application strategies for robots may to some extent become detached from the mentioned considerations of cost and revenue due to personal preferences and interests of the robot users",This goes hand in hand with the following: Service tasks cover a broad area of private and business life: from garbage separation to looking after toddlers or dementia patients,Presumably these tasks are not of equal importance to all people and it is therefore not irrelevant who performs these tasks,"Hence, it is less a question of social acceptance of service robotics as a whole, but the question relates to specific service tasks","The problem of social acceptance has a much stronger effect on forecasts of substitution than in the manufacturing sector where customers have long become accustomed to, e.g., cars manufactured by robots: Who would like to substitute the nanny by a robot in child care, even if the robot manufacturer could credibly demonstrate that the robot can also handle children with specific educational needs? 3.4 Interim conclusion The perspectives outlined above are explicitly or implicitly anticipated by a robot manufacturer: What technological options can be implemented today for a service robot (information technology perspective), what tasks is the robot able to perform in contrast to and/or in collaboration with humans (task-oriented perspective), and what affordable and legitimate needs of private or institutional service providers are met by such a technological artifact (societal perspective)? What is important is that these are mental anticipations of reality, not reality itself","These anticipations on the part of robot manufacturers are also influenced by “trends” in relevant discussions (topics such as Industry 4.0, demographic change, humanoid robots, etc.)",To what extent the service robots manufactured on the basis of these anticipations meet affordable needs of potential customers can certainly not be predicted for service robotics or the service sector in general,"This requires a specification of the service areas and an analysis of foreseeable development of wages/qualification of the employees, of the strategies of local companies and institutions, as well as of the relevant legal regulations. 4 First conclusions for technology assessment of service robots Assuming that robots are developed for services currently rendered by human labor, service robots will probably take over at least parts of these tasks",This brings about changes in the work of humans who will now render services with the support of robots,"In case of a complete takeover of the entire service task, the change would, in an extreme case, imply that humans do not work anymore and become unemployed","Another possible situation is that the collaboration with service robots enables humans to render their services more efficiently, which could eliminate half of all jobs in this service segment","From the perspective of interdisciplinary technology assessment  , it would now be interesting to know whether a statement can be made ex ante on how a partial or entire substitution would affect the work processes of humans and what economic effects this substitution can be expected to have on the labor market",The results presented in this paper demonstrate that no generally valid conclusions can be drawn,"Depending on whether the collaboration between humans and robots constitutes a substitution of tasks or complementary task sharing, the impacts on the labor market can be completely different",Also statements as to the level of personnel put under particular economic pressure by the use of service robots cannot be generalized,"Due to the cost–benefit ratio resulting from the low costs of labor for services classified as simple or low-skilled, modern service robots are not likely to gain market share in this segment","Therefore, it is more likely for service robotics to put people with a medium level of education under some pressure","Also from a work science perspective, the situation is complex",The information technology perspective indicates that robots – with regard to their hardware – generally become less of a safety risk and therefore – unlike industrial robotics of the past – generally enable close collaboration with humans,This progress in the hardware development of robots and their capacity of processing environmental information goes hand in hand with advances in programming service robots that enable also people without training in information technology to control service robots to a limited degree,"Taking a closer look at the tasks service robots are intended to be used for, one can identify areas in which a successful deployment of service robots is already conceivable today","The use of service robots is especially promising for those tasks that imply a lot of effort, health problems, a difficult diurnal rhythm or a specific physical burden for humans","The perspective of social work organization as a whole takes account of availability and remuneration, the expectations of potential customers, the skills of current personnel, as well as the performance capability and purchase and maintenance costs of robotic technology","Still, the fact that according to these parameters a use of robots may be practical does not say anything about the factual acceptance of these service robots",This requires a very detailed analysis of the service segment the robot is to be used in,So the possibly disillusioning conclusion is that generalizing statements cannot be made—neither from an economic nor from a work science point of view,"However, criteria can be derived from both disciplinary perspectives which enable an assessment of specific service robotics systems within their respective operational context",From the analysis of these contexts it is then indeed possible to develop performance criteria which make a successful cooperation between humans and service robots in a service context probable,The work science analysis of these cooperative services and the overall economic analysis allow drawing relevant conclusions for interdisciplinary technology assessment,"This can firstly result in direct recommendations for technological development, for example, for optimizing the human–machine interface for a specific cooperative task","Secondly, there may be indications for legal regulations, e.g., when it must be determined who or what is liable for a damage that occurs in connection with a cooperatively rendered service","Statements regarding service robotics in general are hard to be justified on the basis of this set of criteria because the quality of the services, the potential for standardization of work environments, the design of the human–machine interface, the level of education of the human service providers, etc. are so varied that a contextual analysis is imperative—with a level of detail below the common service sectors","As a consequence, there will be no general technology assessment of service robotics as such, but exemplary studies will have to be conducted for different service robots.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0957417419301903," 1 Introduction At a time when even the largest banks are not immune to distress, credit decision-making is crucially important",The Reserve Bank of India (RBI) and the Finance Ministry has thus far externally controlled and regulated the banking sector,"Deregulation and the decoupling of state control pose new challenges, and intense competition is placing the survival of all but the fittest and the most efficient in doubt",Commercial banks are accordingly striving to adjust to a new economic and technological environment,Sound credit scoring models form an integral part of this adjustment process,This motivates our present purpose which is to propose suitably conceived and designed credit scoring models for personal loans with due allowance for the incidence of default,The novel contribution of the present paper consists in integrating two stages of the decision process with reference to the Indian banking sector,"Firstly, we build credit scoring models for our unique sample of personal loans, provided by one of the largest Indian banks",The sample includes a significant number of bad debts that is consonant with the current and evolving profile of personal indebtedness,"Secondly, we explore in detail the characteristics of the defaulters in our sample",This feature is particularly important given the recent history of rising bad debt,"In both stages, we identify the key predictor variables to be used in building models","Further, we evaluate our models by using   misclassification costs","The sharp increase in household leverage ratios in recent years shown in  
                      a (Leverage Ratios in India) portrays the increase in borrowers’ vulnerability.  b (Growth of Personal Loans and Housing Loans) shows the muted growth of personal loans over recent years up to the end of 2010","However, the year ended March 2011 saw the increase of 17% portrayed in  
                      , against only 4.12% in the year ended March 2010","The rate slightly decreases in the next two years, 2012 and 2013, which is commensurate with the increase of non-performing assets reported on Indian banks’ balance sheets ( )","It should be emphasised that at the end of March 2014 retail credit has increased driven primarily by housing loans, personal loans and auto loans representing 47%, 36% and 14%, of gross credit respectively ( )","Indian market credit bureaux, for example Credit Information Bureau India Limited ( ), collect credit data for the banking industry",CIBIL maintains a repository of the credit history of all commercial and consumer borrowers in the country and it provides information to any bank to facilitate their credit granting decisions,CIBIL’s Consumer Credit Bureau deals with the credit history of individual customers while the Commercial Credit Bureau maintains the credit history of non-individual clients such as corporates,CIBIL provides credit information as distinct from opinions and does not classify any client’s loan as being in default unless the lender has already classified it as such,"While many research papers have discussed credit scoring models for developed countries ( ), relatively few have focused on building such models for developing and emerging markets ( )","While these have addressed a wide range of cases none, to the authors’ knowledge, have examined the Indian banking sector",Given the sensitivity of data access is significant,"Particularly, in the light of past financial crises, banks become increasingly risk reverse due to security and clients data protection laws","Small samples are widely used in building scoring models in the literature, as this issue is well recognised (see for example  )","For instance, consumer loan applications models are regularly built using around1,000 observations or less (see for example  )","In building scoring models, statistical techniques such as discriminant analysis and logistic regression are widely used ( )",The logistic regression model does not necessarily require the assumptions of the discriminant analysis model and may prove to be more robust in practical applications,"Other classification techniques such as classification and regression tree, k-nearest neighbour and support vector machines are also in common use ( )","Various neural networks, including artificial neural networks, multilayer perceptron neural networks and back-propagation neural networks, have also been used in building scoring models ( )","Amongst these probabilistic neural networks provide results which are significantly more accurate in building personal loan scoring models (see,  )",Comparisons between traditional and advanced scoring techniques have been the subject of numerous studies ( ),A substantial number of these studies demonstrate the superiority of neural networks over conventional techniques ( ),"However, there is still a role for conventional techniques such as discriminant analysis and logistic regression in building scoring models for personal loans (see for example,  )",In this paper four statistical modelling techniques are applied to analyse bank personal loans using a data-set provided by an Indian bank,"As motivated by the above literature these are discriminant analysis, logistic regression, multi-layer feed-forward neural networks and probabilistic neural networks","Three different criteria namely correct classification rate, error rates and   misclassification cost are used to compare the effectiveness and predictive capabilities of different models","Moreover, in this paper   misclassification costs, provided by the bank’s own credit officials, are used in preference to the more conventionally used estimated misclassification costs",This underscores the novelty of our contribution,The layout of this paper is organised as follows:   reviews the current guidance note on credit risk management by RBI.   addresses research methodology and data sources.   discusses the empirical results.   concludes and discusses the opportunities for further research. 2 Current credit risk management practices in Indian banks In the 21st Century banks are confronted with an increasingly complex combination of interdependent financial and non-financial risks,"This includes credit, interest rate, liquidity issues, regulatory, reputational and operational risks",These risks need to be controlled and managed by banks’ senior executives,"Further, major decisions about whether or not to implement a centralised or decentralised structure to manage these risks are faced by banks all over the world","In India, banks have been guided by a centralised approach on their credit risk from the RBI “Guidance Note on Credit Risk Management” that was issued in 2002. 
                       These guidelines recommend that banks need a credit risk framework that focuses on policy and strategy, organisational structure and systems, as discussed below. 
                      ",Banks require a board-approved risk policy and strategy that clearly identifies how to manage the bank’s lending portfolio,Strategic plans must establish the credit granting processes that will be utilised by the bank with due consideration for the target market and cost/benefit considerations.  ,"Risk management committees and credit risk management departments are vital structural components in establishing successful risk systems that clearly identify accountability and ensure that responsibility flows from the Board of Directors down to lending officers. 
                       are used to avoid an overly simplistic approach to risk classification and a process that is used to formulate risk-ratings is as follows: 
                   Credit risk modelling techniques encourage a more quantitative and less subjective approach to personal lending",These methods have enhanced the measurement of risk and performance in banks’ lending portfolios,"The modelling techniques suggested by the RBI Guidelines include econometric techniques, neural networks, optimisation models, rule-based or expert systems and hybrid systems","In this paper we explore the first two set of techniques (for details regarding the credit risk framework, see the Appendix)",Credit risk models as described by RBI Guidance Notes encourage the statistical analysis of historical data including the Z-score model and Emerging Market Scoring (EMS) model ( ). 3 Research methodology The main aim of this paper is to investigate whether apposite credit scoring models can lead to more efficiently discriminating creditworthiness evaluation and ultimately towards lower default rates,At an early stage of this research we conducted structured interviews with key decision-makers in a number of private and foreign banks in India,"This included state and regional sales managers, territory managers of personal loans, branch managers, credit approvals and credit default controllers",The importance of doing this was threefold,"Firstly, these interviews enabled us to establish a list of explanatory variables, which are used as part of   lending procedures","Secondly, the results of these interviews form a natural complement to the available academic literature","Thirdly, we were able to establish that there was no set method used in the evaluation of personal loan applications in India",In many cases a predominantly judgemental approach was employed,"In building our proposed scoring models we adopt a two-stage analysis and use four different statistical modelling techniques namely discriminant analysis, logistic regression, multi-layer feed-forward neural networks and probabilistic neural networks","In the first stage, we build our scoring models and, using   misclassification costs, test the predictive capabilities of the various scoring models","In the second stage we focus upon the   cases, using ‘customer began to default’ as a dependent variable, and the same set of explanatory variables as used in the first stage of the analysis","Furthermore, a Variable Impact Analysis is conducted as part of the two stage analysis to identify the key determinants of both successful and defaulted cases. 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   3.1 Data collection and sampling procedures In order to build our proposed credit scoring models, we use historical data comprising 2093 personal loans supplied by one of the largest banks in India","Thus, given the data sensitivity, our sample size is in line with the previous literature (see for example,  )",The significance of our dataset is as follows,"Firstly, based on literature reviews in   and  , our sample size appears to be in the top 20% of the published literature","Secondly, even when reported, larger sample sizes can be misleading",Often studies report results for multiple sub-samples,"Though the average sub-sample size may be higher than our sample, it is common that several of the sub-samples may be significantly smaller than 2000 observations (see e.g.  )","Thirdly, our application is interesting and important in its own right due to its focus upon developing countries","Of the ten papers identified in   as having larger sample sizes than our own, seven focus upon developed countries","In terms of applications to developing countries larger samples are either derived from externally funded research projects ( ; Huang et al., 2006) or, whilst slightly larger, are of a similar order of magnitude ( ; 2765 cases)","Fourthly, it is important to recognise that our sample derives from a real-world credit scoring problem and data we ourselves collected",This stands in marked contrast to a small number of classical datasets that are regularly used in studies of credit scoring (see e.g,Table 3 in  ),"Furthermore, our unique blind data set used in this paper covers a lending range from Rupees ₹ crore 50,000 to Rupees ₹ crore 100,800,000 for its customers from 2009 to 2014, of which 1233 are considered good loans and the remainder  ","Having such a high percentage (41.09%) of bad loans, the dataset can be considered as ‘  (see for example,  )",The Indian bank provide 20 predictor variables which are mainly used in their decision making process,"However, 6 predictors are excluded leaving 14 explanatory variables which are used in building the scoring models, as shown in  
                         ","Having a ‘land line’ is a mandatory decision criterion, without which the application is declined","Similarly, the provision of legal documentation is mandatory","Both ‘state” and ‘pin code’ (equivalent to a postal code in the UK or a zip code in the USA) are considerably highly correlated (i.e. 97.70%) and therefore pin code is excluded. 
                          We also excluded both the ‘starting and the ending actual year’ as we use ‘term’ as an explanatory variable. 
                          The ‘customer begin to default’ variable is excluded when building the scoring models in the first stage","However, this variable is used as a dependent variable when running the sensitivity analysis investigating the incidence of the default cases, 
                          i.e. in the second stage, see  
                      In order to build our scoring models, Palisade Neural Tools, STATGRAPHICS Centurion XVI, IBM-SPSS Statistics 22 and R are used",We use a stratified 10-fold cross-validation technique to test the predictive capabilities of our scoring models,"We randomise the data so that the percentage of bad customers in each group is the same, using R","The training set consists of 1883 cases (except for three folds, which consists of 1884 cases) and the hold-out set consists of 209 cases (except for three folds, which consists of 210 cases). 
                         
                      3.2 Statistical scoring techniques 
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                      3.2.1 Discriminant analysis Discriminant analysis (DA) is a discrimination and classification technique, first popularised in bankruptcy prediction by  ","The following formula can be used for MDA: where, Z represents the discriminant z-score, α is the intercept term, and   is the respective coefficient in the linear combination of explanatory variables,  , for i = 1 to n (see, for example,  ). 3.2.2 Logistic regression Logistic Regression (LR) is a widely used statistical modelling technique, in which the probability of a dichotomous outcome is related to a set of predictor variables in the form: where, 
                             is the probability of default, α is the intercept term, and   represents the respective coefficient in the linear combination of predictor variables,  , for i = 1 to n","The dependent variable is the logarithm of the odds ratio,  (see, for example,  ). 3.2.3 Multi-Layer Feed-Forward Network It is convenient to use Multi-Layer Feed Forward Networks (MLFNs) to represent complex relationships between a set of variables.  
                             presents an example of a MLFN structure as follows: The following formula explains the MLFN function for two hidden layers: where, 
                             = the output of the network;  = conversion function for the output layer;   = connection weighted summation to the output layer from the second hidden layer;   = conversion function for the second hidden layer for node  = conversion weighted summation from the first hidden layer to the second hidden layer;   = conversion function for the first hidden layer for node   = conversion weighted summation from the input layer to the first hidden layer;   = inputs variables for node   = number of nodes in the second hidden layer;   = number of nodes in the first hidden layer; and   = number of input nodes (see,  , p. 102). 3.2.4 Probabilistic Neural Network A Probabilistic Neural Network (PNN) is primarily a classifier, mapping inputs to a number of classifications, which might be imposed into a more general function.  
                             presents an example of a PNN structure, as follows: The Bayesian probability density function, for the respective output from PNN pattern node, can be represented as follows (see,  ): where, 
                            = vector of observed inputs;   = number of training patterns for class  ;   =  th training vector for class  = vector-dimension;  = standard deviation parameter for smoothing purposes;   = category class;  = transposition function for vector; and   = probability","The conditional probability can be written as: for each class, using the basic Bayes’ formula (see,  , p. 100). 4 Empirical results and analysis We present descriptive statistics for our predictor variables followed by our two-stage results","Stage one, focuses on presenting the results of the four statistical models (shown in  .) using the 10-fold cross validation","Then we compare different statistical techniques results predictive capabilities using average classification rates, errors rates and  ","In addition, we present a ranking of the relative importance of the predictor variables","Stage two performs an additional sensitivity analysis of the   cases. 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   4.1 Descriptive statistics 
                         
                          provides descriptive statistics for the categorical variables used in building our scoring models",It can be concluded that ‘state’ is the most important predictive variable as it has the highest information value of 0.021,It is clearly evident that State C has the worst Weight of Evidence (WOE) value of −40.42 compared to 12.52 for State A,This may imply a preference of lending to clients from State A,"Similarly, and counter-intuitively, females (WOE = −23.65) are less creditworthy compared to their male counterparts (WOE = 4.95)","Our descriptive statistics show that other predictor variables are less important, with lower information values, when compared to State and Gender","As to the continuous predictors, five variables are also used in building our scoring models as follows: Age ranges from 23 to 56 years old; Term ranges from 2 to 4 years; EMI ranges from Rupees ₹ crore 1468.5 to Rupees ₹ crore 2960,496; Loan Amount ranges from Rupees ₹ crore 50,000 to Rupees ₹ crore 100,800,000; and Net Income ranges from Rupees ₹ crore 570,000 to Rupees ₹ crore 1310,000","The following sub-sections present classification results, including   Misclassification Costs (AMC), for our scoring models presented in  ","We use actual ratios of 6.5:1.6 and 15:1.7 for 2006 and 2011, respectively, to calculate the AMC associated with Type II and Type I errors",These actual ratios were provided by the Indian bank’s own credit officials,"This offers a refinement of the traditional approximate way of incorporating expected misclassification costs in the literature (see for example,  )","Our unique AMC can be calculated using where, ACR  denotes the corresponding actual cost ratio associated with a Type I error; P  denotes the associated probability of a Type I error; π  denotes the prior probability of good cases; ACR  denotes the corresponding actual cost ratio associated with a Type II error; P  denotes the associated probability of a Type II error; π  denotes the prior probability of bad cases","These actual misclassification cost ratios that were provided, pre credit crunch, demonstrated a more favourable outlook in India with a 2006 ratio of 1.6:6.5 compared to previous studies (see for example,  ) who used a ratio of 1:5","However, the later figures used reflect a clear deterioration in the Indian lending climate with a ratio of 1.7:15 being used from 2011","This deterioration is confirmed by observations that the RBI raised interest rates to tame inflation and, due to worsening credit conditions, asked lenders to double their provisions for bad loans (see  )","Furthermore, as an additional robustness test, for the two neural network models, namely PNN and MLFN, we run the 10-folds cross validation again, this time allowing the 10-folds to be chosen at random. 4.2 Statistical scoring techniques: Stage 1 
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                      4.2.1 Discriminant analysis 
                            
                             summarises the classification results for the 10 DA scoring models hold-out sub-samples using a default cut-off score of 0.50",The Average Correct Classification Rates (ACCR) range from 63.33% to 78.10% with a mean ACCR of 67.61%,Type I errors range from 8.06% to 30.08%; Type II errors range from 41.86% to 60.47%; and Total Error (TE) rates range from 21.90% to 36.67%,"The average mean for Type I, Type II and TE are 21.82%, 47.56% and 32.39%, respectively","Notably, the   misclassification costs for years 2006 and 2011 range from 1.19 to 1.72, and from 2.65 to 3.85, with an average mean of 1.48 and 3.15, respectively (see  )","Clearly, this suggests that AMC has significantly increased over time","This should motivate decision-makers to apply scoring models to reduce default rates. 4.2.2 Logistic regression Results of the 10 LR scoring models hold-out sub-samples using a default cut-off score of 0.50, are shown in  
                            ",The ACCR range from 50.24% to 77.62% with an average mean of 66.03%,Type I error rates range from 11.29% to 50.24% with an average mean of 27.49%,Type II error rates range from 38.37% to 51.16% with an average mean of 43.26%,The TE rates range from 22.38% to 49.76% with an average mean of 33.97%,"As per actual misclassification costs, they range from 1.13 to 1.69 and from 2.47 to 3.47 for years 2006 and 2011, respectively",The average mean for the AMC for years 2006 and 2011 are 1.41 and 2.94 (see  ),"Again, our results show notable increases in AMC over time","These results are in line with DA scoring models results shown in  . 4.2.3 Multi-layer Feed-Forward Networks 
                            
                             and  
                             give the classification results for the 10 MLFN scoring models hold-out sub-samples and the additional 10 MLFN scoring models based on random runs, respectively","As per the former, the ACCR ranges from 63.16% to 76.67% with an overall mean of 67.13%","Type I, Type II and TE rates range from 10.57% to 44.72%, from 19.77% to 54.65%, and from 23.33% to 36.84%, respectively","The overall mean for these error rates are 27.74%, 40.23%, and 32.87%, respectively","For MLFN the AMC ranges from 0.95 to 1.68, and from 1.67 to 3.60 for years 2006 and 2011, respectively","The overall means for these AMC are 1.34 and 2.76, respectively (see  )","As per the latter, our 10 MLFN scoring models based on random runs show slightly better results under each of the previous criteria","As shown in  , the overall means are 70.57%, 23.15%, 39.13%, and 29.43% for ACCR, Type I, Type II and TE rates, respectively","More importantly, the AMC results also improved showing that the overall means are 1.22 and 2.55 for years 2006 and 2011, respectively","These results emphasise that MLFN can offer better results compared to conventional statistical techniques shown in  – . 4.2.4 Probabilistic Neural Networks 
                            
                             summarises classification results for the 10 PNN scoring models hold-out sub-samples",The ACCR ranges from 59.81% to 81.90% with an average mean of 68.71%,Error rates results show that they range from 5.65% to 31.71% for Type I error with an average rate of 23.11%; they range from 36.05% to 52.33% for Type II errors with an overall mean rate of 43.02%; and they range from 18.10% to 40.19% for the TE rates with an overall mean of 31.29%,"AMC results show that they range from 1.01 to 1.70 and from 2.27 to 3.55 for years 2006 and 2011, with average means of 1.37 and 2.88, respectively (see  )","Results shown in  
                             are for the 10 PNN scoring models based on random runs","Clearly, these results are the best amongst our scoring models with exception of the AMC 2011 results","The overall means are 73.20%, 18.49%, 38.73%, and 26.85% for ACCR, Type I, Type II and TE rates, respectively","Furthermore, the AMC results show that the overall means are 1.21 and 2.59 for years 2006 and 2011, respectively","These results demonstrate that our neural network models, namely PNN and MLFN, can lead to further material reductions in default losses. 4.3 Comparison of different statistical scoring models Comparing different models where the same 10-folds are used, neural network models, namely PNN and MLFN, outperform conventional models, namely DA and LR, used in this paper","That is, PNN models show the highest ACCR of 68.71% and the lowest TE of 31.29%; whilst MLFN show the lowest AMC of 1.34 and 2.76 for 2006 and 2011, respectively","Furthermore, when the 10-folds are randomly chosen both PNNran and MLFNran results show improvement under different criteria and both models are still outperform other techniques","On the one hand, PNNran has the highest ACCR of 73.20%, the lowest TE of 26.85% and the lowest AMC of 1.21 for 2006, whilst MLFNran has the lowest AMC of 2.55 for 2011",Our results suggest that the default rate of 41.09% could be reduced to 26.85% using PNNran scoring models (see  ),"We then use a   model, which is a one-way Analysis of Variance (ANOVA), to investigate whether there are significant differences between different models for the scoring criteria outlined above. 
                          The general linear model with categorical variables is formed by setting where, 
                          is the overall mean,  is the  th treatment effect (under the identifiability constraint  ), and the ɛ  are iid  (0,  
                         ) (see for example,  ).  
                          shows our results and there is an evidence of statistically significant differences between the scoring models for each criterion","The graphical illustration (see  
                         ) confirms the findings shown in  . 
                         
                         
                         
                      4.3.1 Importance of different predictor variables used in building the scoring models 
                            
                             shows the Average Variable Impact (AVI) for each of the 14 predictor variables under each of the scoring models applied in this paper across 10-folds","Clearly, alternative models may treat various predictor variables differently when it comes to their impact on loan quality","By averaging the variable impact weight over 60 scoring models, for each predictor variable under each of the statistical techniques, we identified net income (NINC), marital status (MRST) and loan amount (LAMT) as of key importance in distinguishing clients’ creditworthiness","In contrast, vehicle ownership (OVEH), loan duration (TERM) and client’s job (JOB) are the least important determinants of clients’ creditworthiness. 4.4 Sensitivity analysis of default credits: Stage 2 The main aim of this stage is to shed light upon the default cases given that they constitute a relatively large proportion of the entire sample (over 41%, 860 out of a total of 2093 cases)",We use a stratified 5-fold cross-validation technique to explain the timing of the incidence of default,We use the same four statistical modelling techniques shown in  ,We rerun additional 5-fold cross validation with folds randomly chosen by the software for both MLFN and PNN,"However, it should be emphasised that the main focus of this section is to identify the key determinants of the incidence of default","Interestingly, in our sample, default occurs only in the first and second years, and none in later years","We randomise the data so that the percentage of bad customers who start to default in their first year and those who start to default in their second year are the same, using R","The training set consists of 688 cases and the hold-out set consists of 172 cases. 
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                      4.4.1 Descriptive statistics for default customers In building our scoring models, we use the same 14 explanatory variables, as shown in  ","However, the dependent variable used in this section is ‘customer begin to default’ replacing ‘loan quality’ in the original modelling","As to the five continuous predictors, Age ranges from 23 to 56 years old; EMI ranges from Rupees ₹ crore 1469 to Rupees ₹ crore 469,920; Loan Amount ranges from Rupees ₹ crore 5000 to Rupees ₹ crore 16,000,000; Net Income ranges from Rupees ₹ crore 570,000 to Rupees ₹ crore 1250,000; and Term ranges from 2 to 4 years","Nine categorical variables are used in building our models.   the sample consists of 693 males and 167 females; 353 single, 498 married and 9 others; 442 graduates and 418 post-graduates; 251 work in the public sector and 609 work in the private sector","Our sample show that 288 start to default during the first year of the loan facility, and 572 start to default during the second year. 4.4.2 Importance of different variables for the default cases It is crucial for decision-makers to become fully aware of the key determinants of the incidence of default, which in turn may reflect on their final decision.  
                             shows the AVI for each of the 14 predictor variables under each of the models across 5-folds","By averaging the variable impact weight over 30 models, for each predictor variable under each of the statistical techniques, we identified the following three key determinants of the incidence of default, in order of importance: State of residence (STATE); equated monthly instalment (EMI) and actual loan amount (LAMT)","This stands in marked contrast to vehicle ownership (OVEH), previous employment (PEMP) and educational level (EDU) which are the least important predictor variables","Considering both the first and the second stages impact analyses of predictor variables, we strongly recommend the Indian banking sector to take into account the following set of predictor variables when making lending decisions:  ",This can have a demonstrable impact on the loan quality and subsequently on the overall lending decision making process,"We run additional statistical tests to distinguish between early and late defaulters in relation to our key variables namely,  ","There are no significant differences between different MRST sub-categories namely single, married and others","Likewise, there are no significant differences between different levels of income","In contrast, early defaulters are associated with higher levels of EMI and LAMT","Furthermore, none of the residents in State C has defaulted in the first year; however, much larger numbers defaulted in the second year","Finally, the largest number of both early and late defaulters are located in State B","In summary, and as part of our policy implications, recent news report that high default rates, rising bad debts and shrinking cash flows has led to enforced redundancies and the closure of a significant number of branches throughout India ( )","Thus, evidence clearly demonstrates that it would have been less costly for the bank had it adopted our credit scoring models rather than implementing their own strategic decisions to downsize","These lessons are not limited to the Indian bank that provided our loan data-set as confirmed by recent news that four major foreign banks have reduced their exposure to the Indian market ( ). 5 Conclusions and areas for further research The main aim of our paper is to use a two-stage analysis to investigate whether scoring models can efficiently distinguish the Indian banking clients’ creditworthiness, and reduce default rates","Working alongside the bank, our fresh contribution includes the incorporation of   misclassification costs when evaluating our models",Our statistically rigorous analysis also stands in marked contrast to the predominantly subjective approach the bank were using to make lending decisions,"In building our models we use four statistical modelling techniques namely discriminant analysis, logistic regression, multi-layer feed-forward neural network and probabilistic neural network",This is combined with a bespoke data-set with a default rate of over 41%,"As to our first stage, our 10-folds analysis shows that both PNN and MLFN, outperform conventional statistical models",PNN models perform better compared to other models in terms of conventional classification criteria such as ACCR and TE,"However, MLFN models outperform others (including PNN) once   misclassification costs are incorporated achieveing the lowest AMC of 1.34 and 2.76 for 2006 and 2011, respectively","Moreover, when the randomly seclected 10-folds are incorporated, PNNran models outperform all other techniques (including MLFNran) achieveing the highest ACCR, the lowest TE, and the lowest AMC of 1.21 for 2006","However, there is still a role for MLFNran achieveing a marginally lower AMC of 2.55 for 2011",We have evidence of statistically significant differences between the scoring models for each criterion using a g  model,"Out of 60 scoring models, we identified NINC, MRST and LAMT as key determinants of creditworthiness in the Indian banking sector","As to our second stage, we use 5-folds cross validation to build our models using the same set of statistical modelling techniques to explain the timing of the incidence of default","Out of our 30 models, we further identified STATE, EMI and LAMT as key determinants of the timing of default","Moreover, when combining both stages outcomes, we identified   as the most important predictor variables for the Indian banking sector",Further analysis shows that early defaulters are associated with higher levels of EMI and LAMT,STATE level effects are also prevalent in the incidence of default,"This suggests that, in practice, greater care needs to be exercised when granting loans to clients from different states","In summary, by applying our proposed scoring models to the Indian banking sector, and alongside successful implementation, we argue that the challenges facing the Indian market could be significantly reduced","In particular, our best scoring models can significantly reduce our sample default rate by 14.24% (i.e. 41.09%, the original default rate – 26.85%, default rate using PNNran).   problems such as increasing interest rates in an attempt to restructure default debt, inflation and the increased cost of banks’ debt could be mitigated",Other consequences of the high default rates have been the redundancy and branch-closure policies that some Indian banks followed in an attempt to cut costs,We submit that some of these cost-cutting measures could thus ultimately have been avoided,In terms of the theory of expert and intelligent systems our proposed two-stage approach forms a natural complement to previous neural network ( ) and hybrid ( ) modelling of credit risk,We also show that methods such as neural networks can lead to better assessments of credit risk than classical statistical methods ( ),Beyond reproducing aspects of real decision-making our results show that neural network models can lead to improved financial decision-making in industrial applications,"In particular, neural network models may be particularly useful when the distribution of instances in the dataset is unbalanced ( ) or information is scarce ( )",There are a number of opportunities for further work,This includes the application of additional techniques and their possible combination into integrated models with larger sample sizes,"In particular, gene expression programming, fuzzy algorithms, proportional hazard models and SVM etc",Limitations of our study include potential concerns over the accuracy of industry-standard costings and the need for high computational efficiency in industrial-sized financial applications (see for example  ),Results may also be sensitive to the economic conditions associated with the timing of the business cycle (see for example  ),"However, recent financial turbulence in India suggests extending our study to other products including credit cards, business loans and mortgages would also be extremely timely","CRediT authorship contribution statement 
                       Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing - original draft, Writing - review & editing, Visualization, Supervision, Project administration.   Conceptualization, Investigation, Data curation, Writing - original draft.   Conceptualization, Software, Validation, Formal analysis, Writing - review & editing.   Conceptualization, Methodology, Validation, Formal analysis, Visualization","Appendix: grading system for calibration of credit risk In this section, we discuss the rating scales and weighted scoring systems as typically applied in the lending departments of Indian banks. 
                      : 
                   
                      : weighted systems apply a score or grade for risk profiling with suitably applied percentages assigned to each of the risk-ratings to produce a weighted average risk-rating","The example as shown in  
                       below would be considered as a potentially low-risk rating: Clearly the problem is how the Credit Risk Framework (CRF) assigns those weightings",In this paper and as a starting point we are assigning weightings for personal loans based on advanced statistical techniques such as neural networks to avoid any subjective bias in assigning these weightings.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0888613x1400067x, 1 Introduction Association mining consists of two phases: pattern mining and rule generation,"Many efficient algorithms have been developed for pattern mining, however, due to the huge number of patterns generated by the mining process, the challenging issue for pattern mining is not efficiency but interpretability  ",Frequent closed patterns partially alleviate the redundancy problem,"Recently, many experiments   have proved that frequent closed patterns are a good alternative to terms or  -grams for representing text features","Several approaches for post-processing of patterns have also been proposed recently: pattern compression  , pattern deploying   and pattern summarization   have been proposed to summarize patterns",The phase of rule generation finds interesting rules based on discovered patterns and a minimum confidence,This phase is also a time consuming process that can generate many redundant rules,"The approaches for pruning redundant rules can be roughly divided into two categories, the subjective based approaches and objective approaches",The former approaches are to find rules that satisfy some constraints or templates  ,The latter approaches construct concise representations of rules without applying user-dependent constraints  ,"The use of closed patterns has been found to greatly reduce the number of extracted rules; however, a considerable amount of redundancy still remains  ",There are several obstacles to overcome when using association mining in real applications  ,"The first problem is the overwhelmingly large volume of discovered patterns, rules and false discoveries","Various methods have been proposed to tackle this, some of which we now briefly describe","Frequent association mining has been extended to multilevel association mining, which uses concept hierarchies or taxonomy trees to find rules  ",The leaves of a taxonomy tree represent items at the lowest level of abstraction,"Using a top-down strategy, at each level, frequent patterns are calculated based on accumulated counts","Recently, mining flipping correlations   has been proposed to find positive and negative correlations in taxonomy trees","Another paradigm is the filtered-top-  association discovery   which used three parameters: a user specified measure of how potential interesting an association is, filters for discarding inappropriate associations, and   the number of associations to be discovered",The second problem is the lack of semantic information along with the mining process because most algorithms are developed for transaction databases,"The third problem is the insufficiency of knowledge coverage due to the use of two approximation phases (a minimum support for the pattern mining phase and a minimum confidence for the rule generation phase) that miss some specific patterns or rules (e.g., the low-support problem, in which a large pattern is more specific but has a very low support  )","Based on the above overview, the increasing size of patterns or rules rapidly becomes unmanageable, and users are looking forward to new interpretation methods to process and “understand” the knowledge in data sets","Currently there are several different approaches for the interpretation of discovered knowledge based on some sorts of semantic annotations: an OLAP based visualization method  , a generating semantic annotation method  , flow graphs   and multi-tier structures  ","Multi-tier structures use “granules” instead of “patterns” and “rules”, and defined meaningless rules based on the relationship between long rules and their general ones (short rules)","In this paper, we continue to develop multi-tier structures for the interpretation of association rules in databases in order to provide a new solution for the challenging issue","To illustrate the relationship between patterns and granules, we present a method to interpret granules in terms of patterns, and prove that decision rules and max closed patterns are mutually corresponding",We also indicate that small closed patterns can be interpreted as smaller granules,"Moreover, we formalize concepts of association mappings and present efficient algorithms for the constructions of multi-tier structures","Finally, experiments on a real dataset and Foodmart 2005 data collection have been conducted and the results show that the proposed approach is promising","The remainder of the paper is structured as follows: Section   discusses related work, Section   introduces basic definitions, Section   presents a method for the interpretation of granules in terms of patterns, Section   discusses the concept of multi-tier structures and presents a method to estimate patterns' support based on granules, Section   presents definitions and properties for association mappings, Section   proposes efficient algorithms for the construction of multi-tier structures based on association mappings, Section   evaluates the proposed approach and lastly, Section   presents the conclusion. 2 Related work Pattern mining played an important role in the development of association mining","Many efficient algorithms have been developed for pattern mining (e.g., the popular Apriori algorithm)   in transaction databases","Pattern mining has also been developed for use in mining frequent “itemsets” in multiple levels  , and for use in constraint-based techniques  ",Several approaches have been developed to combat the challenging issue of presenting and interpreting discovered patterns,"A concise representation of patterns is a “lossless” representation, examples being: non-derivable patterns  , condensed patterns  , maximal patterns, closed patterns, and regular patterns  ","Pattern post-processing methods have been presented recently, for example: pattern compression  , pattern deploying   and pattern summarization  ","Although theses methods can significantly reduce the number of discovered patterns, they annotate useful patterns with non-semantical information","Several different approaches have been introduced to interpret discovered knowledge based on some sorts of semantic annotations: an OLAP based visualization method   has been proposed to illustrate the meaning of discovered knowledge, a generating semantic annotation method for patterns has been presented   by constructing a context model, flow graphs have been used to generate decision rules  , and granule mining   which uses the semantic information about the data dimensions to generate data granules in databases",This approach has shown particular promising performance in the area of network traffic characterization  ,"Excluding granule mining, all approaches just described work exclusively on a one dimensional dataset and produce patterns or rules at item level","This makes it very difficult for users to identify meaningful knowledge, and these approaches are not efficient for representing associations in very large multidimensional databases",Granular computing is a popular concept that concerns problem solving and information processing at multiple levels of knowledge granularity  ,It utilizes knowledge granularity induced by partitions  ,The concept of granules and decision rules are accepted in the rough set community for forming partitions of database tables,Rough set theory has been developed to deal with vagueness for reasoning precisely about approximations of vague concepts  ,A database table can be compressed into a decision table by using an SQL “group by” operation,"A row in the decision table is a granule or a classification rule, which consists of some relevant attributes",One purpose of the research undertaken into data mining has been for the effective discovery of classification rules by reducing the dimensionality of the attribute set  ,"In this paper, however, we do not direct our research in this direction","Instead, we focus on the methods for describing association rules in relational databases",The rows in a decision table were also reviewed as decision rules if we divided the set of attributes into two groups: condition attributes and decision attributes  ,"Decision rules have been used for rule-based classification  , and for the construction of decision trees and flow graphs  ",The proposed approach will enable users to find useful decision rules through the multi-tier structures for improving the performance of rule-based classification,The advantage of using decision rules is that they reduce the two-phases of association mining (pattern mining and rule generation) into one process,"As mentioned in the introduction, there are three problems that occur when we try to directly use decision rules for data mining",These problems are especially true in the case of association rule mining,The first of these problems is that the relationship between patterns and granules (or decision rules) is not fully understood,The second problem is that decision rules can only represent a small proportion of associations in databases and do not describe the associations crossing different granules,The third problem is that decision tables and flow graphs do not support a way for identifying meaningless rules and for accessing rules efficiently,"In this paper we develop granule mining further, into multi-tier granule mining, in order to enable identification of meaningless rules and efficient access to association rules","We also show how to estimate the support of a patterns based on granules. 3 Basic definitions 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   3.1 Closed patterns and   values A transaction database   is a set of transactions, where each transaction has a unique transaction ID and contains a set of items",We use   to denote the set of all items used in all transactions of  ,"Let   be a set of items, we call   an  ","Its   is the set of transactions   such that  , and its   is the proportion of transactions in   which contain the itemset, i.e.,  ","An itemset   is called a   if its support  , a minimum support, where  _  is an experimental coefficient which is normally decided by users based on experiments",Determining a suitable minimum support is a challenging issue within the data mining community,A larger minimum support may ignore many more specific itemsets; but a smaller one will introduce a lot of noises  ,"A frequent itemset is also called a   in this paper. 
                         
                          illustrates a set of transactions, where  , and its  ","If  , we have   and  . 
                         
                      Given a pattern  , its closure  ","A pattern   is   if and only if  .   is called a max closed pattern if all its super patterns are non-closed, where pattern   is called a super pattern of   if  ","Let  , the discovered patterns (or frequent itemsets) in   are  ,  ,  ,  ,  ,  ,  ,  ,  , and  ; and the closed patterns are  ,   and   only (see  
                         ). 
                         
                      Closed patterns can be further grouped (or summarized) into   clusters (or called pattern profiles  )   based on similarities, where   is normally much less than the number of closed patterns","Let a cluster or a pattern profile   include a subset of closed patterns ( ), and  ","Pattern profile   is then formally represented as a triple  , where   is a probability distribution vector of the items in this profile;   is called a master pattern which is the union of patterns   and  , and   is the support of the profile which equals  ","To measure the accuracy of the summarization, the first step is to estimate patterns' support values according to pattern profiles only","For a given pattern  , its estimated support can be calculated as follows:  where  ","The restoration error rate   is then used to calculate a   value to evaluate the accuracy of a pattern summarization method:  where   is the set of discovered patterns,   is the real support of pattern  , and   is the estimated support calculated using Eq.  ","The smaller the error rate is, the closer is the estimated support to the actual support. 3.2 Decision tables and granules A transaction database can be described as an   
                          whereby columns (also referred to as attributes) consist of all items in  ","For example,  
                          is the corresponding information table of  , where each row represents a transaction, “1” means the presence of an item and “0” means the absence of an item","In contrast to the transaction database, the information table explicitly describes the absence of items",The information table is like an adjacency matrix of a graph and the transaction database is the corresponding adjacency list of that graph,An information table can be further compressed into a   by using the SQL command “Group By”,"For example,   can be compressed into  
                          if we select all attributes","We call each row in the decision table a  , the set of those transactions grouped to form the granule is called its coverset, and its support is defined as the same as the definition for itemsets","In contrast to a pattern, which only represents the appearance of a set of items, a granule explicitly represents both the appearance and absence of a set of items","It is usually assumed (see  ) that there is a function for every attribute   such that  , where   is the set of all values of  ",We call   the domain of  ,"Based on the above example, we can obtain a decision table for any selected set of attributes","Formally, let   be a subset of  .   determines a binary relation   on   such that   if and only if   for all  , where   denotes the value of attribute   for object  ","It is easy to prove that   is an equivalence relation, and the family of all equivalence classes of   is denoted by  ",We call each equivalence class in   a  ,"The granule in   that contains transaction   is denoted by  . 3.3 Decision rules and properties In this paper, we use the formal concept analysis   to define the decision rules and several properties about granules","Given a transaction   and a granule  , we say   is induced by   or   has the property   if  , also written as  , where   is true if and only if  ","Let  ,   and  ","We call granules in   
                         , and call granules in   
                         , respectively, where   and  ","For example, assume   and  ,  
                         
                          show the decision table for   and the decision table for  , respectively","The smallest granules only contain one single attribute, we call these primary granules","A large granule can be generated from some smaller granules by using logic operation “and”, ∧","Every granule in the decision table can be mapped into an   (also referred to as a  ), where the antecedent is a   which consists of attributes in  , and the consequent is a   which consists of attributes in  . 
                         
                      
                         
                      
                         
                      4 Interpretation of granules in terms of patterns As discussed in Section  , a granule can be read as an association rule if we divide attributes into the condition attributes and the decision attributes","In this sub-section, we formally discuss the relationships between granules and patterns in order to understand the meaning of granules. 
                      
                   Based on this definition, a decision rule corresponds to a granule that can also be interpreted as a decision pattern. 
                      
                   
                      
                   
                      
                   
                      
                   In   we prove that decision rules (or decision patterns) and max closed patterns mutually correspond to each other for a given decision table",In   we illustrate that small closed patterns can also be interpreted as smaller granules,These results indicate that granules are significant abstractions of knowledge in databases because they can prune noises that may be contained in some non-closed patterns. 5 Multi-tier structure In this section we present a method to summarize association rules,We firstly discuss the relationship between granules and association rules,"We also present multi-tier structures for representing association rules, and define the concept of general rules (i.e., rules with shorter antecedents) in order to clarify the meaning of meaningless","Finally, we present a method to estimate the support of given patterns based on the multi-tier structure. 
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   5.1 Granule vs. association rules To describe desired associations between different granules, users can divide condition attributes or decision attributes into sub-categories","For example, let   and   be two subsets of  , which satisfy   and  , then a   can be divided into a   granule   and   granule   and have  ","In this paper, we only discuss the method for dividing condition attributes",The analogous idea can be used for dividing decision attributes,Each large granule can be viewed as an association rule if it consists of two small granules,"We call “ ” an association rule, where small granule   is its antecedent and small granule   is its consequent (note the following concepts are also applicable for “ ”)","Its   is  and its   is  where  , the total number of transactions","In the following description, we use the previous example to show the relationship between association rules and granules.   illustrates a decision table of  , which includes granules and their support and covering set",Large granules can be divided into some small granules,"For example, granules  ,  ,   and   can be divided into   and  ","Let   and  .   show the   and  , respectively","Moreover,   
                          can be further divided into  -  and  - ","Let   and   be two subsets of  .  
                         
                          illustrate  -  and  - , respectively",Small granules can also be merged into large granules,"For example,  -  and  -  can be merged into large granules  ,   and  .  
                          illustrates the results of this merge",All large granules can be viewed as association rules,"In the above example,   contain only large granules.  
                          lists all association rules in the above example","Normally, we find that a very large set of rules show the associations between granules of different sizes, and therefore it is very hard for users to understand and efficiently use these association rules if a suitable structure for managing these rules is not provided. 5.2 Interpretation of association rules 
                          can be effectively used to describe the associations between granules, which is usually described as a pair  , where   is a set of granule tiers and   is a set of association mappings that illustrate the associations between granules in different tiers. 
                         
                          illustrates a 3-tier structure, where   are divided into  
                          and  
                          (i.e., the first two levels in the figure), and we have  ","The   tier includes  , the   tier includes  , and the   tier includes  , where  ,   and  ","The 3-tier structure in   includes three association mappings (arrows),  ,  , and   (i.e.,  ), which show the linkages between   and   (e.g., the solid arrows),  
                          and  
                         , and  
                          and  , respectively",These association mappings can be used to generate association rules,"Given a   
                          and a  
                          
                         ,   includes all possible associations (links and their strengths) between   and  ;   includes all possible associations between   and  
                         ; and   includes all possible associations between   and  ",The   is used to describe the extent of the association between granules,"Given granules   and  , the link strength between them is defined as follows:  which is the number of transactions that have the property “ ”","A multi-tier structure, unlike a decision table, enables the discussion of general association rules (rules with shorter premises)","Let   be a  ,   be a   and  ",We call “ ” (or “ ”) a   of rule “ ”,"Using the multi-tier structure, we can define the term “meaningless” for a decision rule based on selected tiers",We call “ ”   if its confidence is less than or equal to the confidence of its general rule,"The rationale behind this definition is analogous to the definition of interesting association rules, where   is an interesting rule if   (conditional probability) is greater than  ","If we add a piece of extra evidence to a premise and obtain a weak conclusion, we can say the piece of evidence is meaningless. 5.3 Support estimation for granules Let   be the decision table of information table ( )","The following equation can be used to calculate the estimated support for a given pattern  : 
                      For a 2-tier structure, let   be the set of  -granules and   be the set of  -granules","A given pattern   can be divided into two patterns   and   such that   and  , respectively, 
                      For other cases with  -tier structures, the estimated support can be calculated by different methods depending on how many tiers of granules the given pattern is derived from",There are three types of the calculation method for the estimation of support in a multi-tier structure,"Where the given pattern is contained by granules in only one tier, the support can be calculated by using the supports of the granules in the corresponding tier",A second method is used for multi-tier structures featuring two tiers of granules in which the same given pattern is contained,"The calculation determining the support in such cases, uses the link strength of the mappings between the two granules",This calculation is done directly in the current multi-tier structure,A third method is adopted for patterns that are contained by the granules of three or more tiers,"To obtain the support for such patterns, the calculation needs to use the mapping information from the 2-tier structure to compute the support through Eq.  ","To demonstrate the estimated support calculation from the multi-tier structure for a multi-tier structure, let a 3-tier structure be   containing three sets of granules that are  ,   and   respectively","A pattern   can be divided into three patterns, namely  ,   and  ","If only one of  ,   or   is non-empty, then the support is calculated through the sum of the support of only one set of granules, as follows: 
                      If one of  ,   and   is empty, then the support is calculated using the link strength of the mappings of  ,   or   as per the second calculation method described","This calculation would look as follows: 
                      Finally, if neither  ,   or   is empty, then the support is calculated through the mappings of  , as described by the third calculation method","In order to use these mappings, the division of   needs to be modified",Let   where   such that the support can be obtained by using a modified version of Eq.  ,"The equation used for this calculation is as follows: 
                      When using the 2-tier structure to calculate the estimated support, zero restoration error rate can be achieved because the 2-tier structure is a “lossless” compression","Further, a zero error rate can also be obtained for a multi-tier structure that has more than two tiers, when using only the mappings of granules from two tiers or in cases where the calculation is performed via the basic 2-tier structure. 6 Association mappings In the last section, we discussed a three tiered structure  , where  ,  ,  , and  ",Association mappings are used to describe the association relationships between granules in different tiers,They can be used to enumerate all association rules between the associated granules,"Usually, there are many possible pairs   such that   and  , and   or   can be further divided into smaller sets","Therefore, it is necessary to use derived association mappings (e.g.,  ) for efficient rule generations in multi-tier structures. 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   6.1 Basic association mapping Let  ,   and  ","Elements of   are called granules, and elements of   and   are called condition granules and decision granules, respectively",Let   and Let  ,"Based on Eq.   and Section  , we have  The associations between   and   can be described as a basic association mapping   such that   is a set of   link-strength pairs for all  ","Formally,   is defined as  , which satisfies  for all granules  , where   is the set of all integers","Of course, supports and confidences of association rules can be easily calculated based on the basic association mapping","Let  ,  , and “ ” be a decision rule, its support and confidence can be derived as follows: 
                         
                      6.2 Derived association mappings An interesting property of multi-tier structures is that one can derive many association mappings based on the basic association mapping, rather than using the original set of transactions",This property is significant in terms of time complexities for rule generation,"To simplify the process of deriving, we first consider the method for deriving association mapping   between  
                          and  
                          based on the basic association  , where   is a set of  
                          integer pairs, which satisfies   and  for all granules  , where  ,  ,   (the set of  
                         ),   (the set of  
                         ), and   and   are relations between   and  , and   and  , respectively","We can also derive the association mapping   between  
                          and   based on the association mappings   and  , which satisfies   and  for all granules  . 
                         
                          illustrates the relations between these association mappings","In this figure, the set of condition attributes are split into two sets,   and  ","The   ( ) are also correspondingly compressed into  
                          ( ) and  
                          ( )","As defined before,   is used to describe the association relationship between   and  ","Association mapping   is used to describe the association relationship between   and  , and association mapping   is used to describe the association relationship between   and  . 7 Construction of multi-tiers One of the advantages of using multi-tier structures is that we can calculate derived association mappings efficiently based on an initial basic association mapping",In this section we discuss the algorithm for the construction of a 2-tier structure,"By using the derived association mappings, we also design efficient algorithms for the construction of multi-tier structures. 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   7.1 Construction of 2-tiers 
                         
                          describes the main procedure of the construction of the 2-tier structure and the basic association mapping","It goes through the set of transactions and constructs the set of condition granules ( ), the set of decision granules ( ) and the basic association mapping ( )","In step  ,   and   are assigned as an empty set","In step  , the algorithm goes through the set of transactions","For each transaction  , the algorithm can be split into two granules: the condition granule,  , and decision granule,   (step  )",The decision granule is then inserted into   (step   to  ),"If a condition granule ( ) is not in the current  , it is added into   and the algorithm assigns the condition granule an initial value,  , as its mapping result (step   to step  ); otherwise, it updates the existing mapping   for the condition granule (step   to step  )","If there is a match for the decision granule  , the algorithm adds one to the corresponding association strengths (step   to step  ); otherwise, it inserts   into   (step   to step  )",Assume that the basic operation in   is the comparison between granules,"Let  ,  ,   and  ","The time complexity of   is determined by the for loop in step  , where checking   and   takes   for every transaction, and checking   takes   if  ",Therefore the time complexity of the algorithm is  since   and  ,"Indeed,   in many cases (see the experimental results in Section  ). 
                         
                          describes the process of the generation of decision rules","Because two “for loops” (in step   and step  ) both traverse pairs in   ( ), and the number of pairs is just  , the time complexity of this algorithm is  . 
                          are comparatively better than Pawlak's method (which only used decision tables  ) in terms of time complexities, since   for any  . 7.2 Construction of 3-tiers After obtaining a 2-tier structure and a basic association mapping, users can further split the condition attributes into any two categories (  and  ) they desire, where  , and  ","The key issue for the construction of a 3-tier structure from a 2-tier structure, is to generate the derived association mappings efficiently","In this section we discuss two kinds of derived association mappings between  
                          and  
                         , and  
                          and  , respectively","We also demonstrate, through the efficient construction of these derived mappings, the algorithms for creating a 3-tier structure based on the basic association mapping and the 2-tier structure","The procedure for this construction is divided into two stages in order to assist with easy understanding of the proposed ideas.  
                          describes the first of the two stages for calculating the derived association mapping   and tiers   ( 
                         ) and   ( 
                         )","In step  , and representing the initial value, an empty is assigned to both the set of  
                          and the set of  
                         ","The algorithm splits each   into two smaller granules: a  
                          
                          and a  
                          
                          (step  )","It then calculates the link strength between the two smaller granules (step   to step  ), and updates the set of  
                          and the value of the association mapping   (step   to step  )","Step   simply inserts   ( 
                         ) into  ","In  , the nested for loop (step   to step  ) traverses the basic association mapping, therefore taking   loops in total","The if-Else statements (step   to step  ) traverse all condition granules and check   in  , therefore taking  ","Step   also takes   and thus, the time complexity of   is  this is much better than directly calculating   from the set of transactions because  . (Notice: if using   to calculate  , the time complexity is  based on Eq.  , where  ,  ,  ,   and  .) 
                         
                          describes the second stage for calculating the derived association mapping  , which describes associations between  
                          and  ","For each  -granule,  , an empty is assigned to   (step  )",The algorithm also composes all paths (through   and then  ) that start from   and end at the same decision granule (step   to step  ),The composition operation ⊕ was defined in   to add two vectors into one,"The time complexity of   is determined by the second for loop (step   to step  ), because it is here that each composition operation, ⊕, is equivalent to the basic operation (the comparison between granules)","Therefore, the time complexity of this algorithm is  this is much better than directly calculating   from the set of transactions. (Notice: if use   to calculate  , the time complexity is  based on Eq.  , where  ,  ,   and  .) We have just discussed an efficient method for the construction of multi-tier structures","Given a 2-tier structure, a 3-tier structure can be derived by dividing the condition granules   into   and  .   (or  ) can also be further split into   and   to generate a 4-tier structure in the same way as described in  , and so on.  
                          lists previously presented algorithms and their complexity, in addition to the present algorithms and complexities we present here","It can be seen that by comparison, the algorithms proposed in this paper are more efficient than previous ones. 8 Evaluation In this section we use two data collections to test the proposed approach",The first data collection is a transaction dataset for a local super market in Australia,"The second is a data cube from an OLAP database. 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   8.1 Experiments on a transaction dataset The transaction data set “dbo.PTGTop20AndBottom15” was generated based on a local super market in Australia and includes 26,590 transactions generated from the original data","For each transaction, if an item attribute appears it is set to one, otherwise it is set to zero. 300 of the most frequent products are first selected as attributes to obtain the basic decision table","We also define condition attributes (P1–P20 in table “dbo.PTGTop20AndBottom15”) as the products with profits more than 50%, and decision attributes (P286–P300 in table “dbo.PTGTop20AndBottom15”) as products with profits equal to or less than 20%","Under these constrains, the dataset is compressed into the basic decision table which includes 2977 granules in total","By splitting these granules into condition granules (tier  ) and decision granules (tier  ), a 2-tier structure is built up firstly based on granules in the basic decision table",We then create a 3-tier structure by slicing tier   into tier   and tier   under the assumption that the products in   have profits more than 90% and that the products in   have profits between 90% and 50%,"In addition, we build a 4-tier structure by further dividing   tier into   tier and   tier such that each new tier includes five attributes while there are ten attributes in tier   and fifteen attributes in decision tier  ",The data is stored in a data warehouse using  ,The retrieval of data uses the   query language which retrieves the desired data in one step,The implementation of the proposed algorithms is on the   platform,The programming language used is  ,"The baseline models are also implemented using the same configuration. 
                         
                          shows the numbers of granules in each tier, along with the number of condition granules, where All means the decision table",In comparison to the decision table there are many fewer granules in each tier of a multi-tier structure because there are only a small number of granules that might be useful for rule generation,"Further, when the granules are divided into shorter granules for building more tiers, the number of granules becomes smaller again","The reason for this is that when the original granule is split into smaller granules, more granules with same attributes are compressed together. 
                         
                          compares these multi-tier structures and the decision table",It is apparent that multi-tier structures can largely reduce the number of granules because they effectively eliminate the redundant information in the decision table,"Additionally, multi-tier structures can describes more types of associations between granules","Pruning meaningless rules is another important feature of multi-tier structures.  
                          depicts the number of meaningless rules found within the 3-tier structure and the 4-tier structure","Column 1 illustrates the percentage of meaningless rules in the 3-tier structure, where we compare decision rules with their corresponding kind of general rule  ","Columns 2 and 3 represent the percentage of meaningless rules in the 4-tier structure, where column 2 uses the general rule   only and column 3 uses both general rules   and  ",The experiment shows that more meaningless rules are likely pruned if more tiers are introduced into the multi-tier structure,"To evaluate the efficiency of the multi-tier structure compared to the decision table, we randomly choose eight granules in  -tier",Their association mappings are generated and then the corresponding granules in  -tier are found and counted,The system records the time for each case separately,"We search the same granules and decision rules in the decision table and calculate the time for both processes.  
                          illustrates the result of this experiment and shows that the efficiency of the multi-tier structure is very impressive",The experimental results demonstrate that the multi-tier structure only uses a small space to store meaningful multidimensional association rules,It can save memory in the system and improve the quality of multidimensional association rules. 8.2 Experiments on OLAP database The data collection Foodmart 2005 contains two databases: SQL Database and OLAP Database,"The data used in this experiment is the customer sales data from the OLAP Database (see  ), which includes four data cubes",The Warehouse and Sales cube contain four measures and here we use the unit-sales measure,"The product dimensions used in the Warehouse and Sales cube consist of eight levels: All, Product family, Product department, Product category, Product subcategory, Brand and Product","We only use the top three levels: All, Product family, and Product department",There are a total of 23 attributes in the   level,"These attributes are categorized into 4 product families:   (Alcoholic Beverages, Baking Goods, Beverages, Dairy),   (Carousel, Checkout, Health and Hygiene, Household, Periodicals),   1 (Baked Goods, Breakfast Foods, Canned Foods, Canned Products, Deli, Eggs, Frozen Foods) and   2 (Meat, Packaged Foods, Produce, Seafood, Snack Foods, Snacks, Starchy Foods)",The transactions used in the experiments presented here consist of the customers' purchase records stored in the fact table of unit sales,"Each transaction is the sum of all product categories from all purchases, for all products, on one day, by one customer","To build up the decision table and multi-tier structure of granules, the transactions of the Unit sales are transformed into an information table using the following procedure","If the customer purchases one or more products from a given product department, the value of the attribute in the product department level is set to 1; otherwise, the value is set to 0","The total number of transactions in the information table is 53,700","The experiments presented test the proposed solution from several aspects, including space and time complexities, and the restoration error rate of estimated support",We use two baseline models for comparison with the proposed theory,"The first baseline model is the decision table, in which the attributes are viewed as two groups: condition and decision attributes","The second baseline model is a pattern summarization model  , which uses pattern profiles to estimate the support of any pattern (see Eqs.   and  ). 
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      8.2.1 Space and time complexity In our experiments, the information table is firstly transformed into a decision table","Multi-tier structures are then constructed based on this decision table and the semantic information of attributes.  
                             shows a special definition of the multi-tier structures, where the semantic relation between attributes are considered","As in  , there are three multi-tier structures: a 2-tier structure (  and  ), a 3-tier structure ( ,   and  ), and a 4-tier structure ( ,  ,   and  )",We also make other 16 definitions of multi-tier structures by grouping the 23 attributes in different combinations,"For each definition, a 2-tier structure (  and  ) is built firstly",Then from it a 3-tier structure is built by dividing the   tier into two smaller tiers   and  ,"Then the   tier is further divided into tier   and   to generate a 4-tier structure ( ,  ,   and  ). 
                            
                             depicts the trends of total granule numbers in the multi-tier structures when the number of tiers increases","It is apparent that for the most part, the number of granules drops largely as the number of tiers increases; there is an inverse relationship between the number of granules and the number of tiers.  
                             shows the number of patterns in the information table based on different minimum support values","In comparison with multi-tier structures, pattern mining results in a large amount of patterns if the  _  is not big enough","Conversely, when the  _  is big enough (e.g., 50 in this example), pattern mining loses lost many large patterns","Multi-tier structures use a very small space to contain all the possible associations for the selected data attributes, thereby eliminating the support issue presented by pattern mining. 
                            
                             shows the results of runtime tests","It is clear that multi-tier structures take far less than does pattern mining, since it is only when the minimum support is set to a very large number of occurrences, that time to obtain frequent patterns looks acceptable",The results also reflect that the time taken to create new tiers from smaller granules is less than that to create new tiers from larger granules,"For example, the time taken to generate a 4-tier structure from a 3-tier structure is 171 ms, while it takes 2593 ms to construct a 3-tier structure from a 2-tier structure","These results show that the proposed theory performs remarkably. 8.2.2 Restoration error rate and meaningless rules The pattern summarization model uses all closed patterns, which are generated from the whole information table with a minimum support of 5, as the input patterns",There are 10 963 closed patterns in total,The restoration error rate   is calculated by using Eq.  ,Several tests are carried out using a different number of profiles,"The number of profiles is set to 200, 500, 750 and 1000 respectively.  
                             shows the error rate results for the pattern summarization model in comparison to that for granule mining","The results show that when using smaller numbers of profiles such as 200, 500 and 750, the restoration error is much higher than that for using granules","To be noticed, using decision table or multi-tier structures to calculate the support for all patterns (see Eqs.   and  ), the   values can remain as zero",Multi-tier structures also provide a special feature for pruning some meaningless rules,In these experiments we firstly generate general rules for the 16 definitions of multi-tier structures,We then filter out the meaningless rules based on their general rules,"We found that on average, about   of rules are meaningless. 
                            
                             illustrates the number of meaningless rules that can be pruned in different tiers","If only one type of general rule is used as the criteria, fewer meaningless rule can be filtered","If two types of general rules, e.g.   and  , are used together as the criteria, then greater number of meaningless rules can be pruned. 9 Conclusion Multi-tier structures provide an efficient way to represent and summarize association rules in terms of granules",This paper continues the development of multi-tier structures,It presents a method to interpret granules in terms of patterns and a method to estimate patterns' support based on granules,It also formalizes concepts of association mapping and presents efficient algorithms for the construction of multi-tier structures,"Moreover, it conducts a set of experiments on the Foodmart 2005 data collection","Compared with pattern summarization, the proposed method of multi-tier granule mining achieves the best performance with zero restoration error rate",The experimental results show that multi-tier structures can be created efficiently and utilize a very small space to store the possible associations generated,This work contributes significantly to improving the summarization of association rules in databases.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0888613x14001133," 1 Introduction A   
                       is a probabilistic graphical model   that is popular in fields such as statistics, machine learning and artificial intelligence",It identifies the nodes of a Directed Acyclic Graph (DAG) with random variables and interprets the graphical structure of the DAG as an assessment of the independencies amongst these variables; nodes that are not connected represent variables that are conditionally independent of each other,"By exploiting these independencies, a global uncertainty model can be constructed easily out of local ones, allowing for a compact representation of the model","Efficient algorithms have been developed for performing inferences in such Bayesian networks, leading to their successful application in a multitude of real-life problems  ","Despite their success, Bayesian networks have an important limitation: the construction of a Bayesian network requires the exact specification of a conditional probability distribution for all variables in the network","In case of limited data or disagreeing and/or partial expert opinions, this requirement is clearly unrealistic and renders the resulting model arbitrary; see Ref.   for numerous other arguments against this ‘precision requirement’","In order to avoid those problems, one can use the theory of  , which, simply put, are Bayesian networks that allow for imprecisely specified local models","Initially, these were taken to be   
                       (closed and convex sets of probability distributions), which explains the terminology","However, as the theory progressed, other imprecise-probabilistic models such as lower previsions   and sets of desirable gambles entered the field as well  ","In the present paper, we use the very general theory of   
                      ",The key idea of this theory is that a subject's beliefs about the unknown outcome of an experiment can be modelled by means of the bets on this outcome—referred to as gambles—that he is willing to accept,"Although sets of desirable gambles are not as well known as other (imprecise) probability models, they have definite advantages","To begin with, they are more expressive than most—if not all—other imprecise-probabilistic models, including the theories of credal sets and coherent lower previsions  : every set of desirable gambles has an associated lower prevision and credal set, but—in general—the original set of desirable gambles cannot be recovered from these derived models",A particularly interesting consequence of this added expressiveness is that conditioning on events with (lower or upper) probability zero becomes non-problematic  ,"Secondly, sets of desirable gambles are strongly connected to classical propositional logic  , thereby providing a unified language for both logic and probability","Thirdly, they have the advantage of being operational, meaning that there is a practical way of constructing a model that represents a subject's beliefs  . 
                       And finally, our experience tells us that it is usually easier to construct proofs in the geometrically flavoured language of coherent sets of desirable gambles than in other, perhaps more familiar frameworks","Three main kinds of credal networks can be distinguished, the difference between them being the notion of independence they adopt: strong independence, epistemic independence or epistemic irrelevance; see Cozman's pioneering work   for an overview","In a precise-probabilistic context, all these approaches coincide and reduce to a Bayesian network","Credal networks under strong independence are by far the most popular ones; see Refs.   for some nice overviews, containing numerous references to both theoretical results, algorithms and applications","In contrast, credal networks under epistemic independence have received almost no attention  , a situation which is likely to persist due to their computational intractability",The current paper deals with the remaining option: credal networks under epistemic irrelevance,"Let us start by stating some of their advantages. 
                   Despite these advantages, credal networks under epistemic irrelevance have received relatively little attention so far; to our knowledge, Refs.   are the main contributions to the field","One of the main persisting problems is that—except for networks that are sufficiently small or have a tree topology—no efficient, exact or even approximate inference algorithm is known",We believe that this is to a great extent due to a profound lack of known theoretical properties,"In the present paper, we start to remedy this situation by providing a firm theoretical foundation for credal networks under epistemic irrelevance",We begin in Section   by providing a short introduction to the theory of sets of desirable gambles,"We then go on to introduce and discuss important concepts such as directed acyclic graphs and epistemic irrelevance in Section  , and use these in Section   to show how assessments of epistemic irrelevance can be combined with given local sets of desirable gambles to construct a joint model for a credal network under epistemic irrelevance",We call this the   and prove that it is the most conservative coherent model that extends the local models and expresses all conditional irrelevancies encoded in the network,"In the remainder of the paper, we develop some remarkable properties of this irrelevant natural extension",Section   presents what we consider to be our main technical achievement: a very general factorisation result and a closely related marginalisation property,"In Section  , we develop a tight connection with the   
                       and show that it corresponds to a special case of the irrelevant natural extension",Our perhaps most important result is presented in Section  : the irrelevant natural extension satisfies separation properties similar to the ones that are induced by d-separation in Bayesian networks,"We introduce an asymmetrical version of d-separation, called  , and show that it implies epistemic irrelevance","Furthermore, since AD-separation is shown to satisfy all asymmetric graphoid properties (all graphoid properties except symmetry), the induced set of epistemic irrelevancies does so as well","We conclude the paper in Section  , comment on how to translate our results to the framework of coherent lower previsions, discuss some algorithmic applications and present future avenues of research","In order to make our main argumentation as readable as possible, all technical proofs are collected in  ",We should note that some of our results have already been published in an earlier conference version of this paper  ,"The current version gives a more detailed exposition of these results, provides them with proofs—which were omitted in the conference version—, and extends them; notable examples of additional results are the connections with independent natural extension, presented in Section  . 2 Sets of desirable gambles Consider a variable   taking values in some non-empty finite set  ","Beliefs about the possible values this variable may assume can be modelled in various ways: probability mass functions, credal sets and coherent lower previsions are only a few of the many options",We choose to adopt a different approach: sets of desirable gambles,We will model a subject's beliefs regarding the value of a variable   by means of his behaviour: which gambles (or bets) on the unknown value of   does our subject strictly prefer to the status quo (the zero gamble),"We give a brief survey of the basics of sets of desirable gambles; see Refs.   for more details, further discussion and connections with other imprecise-probabilistic models. 
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                   2.1 Desirable gambles A gamble   is a real-valued map on   that is interpreted as an uncertain reward","If the value of the variable   turns out to be  , the (possibly negative) reward is  ","A non-zero gamble is called   to a subject if he strictly prefers to zero the transaction in which (i) the actual value   of the variable is determined, and (ii) he receives the reward  ",The zero gamble is therefore not considered to be desirable,We model a subject's beliefs regarding the possible values   that a variable   can assume by means of a set   of desirable gambles—some subset of the set   of all gambles on  ,"For any two gambles   and   in  , we say that   if   for all   in   and   if both   and  ",We use   to denote the set of all gambles   for which   and   to denote the set of all gambles   for which  ,"As a special kind of gambles we consider   
                          of events  .   is equal to 1 if the event   occurs—the variable   assumes a value in  —and zero otherwise. 2.2 Coherence In order to represent a rational subject's beliefs about the values a variable can assume, a set   of desirable gambles should satisfy some rationality requirements","If these requirements are met, we call the set   
                         ","We require that for all   and all real  :  Requirements D3 and D4 turn   into a convex cone:  , where we use the positive hull operator ‘posi’ that generates the set of finite strictly positive linear combinations of elements of its argument set:  Here   is the set of all (strictly) positive real numbers, and   the set of all natural numbers (zero not included). 2.3 Natural extension In practice, a set of desirable gambles is often elicited by presenting an expert a number of gambles and asking him whether or not he finds them desirable, resulting in an   of desirable gambles  ","However, such an assessment is not guaranteed to be coherent","Hence, the question arises whether   can be extended to—included in—a coherent set  ","It turns out that this is easily done; by applying D2–D4, we can use   to infer the desirability of other gambles","The largest set of desirable gambles that can be constructed in this way is  Since   trivially satisfies D2–D4, we see that   is coherent if and only if it avoids null gain [D1]","Furthermore, if   is coherent, then it is the smallest—most conservative—coherent set of desirable gambles that contains   and it is then also equal to the intersection of all the coherent supersets of   
                         ; in that case, we call   the   of  . 3 Credal networks under epistemic irrelevance In order to develop our results, we need to introduce some elementary, but nevertheless essential concepts",We start with important terminology related to Directed Acyclic Graphs (DAGs),"Next, we show how we can use sets of desirable gambles to model our beliefs about the variables that are associated with such a DAG and how to express epistemic irrelevance in this language","Finally, we introduce local uncertainty models and explain our interpretation of the DAG that is associated with a credal network. 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   3.1 Directed acyclic graphs A directed acyclic graph (DAG) is a graphical model that is well known for its use in Bayesian networks","It consists of a finite set of nodes (vertices), joined into a network by a set of directed edges, each edge connecting one node with another","Since this directed graph is assumed to be acyclic, it is not possible to follow a sequence of edges from node to node and end up at the same node one started out from",We will call   the set of nodes   associated with a given DAG,"For two nodes   and  , if there is a directed edge from   to  , we denote this as   and say that   is a   of   and   is a   of  ","For any node  , its set of parents is denoted by   and its set of children by  ","If a node   has no parents, that is,  , then we call   a  ","If  , then we call   a  , or  ","Two nodes   and   are said to have a   between them if one can start from  , follow the edges of the DAG regardless of their direction and end up in  ","In other words: one can find a sequence of nodes  ,  , such that for all   either   or  ","If this sequence is such that   for all   (all edges in the path point away from  ), we say that there is a   from   to   and write  ",In that case we also say that  ,"If   and  , we say that   and write  ","For any node  , we denote its set of   by   and its set of   by  ","We also use the shorthand notation   to refer to the so-called   of  . 
                         
                      We extend these notions to subsets of   in the following way","For any  ,   is its set of parents and   is its set of descendants","The non-parent non-descendants of   are given by 
                          
                          and we also define  ","In general, this last set cannot be referred to as the non-descendants of   since   and   are not necessarily disjoint","We call those subsets of   for which they are disjoint  : a set   is closed if for all   and any   such that  , it holds that  ","For closed  , we find that 
                          
                          and therefore  ","This means that, for closed  ,   can rightfully be referred to as the non-descendants of  . 
                         
                      With any subset   of  , we can associate a so-called   of the DAG that is associated with  ",The nodes of this sub-DAG are the elements of   and the directed edges of this sub-DAG are those edges in the original DAG that connect elements in  ,"For a sub-DAG that is associated with some subset   of  , we will use similar definitions as those for the original DAG, adding the subset   as an index","As an example: for all  , we denote by   the parents of   in the sub-DAG that is associated with the nodes in  ","For all   and  , we have 
                          
                          and  . 
                         
                      3.2 Variables and gambles on them With each node   of the network, we associate a variable   assuming values in some non-empty finite set  ",We denote by   the set of all gambles on  ,We extend this notation to more complicated situations as follows,"If   is any subset of  , then we denote by   the tuple of variables whose components are the   for all  ",This new joint variable assumes values in the finite set   and the corresponding set of gambles is denoted by  ,"When  , we let   be a singleton","The corresponding variable   can then only assume this single value, so there is no uncertainty about it.   can then be identified with the set   of real numbers",Generic elements of   are denoted by   or   and similarly for   and   in  ,"Also, if we mention a tuple  , then for any  , the corresponding element in the tuple will be denoted by  ","We will use the simplifying device of identifying a gamble   on   with its   to  , where  : the gamble   on   defined by   for all  ","For instance, if  , this allows us to consider   as the set of those gambles in   that depend only on the variable  . 3.3 Modelling our beliefs about the network Throughout, we consider sets of desirable gambles as models for a subject's beliefs about the values that certain variables in the network may assume","An important contribution of this paper, further on in Section  , will be to show how to construct a joint model for our network, being a coherent set   of desirable gambles on  ","From such a joint model, one can derive both conditional and marginal models  ",Let us start by explaining how to condition the global model  ,"Consider a non-empty set  , with  , and assume that we want to update the model   with the information that  ","This leads to the following updated set of desirable gambles:  which represents our subject's beliefs about the value of the variable  , conditional on the observation that   assumes a value in  ","This definition is very intuitive, since   is the unique gamble that is called off (is equal to zero) if   and equal to   if  ","Since  , the special case of conditioning on the certain variable   yields no problems: it amounts to not conditioning at all",The connection with the precise-probabilistic version of conditioning is discussed in Ref.  ,Marginalisation too is very intuitive in the language of sets of desirable gambles,"Suppose we want to derive a marginal model for our subject's beliefs about the variable  , where   is some subset of  ",This can be done by using the set of desirable gambles that belong to   but only depend on the variable  :  Now let   and   be   subsets of   and let   be any non-empty subset of  ,"By sequentially applying the process of conditioning and marginalisation we can obtain conditional marginal models for our subject's beliefs about the value of the variable  , conditional on the observation that   assumes a value in  :  Conditioning and marginalisation are special cases of Eq.  ; they can be obtained by letting   or  ","If   is a singleton  , with  , we will use the shorthand notation  ","Since coherence is trivially preserved under both conditioning and marginalisation, we find that if the joint model   is coherent, all the derived models will also be coherent","For additional properties of these marginalisation and conditioning operators, we refer to Ref.  . 3.4 Epistemic irrelevance At this point, we have the necessary tools to introduce one of the most important concepts for this paper, that of epistemic irrelevance","We describe the case of conditional irrelevance, as the unconditional version of epistemic irrelevance can easily be recovered as a special case","Consider three disjoint subsets  ,  , and   of  ","When a subject judges   to be   
                          
                          
                         , denoted as  , he assumes that if he knew the value of  , then learning in addition which value   assumes in   would not affect his beliefs about  ","More formally put, he assumes for all   and   that:  Alternatively, a subject can make the even stronger statement that he judges   to be epistemically   to   conditional on  , denoted as  ","In that case, he assumes that if he knew the value of  , then receiving the additional information that   is an element of any non-empty subset   of   would not affect his beliefs about  ","In other words, he assumes for all   and all non-empty   that:  Making a subset-irrelevance statement   implies the corresponding irrelevance statement  ","Even stronger, it implies for all   that  ",The converse does not hold in general; see  ,"However, as we will show further on, credal networks under epistemic irrelevance are a useful exception: although we define the joint model by imposing irrelevance, it will also satisfy subset-irrelevance","For the unconditional irrelevance case it suffices, in the discussion above, to let  ","This makes sure the variable   has only one possible value, so conditioning on that variable amounts to not conditioning at all. 
                         
                      We consider subset-irrelevance to be the more natural of the two concepts, as it requires   information about   to be irrelevant, which is what—in our opinion—irrelevance should mean","For example, in  , although   is epistemically irrelevant to  , learning that   does affect our belief model for  ; this would be impossible if   were epistemically subset-irrelevant to  ","Irrelevance and subset-irrelevance can also be extended to cases where  ,   and   are not disjoint, but   and   are",We then call   epistemically (subset-)irrelevant to   conditional on   provided that   is epistemically (subset-)irrelevant to   conditional on  ,"Although these cases are admittedly artificial, they will help us state and prove some of the graphoid properties further on. 3.5 Local uncertainty models We now add   to each of the nodes   in our network",These local models are assumed to be given beforehand and will be used further on in Section   as basic building blocks for constructing a joint model for a given network,"If   is not a root node of the network, i.e., has a non-empty set of parents  , then we have a conditional local model for every instantiation of its parents: for each  , we have a coherent set   of desirable gambles on  ",It represents our subject's beliefs about the variable   conditional on its parents   assuming the value  ,"If   is a root node, i.e., has no parents, then our subject's local beliefs about the variable   are represented by an unconditional local model",It should be a coherent set of desirable gambles and will be denoted by  ,"As was explained in Section  , we can also use the common generic notation   in this unconditional case, since for a root node  , its set of parents   is equal to the empty set ∅. 3.6 The interpretation of the graphical model In classical Bayesian networks, the graphical structure is taken to represent the following assessments: for any node  , conditional on its parent variables, the associated variable is independent of its non-parent non-descendant variables  ","When generalising this interpretation to credal networks, the classical notion of independence gets replaced by a more general, imprecise-probabilistic notion of independence","In this paper, we choose to use epistemic irrelevance, as introduced in Section  ; see the Introduction for discussion, motivation, and relevant references","It is useful to know that in the special case of precise uncertainty models, epistemic irrelevance is equivalent to the classical notion of independence, making the interpretation of the graphical structure of the network equivalent to the one in Bayesian networks",Let us state our interpretation more formally,"We assume that the graphical structure of the network embodies the following conditional irrelevance assessments, turning the network into a  ","Consider any node   in the network, its set of parents   and its set of non-parent non-descendants  ","Then   
                         
                          
                          
                          
                         :  For a coherent set of desirable gambles   that describes our subject's global beliefs about all the variables in the network, this has the following consequences","For every   and all  ,   must satisfy: 
                      4 Constructing a joint model We now show how to construct a joint model for the variables in the network, and argue that it is the most conservative coherent model that extends the local models and expresses all conditional irrelevancies encoded in the network","But before we do so, we need to ask ourselves the following question: suppose we have a global set of desirable gambles  , how do we express that such a model is compatible with the assessments encoded in the network? 
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   4.1 Defining properties of the joint model We will require our joint model to satisfy the following four properties","First of all, our global model should extend the local assessments, in the sense that the local models that are derived from the global one by marginalisation should include—be at least as informative as—the given local models:  The second requirement is that our model should reflect all epistemic irrelevancies encoded in the graphical structure of the network:  The third requirement is that our model should be coherent:  Since requirements G1–G3 do not determine a unique global model, we impose a final requirement to ensure that all inferences we make on the basis of our global models are as conservative as possible, and are therefore based on no other considerations than what is encoded in the network:  We will now show how to construct the unique global model   that satisfies all four requirements G1–G4. 4.2 An intuitive expression for the joint model Let us start by looking at a single given marginal model   and investigate some of its implications for the joint model  ",Consider any node   and fix values   and   for its parents and non-parent non-descendants,"Due to requirements G1 and G2, any gamble   should also be an element of  , which by definition means that  ","Inspired by this observation, we introduce the following set of gambles on  :  It should now be clear that   must be a subset of our joint model  .  
                      Since our eventual joint model should also be coherent (satisfy requirement G3), and thus in particular should be a convex cone, we can derive the following corollary.  
                      We now suggest the following expression for the joint model describing our subject's beliefs about the variables in the network:  We will refer to   as the   of the local models  ","Since we know from   that it is guaranteed to be a subset of the joint model we are looking for, it is rather natural to propose it as a candidate for the joint model itself","In the next section, we set out to prove that   is indeed the unique joint model satisfying all four requirements G1–G4. 4.3 Justifying our expression for the joint model We start by proving a number of useful properties of  .   
                         
                      These two propositions serve as a first step towards the following coherence result, which states that our joint model   satisfies requirement G3. 
                         
                      The crucial step in our proof for this result is to consider a specific Bayesian network that has the same topology as our credal network and to use the corresponding joint probability mass function to construct a separating hyperplane argument","In this way, we are using existing coherence results for Bayesian networks to prove their counterparts for credal networks","Next, we turn to an important factorisation result that is essential in order to prove that our joint model extends the local models and expresses all conditional irrelevancies encoded in the network, and therefore satisfies G1 and G2.   
                         
                      Notice that, although G1 only requires   to be a superset of  , the irrelevant natural extension   also satisfies a modified, stronger version of G1:  ",We now have all tools necessary to formulate a crucial result,"It is the first important contribution of this paper and provides a justification for the joint model   that was proposed in Eq.  .  
                      It is already apparent from   that the properties of the irrelevant natural extension   are not limited to G1–G4","As a first example,   implies that for any node  , conditional on its parent variables  , the non-parent non-descendant variables   are not only epistemically irrelevant, but also subset-irrelevant to  . 
                         
                      In the remainder of this paper, we establish a number of even stronger properties of  ","However, before we do so, let us take a small step back to take a look at the larger picture","If we were to drop the irrelevance assessments, that is, if we were only to impose requirements G1, G3 and G4, then the unique model to satisfy these three requirements would be the smallest coherent set of desirable gambles that extends the local models or, in other words, the natural extension of  By including G2, we are applying a more general form of natural extension that combines an assessment of gambles with structural assessments","In our case, these structural assessments consist of a very specific set of epistemic irrelevancies—G2—and the resulting ‘generalised’ natural extension of  , which we call the irrelevant natural extension, turns out to be the ‘traditional’ natural extension of the extended assessment  , which is obtained by combining the original assessment   with the irrelevancies that are imposed by G2","In principle, this generalised form of natural extension can be applied to other structural assessments as well",Cozman   discusses the possibility of considering credal networks with arbitrary assessments of epistemic irrelevance; the special case of credal networks under epistemic independence is discussed in Ref.  ,"However, unlike in our special case, it is not always easy—and sometimes even impossible—to obtain a closed-form expression for the resulting joint model of these credal networks","Finally, one can also combine natural extension with structural judgements other than epistemic irrelevance",For example: Ref.   considers the structural judgement of exchangeability,"That being said, for the remainder of this paper, we focus on the special case of the irrelevant natural extension  . 5 Additional marginalisation properties As explained in Section  , a subset   of   can be associated with a so-called sub-DAG of the original DAG","Similarly to what we have done for the original DAG, we can use Eq.   to construct a joint model for this sub-DAG","All we need to do is to provide, for every   and  , a local model  ",One particular way of providing these local models is to derive them from the ones of the original DAG,The starting point to do so is fixing a value   for the parent variables of  ,"This provides us, for every  , with a value   because  ","For every   and  , we can then identify the local model   of the sub-DAG with the local model   of the original DAG, where  ","In other words, for every   and  
                      
                   For every   and all  , the resulting joint model for the sub-DAG that is associated with   is given by  where 
                   
                      
                   A question that now naturally arises is whether these joint models for sub-DAGs, as given by Eq.  , can be related to the original joint model  ","It turns out that, for subsets   of   that are closed, this is indeed the case. 
                      
                   Our proof for the reverse implication is complex and elaborate, but its core is a simple separating hyperplane argument","Similar to what we have done in the proof of  , we construct a joint probability mass function to perform the separation","However, in contrast with the proof of  , a factorising probability mass function is no longer sufficient",This makes constructing the joint probability mass function that is used in   both complex and elaborate,We consider   to be the main technical achievement of this paper,It is a significant generalisation of   [with  ] and has a number of important consequences,"As a first example, it implies the following generalisations of  .   
                      
                   6 Connections with independent natural extension Let us now consider the special case of a credal network for which the underlying DAG has no edges or, equivalently, consists of disconnected nodes only; see  
                       for an example with four nodes","In that case, these nodes clearly have neither parents nor descendants","Consequently, for every  , the local model   is unconditional and the non-descendants are given by  ","It is therefore easy to see that in this particular case, Eq.   reduces to  Due to  , we know that  , as given by Eq.  , is   
                      , meaning that  Since   also marginalises to its local models ( ), we find that  , as given by Eq.  , is an   of the local models  ,  , that is also coherent ( ). 
                       Moreover, due to   and Eq.  , it is a subset of any other coherent independent product of  ,  ","This makes  , as given by Eq.  , the unique smallest coherent set of desirable gambles on   that is an independent product of  ,  ","This is called the   of the local models  ,  , and is denoted as   
                      ","We can therefore conclude that, for credal networks for which the underlying DAG consists of disconnected nodes only,  a result that was already mentioned in Ref.  ","Consequently, quite a few of the results we obtain in the present paper can be regarded as generalisations of those in Ref.  . 
                       Our next two results show that the connection between our irrelevant natural extension of a network and the independent natural extension, as defined in Ref.  , goes much further than Eq.  . 
                      
                   
                      
                       illustrates this result with a simple example","It should be clear that Eq.   is a special case of  . 
                       
                       generalises   even further. 
                      
                   
                      
                       illustrates this result with an example. 7 AD-separation and its consequences for the irrelevant natural extension In Bayesian networks, there exists a simple criterion, called d-separation, that is capable of detecting independencies in the joint model, based only on the graphical structure of the underlying DAG  ","Due to their close connection with Bayesian networks, credal networks under strong independence inherit this property almost trivially  : every d-separation in the DAG corresponds to a (strong) independence in the joint model","For credal networks under epistemic independence, no such result exists","We do know that for general credal networks under epistemic independence, d-separation does not imply epistemic independence  ","However, it is considered an important open problem   whether or not this holds for the most conservative joint model, also referred to as the independent natural extension 
                       of a credal network  ; see Ref.   for some partial but promising results for Markov chains","For the irrelevant natural extension of a credal network, which is the subject of the current paper, d-separation does not imply epistemic irrelevance; see Ref.   for a counterexample","However, as we will show, it is possible to derive very similar results by employing an asymmetrical version of d-separation, which we call AD-separation","As we will see, AD-separation satisfies all graphoid properties except symmetry","Furthermore, and perhaps most importantly: we show that in the irrelevant natural extension of a credal network, AD-separation implies epistemic irrelevance, thereby establishing an asymmetric version of the classical d-separation result","We should note that our results are inspired by the work of Moral  , who developed similar results in a much more restricted context; we comment on the restrictions he imposes further on in Section  . 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   7.1 AD-separation In probabilistic graphical networks that are defined by means of a symmetrical independence concept, the notion of d-separation is a very powerful tool  ","However, for asymmetrical independence concepts such as epistemic irrelevance, there seems to be no convincing reason for using a symmetrical separation criterion such as d-separation","If learning   is irrelevant to  , must it follow that learning   is irrelevant to  ? We agree with Dawid   that such a requirement is not obvious","Hence, we prefer to consider a modified version of d-separation that does not require symmetry","Moral   speaks of  
                         
                          (AD-separation) and Vantaggi   has introduced the very similar   criterion","Here, we do not use one of these existing concepts, but choose to introduce a slightly modified version, which we will call   (asymmetrical d-separation) as well. 
                          We prefer our version because our definition is weaker than—in the sense that it is implied by—Moral's AD-separation, slightly more general 
                          than Vantaggi's L-separation and yet, it has stronger properties than both of these other concepts","Consider any path   in  , with  ","We say that this path is   by a set of nodes   whenever at least one of the following four conditions holds: 
                      In Moral's version of AD-separation, the notion of a blocked path is very similar","The only difference is condition B1, which he strengthens by requiring that  ","Clearly, our condition is implied by Moral's","Vantaggi uses the same notion of blocked path as we do, 
                          but leaves out conditions B1 and B4","They are redundant in her case, because she does not need to consider cases in which   or   are elements of  . 
                         
                      
                         
                      Now consider (not necessarily disjoint) subsets  ,   and   of  ","We say that   is   from   by  , denoted as  , if every path  ,  , from a node   to a node  , is blocked by  ; see  
                          for an example of AD-separated sets",Moral and Vantaggi define their separation criteria in much the same way,"The only difference with Moral's version of AD-separation is his notion of a blocked path, as explained earlier","Clearly, AD-separation in Moral's sense implies AD-separation in our sense",The difference with Vantaggi's criterium is that L-separation is defined for disjoint sets only,"Notice that if we restrict ourselves to disjoint sets, AD-separation (both our version and the one by Moral) is identical to L-separation. 
                         
                      It turns out that our version of AD-separation satisfies all graphoid properties except symmetry.  
                      This result (and our proof for it) is very similar to, and heavily inspired by, the work of Vantaggi  . 
                          The main difference is that Vantaggi does not include the two redundancy properties, since L-separation is defined only for   subsets  ,   and   of  ","Moral's version of AD-separation does not require  ,   and   to be disjoint, but it does not satisfy direct redundancy, and proofs for a number of other properties are not given  . 7.2 Separation properties of the irrelevant natural extension The reason why we have introduced AD-separation, is because it can be used to state the following very general factorisation result, the proof of which relies heavily on  .  
                      By combining this with  , we can prove a result that is very similar to the classical d-separation result: AD-separation implies epistemic irrelevance in the irrelevant natural extension of a credal network. 
                         
                      We leave it to the reader to show that   is a generalisation of   and that   generalises the first part of  ","In other words: for any closed subset   of  , it holds that  ","What is particularly nice about   is that it allows us to detect epistemic irrelevancies in the joint model in a purely graphical way, without resorting to numerical computations; all we have to do is to check for AD-separation",Note however that AD-separation is only a sufficient condition for epistemic irrelevance,An important—and so far open—question is therefore whether the epistemic irrelevancies that are detected by AD-separation are the only ones that can be detected based on the graphical structure of the network,"In other words, to put it more technically: is AD-separation complete with respect to epistemic irrelevance? We conjecture that it is, but provide no proof",Another important question is whether or not AD-separation can be checked efficiently,We suspect that this is indeed the case and that polynomial time solutions can be obtained by suitably adapting existing algorithms for d-separation  ,"However, this too, we leave as a possible topic for future research. 7.3 A crucial difference with earlier work by Moral Readers who are familiar with the work in Ref.   might have noticed the similarity between Ref.   and the first part of  ","The main difference between our approach and Moral's approach  , besides the fact that we use a slightly different separation criterion, is that he enforces a more stringent version of epistemic irrelevance than we do",He calls   epistemically irrelevant to   if and only if the model   for the variable   is the unique smallest set that marginalises to the marginal models   and   and for which   is irrelevant to   in our sense,He refers to our concept of irrelevance as ‘weak’ epistemic irrelevance,"Consequently,   
                          
                         ","As a simple example: his concept of irrelevance does not allow for two variables to be mutually irrelevant, except in some degenerate uninformative cases","Therefore, his results cannot be applied to a network consisting of two unconnected nodes","More generally speaking, it seems to us his results can only be applied to networks in which every pair of nodes can be connected by means of a directed path. 7.4 Further comments and some clarification As far as the second part of   is concerned, some clarification is perhaps in order",We do not claim that epistemic irrelevance satisfies the graphoid axioms that are stated in  ,"As was proven in Ref.  , epistemic irrelevance can violate direct contraction and both direct and reverse intersection","In fact, we believe that this negative result might even be one of the main reasons why a result such as   has thus far not appeared in any literature","Indeed, in Bayesian networks, proving the counterpart to  —with AD-separation replaced by d-separation and epistemic irrelevance replaced by stochastic independence—is usually done by using the fact that stochastic independence satisfies the graphoid axioms  ","By applying these axioms to the independence assessments that are used to define a Bayesian network, one can infer new independencies, namely those that correspond to d-separations in the DAG of that network","If one tries to mimic this approach in our context, then since epistemic irrelevance can fail some of the graphoid axioms, one might suspect that   cannot be proven","However, it is not necessary to use the axioms: our proof for  —of which the first part of   is a straightforward consequence—uses only   and a number of properties of AD-separation",At no point does it invoke graphoid properties of epistemic irrelevance,The second part of   is then but a mere consequence of the first part and  ,"It states that the family of irrelevance statements that are proven to hold in the first part, are closed under the graphoid properties in  ","So in order to conclude this section: epistemic irrelevance can fail a number of graphoid axioms, which implies that the irrelevance statements that are proven in   do not necessarily hold for every joint model   that satisfies requirements G1–G3","However for the unique one that also satisfies G4, being the irrelevant natural extension   of the network, this family of irrelevance statements does hold, the reason being that for this specific model, one can provide a direct proof that does not invoke any graphoid axioms of epistemic irrelevance. 8 Summary and conclusions This paper has developed the notion of a credal network under epistemic irrelevance within the framework of sets of desirable gambles","By combining local sets of desirable gambles with assessments of epistemic irrelevance, and by doing so in the most conservative way possible, we have constructed an intuitive expression for the irrelevant natural extension of a credal network",We then went on to establish a number of theoretical properties of this irrelevant natural extension,"It satisfies a powerful factorisation property, marginalises in an intuitive way and has tight connections with the independent natural extension","Furthermore, the irrelevant natural extension satisfies a result that is very similar to the classical d-separation result in Bayesian networks","We have introduced the notion of AD-separation, an asymmetrical adaptation of d-separation, have shown that it satisfies all graphoid properties except symmetry and, most importantly, that it implies epistemic irrelevance","As far as future work is concerned, the most immediate task seems to be translating our results to the framework of coherent lower previsions","Although the expressiveness of sets of desirable gambles renders them extremely useful from a theoretical point of view, and as such allowed us to develop the results in this paper, representing them in a computer in order to manipulate them using algorithms quickly becomes overly complicated; see for example Refs.  ","Since part of the expressiveness of sets of desirable gambles is not relevant as far as probabilistic inference is concerned  , it seems preferable to use less expressive frameworks for the development of algorithms","Inspired by the recent linear time algorithm for credal trees  , coherent lower previsions seem to provide the right balance between expressiveness and algorithmic power",We are confident that it is indeed possible to translate our results to the framework of coherent lower previsions,"In fact, we have concrete ideas on how to do so; see Refs.   for some essential theoretical results, establishing an elegant connection between the theories of sets of desirable gambles and coherent lower previsions in a very general setup","For the particular case of credal networks under epistemic irrelevance, some preliminary results in terms of coherent lower previsions (and credal sets) can already be found in Refs.  ","Using these techniques, it is possible to express our more fundamental results—such as those concerned with marginalisation, independent natural extension and AD-separation—directly in terms of coherent lower previsions",We intend to publish this in the near future,Another important avenue for future research would be to try and establish similar results for credal networks under epistemic independence  ,"Do these networks satisfy marginalisation properties such as the one presented in Section  ? Do they exhibit the connection with the independent natural extension that was discussed in Section  ? And perhaps the most important open question related to credal networks under epistemic independence: do they satisfy the same separation properties as Bayesian networks? See the introduction of Section   for some comments on this last question, including relevant references","Our ultimate goal, which served as a motivation to develop the present results, is to develop an efficient inference algorithm for credal networks under epistemic irrelevance whose underlying graphical structure is not necessarily a tree, extending the developments in Ref.  ",We would like to point out that some of the properties in this paper already provide a direct tool to do so,"Similar to what is usually done in Bayesian networks, our marginalisation properties and the separation properties that are induced by AD-separation can be used to reduce the size of the network in which the inference problem at hand needs to be solved; see Ref.   as well","If after such a preprocessing step, the graphical structure of the network is reduced to a tree, the algorithm in Ref.   can be applied","On top of these preprocessing steps, we think that further algorithmic developments could also benefit greatly from our results in Section   on the connection with the independent natural extension, especially since the strong factorisation   property of the independent natural extension has been an essential tool in the development of the aforementioned algorithm for credal trees under epistemic irrelevance  ",Appendix A Proofs of our results This appendix provides proofs for the results in this paper,"It also contains some additional results that are needed in the proofs, a short introduction to maximal sets of desirable gambles (included just before the proof of  ) and two definitions that are used in some of the proofs related to AD-separation (included just before the proof of  ). 
                      
                   
                      
                   
                      
                   
                      
                   Our proof for   and   uses the following convenient version of the separating hyperplane theorem","It is proved in Ref.   and repeated here to make the paper more self-contained. 
                      
                   
                      
                   Since   generalises   without building upon it, it is not necessary to provide   with a separate proof","However, we feel that the complexity of the proof for   obscures the ease with which   can be proved",We therefore choose to provide   with a proof of its own,"As it makes use of so-called maximal sets of desirable gambles, a concept that has not been introduced in the main text of this paper, we provide a short introduction here","A coherent set   of desirable gambles on   is called   if it is not included in any other coherent set of desirable gambles on  —in other words, if adding any gamble   to   makes sure we can no longer extend the resulting   to a coherent set",We generically denote maximal sets of desirable gambles as   instead of using the general notation  ,Maximal sets of desirable gambles have a number of useful properties,"For example, a coherent set   of desirable gambles on   is always the intersection of all the maximal coherent sets   of desirable gambles on   that include it; see Ref.  ","In other words,   if and only if   for every  ","As a consequence, we have the following separation property: if a gamble   is not an element of  , there is at least one maximal set   for which  ","Another useful property is that maximal sets of desirable gambles resolve points: for any maximal set   and non-zero gamble   in  , either   or −  is an element of  ; see Ref.  . 
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   Our proof for   and   makes use of the following two subsets of  ","For all  , we define  and 
                   
                      
                   
                      
                   
                      
                   
                      
                  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s259018851930006x," 1 Introduction In transportation engineering, the topmost consideration should be safety","Therefore, professional scientific effort should be made by both private and public sectors to comprehensively interpret service conditions and the changing mechanism of road roughness levels",The international roughness index (IRI) was initially proposed in a research project conducted by the University of Michigan ( ) for monitoring the overall roughness condition of certain pavements,The IRI mainly measures the longitudinal profile condition of a travelled roadway according to the vehicle vibration condition,The most widely used units for measuring the IRI are meter per kilometer and inch per mile ( ),"Over the past few decades, scholars across the world have made significant effort to study the changing IRI mechanisms, which have been then used to analyze pavement performance deterioration trends ( ).   analyzed the IRI characteristics of cracked and seated concrete pavements and thereby provided maintenance and rehabilitation guidance for pavement management","However, they did not consider the climate conditions, which have significant influence on the roughness progression, especially for hot-mix asphalt overlay pavements ( ).   investigated the roughness condition of low-volume roads using multiple regression analysis.   predicted the IRI over time using Markov analysis, thus providing a probabilistic IRI prediction method instead of a deterministic one.   proposed an exponential IRI regression model by modeling two parameters α and β and used the model to predict IRI trends over time","However, the effectiveness and robustness of the abovementioned approaches are not convincing because most regression analysis methods assume that the dataset follows certain distributions",Typical IRI prediction models have been developed by the transportation departments of Mississippi and Washington as part of the Long-Term Pavement Performance (LTPP) program and by other state agencies based on the Mechanistic Empirical Pavement Design Guide ( ),"The development of these models is a remarkable achievement, and they are still being used for long-term pavement design and management","However, because these models are based on data-driven approaches, they still have the intrinsic drawbacks of statistic-based methods ( )","Thus, the performance of these models relies significantly on the quality and characteristics of the data being used","In addition, traditional approaches are influenced by the low accuracy of data acquisition, and hence, they usually fail to handle the ambiguity and uncertainty inherent in raw datasets ( )","With the advancement of artificial intelligence (AI) techniques, several engineering fields are focusing on using AI approaches to facilitate research and practice tasks","In the field of pavement management, the initial attempt was made by  , who proposed an intelligent method based on artificial neural networks (ANNs) to analyze the deterioration trend of pavements through IRI prediction modeling","Thereafter, researchers have used not only ANNs but also other AI techniques such as support vector machines ( ) to investigate pavement roughness progression","However, these methods consider the IRI as a dependent indicator that is influenced by several other pavement conditions, traffic loading, and environmental factors at a fixed observation time, thereby ignoring the impact of time on the IRI","Time is an invisible space variable that can have an abstract existence and can contain a comprehensive combination of all other visible, traceable, and concrete influences","Hence, this variable has a direct influence on the deterioration of pavement performance, and hence, it affects the IRI","Therefore, fuzzy time-series methods should be investigated to bridge the gaps in current literature with regard to intelligent IRI predictions","Fuzzy time-series prediction methods have been successfully applied to several fields such as stock prediction and weather prediction, and these applications serve as invaluable reference for the IRI prediction research conducted in this study","Considering the background described above, the present study aims to develop an IRI prediction approach based on an innovative fuzzy time-series analysis","To fulfill this purpose, multifactor and multigranularity analyses are considered for time-series forecasting","First, the automatic clustering technique (ACT) is adopted to generate a series interval within each granular space, which should be defined in advance","Next, a second-order fuzzy-trend matrix (SFTM) and fuzzy-trend relationship classification (FTRC) method are proposed to predict the fuzzy-trend of each factor","Then, the fuzzy-trend states of multiple granular spaces are generated while giving full consideration to various uncertainties","Finally, the particle swarm optimization (PSO) technique is employed to forecast the optimal IRI value","Comparative experiments are performed to evaluate the effectiveness of the proposed method. 2 Methodology After a road is opened to traffic, its roughness condition tends to vary over the service time","To outline the changing mechanism of road roughness, IRI values are regarded as fuzzy time-dependent variables.  
                       illustrates the framework of the proposed fuzzy time-series clustering IRI prediction methodology","As shown in the figure, the IRI values are first divided into three granular spaces","Then, the data within each granular space are assigned to a series of intervals, with each interval having a membership degree defined by fuzzy theory","Specifically, the value of the membership degree indicates the belonging probability of the concrete IRI value to the corresponding interval","In this research, the initial intervals are obtained through an automatic clustering algorithm","Thereafter, the fuzzy sets are assigned to the divided intervals according to the subordinate relationships of these sets along with the grouping statistics","Finally, the PSO method is utilized to obtain the optimal weight vector and corresponding predicted IRI value","To provide a more interpretative understanding of the methodology, the key fundamental theories are introduced in this section. 
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   2.1 Fuzzy time series In 1965, Zadeh proposed the fuzzy set theory ( ), which was the theoretical foundation for fuzzy time-series analysis","On the basis of this theory,   developed the first fuzzy time-series model in 1993, which was a remarkable breakthrough","Later, several studies were conducted on the optimization ( ) and application of time-series analysis","A few representative examples are listed here:   aimed to optimize the complexity of a fuzzy time-series model,   proposed a heuristic algorithm ( ) to improve the accuracy of the prediction result of a fuzzy time series, and   proposed a time-series model optimized for computing with words prediction analysis","Here, a fuzzy time series is defined as follows: Define   as a universe of discourse","Then, a fuzzy set   of   can be represented as follows: where   denotes the membership function of the fuzzy set  , which is mostly written in the form   → [0, 1].  ( ) is the membership degree of   within the fuzzy set  , 1 ≤   ≤  ",Assume that a group of fuzzy sets   are defined in the universe of discourse  .  ( ) represents the collection of  ,"Then, the collection   is called a fuzzy series set in the discourse  ","Based on the above definition, the  th  fuzzy time-series relationships for the fuzzy sets   can be expressed as 
                      Therefore, given  , the following logical relationship can be obtained: where   are called the current states, which should be known values in a certain problem.   is called the next state, which is to be predicted by the developed model. 2.2 Interval division using ACT The ACT ( ) is characterized by its adaptive clustering capability, which can help in minimizing the bias caused by manual parameter setting","Therefore, the ACT is adopted to generate the multiple intervals for different granular spaces.  
                          illustrates the key procedures of the ACT, which are data sorting, distance calculation, automatic clustering, and interval generation","Step 1: Sort the raw data in the ascending order, with equal values appearing only once","Then, calculate the average deviation ( ) and standard deviation ( ) of the sorted dataset using   and  . 
                         where   denotes the total number of data points.   denotes the sorted data values with  ",Step 2: Use   to calculate the maximum distance: where   is a constant weight value,Step 3: Cluster the ascending data following the calculations of Steps 1 and 2,"Let  
                          be the current cluster and judge whether the next data point belongs to this current cluster or not","If not, assign it to a separate cluster","That is, if  , then assign   to the current cluster","Otherwise, create a new cluster for  ",Repeat the operations until all the data points are assigned to specific clusters,"The clustered results can be expressed as follows: 
                      Step 4: Convert the clustering results to continuous intervals.  
                          presents the process of converting clusters to intervals",The detailed steps are as follows,"First, convert the first cluster {  to an interval [ 
                         ,  
                         )","Then, make the following two judgements: (1) Judge whether the current interval is [ ) with the current class of { }","If   ≥  , then convert the current class { } to intervals [ )","At the same time, use [ ) as the current interval and the next cluster, which is { }, as the current class","However, if   <  , then convert the current class { } to interval [ ) and create a new interval [ ) between [ ) and [ )","Next, use [ ) as the current interval and the next cluster, which is { }, as the current class. (2) Judge whether the current interval is [ ) with the current class of { }","If so, convert the current class { } to interval [ ,  )","At the same time, use [ ,  ) as the current interval and the next cluster, which is { }, as the current class","According to this principle, process all the data points within the dataset and thus generate all the intervals",Step 5: Calculate the medium values for all the intervals as follows: where   denotes the lower bound of the  th interval,"Here, the superscript   denotes the upper bound of the  th interval. 2.3 Granular computing Granular computing (GrC) is an umbrella concept and computational paradigm for information processing that simulates human thinking mechanisms",Different formalisms and information processing methodologies are included in GrC methodologies,The hierarchy structure of the computing organization mechanism of granules provides GrC with the ability to interpret complex problems from multiple perspectives,"As illustrated in  
                         , the entire GrC structure usually includes several layers with crossed information sharing between the granules","In this figure, the deeper the layers, the finer are the unit data items, which are shown as the dots in the  th layer","The hierarchy property of the granular structure can be observed as well, which characterizes the novelty of GrC compared to other methods","Generally, GrC is realized based on three basic notions: granules, granular space, and hierarchy structures ( )",Granules are the unit elements of a GrC model and are distributed in their specific granular spaces,"The individual granular space (father granular space) can be composed of several subgranular spaces (child granular spaces), and both the father and child granular spaces themselves can be hierarchically distributed as well","Thus, the comprehensive hierarchy of multiple granular spaces can be used to model the multifactor and multiscale organized datasets, especially for handling problems that involve both certainty and uncertainty influences that should be considered simultaneously ( )","For GrC-based system development, several computing models have been proposed as a type of knowledge interpretation methodology","Popular GrC models include the fuzzy set theory, rough set theory, quotient space, and cloud model",Each of them has a specific hierarchy structure suitable for certain problems,The fuzzy set theory is mainly used for handling ambiguous data and is based on the granularity of information and if–then structures,The rough set theory is a mathematical tool for interpreting incomplete data and unveiling the hidden properties of a dataset,Hierarchy rough set structures are a typical characteristic of rough set analysis,"The quotient space is characterized by quotient typology, and new space is created for the given data",The cloud model method involves forward and backward cloud transformation,"Hence, the hierarchy structure of the cloud model is generated by this bidirectional transformation concept",These four typical GrC models are widely used in literature,"Each has its own computing mechanism, but they share the basic ideas of GrC","In this research, the fuzzy set model is selected for IRI prediction analysis because it is more computationally efficient than the other approaches","Specifically, each IRI measurement factor is considered as an individual granule","Therefore, three granules can be defined by three IRI factors, which are the principal factor and two subfactors","Then, we define three granular spaces: (1) the first granular space containing only the principal factor, (2) the second granular space containing the principal factor and one of the subfactors, and (3) the third granular space containing the principal factor and both the subfactors. 2.4 PSO PSO ( ) is a heuristic model optimization algorithm based on the simulation of animal movement behavior",The innovative idea behind this method is that animals are able to select the optimum speeds and directions in moving toward their desired destinations,The heuristic aspect of the PSO method is that no assumptions are required for handling the specific tasks when using this method,The PSO algorithm is a global random search algorithm based on swarm intelligence and hence is more intelligent than traditional methods,The main idea behind this method is the simulation of the migration and clustering behavior during bird foraging,"Various organisms in nature have certain group behaviors; thus, one of the main research areas of artificial life is to explore the group behaviors of natural organisms, thereby constructing intelligent models for solving practical problems with scientific perspectives","In most situations, PSO is used to optimize the solutions of other methods instead of providing independent models","Specifically, potential solutions to a certain problem are regarded as birds, called particles, in the solution space",Each particle has a fitness value that is determined by the defined fitness function and has a velocity that determines the direction and distance from which the particle is to move toward the optimum position,"When the specific optimization problem has been established, a group of random particles (random solutions) is generated to initialize the particles","Then, the moving velocity and positions of the particle are iteratively updated to find the optimal solution","In each iteration, the particle updates itself by tracking two extremums: the optimal solution found by the particle itself, which is called the individual extremum, and the optimal solution found among the entire population in the current state, which is called the global extremum","Finally, the optimized model is obtained when the satisfied fitness value is achieved","Therefore, the primary consideration of utilizing PSO in this research is based on the three abovementioned reasons, which can be summarized as follows: (1) PSO is an intelligent algorithm that outperforms traditional methods, yet few studies have used this approach to analyze pavement roughness conditions. (2) PSO is best utilized when it is combined with other methods to solve certain optimization problems instead of being independently used for model building","In this manner, PSO can serve as an effective approach to solve complex optimization problems. (3) The PSO method has been proven to be significantly effective in solving different types of problems; hence, this research aimed to determine the effectiveness of the PSO method in solving the problem of IRI prediction","Specifically, a particle swarm is formed by a group of particles that represent the potential solutions",Each individual particle has its initial position and moving velocity,"By calculating the objective function, the particle should update its position based on the velocity toward the best position, which represents the optimum solution of a certain problem","To ensure that the final solution is optimum from a global perspective, the individual local best position ( ) and global best solution ( ) should be updated repeatedly during the model optimization process","The global best solution is updated based on the local best solution, which is obtained by revising the current position based on the velocity and direction of the particle","The velocity and position update functions for particle   are defined in   and  , respectively. 
                         where  
                          and  
                          respectively represent the velocity and position of the  th particle in the  th iteration.   denotes the inertial weight coefficient.  
                          and  
                          are the constant parameters that are decided based on the properties of a specific problem","In the present case, we set  .  
                          and  
                          are normalization parameters having Gaussian distributions","The procedures of the PSO optimization algorithm are based on the above definitions and explanations, as illustrated in  
                         . 3 Multigranularity-combination-based IRI prediction This research proposed a state-of-the-art IRI prediction model based on a fuzzy time-series forecasting methodology facilitated by multigranularity PSO","As shown in  
                      , the prediction process is divided into five main steps","In this section, the detailed calculations in each step are discussed","Step 1: ACT-based interval division:  
                       shows the interval division process based on the ACT","Assume that the universe of discourse of the principal factor is divided into   intervals, which are represented as  ","If the  th subfactor belongs to the interval represented as  , then assign it to this interval",This affiliation can be represented as  ,"Step 2: FTRC realization: This procedure can be divided into four steps ( 
                      ): (1) define the fuzzy sets, (2) fuzzify data, (3) construct an SFTM, and (4) build FTRC for each factor","The details of these steps are as follows. where   and   denotes the intervals divided by the ACT. 
                   Based on this equation, the FTRC can be obtained, as listed in  
                      ","Step 3: Fuzzy-trend calculation for the individual factor: Construct the fuzzy-trend matrix ( 
                      ), which is used to predict the fuzzy-trend for each factor.  
                       illustrates the procedures of this method","Based on the fuzzy relationships presented in  , a fuzzy-trend matrix with a size of 3  ×  6 is constructed ( ) and assigned to each factor","In  ,  
                       represents the sum of the descending trends for group  , and  
                       represents the total number of descending relationships.  
                       represents the sum of the equal trends of all SFTMs for group  , and  
                       represents the total number of equal relationships.  
                       represents the sum of the ascending trends for group  , and  
                       represents the total number of ascending relationships",The calculation principles are as follows,"Consider the principal factor whose SFTM is expressed as  
                      ,  
                       →  ","If  
                       >  , then update the corresponding values based on the following equations: 
                      
                      
                      
                   Otherwise, if  
                       <  , then  
                       and  
                       should be updated according to the following equations: 
                      
                   Assume that the SFTM of the current state belongs to group  ","Then, the fuzzy-trend in the next state can be predicted by the following equation: 
                   Step 4: Fuzzy-trend calculation for multigranular spaces: Divide the original dataset into   granular spaces and predict the fuzzy-trend for each granular space based on the processes illustrated in  
                      ","In this study, the first granular space is defined as the principal factor, the second granular space is composed of the principal factor and one subfactor, and the (  + 1)th granular space is composed of the principal factor and   subfactors","Here,   denotes the number of subfactors","In this procedure, the fuzzy-trend   and corresponding predicted value   of the (  + 1)th granular space can be calculated using the following equations: 
                      
                      
                      where   represents the raw data of the principal factor at the time point  , Δ  is the predicted fuzzy-trend of the principal factor, and Δ  is the predicted fuzzy-trend of the  th subfactor","In addition,   is the correlation coefficient between two factors: (1) the average trend variation vector of the principal factor   and (2) the average trend variation vector of the   subfactor  ","Step 5: PSO-based optimum solution calculation: The optimal weight vector should be trained to calculate the final predicted result.  
                       shows the detailed process","First, the training dataset should be expressed as   according to the following equation: where   is a matrix formulated by the predicted values of   observations.   is the vector constructed by the corresponding ground truth values.   denotes the total number of granular spaces.   and   denote the predicted values and corresponding ground truth values for the  th observation in the (  + 1)th granular space","Next, the optimum weight vector should be determined by the PSO method","In this procedure,   particles are generated within the   granular spaces","The position of the  th particle is expressed as  , which should meet the condition  
                       ∈ [0, 1] while  ","Similarly, the velocity of the   particle is expressed as  , which should meet the condition  ",Both the position and velocity of each particle are initialized with random values,"In this study, the fitness function is defined as where  ( ) denotes the fitness value with 1 ≤   ≤  ","Here,   represents the number of particles generated in   granular spaces.   and   are defined in the same manner as that in  .  
                       denotes the position vector","For an individual particle, after the update of each position and velocity, the corresponding fitness value should be calculated by  ","When the best fitness value is obtained, the forecasting trend and forecasted value can be calculated by   and  , respectively. 
                      where   denotes the fuzzy-trend of the (  + 1)th granular space. Δ  denotes the corresponding forecasted fuzzy-trend","If Δ   > 0, the next state is considered to be in the ascending trend","Otherwise, it should be considered to be in the descending trend. 4 Implementation details 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   4.1 IRI data extraction and analysis IRI prediction experiments are performed using the proposed multigranularity fuzzy time-series method",Original IRI data are extracted from the LTPP database,"Different from the traditional methods, which consider only a single factor when predicting IRI changes, the proposed method utilizes multiple factors","Specifically, the average IRI values are selected as the principal factor, and the IRIs measured in the left and right wheel paths are used as the subfactors",The model predicts the average IRI in the next state with comprehensive consideration of the IRI values in the current state,"Therefore, the results are more reliable","Two significant phenomena are observed in the raw dataset: 
                      4.2 IRI forecast experiments Because the IRI distribution is different for different regions, this research extracts datasets from 38 provinces/states in North America ( 
                         )","Thus, more than 20,000 data items are obtained to train the model, and hence, the robustness of the method can be assured","The experiments are discussed in detail below. 
                      First, the original data are sorted in the ascending order",The average and standard deviations of each factor are calculated,"Then, intervals are generated based on the proposed ACT.  
                          presents the generated intervals. 
                      First, the fuzzy sets are formulated as follows: 
                      Next, FTRCs are built based on the SFTM.  
                          presents a part of the FTRCs for different factors","According to the fuzzy-trend relationships defined in  , Group 2 indicates equal trending between two adjacent intervals","Therefore, the entire data presented in   have equal trending, which indicates that the roughness conditions of these road sections have no significant variance between each other","It should be noted that no groups are assigned to the values in the first two rows of  , which are indicated by a slash (/)","These two values are not assigned to any groups because the group of current values is decided based on the evaluation of the two values ahead of the current values. 
                      Three fuzzy-trend matrixes are generated according to the principle used to obtain the matrix in  ","Accordingly, the matrixes presented in  
                          (a)–(c) are obtained","From these results, it can be inferred that few IRI values show descending trends, i.e., negative values","Most values remain constant, which is indicated by the number 0","However, the IRI values for some road sections show ascending trends",These phenomena indicate that the roughness changing mechanisms in different road sections are significantly different even though they are distributed within limited ranges,"Thereafter, the fuzzy-trends of these three factors are calculated using  ","Here, the calculated fuzzy-trends are  ","These results indicate that all the fuzzy-trends tend to increase with time because all of them are above 0. 
                      The fuzzy-trends are constructed in multiple granular spaces","As discussed before, the first granular space includes only the average IRI values, the second granular space includes the average IRIs and one of the wheel-path IRIs, and the third granular space includes all the IRIs","Then, the fuzzy-trends and values of these granular spaces can be calculated using  – . 
                         
                         
                      In this study, 25 particles are used for model optimization",The optimum results are obtained after 200 iterations of parameter updating,"Then, according to   and  , the optimal weighting vector is calculated as  ","Finally, the predicted IRI value is obtained based on   and  ","The calculated values obtained from the experiment are 
                         
                      Here, the calculated Δ  is higher than 0, which indicates that the IRI tends to increase in the next state.  , which is the forecasted IRI value in the next state, is 3.3764. 5 Results To further verify the effectiveness of the proposed method, comparative experiments are performed","The polynomial fitting, autoregressive integrated moving average (ARIMA), and backpropagation neural network (BPNN) methods are selected for IRI prediction, and the errors of these methods are compared with the results of the proposed method",Two error indexes—the root mean squared error (RMSE) and relative error (RE)—are used to evaluate the feasibility of each method,"These two indexes are calculated as follows: 
                      where   denotes the number of the dataset.   and   denote the   predicted IRI value and ground truth IRI value, respectively","As indicated by the error distributions shown in  
                      , the proposed method achieved the smallest error indexes ( ), followed by the ARIMA method ( )","The polynomial fitting method has the least performance, which might be caused by the fact that traditional fitting methods are less capable of extracting comprehensive information from big datasets. 6 Discussion In this research, IRI data for 19 years (1997–2015) are extracted from the LTPP database for modeling",The IRI values were collected by the Federal Highway Administration,"We select the IRI values for the pavement section Ontario 0901, Canada, to illustrate the entire prediction process","In addition, the IRI values for the pavement sections Florida 3804, Alabama 0103, and Alabama 0163 are extracted to perform comparative experiments","Because times-series prediction relies highly on historical data, the data from 1997 to 2014 are collected for model training","Then, the trained model is used to predict the IRI value for the year 2015",The IRI values extracted from the LTPP database are considered as ground truth values to calculate the errors of the model,"Thus, the IRI values used for model training are considered as the current state, whereas the IRIs for the year 2015 (the last year) are regarded as the next state to be predicted","Moreover, the prediction performance of the proposed method is compared with those of other widely used methods to verify the effectiveness and reliability of the proposed method","In summary, the following conclusions can be drawn from this research: 
                   Data availability statement The raw dataset used in this research can be downloaded from the LTPP InfoPave website, which can be accessed through the following link:  ","Funding This research is funded by   [Grant number:  ], Fundamental Research Funds for the  ,   [Grant number:  ,  ], China","CRediT authorship contribution statement 
                       Conceptualization, Methodology, Validation, Formal analysis, Writing - original draft, Writing - review & editing, Funding acquisition.   Conceptualization, Methodology, Validation, Formal analysis, Data curation, Writing - original draft, Writing - review & editing.   Methodology, Validation, Formal analysis, Data curation, Writing - original draft, Writing - review & editing.   Validation, Formal analysis, Writing - review & editing, Funding acquisition.   Validation, Formal analysis, Data curation, Writing - review & editing","Acknowledgements Meanwhile, the authors would like to express sincere appreciations to the LTPP staff for maintaining the Website and making the data so readily available","Moreover, the authors would like to thank the anonymous reviewers for providing their invaluable comments",Conflicts of interest The authors declare no conflict of interest.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0888613x1300234x," 1 Introduction Computing with words (CWW) was introduced by Zadeh   as a methodology for reasoning and computing with human-sourced information described in natural language, of which the idea was actually rooted from his previous work on linguistic variables, fuzzy constraints and fuzzy if-then rules  ","During the last decade or so, CWW has attracted considerable attention of the fuzzy set community","In order to establish a mechanism for automated reasoning, computing or decision-making with words, it is necessary to establish appropriate mathematical models for representing linguistic information and perceptions, which would be able to capture certain semantic characteristics of words that underline the way human beings reason or make decisions using natural language","This is truly a challenging task because of the flexibility (e.g., context/culture dependency) of as well as fuzziness and uncertainty associated with semantic characteristics of words used in human reasoning",All of those make the process very difficult to model and represent the meaning of words and perceptions in order to carry out meaningful computing,"This makes CWW a comprehensive research area being open to interpretations and different instantiations, as intensively discussed in  ","So far, there have been numerous models developed for CWW with applications to a spectrum of practical human-centric problems","From the perspective of modeling and reasoning, Zadeh's seminal work   is the first one that provides a general framework for CWW in which the use of fuzzy sets becomes crucial as they provide a means of modeling the fuzziness inherent in natural language utterances",Computational mechanism underlying the CWW is essentially based on the so-called extension principle in association with generalized rules of inference in fuzzy logic,"Interestingly, in  , Lawry proposed an alternative approach to CWW based on mass assignment theory   and probability theory and provided a mechanism for reasoning with linguistic descriptions endowed with imprecise probabilities that avoids the computational complexity incurred by applying the extension principle in Zadeh's theory of CWW","As a matter of fact, Zadeh's general framework could be further developed and detailed for various applications to human-centered modeling and reasoning problems in practice","As effective and efficient paradigm serving for handling imprecise information in natural language, the authors of   expect that CWW can find applications in the areas related to computational linguistics, and argue that CWW could have high implementation potential in natural language generation, processing or understanding, illustrating by an application of their proposal to linguistic data summaries","From the perspective of decision-making application, over the last decade much work has been done so as to develop CWW approaches for solving decision problems involving vague and imprecise information","Typically, in decision-making applications, CWW is mainly involved with the problem of how to represent and aggregate linguistic information in decision making","In recognizing that “ ”, Mendel   proposed to use type-2 fuzzy sets for modeling words in CWW for assisting people in making subjective judgments","While most early methods for dealing with linguistic information in decision-making were making use of fuzzy sets as a means of modeling linguistic terms and the corresponding CWW models were based on Zadeh's extension principle, e.g.,  ","Clearly, the computing results coming by using these methods, classified as semantic models  , come in the form of fuzzy sets that in general do not match exactly pre-defined fuzzy sets of linguistic terms and, therefore, a linguistic approximation process must be applied to obtain linguistic recommendations for the problem at hand","Consequently, such linguistic approximations may result in loss of information and lack of precision of the final results","This has motivated Herrera and Martínez   to propose a so-called 2-tuple linguistic representation model as a tool for CWW, which aims at overcoming the limitation of the loss of information caused by the process of linguistic approximation in fuzzy set-based approaches","The 2-tuple linguistic model has been also discussed in decision making problems present in various application areas, including group decision making, distributed intelligent agent systems, information filtering, information retrieval, and engineering management  ","In the 2-tuple linguistic representation model or its variations such as proportional 2-tuple representation model  , which are classified as symbolic models  , each set of the linguistic terms under consideration, denoted by  , is assumed to be strictly ordered, i.e., we have  ",In this study the set   is called a linguistic scale,"Basically, symbolic models aimed to map a linguistic term set into an appropriate numerical scale and then computation for linguistic information aggregation is performed over this numerical scale so that many available numerical aggregation operators can be applied in a direct manner","Finally, the computing results are converted back to linguistic 2-tuples","As we have observed, in the fuzzy-set-based approaches, the semantic representation of words makes them to become complicated in terms of underlying computations and, in addition, the ordering relationships between terms of the scale become blurred as well","While in the symbolic-model-based approaches, although they allow directly performing computations on the set of linguistic values in which only a totally ordered structure is assumed by mapping the set of linguistic values into suitable numerical scale, we might be losing much of the information we have purposely been keeping at the structural phase of linguistic decision problems","Note that the use of a linguistic approach is only necessary when the information in decision situations cannot be assessed precisely in a quantitative form (i.e., in terms of numerical values)","Moreover, let us note again that most linguistic scales used in the previous studies of linguistic decision analysis are assumed to be totally ordered","That is, one can recognize an order between the terms in the scales based on the qualitative semantics of terms, called inherent order-based semantics, which are directly associated with the string expression of terms regarded as their syntax","Obviously, this qualitative semantics of terms is present in any natural language","When experts provide their linguistic assessments, they focus on the ordering semantics of terms completed in comparison with some other terms of the linguistic scales","Therefore, we may refer here to qualitative linguistic scales to indicate that the linguistic terms of the scales must convey their inherent order based qualitative semantics",Quantitative semantics of terms is required from computational standpoint and it is a basis to establish a computational structure or semantics domain that will be called in this study semantic linguistic scale,"It is obvious that there is no explicit connection between the qualitative linguistic scales and their respective quantitative ones while, as pointed out by Zadeh in  , one of the key rationales underlying CWW is “much of human knowledge is described in natural language”, and hence words are objects of CWW","Therefore, it is naturally desirable to establish formal linkages between the former scales, connected immediately to the respective linguistic scales, with the latter ones","However, this can be done only when linguistic variable domains can be formalized in a manner that the qualitative semantics of terms determines their quantitative semantics, similarly as a guideline mentioned by Mendel in  , “A word must lead to a membership function rather than a membership function leading to a word”","These linkages can be established by following the idea of the denotational semantics found in programming languages, in which every phrase (linguistic label), as a syntax expression, in a language is interpreted as a denotation, which is often a mathematical object  ","That is that “The idea of denotational semantics is to associate an appropriate mathematical object, such as a number, a tuple, or a function, with each phrase of the language”   and, then, semantic worlds or domains “are ‘sets’ of mathematical objects of a particular form”  ","In particular, they may be mathematical structures, which are suitable for developing semantic linguistic scales with expected computation features","Since hedge algebras are mathematical models of linguistic term-domains, their elements combined with their quantitative characteristics can be utilized to represent the semantics of linguistic terms by 4-tuple linguistic representation, which may be considered as a generalization of the linguistic representation of linguistic labels in the 2-tuple approaches",Based on this we can develop semantic linguistic scale viewed as “semantic domains” in the above sense of the linguistic terms in linguistic scales,"Note that, as being objects of computation in CWW, terms (words) and propositions drawn from a natural language together with their meaning and fuzziness/vagueness representation seem to play an important role in the development of CWW paradigm  ","Therefore, such a connection between two semantics could result in the linguistic computational models developed in a more interpretable and convincing fashion","However, it seems that this observation has been overlooked in the previously developed models for linguistic decision-making","In light of the discussion presented above, the main objectives of this paper are as follows: 
                   The paper is organized as follows","Section   discusses the essential qualitative semantics of terms and the semantic linguistic scales that are useful from computational perspective, and then proposes some requirements for their construction","In Section  , the order-based qualitative semantics of terms will be discussed based on the basis of order-based structure of hedge algebras",Quantitative semantic aspects of linguistic terms previously developed will be reviewed to offer necessary background knowledge for the study,The concept of a 4-tuple semantic representation model and 4-tuple semantic linguistic scales exploiting numeric quantitative semantics and interval-semantics of terms are introduced and systematically developed in Section  ,"To show the advantages of the 4-tuple semantic linguistic scales, a comparative study based on a simple multi-criteria decision problem is examined in Section  ","Finally, some conclusions are presented in the last section. 2 A concept of semantic linguistic scales and some essential requirements for their construction When dealing with linguistic scales or, more generally, with CWW, the following two aspects should be considered: 
                   With regard to this viewpoint, it can be observed from the above analysis that the concept of linguistic scales is still not obvious and fully unified","Thus, these two aspects should be investigated in a systematic manner. 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   2.1 A concept of semantic linguistic scales Our starting position to define this concept is to consider each linguistic scale as a mechanism to generalize a numeric scale","For example, let us consider a numeric scale consisting of the numbers in the interval [0, 10] to assess the mathematical ability of students on a basis of their grades obtained in mathematics examinations","Intuitively this numeric scale exhibits two main characteristics: 
                      Now, in order to relate numeric scales with their respective linguistic ones, we may imagine how a teacher gives an assessment, for instance, of 7.5 to a student's examination","Here, the teacher realizes this assessment in a fuzzy environment   and usually he used to utilize vague concepts in his examination assessment process",Then he has to convert his linguistic assessments into appropriate numbers positioned on the numeric scale,"That is, linguistic terms are present in the teacher's assessment process","In practice, at the end of the study-year, students are classified into groups, named by linguistic terms, in accordance with an aggregation of the results of their mathematical examination, by using the given quantitative-linguistic scale","For example, in the Vietnamese colleges, such a scale is defined as follows:  where the intervals shown in  , called in this study   of terms, are the user declaration of the quantitative semantics of the linguistic terms of the scale","This means, for instance, that a student is classified into the group “ ” with respect to the mathematical abilities if the aggregated result of all her/his mathematical examination assessments in the study-year falls into the interval  , which is declared as the interval-semantics of “ ” as shown in  ","As such, the set in   gives an example of the semantic representation model for the concept of “semantic linguistic scales”, which can be considered as a generalization of the numeric scale  , although it should be extended to include richer quantitative semantics of terms for computing with linguistic terms","Since the terms present in   are only labels, treated as the  , an extension of the semantic representation model given in   must be adequate to guarantee that we can develop operations working usefully on the semantics of the linguistic scale, including the ordinary aggregation operations","In light of this observation, it is natural to require that, for each linguistic scale, the so-called   associated with it that we should develop has to come up with the following two main properties: 
                      In addition, as we require in the study that the proposed linguistic computational model would exhibit a proper relationship between the qualitative semantics of linguistic terms and their suitable quantitative semantics, it is necessary to propose some requirements for constructing semantic linguistic scales as discussed in the following subsection. 2.2 Some essential requirements of the construction of semantic linguistic scales As aforementioned, the relation of syntax, i.e. label representation, and semantics of the linguistic terms of a linguistic variable is a fundamental problem of linguistic decision analysis","However, it seems to be not easy to deal with this problem for imprecise linguistic information","Therefore, it becomes important to discuss what the actual semantics of this kind of information is","Conventionally, fuzzy sets assigned to linguistic terms can be considered as their semantics based on the viewpoint of a generalization of crisp-concepts","By this it is difficult to explain, for instance, what the semantics of linguistic hedges is","On the other hand, another viewpoint on the semantics of terms can be observed when we follow the concept of semantics in formal logics","Normally, the meaning of a word or a phrase is a collection of objects or phenomena present in real world that the terms or phrases point at","The question is at what points a vague term in a linguistic domain of an attribute, which comprises terms generated from primary terms (atoms) by using hedges. “ ” or “ ” do not indicate concrete items or phenomena in the real world, because we still do not know whether they indicate human, or animals or other things","However, they realize a qualitative semantics and may be used to describe different properties of different objects",The presence of these terms in natural language aims   properties  ,This semantics seems to be very crucial for human decision making as it will be discussed next,The same argument can be applied to explain the presence of hedges in natural languages: their presence is due to the need of the comparison of alternatives in human decision making,"So, every hedge aims to intensify vague terms and generate new terms, which is comparable with the original ones",For example “ ” is comparable with “ ”,"Therefore, another characteristics of the term semantics is what that provides us a basis to identify the order relationships of the terms in a term-domain","This characteristics of the term-semantics seems to be more essential, when we observe that there are fundamental facts supporting it: natural language is a vehicle to recognize reality and offer a communication vehicle within the community","Subsequently, natural language is rich enough to fully describe phenomena of the real world serving for human activities",Life is composed of a series of decisions,"The aim of decision making in human daily life is to choose an alternative, which is better than others","Therefore, in natural languages there should be elements to describe preference of alternative in comparison to other alternatives in question","Linguistic terms with their own semantics and, in particular, hedges, are elements facilitating this process","In turn, such feature shows to be an essential characteristic of the semantics of linguistic terms","As a consequence, we consider order-based semantics of terms as their natural intrinsic qualitative semantics","To ensure the soundness of the linguistic scale, we introduce three intuitively appealing requirements for construction of semantic linguistic scales","We do not consider them as criteria, since it is not easy to define the proper semantics of vague linguistic terms","It depends on what the starting point of view is and there is no exact condition for this task in such an uncertain environment. 
                         
                      This requirement seems to be natural as linguistic scales are ordered by the semantics of their terms","In addition, as discussed above, our starting point viewing the semantics of terms comes from the demand of decision-making tasks: Ranking the alternatives in question in accordance with certain criteria based on expert linguistic evaluations",This implies that establishing an appropriate formalized mechanism for comparison of the alternatives becomes highly relevant,"Following this, the semantics of terms based on their inherent order relationships seems to be intrinsic to this research field and, thus, this requirement is necessary. 
                         
                      Since the computational operations on linguistic scales should work on the quantitative semantics of terms and there is a closed relationship between the qualitative semantics and the quantitative semantics of each term, the above requirement is obvious","The inherent qualitative semantics of terms is essential and, in principle, it determines their quantitative semantics","Since, up to now, there is no a mathematical structure, except the structure of hedge algebras, to model the inherent qualitative semantics of terms, we have no formalized mechanism to relate the qualitative semantics of terms with their existing quantitative semantics examined in this field","The lack of this relationship implies the lack of the basic criteria to decide, which linguistic scales are better than the others with respect to this requirement, and the lack of formalized basis to construct appropriate semantic linguistic scales for particular applications. 
                         
                      To show the usefulness of the proposed requirements, in the next section we construct semantic linguistic scales based on the order-based semantics of term. 3 Order-based qualitative and quantitative semantics of linguistic terms As discussed previously, it is observed that linguistic scales are always assumed to be totally ordered using the inherent term meaning that is recognized naturally by a human community","This suggests us to consider this term meaning as qualitative semantics of linguistic terms of a linguistic variable, which can be formulated in terms of an order relation on a term-set under consideration",That is the semantics of a term of a linguistic variable is determined by a collection of ordering relationships between this term and the other ones in a term-domain of the linguistic variable,"Besides, in practice, a linguistic scale is always associated with a numeric scale, to the values of which have semantic relations the terms of the scale",Any value of the numeric scale has to relate to the semantics of the terms of the linguistic scale in some way to a certain degree,"For instance, the quantitative semantic representation of terms appearing in   is a way to specify that the values of the interval   relate more preferably to the semantics of the term “ ” than to the others","We aim to establish a formal basis to relate the qualitative semantics of terms in linguistic scales to the values of the respective associated numeric scales, using appropriate representation of quantitative semantics of terms","There is a mathematical foundation for realizing this purpose, since, as it will be seen, every term-domain of an attribute can be considered as a subset of a hedge algebra under an isomorphic mapping preserving semantic order-based structure  ","A computational semantic representation model of linguistic scales can be developed based on the quantification of hedge algebras with the concepts of fuzziness measure, fuzziness intervals, similarity intervals and numeric quantitative values of linguistic terms  ",We will give a short overview of necessary concepts of hedge algebras and their quantification tasks,"Regarding more details and a formal presentation, the reader can refer to  ",The motivation of the development of hedge algebras is to discover semantic properties of vague linguistic terms by means of order relation that can be considered as the inherent semantic order relation on term-domains of linguistic variables,"This approach to linguistic semantics seems to be, on an abstract level, compatible with the natural term semantics and quite sufficient for human fuzzy decision making tasks","From this viewpoint, the term-domain of a linguistic variable  , denoted by   – the set of all terms of  , can be considered as an order-based structure, the term-set   becomes a poset, denoted by  ","Moreover,   can be viewed as an algebraic structure, denoted by  , where 
                   On this view,   is just the set generated from the primary terms by using hedges acting on them in concatenation, i.e. each term in   can be written in a string of the form  , where   and  ","Then, each term in natural language can be regarded as an element of  ","For example,  , a term in natural language, can be considered as an element of  , which is viewed as a string consisting of two hedges   and   in concatenation and a primary term  ","So, the string representation of the elements of   is identical with the string representation of the term expressions in natural (English) languages and, hence, we call its elements also terms for convenience",Denote by   the set of all terms generated from the terms in   using the hedges in  ,"We will often use in the sequel the notations  , i.e.  , and   being equal  ",Hedge algebras were axiomatized in such a way that they can be regarded as the isomorphic images of the ordered-based structure of certain term-domains of linguistic variables,This requires that the axioms of hedge algebras should suitably be selected from the natural properties of the set   that can be formulated in terms of the inherent semantic ordering relationships between linguistic terms,"In this way, each element of   can be understood as conveying the qualitative semantics of the respective linguistic term of  . 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   3.1 Some descriptions of qualitative term semantics We intend to show that the qualitative semantics of linguistic terms can be formalized based on the inherent semantic order relation and, hence, it is called also the order-based semantics of linguistic terms","Indeed, let us consider an order-based structure   of a linguistic variable  , as described above","The starting point is that since the function of hedges is to intensify linguistic terms, for any hedge   and any term  ,   and   should be comparable, i.e. the (order-based) effect of   acting on   is expressed by the fact that either   or  ","In the case that   or  , for certain   or   (and hence, in reality, for all  , either   or  ), we say that the effect of   is greater than  , and write  ","For example, in this sense, we have  ",That is the semantics of hedges of   can be formulated in terms of the semantic order relation ⩽,"The inherent order-based semantics of terms and the linguistic hedge lead to discovering many new notions and properties of linguistic term formulated in terms of ⩽. 
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      3.1.1 Linguistic terms possessing their own “algebraic” sign We observe that the semantic order-based semantics of terms leads to the fact that terms and hedges have the so-called semantic tendencies that can be recognized as follows: 
                         The meaning of   is expressed as follows: 
                         For example, since  we have  . 3.1.2 Semantic heredity – an essential meaning of linguistic hedges An essential property of hedges is the so-called  , which states that the terms generated from a given term   by using hedges must inherit or contain the (genetic) core meaning of  ",This implies that hedges cannot change the essential meaning of terms expressed in terms of the   ⩽,"Thus, it results in the following: 
                         Assuming that   and  , where  , the artificial hedge identity, and   and  , the hedge heredity leads to the following: 
                         The above listed properties show already that term-domains of linguistic variables with such qualitative semantics of terms possess a rich order-based structure","Therefore, we may observe that hedge algebras are viewed as just formalized structures of the qualitative semantics of term-domains, noting that the meaning of a term represented by its order-based relationships with others in such a formalized structure carries much information than by a fuzzy set itself. 3.1.3 The comparison criterion of hedge algebra  
                         The following criterion for comparing any two terms of   can be proved in these formalized structures: Let   and  ,  , be the canonical representations of any two given terms   and  , respectively","Then, there exists a greatest integer   such that  , for all  , and, moreover, putting  , called the  -suffix of  , we have 
                         3.2 Semantic quantification, fuzziness intervals and fuzziness measure of terms In order to develop a formal basis for quantifying hedge algebras, from now on we always assume that the hedge algebra under consideration is free, i.e.  , for every  ,   and  ","So, each string representation of the algebra,  , is canonical and, hence, it defines a unique term and  ","When we consider a term-set   for a specific application problem,   is assumed to be embedded in  ",That is the meaning of the terms in   should be considered in the context of the whole  ,"In algebraic approach, the quantitative semantics of terms are characterized by several concepts: numeric quantitative semantics, fuzziness measure, fuzziness intervals and similarity intervals of terms",They have close relationships with each other and are completely determined providing fuzziness measure values of the primary terms and hedges,"Numeric semantic quantification of a term   is realized by giving a mapping  , which assigns a numeric value in   to  , where the unit interval   is the normalized domain of the universe of discourse of  ","This mapping has to satisfy the following conditions and is called a   (SQM): 
                      Given an SQM  , denote by   the least sub-interval of   containing the image  , which is closed on the left and open on the right, except the right equal to 1","This definition ensures, by  , that 
                      Since   consists of the terms, which intuitively still contain a core meaning of  , it can be considered as a fuzziness model of   
                         ","Hence, the interval   is called   of   and its length,  , is interpreted to be   of the term  , denoted by  , for every  ","Semantically,   consists of numeric values that are compatible with the semantics of   to a degree indicated by   and, hence, it is called also the  -fuzziness interval of  ","The larger the length of   is, the more compatible the values of   with the semantics of   to a degree   than with the semantics of any other  ","When particular terms associated with these intervals are not necessary to be explicitly mentioned, the intervals in   are called simply fuzziness intervals of a degree   of  ","Assuming the hypothesis that the proportion   does not depend on the particular term  , this quantity is called  , denoted by  ","Thus, in this approach the quantitative notions of fuzziness of terms and hedges,   and  , can be defined formally and we have the following properties: 
                      The fuzziness measure of the terms and the hedges, one of the aforementioned concepts of the quantitative term semantics, can be defined based on a formalization of the order-based semantics of terms","This concept will play a centric role in determining other quantification concepts of the terms: The quantities   (or  ) and  ,  , are the only parameters, called   of  , for calculating the all characteristics of the quantitative semantics of terms","This may be compatible with the intuitive idea that fuzzy information should be characterized by its own fuzziness. 3.3 SQMs induced by fuzziness measure The SQM value of   to be defined comes with a meaning that it is the core value of the fuzziness interval  , which is similar as the core of a fuzzy set","However, in this approach, one can compute the SQM-values of terms, when the fuzziness parameter values are given","To establish a formula for computing an SQM  , for a given fuzziness measure   of  , we look at the inequalities present in   and   and note that   preserves the order relationships between terms","Then, we will see that   should be defined to assume the value lying in between the fuzziness intervals  ,  , and the ones  ,  , and, hence, in between   and  ","Then,  -values can be calculated recursively as follows: 
                      This shows that the concept of SQMs can be defined by the fuzziness concept of terms","The SQM-values of terms represent   of terms, a characteristic of the quantitative semantics of terms, which carry also much information about the meaning of the terms of  , since this approach guarantees that they are most compatible with their respective terms than other values in the universe of discourse. 3.4 
                         -similarity intervals of linguistic terms A similarity relation  , which is an equivalence relation on a universe of discourse, is a fuzzy concept that is useful to manipulate fuzzy data in fuzzy databases, but it has no relation with the inherent order-based semantics of terms","To link this term semantics with this concept, the similarity intervals of terms were defined in such a way that they are built up from fuzziness intervals of  , for some   and a given fuzziness measure   of   
                         ","The problem is formulated as follows: for a given   of  , a   and a finite set of terms  , construct a set of intervals,  , of the normalized reference domain  , called  , that satisfy the following conditions: 
                      The condition (S1) guarantees that the set of  -similarity intervals of the terms in   determines a similarity relation on   in the sense of Buckles and Petry  ","A crucial and important difference here is the semantic property (S2), which states that the numeric values in each equivalence class of this similarity relation, i.e. in each  -similarity interval, are similar to the semantics of a term in   to a degree  ","This problem can be applied not only in the construction of semantic linguistic scales in decision making, but also in many other fields, e.g. in fuzzy classification   and, possibly in fuzzy databases",The idea for solving this problem is as follows,"The fuzziness intervals in   of degree   of the terms of  ,  , which in terms of topology are finer than the fuzziness intervals of degree  , will be utilized to form a neighborhood basis for a topology for our constructing  -similarity intervals","Similarly as generating open sets of a topology,  -similarity interval of every term   of   will be constructed by taking the union of certain fuzziness intervals of  , for a selected  , that lie around the SQM-value  ","Note that every fuzziness interval of degree   comprises the values of  , which are similar with each other with a degree  ","Thus, the values of each resulting interval that is the union of some others can be considered as similar to each other and, hence, to   with a degree lower than  , but not lower than  ","So, they may be considered as being similar to each other with the degree  ","Following this idea, we construct  -similarity intervals of the terms in   by partitioning the set   into clusters  ,  , that satisfy the following conditions: 
                      Such clusters   always exist for  , if   and   2, and for  , if otherwise","Similar as in  , for simplicity, in this study   is selected to be the minimum, which satisfies the above conditions","Then,   is defined to be the set union of the intervals in  ,  ","Clearly, these intervals satisfy conditions (S1) and (S2) and, hence, they are  -  of  ","For simplicity, to construct  -similarity intervals of   we consider in this study only the case that  , e.g.   and  , and, hence,  ","Then, the calculation of the similarity intervals of terms can be established by a simple formula","In fact, since the hedges   and   have different signs, for every term  , one of the terms   and   is smaller than   and the other is greater than  ","Therefore, it can be seen that the term-set of   generated from a primary term   can be represented as a full binary tree: (i)   is assigned to the root of the desired tree. (ii) For every   the smaller term among   and   is assigned to the left child of the node   and the greater one is assigned to the right child of  ",The graph   representing the term-set of   comprises isolated nodes of the constants   and the two trees associated with   arranged from left to right in accordance with their order,"It can be checked that this graph has the following properties: 
                      Given  , for  , let us denote by   and  , respectively the left and the right adjacent terms of   defined in the context of   and, hence,  ,   and   are consecutive in  ","By (ii) of Gr2), there are exactly two terms   and   in   satisfying the inequalities   and the terms in these inequalities are consecutive in  ","Note here that the sub-indexes   and   indicate that the respective term lies, respectively, on the left or the right hand side of  ","Similarly, again by (ii) of Gr2) applied to the set  , there are exactly four terms  ,  ,   and   in   satisfying  ","Since the cluster   is defined based on the fuzziness intervals of  , it follows that  ,  , as these fuzziness intervals have the common end-point  ","By the fact that the SQM   preserves the order of the terms of  , these fuzziness intervals satisfy   (refer to  
                         )","Therefore, we have 
                      
                         
                      4 4-tuple semantic representation of the terms of linguistic scales and 4-tuple semantic linguistic scales In this section, we develop a mathematical model for representing the semantics of linguistic terms and semantic linguistic scale for a given linguistic scale, counted as a qualitative scale",Qualitativeness being meant here is different from the qualitativeness related to the formalization and conceptualization of similarity that emerges from the feature-theoretical approach to the analysis of similarity relations between objects characterized by their features,"Its formalization is based on now-famous model of similarity worked out by Tversky, which is applied in many fields including human judgment and decision-making under uncertainty  ","The qualitativeness discussed in our study is related to linguistic terms, whose qualitative semantics are drawn from the inherent order based semantics of terms that the people of a community can recognize and communicate","Therefore, the terminology “qualitative linguistic scale” is used in the paper",The question is in which way we can represent such a qualitative semantics serving for the study,"The formalization of term-domains allows us to exploit the idea of denotational semantics of linguistic terms, as simple syntax expressions of a language, with which one gives these terms meaning by mapping them into suitable objects of a mathematical structure","This suggests that we first examine a foundation to introduce a linguistic representation of linguistic terms, a 4-tuple representation","Then, develop 4-tuple semantic linguistic scales that should form a mathematical structure, counted as semantic domains of term semantics as well as of the numeric values of the reference domain",Note that the combination of numeric and linguistic information was examined in  ,The 2-tuple approach to multi-criteria decision making (MCDM) problem suggests the study of 3-tuple representation model of linguistic terms in   and the introduction of 4-tuple representation model in this study,The 3-tuple representation model is examined based already on order-based semantics of terms,"However, the given linguistic scales are assumed to be of the form   and, hence, the quantitative semantics of terms is simply their  -similarity intervals and the developed 3-tuple linguistic scales are not closed with respect to aggregation operations","In this study, for a given free linear hedge algebra  , the term-set   of every linguistic scale is assumed to be a subset of  ,  ","With 4-tuple representation model of vague linguistic terms, it is expected that the 4-tuple semantic linguistic to be developed may represent much more semantics of the given linguistic scale and useful to solve MCDM problem, relying upon the qualitative and quantitative semantics of terms presented in Section  ","As a starting point, it can be observed that the order-based structure of the term-set of linguistic scales is very poor to develop computing operations working directly on linguistic terms for solving MCDM problem","In this situation, we have to develop a semantic linguistic scale or, for short, semantic one, associated with the given linguistic scale in such a way that 
                   To construct a semantic scale to meet all these requirements is a difficult problem","Therefore, the study is limited in defining aggregation on the scale based on numeric aggregation operations","The crucial problem for developing semantic scales is to examine 4-tuple representation model, whose components represent in turn a linguistic term, its interval-semantics and numeric semantics and a numeric value for representing also a numeric assessment as well as the numeric computation result related to aggregation operations","In the rest of the section, we develop a strict formalism for constructing 4-tuple semantic scale for a given linguistic one, exploiting the context-dependent semantics of terms in the context of the linguistic scale. 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   4.1 4-tuple semantic representation model of linguistic scales As mentioned previously, the semantics of terms is context-dependent","More exactly, it depends how their adjacent terms appear together with them in a given linguistic scales","Therefore, the semantics of each term in a linguistic scale should be determined in the context of its adjacent terms in the scale","From this viewpoint, we discuss fist about semantic properties that the term-set of a linguistic scales should have","We always assume that the term-set   intuitively should have the properties that 
                      Property (i) and (ii) describe a context-dependent semantics of terms in linguistic assessment reality","In fact, when a teacher, for instance, gives a linguistic assessment value “ ”, he/she should think of the “ ”, the value generates “ ”","Analogously, when he/she gives an assessment value “ ”, he/she should think of the value “ ”, also the value generates “ ”",That is Property (i) is necessary,"And when he/she want to give a very high linguistic assessment value of student ability the best value   should be considered, and so on",That is Property (ii) is also needed,A term-set having both Properties (i) and (ii) is called  ,"For example, the set   is not superior-closed since it contains the terms   and   but not the term  ","However, the term-set   is superior-closed","To construct the interval-semantics of such a superior-closed term-scale  , we assume that the  -similarity intervals of   have been constructed, which form a partition of  ","Thus, we observe that in the case   is a proper subset of  , for  's ∉ , we lost their  -similarity intervals to cover the whole interval  ","Since the interval-semantics of   to be constructed must cover  , the lost intervals can be retaken based on determining those terms among the leaves of   that are superiors to some terms of  , viewed as a graph, whose  -similarity intervals are lost","Then, the lost  -similarity intervals are retaken by adding them to the  -similarity interval of their determined superior term","For instance, the meaning of   appearing in the context of its adjacent terms   and   in   mentioned above is more general than  , which is absent in  ","Therefore, the 2-similarity interval of   that may be lost can be gotten back by calculating the interval-semantics of   in the context of  , based on the fact that   is the leaf of  , which is a superior to  ",This suggests the following way to define the interval-semantics of the terms in  ,"Since the qualitative semantics of a term   in   depends on the presence of its left and right adjacent terms in  , the interval-semantics of  ,  , with  , will be defined based on the determination of its left subinterval   and its right subinterval  ","These subintervals can be defined by utilizing the  -similarity intervals of  , for  ","Since the specificity of terms characterized by the number of the occurrences of hedges in their canonical representation is an important characteristic in this definition, we suppose that   is of specificity of  , i.e. there is a term   such that  ","Given a set of the fuzziness parameter values of  , the  -similarity interval of  , for every  , are completely determined and we denote the left end-point of its similarity interval   by  
                         , where   is an abbreviation of “left end-point”, and put  ","Similarly, the corresponding notations for the right hand side of   are   and  , where   is an abbreviation of “right end-point”","For each  , to define   and   we must determine the left adjacent term   and the right adjacent term   of   in  , utilizing their  -similarity intervals","In this study, we suppose that the sets of hedges   and   are singleton for simplification of the proofs of the next propositions","In addition, this restriction is still sufficient for several application problems   and for the construction of the linguistic scales examined in the study, since for   we have already a term-set with  ","Besides, if necessary, we may extend to consider a larger set  ","First we show some properties of the structure of the superior-closed term-scales  . 
                         
                      
                         
                      For illustration, consider two consecutive terms   and   of the linguistic scale  , whose terms generated from the term “ ” are shown in boldface in  
                         ","So, we have   and  ","It can be verified that (i)  , (ii)   and (iii)  , which shows the validity of (L2)","Before proving successive propositions, we show the validity of the following lemma: 
                         
                      
                         
                      From now on, for every  , the left and the right adjacent terms of   in   are denoted respectively by   and  . 
                         
                      
                         
                      
                          comes with some practical relevance","As on the aforementioned discussion, the interval-semantics of a term   in   should be defined in the context with   and   and the  -similarity interval   of a term   in   is interpreted as an interval-semantics of   defined in the context of  ","Therefore,   provides a basis to determine the interval-semantics of  ,  , in the context of  ","That is it is defined by the equality  , where   and  , remembering that  ,  , can be computationally determined for given   and fuzziness parameter values of  ","For example, let us consider again the term-set   with the specificity 4 represented in   and the term  ",It can be seen that   and  ,"Hence,  ,   and  ","However, for   we can see that   and   and, therefore,  ","This suggests us to introduce the following definition, which is correct based on  . 
                         
                      The interval-semantics of   defined in this way exhibit a desired meaning","Obviously, the numeric quantitative semantics   of   lies in   and the numeric values of   can be regarded as to be similar to the semantics of   (i.e. with its numeric quantitative semantics  ) with a degree indicated by  ","Note that, based on the way of the construction of the similarity intervals of terms, the numeric values in   are similar to   with a degree  ","Meanwhile, the numeric values in   are similar to   with a degree  ","Since it is known that the larger the values   and  , the higher similarity degree of the numeric values of the internal it indicates,   should be defined by min operator","Particularly, if the adjacent terms   and   of   in   are also the adjacent terms of   in  , then  , where  ","However, we have  , in general","For example, in the example given above the degree of similarity of   is  , while the one of   is  . 
                         
                      
                         
                      So, each term   of   can be associated with an interval   which represents an interval-semantics of  ","This and the concept of 2-tuple representation   suggest us to introduce the following general notion of 4-tuple semantics representation model of linguistic terms. 
                         
                      The 4-tuple representation   can be considered as an extension of the practical linguistic representation of terms given in   that one has used for student assessment","In general, this 4-tuple semantic representation model of linguistic terms represent a compound relationship, which relates the quantitative semantics with the qualitative semantics of terms","The meaning of the components of   is that   carries a qualitative meaning, which is determined in a semantic order-based term-set",The second component represents an interval-semantics of  ,"The third represents its numeric quantitative semantics of  , considered as to be the value most compatible with   among the values of the interval-semantics of   or   of this interval-semantics","The fourth is devoted to represent a numeric assessment, which represents a user numeric assessment considered as his approximation of the semantics of  ",It is a way to represent the relation between a numeric with a linguistic value,The numeric value in the fourth component enables to represent by the tuple a semantic relation of this value with the term  ,"Similarly as in the case of 2-tuple linguistic representation  , the quantity   reflects how large difference of a numeric assessment from the core of the interval-semantics of  ",That is the 4-tuple semantic representation given in   may bring useful information,The concept of 4-tuple semantic representations of terms permits a possibility to unify the ordinary numeric scales associated with the linguistic ones,This may be a potential demand of many practical applications,"For example, when we are required to deal with a data warehouse of historical project bid data, the one part of which may include   given by certain experts for a certain criterion, while the other part of which may include   for the same criterion given by other experts","It seems to be practical to consider that in the case of a social investigation on social networks, for flexibility, we may design two parallel numeric and linguistic scales to permit people to express their opinion by choosing numeric or linguistic assessments","In the case someone chooses a linguistic assessment, say  , his opinion is automatically represented in the system as the 4-tuple  , since   and   have been declared in advance","In the case someone chooses a numeric assessment, say  , where   is a normalized interval of the numeric assessments, we will show below that this opinion will be automatically represented uniquely as a 4-tuple  , called 4-tuple linguistic representation of   (see  ). 4.2 4-tuple semantic linguistic scales and their computation features In this subsection we show that semantic linguistic scales using the above 4-tuple semantic representation of terms will have a rich functionality","In this study, we distinguish between linguistic scales, whose carry their own order-based semantics the experts use to express their assessments, and their respective semantic linguistic scales expressed in terms of 4-tuple semantic representation model, whose 4-tuples form an underlying set of a rich computational structure","Since the terms in linguistic scales are elements of a hedge algebra associated with the linguistic variable in question, we introduce the following definition, noticing that a term   is more specific than a term   if  , i.e. the number of hedge occurrences in   is greater than the one in  ","Thus, the length of terms characterizes their specificity level. 
                         
                      Note that since   is free and hence   is infinitely countable","Therefore, the assumption   means that the qualitative semantics of its terms should be considered in the context of   and, therefore, it is requires that their 4-tuples should be computed in the context of the whole algebra  , as examined in Section  ","It can be observed that the condition (i) is a natural requirement for the notion of the specificity level. (ii) is necessary, since they are elementary vague concepts of a linguistic variable to determine other terms and their semantics dependent on the context in which are present the elementary vague concepts","For example, one cannot give the assessment “ ” if the term “ ” is absent in  . (iii) is required as the previous discussion about the superior-closed notion in the beginning of Section  ","As   is not a computational structure, a semantic linguistic scale with necessary computational features in company with   will be introduced based on the 4-tuple semantic representation model of terms, called 4-tuple semantic linguistic scale. 
                         
                      The above concept of 4-tuple representation of linguistic terms has some significant advantages","It is well-known that words are objects of computation in CWW and, as mentioned by Lawry in  , a fundamental aim of CWW is to enable representation of fuzzy linguistic information","So, it is important to develop a mathematical model to represent the semantics of linguistic terms as proper as possible","As the underlying term-sets of hedge algebras are regarded as term-domains of linguistic variables and note that the string representation of elements of hedge algebras is the same as linguistic expressions of a natural (English) language, the linguistic terms in   can be counted as just elements of a hedge algebra","Thus, we can understand that every linguistic term in  , as a syntax expression, is mapped into itself counted as its denotational semantics, which is an element of a hedge algebra, a mathematical object","So, each term   present in a 4-tuple has an explicit semantics and expresses an order-based semantics defined in the whole free hedge algebra   associated with  ","This viewpoint makes sense as according to the development of the 4-tuple representation model presented in Section   every term   with semantics defined in the context of   determines its interval semantics  , calculated based on the whole structure of  , and produces the value  , counted as its numeric semantics","This allows to exploit the specificity versus the generality of terms caused by the use of hedges, which implies for instance that if   is of high specificity, i.e.   is large, the length of   is small","So, the proposed 4-tuple representation model represents a strict connection between the qualitative semantics of the terms in the given scale and their quantitative semantics expressed by their interval and numeric semantics","In addition, as we will show next, it enable to unify a linguistic scale with its numeric reference scale to become a unified  SLS for the user to give his assessments and every  SLS forms a mathematical structure closed with respect to the proposed aggregation operators as required by the denotational semantics of a (formal) language",Dealing with words is a challenging problem and it seems that any linguistic representation mathematical model proposed for an approach to represent the semantics of words will impose a constraint on the approach,"Therefore, the 4-tuple representation has also certain limitation that will be discussed in the conclusion section","The following proposition shows the existence of a 4-tuple semantic representation for any given numeric assessment and describes the relationship between numeric assessments and linguistic assessments utilizing the scale  . 
                         
                      
                         
                      Note again that   and   have been examined for the general case, in which one does not know in which way the interval semantics and the numeric semantics of the terms in   are declared","In our approach, we can prove the following statement","First, note that, for given fuzziness parameter values of  ,   is completely determined, where   is defined as in   and the SQM   is induced by the fuzziness measure defined by the given fuzziness parameter values. 
                         
                      
                         
                      Before the examination of the computational structure of 4-tuple semantic linguistic scales, we consider the following example. 
                         
                      4.3 The computational structure of 4-tuple semantic linguistic scales We show that the proposed semantic linguistic scales exhibit a desired computational structure","For simplicity, the indexes of interval-semantics of terms indicating its similarity degree will be ignored, if they are not necessary to be declared explicitly. 
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                      4.3.1 Order relation on 4-tuple linguistic scales 
                            
                         It can easily be seen that from the inequalities   and   it follows that  , i.e. all their components are respectively identical","Moreover, the linearity of the sets   and   implies the linearity of the 4-tuple semantic linguistic scales","As an immediate consequence of the above definition we have: 
                            
                         4.3.2 Aggregation on 4-tuple linguistic scales One of the main aims of the study is to construct a semantic linguistic scale on which we can realize easily necessary operations to aggregate linguistic as well as numeric assessments of experts properly","To realize this, we introduce the following definition. 
                            
                         This definition is very similar to the aggregations used in many practical decision making applications","For instance, in evaluation of students or of bids for project contract, experts may evaluate them with respect to different criteria using numeric scales and then aggregate their assessments","The objects under evaluation will be classified into groups labeled by linguistic terms, whose semantic intervals contain the aggregation result of the numeric assessments of properties of the respective objects","However, with 4-tuple semantic linguistic scales experts may express their assessments by either numeric values or linguistic values present in the scales. 
                            
                         
                            
                         Now, we are ready to show that the proposed 4-tuple semantic linguistic scales meet well all three requirements discussed in Section  . 
                         5 A comparative study using group multi-criteria decision making The main aim of the study is to introduce an approach to construct the so-called sound semantic linguistic scales, in which the qualitative semantics of terms of a linguistic variable induces and determines the quantitative semantics of the linguistic scale terms and their 4-tuple representation",This characteristic distinguishes this approach from the existing ones in establishing linguistic representation models of terms,"Consequently, we may expect that the decision results depend mainly on the expert linguistic assessments, i.e. mainly on the qualitative semantics of terms","To show this, in this section, we consider a simple multi-criteria decision problem and demonstrate that, for given two linguistic scales with different cardinalities, the decision results based on the   assessments expressed by the terms present in the both linguistic scales given by an expert are identical when we use their corresponding 4-tuple semantic linguistic scales","However, it is not the case when the terms of these linguistic scales are represented based on the 2-tuple model proposed in  ","A method for solving group multi-criteria decision making problems comprises two main tasks: (i) collecting assessments of experts with respect to criteria for the alternatives in question, utilizing the given linguistic scale, and (ii) aggregating the collected expert assessments","For simplicity, we will consider multi-criteria decision making problems with a unique expert, since in the case with a group of experts it is required to implement an additional aggregation scheme","Let us consider a decision making problem with two alternatives   and   and three criteria  ,  ","For simplicity, we assume that the expert use the same linguistic scale to express the assessments of her/his evaluation of all the alternatives under consideration with respect to these distinct criteria","In addition, to make a clearly visible difference of the proposed approach from the 2-tuple based approach, two linguistic scales, the one is a proper subset of the other, that will be applied in turn are given as follows:  where,   
                      ,   ≜  ","Here, some explanation of the semantics of terms when   with 9 linguistic terms is reduced to its subset   with 5 linguistic terms of the same linguistic variable, say  , is needed","Since in our approach the linguistic terms of   convey the inherent qualitative semantics, every term of   has its own meaning in the context defined by  ","So, we believe that when an expert expresses his linguistic assessment, say the term  , he/she does mainly think of the meaning of   in the context of  ","Therefore, when, for example, a teacher gives his assessment “ ”, which is present in both   and  , of the mathematics ability of a student in the context defined by  , he will keep his assessment, “ ”, in the context of  ","Keeping this situation in mind, the linguistic assessments of the two alternatives in question of the expert as shown in  
                       can be considered as his assessments in the context of each of the two scales   and  ",Note that the weights of the criteria are also given in the table assuming that the selected aggregation operation is the weighted average,"Then, the 4-tuple semantic linguistic scale associated with   constructed by the method as described in Section  , using the same fuzziness parameter values given in  , is exhibited as follows: 
                   Since   is a proper subset of   and they are of the same linguistic variable, we have to assume that the hedge algebras for representing the terms in the two given linguistic scales and the fuzziness parameter values of the linguistic variable for constructing the 4-tuple semantic linguistic scales are the same","With these assumptions, the 4-tuple semantic linguistic scale associated with   can be constructed and it consists of the following 4-tuples: 
                   The above assumptions and formula   ensure that the fuzziness intervals used to compute the interval semantics of the terms of both   and   are the same","So, their numeric semantics in both cases are the same and their interval semantics in   includes or, in accordance with their qualitative semantics, is of more generality than the respective one in  ","Therefore, the linguistic assessments of the expert in the context of either   or  , with their quantitative semantics represented in the respective  SLS, can be regarded as being almost the same assessments of himself. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   5.1 Case 1 – the use of scale  
                      Now, we are ready to compute the results of aggregation for both approaches and their aggregation results are represented in  
                         ","For the 2-tuple approach, the aggregation result of   and   are represented by ( , 0.48) and ( , −0.45), respectively, and, therefore,   is the best alternative","For the 4-tuple approach, the first alternative   is preferable over  , as  , where the first 4-tuple is the aggregation result of   and the second one is the aggregation result of  ","Note that since his assessments are linguistic, the fourth components of the 4-tuples representing his assessments are identical with the third components, i.e. the SQM-values of his linguistic assessments","So, the two approaches produce different preferable alternatives","Note that, using the  SLS, the aggregation utilizes operations on the numeric semantics (i.e. the SQM-values) of the terms, which is determined in the context of the whole linguistic variable","While, using 2-tuple linguistic representation model, the aggregation utilizes operations on the indexes of their position in the linguistic scales that are clearly dependent on the cardinality of the scale. 5.2 Case 2 using the scale  
                      As discussed above, the expert's linguistic assessments given in   that are included in   can be considered as his linguistic assessments in the context of each of the scales   and  , although their semantics may be changed a bit by the influence of a possible change in their left and right adjacent terms in each scale","Now, we show that while his decision using the 4-tuple semantic linguistic scale remains the same as it is determined in the case of  , it will be changed when linguistic 2-tuples are applied","In fact, since in the 4-tuple approach the SQM-values depend only on the linguistic variable   and its fuzziness parameter values provided, the SQM-values of the expert's linguistic assessments are identical with those examined in Case 1","Therefore, the results of the expert's assessment aggregation produced by the numeric aggregation operation in question, which acts on the SQM-values of the respectively terms appearing in the forth components of the 4-tuples of the  SLS of  , must be the same as those computed in Case 1","Only the interval-semantics of the terms in   here are changed a bit as some terms of   are absent in  , as discussed above, and they include the interval-semantics of the respective terms in  ","Consequently, the interval-semantics of these terms in   is more specific than the respective one of  ","For instance, the interval-semantics   of   in the context of  , whose semantics is more specific than the semantics of the very term in the context of  , is included in  , the interval-semantics of   in  ","Hence, for the linguistic scale  , the 4-tuple representing the aggregation results of the alternatives   and   are given respectively as follows:   and  , noting that their fourth components are, respectively, the same as those calculated in Case 1","So,   is still more preferable than  ","However, in the 2-tuple approach the aggregation results of these alternatives are given in  
                         , which imply that   is more preferable over  , a solution result different from the one obtained in Case 1. 6 Conclusions The semantics of vague linguistic terms is a complicated issue","Although in CWW we have witnessed a significant progress, there are, in our opinion, still some significant open questions","For instance, while we emphasize the role of linguistic terms or expressions and their meaning's representation in CWW, explicit relationships between the qualitative semantics of linguistic terms, like the order-based semantics, that human being can observe, and the mathematical models that one uses to represent the meaning of linguistic terms are still not deeply discussed and formalized","One reason for this is that term-domains of linguistic variables are not considered as mathematical structure, based on it one may develop a formal linkage of expected meaning's representation model with the qualitative semantics of linguistic terms drawn from this mathematical structure","In the literature on fuzzy decision-making, there is a lack of investigations on the inherent qualitative semantics of vague linguistic terms expressed in the linguistic scales",The semantics of terms should be able to serve comparison tasks of alternatives in order to help decision makers to choose the best ones,"Thus, the meaning of terms in a term-domain aims to express semantic order based relationships between them","Note that to express expert opinions, the experts try to focus to choose appropriate terms of the scale based on their semantic order based relationships, i.e. based on their qualitative semantics",This shows that the qualitative semantics of terms should play a pivotal role in the development of sound semantic linguistic scales,"So, a sound formalized qualitative semantics of linguistic terms will increase the interpretability of a meaning's representation model","In this study, we have proposed to interpret the inherent order-based semantics of terms of a linguistic variable as qualitative semantics of linguistic terms that are directly associated with the term string expressions regarded as their syntax","The quantitative semantics of terms serves as fundamental basis for computation on words, but it should represent the term qualitative semantics in a certain formal way","Therefore, it becomes necessary to establish a formal bridge to connect the quantitative semantics of terms with their qualitative semantics, i.e., their inherent order-based semantics","However, up to now, the examination of the semantics of linguistic terms has not been carefully taken into consideration and the qualitative semantics of terms still have not been explicitly declared and, hence, there exists a visible gap between the qualitative semantics of terms and their quantitative semantics in the literature","In light of this complex situation, we articulated some requirements of general nature for construction of semantic linguistic scales for decision-making problems",These requirements are proposed based on the analysis of the actual relationships between the qualitative semantics and the quantitative semantics of vague linguistic terms and of the relationship between linguistic scales the experts use to express their linguistic assessments and their associated computational semantic linguistic scales considered as domain of desired operations for solving decision-making problems,These questions may be crucial to the development of a sound and legitimate semantic linguistic scale associated with the given linguistic scale of a linguistic variable,"We demonstrated that hedge algebras, which aim to model qualitative semantics of terms, can be applied to solve these questions","By this, the original 2-tuple linguistic representation model   can be generalized to become the 4-tuple linguistic representation one","Based on this the 4-tuple (quantitatively) semantic linguistic scale has been developed, which is able to meet all the three proposed requirements for the construction of legitimate semantic linguistic scales discussed in Section  ","The important thing has been shown that a strict mathematical formalism has been established to construct a 4-tuple semantic linguistic scale, utilizing the quantitative semantic aspects of terms, based on the proposed formalized qualitative semantics of terms and the quantification of hedge algebras","It is natural and practical that the qualitative semantics of the terms in a linguistic scale is viewed as being context-dependent, i.e. the semantics of a term in the scale depends on which are its adjacent terms in the given scale, taking the generality and the specificity of terms, caused by the use of hedges, and their fuzziness into consideration",It should be emphasized that the hedge-algebra-based qualitative semantics of terms provides a formalism to solve this problem,"Hence, these 4-tuple semantic linguistic scales can be computationally determined by specifying the fuzziness parameter values of the linguistic variable under consideration",It is a basis to construct an optimal semantic linguistic scale suitable for a specific application,"This demonstrates that it is possible to establish a formal bridge to link the quantitative semantics of terms in the linguistic scales to their qualitative semantics determined in the context of the respective linguistic scales, based on hedge algebra structures",We showed also a number of their immediate advantages as discussed in Section   and Section  ,"In general, the proposed methodology for construction 4-tuple semantic linguistic scales can be considered as being characterized by several distinct aspects of the semantics of terms and is based on a strict mathematical formalism","These semantic linguistic scales can be viewed as a generalization of the respective numeric scales and, therefore, this concept becomes natural and easily understood","They are completely determined based on the fuzziness intervals of terms, when the user provides numeric values of the fuzziness parameters of linguistic variables and, hence, they can be produced computationally","Therefore, to construct an efficient semantic linguistic scale for a decision-making problem, the user can concentrate on the selection of linguistic hedges for the generation of suitable linguistics scales and the determination of appropriate values of the fuzziness parameters, including by optimizing these parameters",All of these observations point out that the semantic linguistic scales are very useful in addressing a variety of problems of decision making,"In fact, it is important for application that the semantics of 4-tuple semantic linguistic scales is compatible with the term semantics the expert uses in reality, since they are developed based on the inherent context dependent qualitative semantics of terms","Further, the aggregation operations induced by the corresponding numeric aggregations can work in a simple way on the developed 4-tuple semantic linguistic scales, as they are closed with respect to these aggregations",An additional linguistic approximation is not required,"Since these scales can be considered as an immediate generalization of the respective numeric scales, in further applications one may allow the experts to express their assessments in terms of linguistic terms as well as in terms of numeric values in their respective scales",A comparative study examined by an example of a multi-criteria decision making problem (Section  ) illustrated these advantages in a compelling way,To conclude this section we state that the main advantage of the proposed 4-tuple semantic linguistic scales is that they are determined based on the formalized qualitative semantics of terms,"However, they have also some disadvantages, as every (exact) mathematical meaning's representation model is a constraint on the way we capture the term meaning and manipulate computationally fuzzy linguistic terms","In addition, the use of mathematical object to represent fuzzy information simpler than fuzzy sets to avoid computational complexity in handling fuzzy information will cause certain lossless information","The interval semantics of a term  ,  , and its numeric semantics   appearing in the second and third component of the 4-tuple of  ,  , are very simple mathematical objects, and they are exactly calculated based on the qualitative semantics of the terms of the given scale  ","The interval   means that all its numeric values are compatible to the meaning of   with   indicated by  , which denotes the length of the string  ","Therefore, they cause certainly certain lossless information, which is the cost to pay the exactness of these data determined in the approach, according to the third principal rationale stated by Zadeh saying that “precision carries a cost”  ","Analogously, it seems that the simplicity in handling linguistic information carries also cost","The requirement that the semantic linguistic scales should be closed with respect to aggregation operators is very strict, otherwise a linguistic approximation method is needed","However, when we interpret the desirable aggregation operator as to include the selected definite linguistic approximation operator (algorithm) for a semantic linguistic scale, this requirement is natural and necessary","Thus, the definition of the proposed aggregation operators induced by the corresponding numeric aggregation operators handling fuzzy linguistic information in a simple way is also another limitation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0888613x14000607, 1 Introduction The paper   describes an appealing method for learning models from imprecise data that improves extension principle-based approaches,The empirical loss of a model on interval-valued datasets is defined as the lowest loss over all the possible crisp instantiations (selections) of the uncertain items in the training data,The loss function of a model with fuzzy data is defined as an average over the different level cuts of the data,"The model with a best empirical loss is searched for, thus a minimin criterion is adopted",It is shown that this strategy is related to the optimization of certain loss functions used in machine learning,"This discussion focuses in on three particular aspects of the paper where further developments may be possible: extension principle-based models, the use of the aforementioned minimin criterion and possible links between data disambiguation and data imputation. 2 Application of the extension principle in the context of learning from data In this section, a nuanced discussion of Prof",Hüllermeier's comments about the use of the extension principle in the context of machine learning is given,We agree that a direct generalization of standard machine learning techniques to imprecise data via the extension principle will be troublesome in practice,"However, if a few minor modifications are introduced, this kind of generalizations can still be useful","Many uncertain datasets contain outliers, understood as imprecise items that do not fulfill the assumptions of the model for any of their possible instantiations (selections)","In this case, all crisp selections of the dataset may in turn contain outliers","Apart from that, if some items in the imprecise dataset had an abnormally high uncertainty, artificial outliers might also be made up for certain instantiations","This might pose a problem with universal approximators, that might try to learn these extreme values",A learning algorithm which optimizes a regularized loss function could be used to prevent this,"Alternatively, one might also conceive a definition of an empirical loss where some of the training instances that are furthest from the model are ignored (see  
                      )","A different model is fitted to each selection, thus these items that are discarded will not belong to the same imprecise instances","As a consequence of this, such an empirical loss function cannot always be computed by removing the outliers of the dataset and then applying the extension principle, neither it is equivalent to the application of the extension principle to a regularized loss function","This idea was introduced in Refs.   and  , where models for precise and imprecise data were proposed that do not produce a point estimate but a crisp or fuzzy set that approximates the union of all the outputs of the models learned from these partial instantiations","In this respect, while we agree that the sentence “We argue, however, that the application of the extension principle is not very meaningful in the context of learning from data” is mostly true, we also think there may be cases where a worthwhile application of the extension principle is possible","Let us remark that the concept of instantiation was not referred to as such in these works, however in the following of this section an effort has been made to cast the ideas introduced in   and   into the theoretical framework introduced in the paper being discussed","As mentioned, algorithms   and   produce crisp or fuzzy families of models, respectively determined by crisp or fuzzy subsets of the parametric space","We will denote them by   or  , respectively",The fuzzy case is the most general of these two and it is summarized here,"Given a crisp input   and a fuzzy subset of the parameter space  , the combined output of the model family   is a fuzzy set with membership  and given a fuzzy input  ,  Let ALG be a learning algorithm that builds a model   given a crisp dataset  , with  ","The model   is defined in turn by an estimation   of the value of the parameter:  Given a fuzzy dataset  , with  , a fuzzy estimate   of   could be thought of that only involves complete instantiations,  where  ",This kind of estimation we agree with Prof,Hüllermeier that it is not a good approach because more often than not we would end up with a set of predictions which is highly nonspecific,"On the contrary, an estimation restricted to certain subsets of the instantiations may be useful","In short,   will be determined as the set that minimizes a functional of the parametric model  , which is not derived from an extension principle-based generalization of the crisp loss","A set   is searched for such that the average nonspecificity of the (extension principle-based) fuzzy output of the model   is minimum, constrained to the fraction of covered instances being higher than a threshold","The fraction of covered instances is not precisely known but a fuzzy set  , defined by its  -cuts,  serves as an approximation","In  
                       (taken from  ) a graphical example of this learning algorithm is given","For simplicity, crisp data is used thus   collapses to a crisp number and the set of parameters   is also a crisp subset of the parameter space","The envelope of the set of outputs of the parametric model family   is plotted along with the results of applying genetic programming, neural networks and fuzzy rule-based systems to the same data, showing that the here proposed extension principle-based model over partial instantiations is resilient to outliers while other (non-regularized) universal approximators are not",A model based on the estimation   (see Eq.  ) is not drawn in this figure but it would resemble that of the fuzzy rule-based system,"To sum up, we suggest that there are certain extension principle-based models that make sense because they are robust under the presence of outliers",These models are not the result of launching a learning algorithm on any crisp selection of the imprecise dataset,"Also, sometimes a confidence interval of predictions is preferred to a punctual estimate (that can be produced nonetheless by this technique if the fuzzy subset of the parameter space is replaced by its modal point)",Links might also exist between these models and  -insensitive loss functions/regularized/support vector regression. 3 Combination of model induction and data disambiguation The idea of a fuzzy loss function is intriguing,"However, the definition of any ranking between fuzzy losses may be arbitrary to a certain degree",In other works   a partial preorder was suggested instead,"In addition to this, rankings could also be defined between interval or fuzzy losses that are different than the minimin approach proposed in the work under discussion  ",The reasons why we think that some rankings exist that may improve on the minimin criteria are two,"On the one hand, different models seemingly share the same minimum empirical loss",A ranking that takes into account the whole set of losses could make for a finer distinction,"On the other hand, some cases exist where the lower bound of the loss of a model may not produce the best disambiguation",Let us describe this with the help of a simple example,"Let  , and let the model class   comprise linear models of the form   (see  
                      )","The set-valued dataset   is considered, where the uncertainty is in the output variable",This dataset was built assuming that the true model is   with an added uncertainty of ±1 units in the output variable,"All models of the form   with   have the same minimum loss in this dataset, thus the data could not be completely disambiguated; the most specific disambiguation would be another interval-valued dataset  . 
                      
                   Furthermore, while all these models are identical according to the best case, their empirical loss in the worst case (the minimax criterion) is different",We realize that the minimax criterion is as arbitrary as the minimin and datasets can be found where the minimin is preferred,"However, we suggest that many rankings exist that take into account the whole set of empirical losses and exploit better the available information in uncertain datasets","For instance, according to our own experience the strong domination defined in   is a good compromise for many practical problems involving interval-valued data. 4 Disambiguation and data imputation We suggest that the concept of disambiguation introduced in this paper may be connected to multiple data imputation  ",Data imputation is primarily designed for problems with missing data,"However, one can think of censored, interval-valued or fuzzy datasets as of having partially missing instances","The imputation of censored and doubly censored or interval-valued data has been discussed elsewhere  , although up to our knowledge methods for fuzzy data imputation have not been proposed yet","Nonetheless, disambiguation can be used to simultaneously remove the uncertainty in both input and output data, thus it may be preferred to the double task of imputing first and then evaluating the model over the imputed data",The author may also benefit from the discussions about the differences between the analyst model and the imputation model,The set of properties of the noise and about the joint distribution of the input–output data that are assumed in the imputation stage use to be different than those implicit in the model being learned  ,The current definition of disambiguation may be related to methods where the same set of assumptions for both the analyst and the imputation model are made; perhaps the interdependence between the assumptions that need to be made when removing the uncertainty in input and output variables might be explored in future works.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s1071581919300552," 1 Introduction The concepts of automation, and mechanized and automated work have been around for decades","According to the Britannica encyclopedia, automation is “  ( )",The above definition of automation does not involve the requirement of a computer processor,"However, many modern forms of automated (or sometimes: autonomous) machines, such as power plant monitoring devices, automated cars, drones, robots, and chatbots, do involve computers","These computer-automated systems are used by humans, and humans are expected to remain essential contributors to artificial systems and automated systems in the future ( )","The study of human-computer interaction, or more specifically human-automation interaction, therefore continues to remain relevant as automated systems are used to support more and more everyday activities, overseen by non-technical and non-professional end-users","In this special issue to celebrate the 50th anniversary of the International Journal of Human-Computer Studies, and its predecessor the International Journal of Man-Machine Studies (from now on collectively referred to as IJHCS), we review the contributions that IJHCS has made towards the study of human-automation interaction",We therefore analyze published work from the journal to distill historic trends,"Our analysis shows that human-automation interaction is a field that keeps expanding into new domains and contexts (what we refer to as “breadth”), and also keeps improving its performance within domains and contexts (what we refer to as “depth”)","Given these expansions, and the exposure to more contexts and to a wider and more diverse group of end-users, there is a potential for the broader human-computer interaction community to contribute skills and knowledge to create and evaluate safe, engaging, and productive automated systems","We close our analysis by discussing eight trends that we deem of particular relevance for this community, classified in two segments","First, we discuss trends that have been around for a while but continue to remain important: (1) function and task allocation between humans and machines, (2) trust, incorrect use, and confusion, and (3) the balance between focus, divided attention and attention management","Then, we discuss emerging themes: (4) the need for interdisciplinary approaches to cover breadth and depth, (5) regulation and explainability, (6) ethical and social dilemmas, (7) allowing a human and humane experience, and (8) radically different human-automation interaction. 2 History of human-automation interaction To gain an overview of the number of articles that were published on the topic of human-automation interaction in IJHCS over its 50 year existence, we conducted a Scopus search on January 14th 2019","We collected all articles that had the word “automation”, “automated”, or “autonomous” in either the title, abstract, or keywords.  
                       reports the number of articles that matched the search query per topic and decade, together with the total number of articles that was published in IJHCS that decade","The topic of automation covers a substantial subset of the published work in IJHCS: 4–11% of published articles in each decade, with around 5–6% of the articles in the last two decades","These percentages should be interpreted as approximate values, as the count is limited by the keywords that authors used in their paper's title, abstract and keywords section","There might be false alarms (papers that were returned based on keywords, but that did not directly address research on human-automation interaction) and misses (papers that are relevant for the field of human-automation interaction, but did not include these specific keywords)","To gain a richer understanding of the themes that are discussed in IJHCS papers on human-automation interaction, our initial keyword search was followed by a qualitative analysis","For this analysis, we sorted the IJHCS papers on human-automation interaction by year of publication",We then read the titles and abstracts of these papers to pick up common themes per decade,"This revealed four themes which align well with more general trends in artificial intelligence (e.g.,  , chapter 1) and human-computer interaction (e.g.,  )","However, as the analysis method is subjective in nature, and limited by the papers that were published in IJHCS, we do not claim that we have identified all strands of human-automation interaction research that occurred over the last five decades","We do claim that we identified relevant themes, which are discussed in more detail next. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   2.1 Start: automation for dedicated domains Publications on automation in IJHCS largely started off with the study of dedicated, domain specific systems","In the 1970s and 1980s a large proportion of published work (around 25 papers) focused specifically on the development and evaluation of automated psychological tests (for overview papers, see e.g.  )","The widespread introduction of computers allowed psychology researchers to conduct interactive tasks on computers, instead of just pen-and-paper tests or subjective assessment","Nowadays, digital testing is common in experimental studies involving human participants, and has given rise to opportunities for conducting large-scale studies using crowdsourcing platforms, like Amazon's Mechanical Turk (see   for a review)","Given the rise and ubiquity of personal computing devices, the idea of completing an online survey would now hardly qualify as an example of “automation” anymore",A second dedicated domain in which automation was researched is knowledge acquisition ( ),"As reviewed in a previous IJHCS special issue ( ), one of the main aims within this domain in the 1980s was to be able to develop methods to ‘extract’ knowledge from experts that can be represented in machines","Among our dataset of papers on automation, the top-cited papers from the 1980s all proposed methods for knowledge elicitation (e.g.,  )",Since the 1980s there has been a general shift in perspective that successful knowledge acquisition and knowledge engineering requires more than extracting knowledge,"Considerations of systems engineering and allowing smart inferences based on multiple sources (e.g., through the internet) are now seen to be key, with modern day knowledge acquisition research taking on a broad and multi-disciplinary perspective (see also  ). 2.2 Time-sensitive and safety-critical settings Throughout the last five decades of IJHCS, automation research has branched out into more domains and settings","One distinct class of research is on tasks that are time-sensitive (i.e., require a response within a finite, short time interval) and/or safety-critical (i.e., where an incorrect action can have disastrous consequences)","Work in this area has been published in every decade, but particularly in the 1990s and early 2000s","The range of settings in which time-sensitive and safety-critical tasks have been studied is diverse and varied: from monitoring dynamic processes in factories (e.g.,  ), power plants (e.g.,  ), and other professional settings (e.g.,  ), to flight monitoring (e.g.,  ), and semi-automated driving (e.g.,  )","The diversity of domains (and the importance of preventing incidents) has allowed an exploration of deep general topics throughout the history of IJHCS, which remain relevant for today's research","They include topics such as how to distribute or allocate tasks between humans and machines ( ), finding the right levels of workload to avoid under- and overload ( ), how to promote appropriate levels of trust in automation ( ), and how to avoid incorrect use and (human) errors such as through complacency ( ) or (human) biases ( )","We will return to the current status of these topics in more detail in our section on the future of human-automation interaction. 2.3 Embodied, situated agents Since the 1990s there has been a gradual shift away from static systems for specific domains (e.g., expert systems, systems for psychological testing) to systems that involve a dynamic intelligent agent that performs a task (e.g.,  )","This continues in the 2000s, with a rise of papers on automated systems that act in a dynamic, physical world","This parallels the popularization in Artificial Intelligence (AI) research of embodied, situated agents ( ): systems that have their own sensors and that depend on interaction with the environment for performance","For example, in the 2000s IJHCS published various studies on physical robots (e.g.,  ) and cars ( )","In parallel, there is also research published on affective interaction with robots, and automated (emotion) feature detection (e.g.,  )","These topics continue in the 2010s, but also broaden out to include, for example, research on human-robot interaction with multiple robots ( )","The relevance of considering embodied and situated robotics and automation explicitly is that the actions of embodied, situated systems (at least in part) depend on how the world is perceived through the machine's sensors, and through the environment in which the machine interacts ( )",Different machines can (learn to) act differently if either their sensors have different capabilities or if they are trained in different kinds of environments,"Generalization to unknown settings, and adaptation to new settings, requires extensive training for these embodied, situated robots","Automated vehicles are an example of an embodied, situated robot that acts in and adapts to unknown settings","For automated vehicles, training typically consists of a combination of extensive experience under real-world driving conditions, as well as extensive simulated training sessions to learn how to act in other potential worlds ( )","By contrast, earlier simpler automated systems, such as, closed-world factory systems, or virtual systems such as a digital psychological test or expert system, require relatively less extensive testing due to their reliance on the assumptions of a closed world. 2.4 Rise of the non-professional users As chips get smaller and gain more capacity, smart and automated technology is becoming more widely available for use by non-professional users",These users have often not been trained in how to use or operate the system and often do not have a detailed technical understanding of how the automation works and the limitations on its successful operation,The last trend that we observe is then that there has been an increase in research on automation for use outside of professional settings,"For example, the availability of smart phones and other smart devices that are connected to the internet and allow users to interact with automated systems and processes","Some examples that are covered in IJHCS include electronic shopping (e.g.,  ), robots as social companions (e.g.,  ), and control of semi-automated vehicles (e.g.,  )","While many of the topics that apply to professional (skilled) users of automated systems also apply to non-professional users, there are some additional considerations that come into play for research on how non-professional users interact with automated systems","For example, for non-professional users one cannot rely on extensive training and experience with the technology, and the technology might be used in a wider set of context than that which can be predicted by the profession","Study of use by non-professional users is therefore an emerging setting, discussed in more detail below that requires the full breadth of HCI expertise","Moreover, the use by non-professional users requires further consideration of more ethical topics such as human attitudes towards and acceptance of autonomous systems ( ) and how to handle security and hacking ( ). 2.5 Summary of human-automation interaction research to date In summary, our analysis of publications in IJHCS on the topic of human-automation interaction shows that research has expanded beyond the use of automation in dedicated domains such as factory assembly lines and automated psychological tests","In particular, there are distinct research lines that investigate the use of automation in time-sensitive or safety-critical settings, through embodied situated agents, and by non-professional users. 
                         
                          provides a Venn diagram with examples of automated systems for each of these research lines",The Venn diagram also makes explicit how these different areas fit together,"Specifically, it identifies that there are many domains and settings in which two or more of these research lines come together","A prime example is the automated car, which involves automation in the form of an embodied, situated agent, which is used by non-professional users in a time-sensitive, safety-critical context","For embodied, situated systems some form of automation (or autonomy) is almost always required (although by definition, humans can also be considered embodied situated agents,  )","Hence in our Venn Diagram of  , embodied, situated agents are represented as a subset of the larger automation category","Moreover, whether something is considered embodied and situated might at times be open to interpretation","For example, we opted that a power plant monitoring system is not labeled as embodied and situated, even though such systems can sense and act to maintain a balance in the power plant's processes (e.g., increase or decrease cooling)","Our motivation for not including it as a fully embodied, situated agent was that—from our understanding—these systems tend to rely on if-then rules and are less open to dynamic situations that our other examples (e.g., cars and military drones) face. 3 Future of human-automation interaction: evergreen themes We now turn our attention to the future of human-automation interaction research, by describing themes that are important for future work","We start by describing three themes that are “evergreens”: themes that were also covered in the past, but that continue to be important areas for research","In particular, these themes require further expansion due to the breadth of domains and users that are involved in automated settings","After discussing these evergreen topics, we go on to discuss five new topics in human-automation interaction that we expect to increase in importance over the coming years. 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   3.1 Function and task allocation between humans and machines The first theme that has had persistent attention in IJHCS research on automation is the distribution or allocation of tasks between humans and automated systems (e.g.,  )","A simple, naive understanding of the introduction of automation might be that automated systems take over the execution of tasks from humans, and thereby simply ‘reduce’ the amount of work or attention that humans need to dedicate to that task","A colloquial understanding is for example that people are better at some tasks (e.g., to exercise judgment) and machines are better at other tasks (e.g., to perform repetitive routine tasks;  )","However, as analyzed in detail by  , achieving such allocation in practice is a hard problem, as researchers differ in what they set as appropriate criteria for the function allocation","In line with this view, it is important to consider the so-called “irony of automation” ( ), which states that introduction of automation can radically change how people perceive or act in a specific context","People do not merely reduce what they work on when (part of) a task is automated, but use different strategies for working on that task altogether","For example, one intention of semi-automated vehicles is that the human driver is responsible for fewer basic control-monitoring tasks (e.g., steering, pressing the gas), and can therefore switch his or her attention to monitoring the traffic environment and the vehicle","However, a meta-review of research on driving assistance systems suggests that the introduction of automation increases the likelihood that drivers perform non-driving related tasks, which reduces their situational awareness and response time to alerts ( )","Although the problem of function allocation, and related themes, such as the irony of automation, have been known for decades, the associated research questions gain new urgency now that automation is being used by non-professional users in time-sensitive and safety-critical contexts","An underestimation of user interaction in these domains can lead to incidents, and non-professional users might lack the training and experience to cope with system failures","Moreover, they might underestimate risks or misplace their trust in the system","For example, in the first deadly incident with a Tesla model S (a partially automated vehicle), the human driver had a prolonged period of visual distraction shortly before the crash ( )","Although the cause of this distraction is unknown, misplaced trust in the automation might have been a factor","Automation might also change how, when, and where tasks are performed","For example, if cars become more automated, will they turn into mobile offices ( ), or areas of fun and play ( )? That is, automation might be a radical disruptive innovation that changes more than just the task itself. 3.2 Trust, incorrect use, and confusion The second major theme of human-automation interaction to have received persistent attention in IJHCS over the years is how to promote appropriate levels of trust in automation ( ), how to avoid incorrect use and (human) errors (e.g.,  ), and how to avoid confusion. 
                          introduced four distinct types of use of automation that can impact a user's trust in a system","Initial   might already depend on trust, but on top of that users and other stakeholders of automation might   the automation (i.e., show overreliance, or too much trust),   it (i.e., under rely on the automation and distrust it, for example due to false alarms), or   it (i.e., introducing the automation without considering all the consequences of it, in line with the irony of automation,  )","These four forms of use, and their impact on trust are still relevant today",They are particularly relevant now that non-professional users are using automation in more settings,"As they lack the training and experience of professional users, they might bring in incorrect expectations of the capabilities of the automated system, resulting in misuse or disuse","How a user uses automation, and how they perceive trust can also be looked at more dynamically, based on a user's understanding of the system's mode of operation over time","The mode, or state, of an automated system determines its response to user input and to changes in the overall context of the system","For example, in automated vehicles, cruise control and adaptive cruise control can be two automation modes","When human drivers or operators engage adaptive cruise control, their vehicle will attempt to maintain a given speed, but will slow down if there is slower traffic ahead; in contrast the same vehicle with (non-adaptive) cruise control will not slow down for slower vehicles ahead","The human operator needs to keep track of mode changes, and also remember how the system will react to user input and context changes in the current mode","Mode confusion (mode error) occurs when the human operator is confused about the current mode of the system, or cannot remember how the system will react in the current mode ( )","Mode confusion is highly consequential for safety-critical systems, such as road vehicles, power plants, airplanes, robotic wheelchairs, and flight control systems","In the above example, if the driver mistakenly believes that the vehicle is in the adaptive cruise control mode, when it is actually in (non-adaptive) cruise control (i.e., a form of misuse of automation in Parasuraman and Riley's terms), the result can be a crash.   discuss this issue in the driving domain by introducing a probabilistic (Hidden Markov Model) framework that relates driver beliefs of the system's mode to actual system modes","Such frameworks make explicit in what system states mode confusion might occur, and can aid in the (re-) design of safety-critical systems","Mode confusion can also happen in other contexts.   point out that power plants are highly complex systems, which means that some part of the plant will always be under repair or in a state of being modified","This effectively changes the mode, or state, of the plant, and requires operators to act accordingly","Mode confusion might result in a misinterpretation of alarms: depending on the mode of the power plant, an alarm might indicate an actual problem or an expected state of operation","In the coming years, human interactions with automation will continue to be subject to mode confusion","First, automation is not the same as autonomy: our automated systems will be very good at what they do, but in some difficult cases, or in legally mandated situations, they will require human intervention","Second, automated systems will continue to be applied in a variety of complex situations—after all, that is where they are the most useful","However, use in complex situations will result in multiple modes of operation ( )","Researchers need to focus on creating models of mode confusion for different application areas, (e.g.  )","Such models can then be used in the design and evaluation of systems that reduce the frequency, and the consequences, of these errors. 3.3 Focus, divided attention, and attention management A third theme that has had persistent attention in IJHCS research on automation is creating appropriate workload levels for the human interacting with automation so as to avoid under- and overload ( )","Taking a broader perspective, one can say there is a need to understand focus, divided attention, and attention management","As automation continues to improve, automated tasks might require less human attention and intervention","This allows humans to focus on other activities, such as (other) work and play","At the same time, researchers expect that humans will continue to play a role in automated systems such as cars, even under higher levels of automation (e.g.,  )","For example, occasional human aid might be needed if the automated system encounters an off-nominal scenario","In such a case, humans need to revert their attention to the automated task, even though they might feel that their preceding task was more urgent to them","These situations require a detailed understanding of multitasking and interleaving processes (see also special issue in IJHCS,  ), and a new view on attention management","Focusing on automated vehicles, a large body of research has investigated the effectiveness of providing last-minute alerts to warn drivers about situations where human assistance is needed","However, in such automated circumstances, people's susceptibility to alerts is reduced ( )","Moreover, even if an alert is processed, mode confusion might limit the human driver's understanding of their role and limit their ability to take the right action ( )",Novel perspectives on attention management might be needed to minimize these dangers,"For example, in our own work we have investigated the use of earlier warnings (pre-alerts) to warn drivers before their action is critical ( ; see also  )","Beyond simply providing warnings, more research is needed into how the human and the machine can be   in a task, instead of one taking over the task of the other and only warning in case of emergency","The success of such systems will rely both on the system's ability to assess (e.g., model and predict) the human state and understanding, and also on the human's ability to understand the system's functioning. 4 Future of human-automation interaction: emerging themes To close, we discuss five themes that are emerging as important topics in automation research, and which we expect to increase in importance over the years to come. 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   4.1 Interdisciplinary studies to cover breadth and depth of domains and users Our review of the IJHCS literature has shown that over the past five decades, research on human-automation interaction has broadened out into different areas",We expect that automated systems will continue to broaden out into new domains as the principles and methods behind automated technologies aimed at professional users start to penetrate the broader consumer market aimed at non-professional users,"For example, automated features from commercial airplanes might make it over to non-commercial airplanes that are used by trained, but less experienced pilots","At the same time, even though technology branches out, in a sense automated technology is often still specialized and limited, and its accuracy can be improved","In the home environment there are dedicated machines for vacuuming, lawn mowing, or playing music, but few devices that combine such tasks","Personal virtual assistants like Amazon's Alexa, Apple's Siri, or Google Assistant can aid in many tasks, but have limited capabilities (e.g.,  )","On the road, automated cars can tackle ever more complex and demanding situations, but still have exceptions where human assistance is needed","In other words, there are opportunities for improvements in both the “depth” (i.e. improving performance on specific tasks) and the “breadth” (i.e., how many tasks and contexts they can handle) of studies on automated systems","As part of the branching out, automated systems will be used more frequently by non-professional users and with this comes a set of important questions about human-automation interaction","For example, how are users trained to work with automated safety-critical devices? How are their skills on a task retained if it is not put to use frequently (see also  )? How are different cultures, and different norms, customs, and conventions facilitated? Will the adoption and use of automated systems benefit a variety of user groups (e.g., automated vehicles hold the potential for improved mobility for people who cannot drive or do not have access to their own vehicle)? 4.2 Regulation and explainability The regulatory landscape for automation depends heavily on the application area","Thus, regulation is well-developed for established fields, such as for relatively simple medical devices","However, new interconnected medical devices present a challenge for regulation ( )","Even more so, medical robotics, where automation can take on various forms, presents a significant challenge for regulators—in fact, autonomous robots will not only be medical devices but also entities that practice medicine, and it is not yet clear who would be in charge of regulating them ( )","Similarly, regulation is still under development for cars, where automation is only now making significant advances ( )",A large push on automation research comes from European legislation on “explainability”,"In the context of recent data protection laws, European laws now require that decisions that are made for humans by automated systems are explainable to the humans ( ; see also  )","Automated system and (machine learning) algorithms make many decisions, but the reasons for these decisions might be opaque to the end user ( )","Moreover, the (decision) models that the algorithms create to inform their actions necessarily abstract away from some details in the world",Such abstraction can result in ‘traps’ ( ) such as an inability to take all of the relevant features into account in decision making (as some were left out in the abstraction) or to transfer learned behavior to new settings (where other features are perhaps more important),"Explainability is not always straightforward for embodied, situated automated systems such as automated cars, as these systems make many decisions over time","For example, at any given time there is an explicit or implicit decision to accelerate or decelerate, and whether to make a steering adjustment (i.e., Michon's control level;  )","Should cars be able to explain these decisions continuously? And should this be done in real-time? Or should only more strategic decisions ( ) such as why particular routes were chosen be explainable? Or is only hindsight explanation needed surrounding (near-) accidents? Although ideally a system should be able to make multiple explanations, whether they do this can impact a user's attention, and might also have impact on system performance (i.e., when dedicating capacity to the storing of decisions)","From a human-computer interaction perspective, explainability of automated systems should at least be present to avoid mode confusion ( ) and to avoid alert fatigue and the so-called “cry-wolf effect” ( )","Like humans, automated machines are not always “perfect”","The algorithms behind automated systems often get trained on data, and the resulting decision systems might be limited by the data (“Garbage in, garbage out”)","Specifically, through the training set, the algorithms might pick up on biases or inequalities that exist in society, which can have consequences for the end users","For example, if a gender classification algorithm is trained to classify people based on their physical features, it might overlook that biological sex and self-identified gender labels might not align, and the resulting misgendering might have negative impacts on mental health ( )",Humans might be able to help learning systems to overcome their biases,"For example, in recently proposed guidelines for human-AI interaction, five of the eighteen guidelines focus on ways to help users correct the mistakes of an AI system ( )","However, it is an open question how to design such systems in practice, in particular as there might be a disconnect between the low-level features that a system needs to adjust to improve, and the high-level concepts that a user (incorrectly) thinks they need to adjust (e.g.,  )","From a legislative perspective, an important question is then also who is to blame when an accident or incident occurs involving an automated system in a safety-critical setting","An initial thought might be to think locally, with the human operator or the producer, programmer, or seller of the technology","However, the introduction of automation is sometimes motivated by a narrative to reduce the frequency or probability of accidents and incidents","Approaching these from a probabilistic viewpoint raises the question of what is an acceptable probability of risk, and how this risk is spread over the population","The consideration of risk at the population level, then turns the question of “who is to blame” into a question that is probably larger than one individual. 4.3 Ethical and social dilemmas As automated machines achieve more functionality, various ethical and social dilemmas become more urgent and prominent",Our overview of the history of IJHCS already touched on one such issue: are increasingly autonomous systems socially accepted as equals ( )? Another ethical and social consideration is that of the future of work and job security,"A model by   predicts that low-skill and low-wage jobs, such as in transportation, logistics, and office work, in particularly are likely to be replaced by automation",Frey and Osborne predict that this will require a shift in skillsets by human workers to tasks that require creativity or social skills,"From our perspective, it is unclear whether this prediction will hold, as our literature review of IJHCS articles indicates that research is already investigating topics such as emotion classification and social interaction between humans and robots (e.g.,  )","Therefore, we expect that in the years to come there will be more progress on (partial) automation of creative tasks and social interaction settings than anticipated in the report by Frey and Osborne","If this happens, the ethical and social question of job security will be plainly evident","Moreover, automation might not increase at a steady, linear pace","For example,   predicts that the pace of improvements in automation might also accelerate as time goes on, thereby making it ever harder for people to catch up with the increasing changes in automation and to adapt their skillset","How are humans then equipped for these societal changes? How do we make sure that we create devices that are there for human users? But also, how can technology help to achieve a world that provides opportunity for all, and not just for a fortunate minority? Another ethical consideration is what decisions automated systems should take in complex life-or-death situations that are imminent in safety-critical scenarios","Survey research shows that humans would like automated machines to make morally just decisions in principle, yet they also want the system to deviate from this moral path if a moral action would require sacrificing their own life or that of their family members ( )","Moreover, the survey research shows that there are individual and cultural differences in what is considered morally just ( )","Given that humans cannot agree on moral conflicts, a lot more research is needed to guide the regulation of automated systems","For example, the Ethics Commission on Automated and Connected Driving, which was appointed by the German government, has developed a set of twenty ethical rules related to the design, deployment, legal issues, and use of automated vehicles ( )","Taken together, the full set of social and ethical considerations also poses a fundamental question: whether to automate at all or not? In most safety-critical scenarios where automation is introduced, such as automated driving, the intention is that introduction of automation or automated support can save lives and reduce incidents","However, the new technology can also introduce new problems and incidents",A moral judgment is needed whether the benefits weigh up against the challenges,"Although the inclination of some researchers might be to minimize   incidents, this might overlook the benefits of automation (see also  ). 4.4 Continued and improved human and humane experiences Implicit in the previously discussed trends is the need to consider human experience","With automation improving, how can we continue to maintain a fair and humane interaction (see also section on ethics)? Which aspects of tasks do we automate, and which tasks do we leave to the human? In line with the historical trend of automated testing (e.g.,  ) and expert systems ( ), we might expect more software tasks to become automated in the coming few years","But which parts are automated? How is creativity and expertise embedded correctly? If creativity is essential for human contributions to an automated task, how do we ensure that humans can contribute this, and how do we know when and where it is needed? Or, if humans would like to focus on other aspects of a task, apart from creativity, how do we continue to allow them to do so? For example, in a world where automated vehicles have penetrated the market, will we allow occasional human driving “just for fun”? How can this be done in a world where other cars might rely on the predictability of non-human actions to maintain a stable driving trajectory? If we do not allow humans to contribute to such tasks and activities, how do we allow a humane experience in other ways? The answers to these questions are not yet clear, but needed. 4.5 Radical changes to human-automation interaction As we look into the future, technological advances in human-machine interaction, automation, artificial intelligence, and related disciplines are likely to usher in dramatic change in how we live with computing devices","Although such radical shifts are hard to predict accurately, some suggestions and trends are noticeable",One such change is imagined by Yuval Noah Harari in his book “21 lessons for the 21st century” ( )—he envisions a world in which AI will become better than we are at many tasks,"If this happens, then one question for human-automation design will be how human users can best use such super-smart AI","Will the humans enjoy the interactions and engage in them? Will they engage with AI while having the appropriate level of trust, taking into account both the benefits and the potential costs of the interactions? Or will they act like the humans in   novel “Caves of Steel,” where the people of the Earth of about 1000 years in the future fear and reject robots, and the comforts that robots can provide humanity? Another dramatic change is envisioned by the futurist Mark Pesce—he expects that we will be able to associate digital data with physical objects and view this data through augmented reality glasses ( )","Pesce expects that this will lead to the emergence of what he calls ‘supertools’: tools that can allow us to interact with computing objects, and thus with the automation around us, while having at our disposal vast amounts of data about all aspects of the work of automation",One significant question for human-computer interaction design in this case is how to allow users to interact with this vast amount of data,"Simply put, there will be too much data available for users to be able to handle it all, which means that human-computer interaction design will need to create focused views of the data","Turning to art again, and specifically the science fiction of Asimov: imagine what it might be like to interact with automation if our interface technologies can go beyond showing us information with augmented reality! What if the interfaces could make us feel like the machine is an extension of our body? This is what it feels to operate an advanced starship in   “Foundation's Edge”—the effort required to accomplish something is about as much as to think about the goal",Perhaps Asimov overestimated the probability that machines will eventually be able to literally read our minds,"But, we can still expect that our minds and the machine automation will not always be separated by keyboards, screens, and brittle speech interfaces","How will radically more capable interfaces affect how we can control automation, and just as importantly, how we perceive automation and its place in our lives? As we contemplate the inevitable radical changes in human-automation interaction, it is important to keep asking questions","What are the economic and societal forces that are driving the changes? How will new technologies shape what is possible for these interactions? And what are the economic and broad societal implications of these dramatic changes? The answers to these questions will be found through interdisciplinary work that incorporates a clear understanding of human-automation interaction, and leverages it effectively","Many previous eras of human development have included radical change in technology, but we expect the change to be faster than it had been in the past","Where will this change lead us? For all of the themes we mentioned in this document, except for this last one, we have reasonably clear plans for how to move forward","For some of them, our horizon extends relatively far, for others not that far","In sum, human-automation interaction research has been an area of exciting and impactful work for many decades","The readers of IJHCS, and more broadly the scientific community, should expect this trend to accelerate in the coming years",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0957417419301812, 1 Introduction Investors make use of many online discussion channels when deciding to make investments on stock markets,"Such information is presented within Financial Discussion Boards (FDBs), news corporations (e.g","Financial Times), broker agency websites, and social media platforms","Recently, Twitter has become a popular platform for investors to disseminate stock market information and discussion ( )",Many large organisations are also using Twitter as a platform to obtain and share information relating to their products and services ( ),"Companies are identified on stock markets through the use of ticker symbols, which are typically one to four characters in length (depending on the exchange) and are unique to an exchange, e.g. the TSCO ticker refers to Tesco PLC on the London Stock Exchange (LSE)",The use of these ticker symbols within tweets on Twitter are referred to as cashtags and allow investors to participate in discussions and view news regarding a specific company at a moment's notice ( ),"Cashtags are clickable links embedded within tweets which mimic the company's ticker symbol, prefixed with a dollar-symbol (e.g. $TSCO cashtag on Twitter refers to Tesco PLC) ( )","Cashtags were originally introduced by Stocktwits 
                       to allow users to link companies with their posts",Twitter introduced the feature of cashtags in 2012 to allow their users to associate specific companies with their tweets ( ),"A tweet can contain multiple cashtags, with the only limitation being the character limit imposed upon Tweets, which was recently increased to 280 characters","The main limitation of cashtags is that they are susceptible to colliding with an identical cashtag belonging to a company listed on another exchange, a phenomenon we refer to as a cashtag collision","As tweets are typically short in length, they can be an indispensable tool for investors to discuss recent events relating to companies","The presence of colliding cashtags, however, can result in investors having to decide if the tweets returned via their cashtag search actually relates to the company in which they are interested in",Investors not aware that Twitter does not distinguish multiple companies over different stock exchanges with identical ticker symbols could have made investments based on information which is not pertinent to the company in which they thought it was,This is even more problematic if investors use automatic analysis tools to measure the popularity of a certain cashtag or other social media metrics,"Throughout this paper we refer to a cashtag collision as one of two scenarios: (1) two identical tickers which refer to different companies (e.g. $TSCO refers to Tesco PLC on the LSE, but also refers to the Tractor Supply Company on the NASDAQ) and (2) two identical tickers which refer to the same company which has multiple listings on different exchanges (e.g. $VOD refers to Vodafone Group PLC on both the LSE and the NASDAQ)","We anticipate that the second scenario will be particularly difficult to detect and resolve, as the same company which is listed on multiple exchanges does not have many features which can distinguish them apart (e.g",VOD on both exchanges will have the same company name and CEO),"The issue of colliding ticker symbols is not just isolated to Twitter, several other news websites which depend on the automatic assignment of news articles to specific companies based on their ticker symbols can also suffer from incorrect assignment of news articles","Yahoo! Finance, for example, incorrectly associates Tesco PLC's (LSE) Regulatory News Service (RNS) statements with the Tractor Supply Company (NASDAQ), which could sow confusion for potential investors who depend on such news sources",This paper introduces a novel methodology for the detection and resolution of colliding cashtags on Twitter,We train traditional supervised machine learning algorithms twice on each tweet to classify if a tweet relates a specific exchange-listed company or not,"One classifier is trained on a sparse vector of the tweet text alone, while a second classifier is trained on both the sparse vector and other features contained within a company-specific corpus",The cashtag collision resolution methodology introduced in this paper is a generalised approach which can be applied to any stock market,We validate the cashtag collision resolution methodology by carrying out an experiment involving companies listed on the LSE (discussed in detail in  ),"The main contributions of this paper can therefore be summarised as follows: 
                   These contributions address a problem which has yet to be discussed within the literature",Several previous works involving the analysis of cashtags could have been susceptible to incorrect analysis and results due to the subtlety of colliding cashtags,"The remainder of this paper is organised as follows:   introduces the main motivation of this paper, challenges associated with colliding cashtags, and the research questions we aim to answer.   explores the related work involving cashtags, disambiguation on Twitter, data fusion, and the use of custom corpora.   provides an overview of an experiment which has been designed to validate the cashtag collision resolution methodology.   provides an overview of the data used in this experiment.   introduces the company corpora creation and data fusion methodology.   provides a high-level exploratory analysis of the data.   details the cashtag collision resolution methodology for classifying a tweet as belonging to a specific exchange or not.   discusses the results of the experiment.   draws a conclusion and proposes future work relating to cashtag collisions. 2 Cashtag collision challenges This section presents the motivation, challenges and the research questions this paper will answer. 
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   2.1 Motivation Although the main limitation of cashtags is Twitter's inability to distinguish between identical cashtags which refer to companies listed on different exchanges, it is also important to mention that the structure of ticker symbols differ across the internet","As Twitter does not adopt or enforce a way for users to include the exchange symbol when referring to a company ticker symbol, as other websites do, a methodology for classifying a tweet as belonging to a specific exchange would benefit both individual investors and businesses alike","Currently, tweets need to manually analysed by the human eye to determine what company is being referred to if no exchange-specific information is available in the tweet, wasting precious time. 2.2 Key challenges The reason that collisions occur on Twitter is that Twitter has yet to formalise or enforce rules relating to embedding cashtags in tweets","Similar to hashtags, users are free to create their own cashtags by simply prefixing any word with a dollar-symbol, meaning no exchange-specific information needs to be present in the tweet for it to be published","When news is published on websites such as Google Finance and Reuters, a pre-determined rule is often adhered to, in that the exchange in which the company sits on is featured in the ticker symbol","Companies are identified on Reuters, Bloomberg, and Google Finance by the formats shown in  
                         , all of which feature the exchange of the company within the ticker symbol",Another challenge is that some of the more popular ticker symbols (e.g,"WEB) can feature on multiple exchanges ( 
                         ), making it increasingly more difficult for an investor to decipher which company a tweet refers to",A challenge relating to the application of Natural Language Processing (NLP) to this field is that text classification is often performed on documents which contain a large collection of words to assist a classifier in determining which class a document belongs to,"Tweets, however, are limited to only containing a limited number of words due to the character limit ( ), meaning tweets may not feature enough information within them to provide an accurate classification as to whether or not the tweet relates to a specific exchange company","The lack of textual information in tweets can be overcome by creating a custom corpus for each exchange-listed company via data fusion techniques, which can then be consulted to assist in the classification process. 2.3 Research questions This paper will answer the following research questions, which will be referred to as RQ1 and RQ2 in subsequent sections: 
                      With the motivation and research questions outlined, in the next section we discuss the work relating to our proposed methodology and the experiment designed to validate it. 3 Related work To our knowledge, there has been no related work on the identification or resolution of cashtag collisions","There has, however, been extensive work in other areas related to this research, which include experiments involving cashtags ( ), word disambiguation on Twitter ( ), the fusion of different data sources ( ), and the use of custom corpora ( ). 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   3.1 Cashtags Previous work on the analysis of cashtags is relatively scant within the literature","Existing work has focused on sentiment analysis of tweets which contain cashtags for the purposes of stock market price prediction, analysing the impact of financial events on Twitter, and uncovering spam bots on Twitter ( ). 
                          collected tweets over a two-month period which contained cashtags for Apple Inc. ($AAPL), listed on the NASDAQ, and Johnson and Johnson ($JNJ), listed on the NYSE, for the purpose of stock market price prediction",Tweets containing these cashtags were then divided into two categories – tweets created during the opening and closing times of the exchanges respectively,"A Feedforward neural network was then implemented which took the average sentiment scores for tweets within these categories to predict the opening and closing market prices, reporting a high accuracy","The main limitation of this work is that it only took into consideration two companies, both of which sit on different exchanges. 
                          analysed the impact of financial events on Twitter","Tweets containing the keyword “tesco”, the hashtag #tesco, or the cashtag $TSCO were collected before and after Tesco PLC announced its merger with Booker Group PLC (both LSE companies)","Their findings provided promising evidence that Twitter was permeable to financial events by analysing the rapidness in which Twitter was able to respond to financial events. 
                          carried out a large-scale analysis on the presence of spam bots on Twitter",They collected over nine million tweets which contained at least one cashtag of a company listed on one of the five main financial markets in the US over a five-month period,"They found that large volumes of tweets containing cashtags of low-value stocks also featured cashtags of more popular, high-value stocks, showing that users attempt to use the popularity of high-value cashtags by “piggybacking” onto them and spreading news of unrelated low-value stocks","They also concluded that large spikes were due to mass, synchronised retweets, showing the presence of bots and that an analysis of retweeting users classified over 70% of them as bots. 3.2 Word disambiguation on Twitter There have been several studies on word disambiguation on Twitter in recent years ( ).   proposed an approach to disambiguating company names which are mentioned in tweets","Their approach relies on positive and negative filter keywords which, when found within the text of a tweet, can help to establish if a tweet refers to a specific company","For example, the term “ipod” is considered a positive filter keyword for the company Apple, whereas the word “crumble” has a negative shift",They identify keywords for specific companies by automatically collecting terms listed on the organisation's Wikipedia page and the company URL and then manually associate positive and negative terms with companies,"Tweets classified by such keywords were then used with a supervised machine learning algorithm, obtaining a classification accuracy of 73%",Research which involves the use of performing NLP on tweets often use NLP models which are specially trained on a corpus of tweets ( ). 3.3 Data fusion Data fusion is a well-known technique which can be used to enhance the quality of data ( ),"The fusion of heterogeneous data has been considered for a wide variety of problems, including navigation systems, military, habitat mapping, and the fusion of heterogeneous financial market data ( )","Data fusion can be a challenging task to undertake for reasons such as disparate and heterogeneous data which cannot easily be combined together, specifically if the fusion needs to be performed over a varied temporal space ( )","Bharath   provides five broad categories of tweets (opinions, private messages, deals, news, and events) for the purpose of improving information filtering (associating tweets with a specific category or topic)","They first trained a Naïve Bayes model on a Bag of Words (BoW) alone, and then combine this BoW with other features such as the author name of the tweet and occurrence of user mentions within the tweet","They were able to obtain improved classification accuracy scores when the Naïve Bayes model considered both the BoW and the supplementary features combined, showing that the consideration of supplementary features can be of benefit to a classification task. 3.4 Custom corpora Several previous works ( ) have utilised custom-made corpora for tasks in which ready-made or “generic” corpora are not sufficient for the task at hand due to domain-specific vocabulary.   proposed a technique to create domain-specific corpora to convert source code identifiers to their equivalent full name counterparts (e.g. a method named “strcmp” can be split into the words “string, compare”)","Their work did note limitations in that, without a domain corpus, translations between source code identifiers to full words can be difficult to achieve",This paper attempts to address several of the challenges outlined in the related work we have just explored,"In regards to cashtag analysis, we consider a larger cashtag space than that explored in ( ) by examining 100 company cashtags","Although we do not attempt to disambiguate between specific keywords found within tweets, we do attempt to disambiguate tweets by classifying tweets as relating to an exchange-listed company or not","In regard to data fusion, we do not attempt to fuse data based on time","Instead, we fuse company-specific information together from three different external data sources in one batch, eliminating the challenges associating with real-time data fusion",This fusion process supports the creation of custom company corpora which will contain information that is specific to each company,"The next section will provide a high-level overview of an experiment to validate the cashtag collision resolution methodology. 4 Experiment details An experiment ( 
                      ) has been designed which involves creating a custom corpus of company-specific information for 100 pre-selected companies. 
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   4.1 Experiment preparation For the purposes of this paper, we validate our cashtag collision resolution methodology by performing an experiment using 100 LSE companies (listed in  )","The LSE has been chosen due to having a popular FDB associated with it which is dedicated to LSE-listed companies, allowing web scraping techniques to yield information specific to companies listed on that exchange",The LSE is formed of two sub-markets; the Alternative Investment Market (AIM) and the Main Market (MM),"The AIM is suited for growing businesses and has a more flexible regulatory system than the MM ( ). 4.2 Company selection In regards to the 100 companies used in our experiment, we select 50 companies from each sub-market (25 of which have a known collision with another company listed on one of the exchanges in  
                         , the remaining 25 with no known collision with the exchanges)","Companies are selected randomly from each of the LSE's ten different industries (basic materials, consumer goods, consumer services, financials, health care, industrials, oil & gas, technology, telecommunications, and utilities)","Only companies which have been listed on the LSE for at least two years were eligible in this selection process, to ensure that they are well-established and to maximise the chance of collecting tweets containing cashtags relating to LSE-listed companies. 
                         
                         
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                         
                      4.2.1 Data collection In order to ascertain if a tweet relates to a specific exchange-listed company, such as the LSE, data from multiple, reputable sources will be collected and combined to ensure a reliable reference to each of the LSE-listed companies is available","Tweets pertaining to the 100 experiment companies are collected in real-time via the Twitter Streaming API, which collects no more than 1% of all tweets tweeted in real-time ( )","Descriptions for each of these companies are web scraped from Reuters so that certain keywords associated with the LSE-listed cashtag company can be obtained, which will be beneficial later to ascertain how many words within the tweets are also found to be in LSE-listed company's biography","FDB posts are then collected from an FDB which is dedicated to LSE companies, allowing us to collect posts which are specific to the LSE companies used in this experiment","Finally, a share price for the company is collected to assist in the manual annotation of the tweets, this can be a helpful attribute if a tweet contains a reference to a share price when little other information is available.   will provide more details on the data collected for this experiment. 4.2.2 Data fusion The company descriptions, FDB posts, and the company share prices are combined to create a company corpus for each of the experiment companies",These corpora will assist the machine learning classifiers later to establish if there is any correlation between the features present within the tweet and the features present in the associated LSE-company corpus.   provides a detailed overview of this corpora creation methodology. 4.2.3 Machine learning Traditional supervised machine learning algorithms are trained twice on each tweet ( ) to classify if a tweet relates to an LSE-listed company or not,"One classifier is trained on a sparse vector of the tweet text alone, while the second classifier is trained on the sparse vector and other features made available from the custom corpora.   contains more details on the classifiers used for this experiment, including the results obtained","We hypothesise that the classifiers which are trained on the combined features will perform better in respect to the traditional performance metrics (accuracy, precision, recall)","In the next section, we provide an overview of the different data sources used in this experiment, along with the motivation for their use in being fused together to create company-specific corpora. 5 Data sources We now introduce the data sources, beginning with Twitter, and then the fusion data sources which will be fused together to create company-specific corpora, which will be utilised in   when the data fusion methodology is introduced","A complete list of the data sources, along with the methods of collection, and dates in which the data is collected, is provided in  
                      . 
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   5.1 Twitter We only collect tweets which have at least one occurrence of a cashtag belonging to at least one of the experiment companies","In total, we have collected 86,539 tweets, which include tweets having collisions and tweets without","These tweets cover a one-month period from 16/4/2018 to 16/5/2018. 5.2 Fusion data sources The data sources listed below are used specifically in the fusion process, company-specific information from Reuters, an FDB (specifically for our experiment, London South East), and AlphaVantage will be used to create company-specific corpora","Pre-processing techniques are explained in  , when the data fusion methodology is introduced. 
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                      5.2.1 Reuters The Reuters finance section contains a description for every company listed on all the major stock exchanges around the world","The description typically consists of a brief paragraph which details relevant company information such as the company industry, location of operation, and other pertinent information",Keywords found within the description could help to establish if a tweet relates to an LSE-listed company or not,"The description for each company has been scraped via BeautifulSoup, 
                             a Python library suitable for scraping websites. 5.2.2 Financial Discussion Board – London South East A popular FDB used by investors trading on the LSE, London South East features a sub-forum for every company listed on the LSE in which investors can discuss news and events for a specific company",FDB posts can help determine what topics are being discussed by investors in relation to the specific company and its corresponding subforum,"As financial posts span across multiple pages, the open-source web crawling framework, Scrapy, 
                             has been used to extract the posts of each of the discussions for the 100 sub-forums",London South East records stock discussion posts going as far back as one year,"We have collected all of the posts available for each of the experiment companies. 5.2.3 AlphaVantage AlphaVantage 
                             offers real-time stock market prices for shares listed on stock exchanges","We have collected a recent share price for each of the experiment companies, which may prove to be a valuable source of information if tweets are found to frequently feature share prices, as this could help to distinguish which company is being referred to","Now that the different data sources have been introduced, we now present the methodology for creating individual company corpora through the use of data fusion. 6 Company corpora creation & data fusion methodology This section will present the methodology ( 
                      ) for creating company-specific corpora through the use of data fusion","We begin by describing the corpora creation steps and exploring the benefits and associated challenges of performing this data fusion on the different data sources. 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   6.1 Corpora creation This section will provide more details on the corpora creation methodology, which includes the features from each data source to be collected, the collection method, selected fusion features, and the data pre-processing steps to be carried out on each of the fusion data sources. 
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                      6.1.1 Feature selection & collection The first step of the fusion process is to collect each of the fusion data sources listed in  ",The Reuters company descriptions for each of the experiment companies have been collected via the BeautifulSoup library,"FDB posts have been collected via the Scrapy library, with the share prices being collected using AlphaVantage's API. 6.1.2 Fusion features Although the Reuters company descriptions and the FDB posts contain several features which are being stored, not all of these features will provide benefits when being contained in a company's corpus. 
                            
                             Outlines the features to be fused and contained within a company corpus, along with the reasoning behind these choices. 6.1.3 Data Pre-Processing An important part of the fusion process is to perform common pre-processing techniques before the fusion process begins","This includes reducing the dimensionality of the data by removing commonly occurring low-value words and transforming them into their non-inflected form.  
                             summarises the pre-processing and other cleaning techniques performed on each of the data sources. 
                            
                            
                            
                         
                            
                            
                            
                         
                            
                            
                            
                         6.1.3.1 Named Entity Recognition The lack of context in short queries (i.e. tweets), due to the character restriction, makes the task of recognising entities particularly difficult for full-text off-the-shelf Named Entity Recognition (NER) ( )",We have utilised NER by selecting the 20 most frequent proper nouns from each of the FDB company sub-forums,"A proper noun being defined as “a name used for an individual person, place, or organisation, spelt with an initial capital letter”","This allows us to capture names of people and organisations being mentioned in user posts which can then be used later to record the number of LSE-listed company FDB proper nouns present in the tweets. 6.1.3.2 Stop word Removal The removal of stop words in the tweets, FDB posts, and Reuters company descriptions has been performed using Python's NLTK package, 
                                which includes a pre-built corpus of common English stop words which we use to perform stop word removal from each data source. 6.1.3.3 Lemmatisation The NLTK has also been utilised to perform lemmatisation on the Reuters company descriptions and all of the tweets’ text in order to reduce the number of words, allowing us to reduce the sparsity of our bag of words (discussed in  ) ( ). 6.2 Data fusion challenges One of the key challenges present in this data fusion process is the heterogeneity of the three data sources",Reuters descriptions are static in the nature that this description will likely stay the same for years,FDB posts are dynamic in the sense that investors will likely be discussing recent news and events relating to a specific company,"As our approach relies on freely-available public data sources, there is the added risk that any of these data sources could suddenly become unavailable, meaning alternative features from other sources may need to be relied upon",Web scraping techniques in particular are susceptible to failing should the structure of a web page change,"Utilising services which provide structured data, such as AlphaVantage, also run the risk of service shortages or their associated APIs becoming unavailable or deprecated",Each of the data sources considered for this experiment do have reliable alternatives,"Descriptions for companies can also be obtained from other reputable financial market news providers, such as Bloomberg","There are also other FDBs which do focus specifically on the LSE, although the structure for scraping posts from this FDB is significantly more challenging due to the way the websites structures its web pages","Share prices from AlphaVantage could also be obtained from web scraping, although share prices obtained in this way would likely be outdated when compared to real-time market prices","In the next section, we perform a high-level exploratory data analysis of the collected data in order to better understand the nuances of the dataset of tweets and FDB posts. 7 Exploratory data analysis This section will present a high-level overview of the Twitter and London South East datasets",This analysis is based on all of the tweets and FDB posts gathered for the experiment companies ( ),"The goal of this exploratory data analysis is to gain a better understanding of the scale of cashtag collisions, in addition to identifying any particular nuances present in the dataset which may be of importance in the annotation process ( ). 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   7.1 Twitter We begin by exploring the Twitter dataset with an exploration of the cashtags within the tweets","A total of 86,539 Tweets have been collected over a one-month period from 16th April 2018 to 16th May 2018","Taking into account the full twitter dataset of 86,539 tweets, we begin the analysis by checking how many tweets contain a cashtag which collide with one of the exchanges in  ","In total, 55,543 (64.2%) contain a colliding cashtag (based on our definition in  )","This highlights the scale of the problem, which this research is attempting to address. 
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                      7.1.1 Cashtag distribution The number of cashtags present within the tweets in our dataset falls between 1 and 50 ( 
                            ), with significant hikes at 10, 20, 24, 30, and a dramatic increase at 32 which almost exceeds that of tweets containing a single cashtag","It is a reasonable assumption that the majority of tweets should contain one cashtag, as tweets are limited to 280 characters, allowing only a limited amount of information to be shared","There is no immediate indication as to why there is such a surge of tweets containing 32 cashtags. 7.1.2 Irregular cashtag – BTG The most dominant cashtag in our dataset is $BTG ( 
                            ), present 58,733 times (tweets can contain duplicate cashtags)","A large portion of these BTG tweets (13,309) contain the exact same textual content when not considering hyperlinks embedded within them ( 
                            ), indicating the presence of tweets created by bots","All of these tweets contain 32 cashtags, which explains the hike of cashtag distribution in  ","The most frequent word found in BTG tweets (“binance”) refers to Binance Coin, a cryptocurrency which is currently ranked in the top twenty of all cryptocurrencies in terms of market capitalisation","There are currently over 1600 cryptocurrencies according to CoinMarketCap, 
                             all of which feature their own symbol which can be converted into a cashtag on Twitter, similar to stock market ticker symbols","The Twitter streaming API provides a structured JSON object for each tweet which contains details relating to the tweet, author, location, amongst other items","A useful attribute for detecting how a tweet was published to Twitter is the   field, which provides the medium used to publish a tweet",A breakdown the most popular Tweet sources in our dataset ( ) shows a clear presence of unofficial apps generating tweets,"We can now therefore conclude that the popularity of BTG cashtag in our dataset is due to the prevalence of automated cryptocurrency bots on Twitter, and that other cashtags may also be susceptible to such noise","As a substantial number of tweets come from automated bots, this leads to a considerable amount of noise in our dataset","We do not remove these tweets from our dataset, as these tweets are clearly not related to any specific exchange, meaning the word patterns used can be of use when attempting to classify a tweet as being related to a specific exchange or not. 7.2 Financial Discussion Board (London South East) posts Analysis of London South East company forums is significantly easier to undertake when compared to tweets, as each sub-forum is dedicated to a particular company listed on the LSE, meaning investors choose a sub-forum to discuss a specific company, thus collisions cannot exist in this domain. 
                         
                         
                         
                         
                      7.2.1 Sector posts The average number of posts per user of the experiment companies ( ) shows that companies listed on the AIM feature more active discussions across most sectors than their MM counterparts","Armed with a better understanding of the Twitter and London South East datasets, the next section will introduce the methodology of resolving cashtag collisions. 8 Cashtag collision resolution methodology The methodology of determining if a tweet contains a colliding cashtag ( ) involves the vectorisation of the tweet text into a sparse vector (Feature 1 – F1) and combining other supplementary features such as the number of exchange-specific (F2) & non-exchange-specific cashtags (F3), the count of Reuters company description words (F4), and FDB words (F5) found within the tweet so that traditional machine learning classifiers can make correlations between these features","We now proceed with the different steps in which we detect and resolve a cashtag collision, beginning with an explanation of our annotated tweet dataset 
                      . 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                   8.1 Annotated tweet dataset In order to answer RQ1&2 ( ), a labelled dataset of tweets must be created in order to assess the predictive power of the different machine learning classifiers to be trained in  ","As the cost of creating a manually labelled dataset is time-consuming, particularly when the labelling requires the inspection of each tweet's text and author details, we have manually annotated 1000 tweets with the labels listed in  
                         ","Although this is a laborious task even for a relatively small corpus of tweets, this is consistent with previous works relating to tweet annotation ( )","As the exploratory data analysis showed a heavy presence of cryptocurrency-related tweets, we use three labels to annotate our dataset","A label of zero (0) indicates the tweet does relate to a stock exchange, but not directly to the LSE",A label of one (1) indicates that the tweet directly relates to a company listed on the LSE,A label of two (2) indicates that the tweet references cryptocurrency,"In order to ensure consistency in this annotation process, and to ensure high-quality labels ( ) are generated, all of these tweets have been manually annotated by a single individual experienced with annotating tweets. 
                         
                         
                         
                      8.1.1 Tweet selection As evident from the exploratory analysis of the tweets in  , the sheer dominance of the BTG cashtag means that any random selection of tweets will favour tweets containing the BTG cashtag, meaning the classifiers would generalise towards cryptocurrency tweets","To ensure fairness when selecting the 1000 tweets, we first attempt to collect ten tweets for every experiment company ticker ( )","This provided 767 tweets (as some company tickers are not as actively used in tweets compared to others), for the remainder, we collect a random sample of tweets over the one-month time period for a total of 1000 tweets. 8.2 Steps 1–3: Feature design choices We now provide a motivation for the features used to train the classifiers","Beginning with the sparse vector to represent the text of each tweet. 
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      8.2.1 Feature 1 (F1) – Sparse vector of tweet text The first stage of our proposed methodology involves the conversion of all of the tweet text into a sparse matrix","After the removal of stop words and performing lemmatisation, the dimension of our sparse matrix is 1000 × 1860",This sparse matrix is featured in the training of both classifiers,"As the cashtags themselves are treated as words, the classifiers will be able to make correlations between the different kinds of cashtags present within a tweet","In regard to performing such NLP tasks on tweets in preparation for the machine learning classifiers, we elected to use the more general Python NLTK to perform this task","Although Twitter NLP-trained models do exist, none of these models have been trained to deal with the nuances present in our dataset","Although the related research ( ) surrounding NLP on tweets found that the performance of standard toolkits (such as NLTK) do not perform as well as Twitter NLP-trained models, this research did not take into account tweets relating to stock discussion, where low-character words such as stock symbols and floating-point numbers are particularly prevalent. 8.2.2 Features 2 & 3 (F2 & F3) – Count of LSE & Non-LSE cashtags in tweet The number of exchange & non-exchange cashtags present within a tweet can be a strong indication as to whether that tweet relates to a company listed on a given exchange","If a tweet contains one cashtag which relates to the LSE, but also contains a large amount of other cashtags not listed on the LSE, this will undoubtedly assist the classification of such a tweet as being non-LSE related","As all of our tweets contain at least one LSE cashtag, the count of LSE cashtags will always be a minimum of one","As is evident from the exploratory analysis in the preceding section, cryptocurrency tweets have a substantially higher count of cashtags in them","We have downloaded a list of all ticker symbols relating to the experiment companies listed in  . 
                             We then cross-check each tweet to see how many cashtags within the tweet relate to an LSE-listed company, with the remainder of cashtags being non-LSE cashtags. 8.2.3 Feature 4 (F4) – Count of Reuters description keywords in tweet The count of words in the tweets which also feature in the tweet's corresponding company corpus can provide strong evidence that a tweet relates to the LSE-listed company","As low-value words have been removed from the description prior to being stored within a company's corpus, words found within the tweet text which also feature in the company description can provide a high correlation that the LSE-listed company is being referenced in the tweet","The LON:TSCO corpus, for example, features words which are able to distinguish it from its colliding company on the NASDAQ, such as “food”, “retail”, and “united kingdom”, which would not be commonly found in tweets referencing the Tractor Supply Company","Naturally, if two or more companies with a colliding cashtag belong to a similar sector, then this feature of counting the number of word occurrences will not provide as much value","For example, LSE:ABC (Abcam PLC) and NYSE:ABC (AmerisourceBergen Corporation) are both in the Healthcare sector, meaning their respective Reuters biographies will contain similar terminology","To alleviate this, a feature which relies on user-generated terms could be of use, this is our motivation for our final feature. 8.2.4 Feature (F5) – Count of FDB proper nouns in tweet The final feature we have proposed is to use the most frequent proper nouns found within the FDB posts for each of the LSE-listed companies",The number of FDB proper nouns contained within the tweets could be a helpful indication to establish if a tweet refers to a specific exchange-listed company or not,"The sub-forum for Tesco (LSE), for example, has frequently-discussed proper nouns such as Lidl and Aldi – Tesco's chief competitors, allowing a further distinction between LON:TSCO and NASDAQ:TSCO",This feature will be particularly more helpful to solve the more complex collisions in which two or more companies with the same ticker have the same company name but are listed on different exchanges,"In respect to these five features, we believe that, when combined ( ), they provide a more robust approach to detect a colliding cashtag tweet, versus using any single feature in isolation. 8.3 Step 4: Classifier training After a tweet has been represented numerically by transforming it into a sparse vector, and the count of LSE, Non-LSE, Reuters, and FDB keywords have been recorded, this can then be used to train the classifiers","Based on previous works which have seen varying levels of success ( ), we have chosen to train Logistic Regression, K-Nearest Neighbours, Support Vector Machine, Naïve Bayes, Decision Tree, and Random Forest classifiers",These are each discussed in  ,Each of the aforementioned classifiers is trained and tested twice independently,"The first classifier (C1) is trained on just the sparse vector of the tweet text (F1) alone, and the second classifier (C2) is trained on the sparse vector and other supplementary features (F1–F5) contained within the company corpora. 8.4 Step 5: Performance evaluation The final stage of our proposed methodology involves comparing each of the classifiers to determine if a classifier benefits from being trained on the additional features","We compare the performance between the classifiers using the Matthews Correlation Coefficient score, a metric used to assess the performance of a binary classifier which has a class imbalance, discussed in further detail in  ",The next section contains the results and discussion of the experiment results. 9 Results and discussion This section will explore if the consideration of additional features improves the classification performance over the traditional approach of using a sparse vector alone,"The classification of tweets in this experiment is a binary classification problem – a tweet either relates to the LSE ( ), or it does not (0)",All of the cryptocurrency tweets (labelled 2) have been labelled zero for the training of all of the classifiers,"This section will introduce a number of suitable supervised machine learning classifiers, along with their respective benefits, drawbacks, and performance on the annotated dataset. 
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                   9.1 Accuracy paradox Before delving into each of the classifiers used in this experiment, it is important to note why we do not blindly depend on the accuracy of the models as an indication of their respective performance",High accuracy scores can often be misleading as to the predictive power of a classifier,A binary classification problem which features a dominant label can often lead to a misleading accuracy score,"In our labelled dataset of 1000 tweets, 642 tweets do not correspond to the LSE, hence being labelled zero","This means if we choose to abandon our machine learning models and predict zero every time, we would achieve a 64% accuracy for free, giving a false indication of predictive power, referred to as the accuracy paradox ( ). 9.2 Matthews Correlation Coefficient A more practical approach to evaluating the results of a binary classifier in which there is class imbalance is the Matthews Correlation Coefficient (MCC) ( )","The MCC score ( ) is calculated by using the Confusion Matrix (CM) results using the equation below (where TP = true positive, TN = true negative, FP = false positive, and FN = false negative): 
                      The MCC score returns a value from −1 to +1","A value of +1 indicates the model makes perfect predictions, 0 indicates the model is no better than random chance, with −1 representing the classifier has made incorrect predictions across the board ( )","Once each of the classifiers’ performance has been discussed, we compare the two best performing classifiers (in respect to their MCC score), to determine if the results between the two best performers are statistically significant","H  denotes the null hypothesis, which we will attempt to reject at a significance level of five percent","H  denotes the alternative hypothesis, which we will attempt to lend support to if we are able to reject H . 
                      9.3 Machine learning classifiers All of the classifiers have been implemented using the skikit-learn library within Python","Each classification model has differing hyperparameters which can affect the performance metrics of the classifier, we find optimal hyperparameters for each classifier through the use of a grid search, which explores a user-specified parameter space to determine the most efficient combination of hyperparameters in respect to a scoring metric (we elect to choose the best hyperparameter combinations based on the MCC score) ( )","A common approach suggested by   is to start with a coarse grid search covering a wide parameter space, and then a finer grid search based on the best values found – we have adopted this approach",Internal 10k-fold cross validation has been used for each classifier using an 80/20 train/test split,"A complete table of results for each classifier is provided in  . 
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                      9.3.1 Logistic Regression The first classifier we consider is Logistic Regression (LR), due to its suitability for relatively small training sets ( )","The LR results ( 
                            ) show an observable increase in the MCC score when the classifier is trained on the combined features when compared to just the sparse vector alone. 9.3.2 K-Nearest Neighbours The next classifier trained is the K-Nearest Neighbours (kNN) classifier","The kNN results ( 
                            ) show that the classifier trained on the combined features does not yield a better MCC score compared to the sparse vector alone. 9.3.3 Support Vector Machine SVMs have had successful applications in fields such as text classification, handwritten digit recognition, and object recognition ( )","The results of the SVM classifiers are reported in  
                            ",The SVM has outperformed kNN by a wide margin and has also significantly outperformed LR,"The SVM trained on the combined features is the top-performing classifier so far. 9.3.4 Naïve Bayes Next, a Multinomial classifier has been trained, due to its suitability with text classification tasks ( ), with the results reported in  
                            ","Although the Naive Bayes has outperformed kNN, it still trails behind LR and SVM. 9.3.5 Decision Tree The Decision Tree (DT) results ( 
                            ) show that there is a minimal difference between both classifiers, with the classifier trained on the combined features marginally ahead in terms of the MCC score. 9.3.6 Random Forest Random Forest (RF) classifiers have become increasingly popular, due to being more robust to noise than single classifiers ( )","The RF classifier results ( 
                            
                            ) perform almost identical, suggesting that the consideration of combined features does not impact the performance of the RF classifier. 9.4 Discussion of results Our preliminary results show that the top performing classifiers, in respect to their MCC score, are LR and SVM, both of which perform significantly better when considering additional features granted by the company corpora. kNN and DT perform slightly worse when considering features present in the company corpora",The experiment results have concluded that   (can a tweet's text alone be used to classify a tweet as belonging to an LSE-listed company?) is a resounding yes,"All classifiers trained have yielded a respectable performance, not only in terms of the traditional metrics such as accuracy, precision, and recall, but also in respect to their MCC score","In regard to   (can the creation of company-specific corpora, created through data fusion, improve the classifiers’ performance?), this is dependent on the classifier in question",LR and SVM both perform significantly better when trained on both the sparse vector and addition features granted by the data fusion process,We can now examine whether the results between LR and SVM are statistically significant in terms of their respective performances between their two classifiers (sparse vector vs. combined features). 9.5 LR vs,"SVM As evident from the initial experiment results, LR and SVM appear to be the best performing classifiers when trained on the combined features","To test if the results are statistically significant, we perform the non-parametric McNemar's test, proposed by ( ), to test our hypotheses",The McNemar's test is a statistical test used to compare two paired samples when the data are nominal and dichotomous ( ),"The p-value result of performing a McNemar's test on the contingency table below ( 
                         ) is calculated at 0.016","This indicates that the performance between the two classifiers, in respect to when they both predict either 0 or 1, is significantly different to each other","As we know the MCC score for SVM is slightly higher than LR, we can conclude that SVM is the best performing classifier for detecting a colliding cashtag tweet. 9.6 Implementation of cashtag collision The methodology to detect a colliding cashtag presented in this paper has involved the manual annotation of tweets as belonging to a specific exchange (1) or not (0)",A company or investor wishing to use this technique could do so with relative ease by collecting data from multiple data sources to assist in the classification process,"As we have only collected tweets from a specific list of 100 company ticker symbols, the classifiers presented in this paper have been generalised to tweets containing such cashtags",This means that any classifier needs to go through a re-training process whenever a new company ticker symbol is introduced on the exchange a company/investor wishes to detect collisions on,"Such annotation should be performed by an expert who is able to distinguish between an exchange-specific tweet and a tweet which does not contain exchange-specific information. 10 Conclusion & future work Prior to this experiment, the scale of colliding cashtags was relatively unknown",We have highlighted that a small sample of just 100 ticker symbols contain a large collision space in Twitter,We have also demonstrated that cashtag collisions are not just isolated to companies listed on stock exchanges but are also impacted by the increasingly dominant cryptocurrency tickers,"We have also shown that although the classification of a tweet belonging to a specific exchange can be achieved using the tweet text alone, significant increases in a classifier's MCC score, particularly LR and SVM, can be achieved by providing supplementary features to the classifiers",The novelty of this experiment lies in the feature design choices of the machine learning classifiers,Each of the features benefits the classification task in different ways,"The count of Reuters keywords embedded in a tweet can assist in the resolution of the first type of collision outlined in   (two or more companies with the same ticker, but different company names)","The second type of collision (two or more companies with the same ticker, and the same company name), is benefitted from the number of FDB proper nouns found within the tweet, as FDB posts are user-created and reflect recent news and discussion surrounding a specific company","Although the NLP pre-processing techniques used in our experiment have enabled the training of robust classifiers, other NLP techniques used on the various data sources could also have a positive influence on the performance metrics of the classifiers","There may also be other features which can further benefit the classifiers’ performance, such as scraping recent news article titles for relevant company keywords and storing such keywords within the company corpora and making use of these when training future classifiers",The supplementary features used to train the second set of classifiers could also provide different degrees of informative power – the count of FDB proper nouns found within the tweet could be of greater benefit than the count of Reuters keywords,Further work in this regard could include quantitative analysis on each of the features to assess how each of these features in isolation benefits the classifiers’ performance,"Ideally, a universally-agreed method for referring to a company through the use of its exchange and company ticker should be adhered to","Although Twitter has yet to address this – since cashtags function identical to hashtags, in that users are free to create their own",Our results have shown that this issue is problematic in the sense that 64.2% of tweets collected over a one-month period contained at least one colliding cashtag,"As previously stated, the current implementation of cashtags on Twitter can sow confusion for investors who are not aware of the problem of colliding cashtags",The proposed cashtag collision methodology presented in this paper can positively impact businesses and investors by deciding if a tweet relates to a specific exchange or not,The proposed methodology can save businesses and investors precious time by eliminating the need to manually examine tweets for relevant keywords,The solution to the cashtag collision problem presented in this paper will be utilised in the future by an ecosystem which will aim to monitor multiple communication channels for irregular behaviour relating to stock discussions,"Credit author statement 
                       Conceptualization, Methodology, Software, Formal Analysis, Investigation, Resources, Data curation, Writing – Original draft, Writing – Review & editing, Visualization, 
                       Conceptualization, Methodology, Validation, Resources, Writing – Review & editing, Supervision, Project Administration, Funding acquisition 
                       Conceptualization, Methodology, Validation, Resources, Writing – Review & editing, Supervision, Project Administration 
                       Conceptualization, Methodology, Validation, Writing – Review & editing, Supervision, Project administration Appendix A. 100 LSE companies 
                      
                      ,  
                      ,  
                      ,  
                      
                  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0020025519304864," 1 Introduction I n computer vision, contrast enhancement is the only method that is used to improve the visibility of underexposed image details","Due to the limitation in the imaging sensors (viz. camera), low illumination, poor quality imaging acquisition systems and improper settings (viz. lenses), the contrast of the captured images can be distinct from ground-truth","For a better human perception and understanding, improvement in the quality of the acquired images is a mandatory requirement","To obtain an enhanced image with better details from these low dynamic range images, contrast enhancement is performed on the basis of image processing criteria  ","Presently, the application of unmanned aerial vehicles (UAVs viz","Drone) is rapidly growing in various fields such as surveillance, military, agriculture, etc. in incommodious environments",Object detection and tracking are very challenging tasks in the field of intelligent transportation and automatic monitoring  ,"Typically, the targets often include vehicles, vessels, crafts, traffic signs, cars or forest; such targets are specifically highly informative, which can be used to improve the accuracy of target tracking (e.g., vessel shape and grouping activity are used in order to differentiate and predict the vessel), and prior spatial information aid to recognize the detection of target objects (viz. vessels, trees, house)  ","Due to the expensive nature of drone devices, the processing cost is very high for fast dynamic motion and transmission","Moreover, the projected drone images can be deteriorated by various noises, degraded normal illumination and visual quality  ","Consequently, it is essential to design and implement a drone efficient image enhancement algorithm with higher accuracy to understand and recognize target objects so as to address the better discrimination of specific objects in dronogram without increasing the burden of the hardware costs  ","In past few decades, researchers have tried to solve the exact contrast enhancement problem for colour imaging using several approaches  ",Improved contrast and image details are often required in a wide range of computer vision applications but a generic solution to this problem has not been discovered,"Several enhancement methods have been suggested such as spatial filtering, histogram equalization (HE), wavelet decomposition, or soft computing (fuzzy sets, neuro-fuzzy, convolutional neural networks) theory  ",All these methods are broadly classified into spatial-domain and transformation-domain methods,"Detailed reviews can be found in  . 
                       These methods are built on the straightforward manipulation of pixel intensities in source images  ","Mainly, statistical based sub-band filtering approaches have been utilized to enhance underexposed and low illumination in target images by suppressing noise  , but these techniques perhaps cause edge blurring and detail loss  ","Thus, adaptive enhancement approaches are beneficial to improve the contrast while maintaining edges and details of dronograms  ","The adaptive density weighted contrast  , or first derivative and local statistics   are common adaptive based approaches","Though histogram equalization (HE) techniques retain the predominant view in the field of enhancement, they possibly result in unnecessary contrast enhancement or over-fit brightness effect resulting from the lack of constraint on the level of enhancement  ","Thus, some techniques are investigated to overcome those lacks, e.g., the adaptive HE (AHE), and contrast limited AHE (CLAHE)   in literature","Unsharp masking (UM) is useful for enhancing exact details of dronograms, but increases noise and exceeds steep details at the same moment  ","Later, few revised methods such as the rational UM   and nonlinear UM (NUM)   have been suggested to overcome those problems","Recently, deep learning is achieving impressive state-of-the art performance for different image processing tasks such as image segmentation, image enhancement etc.  ","Deep convolutional neural network architecture is quickly becoming prominent in image processing since it provides the ability to efficiently encode spectral and spatial information based on the input image data, without any prepossessing step",It consists of multiple interconnected layers and learns a hierarchical feature representation from raw pixel-data,It discovers features at multiple levels of representations,"Several researchers have suggested various deep learning based image enhancement methods such as Deep Bilateral Learning  , deep convolutional neural network based image enhancement  , Dehaznet for image enhancement  , MSR-net   etc",All the deep learning based image enhancement methods produce remarkable results for real time images and low illuminated images,"Therefore, deep learning method is most robust but lots of synthetic data as well as high computational resources are required to perform the deep learning algorithms","Sometimes, deep learning approaches suffer from high bias and over-fitting problem due to the nature of data. 
                       In this case, all existing techniques are designed based on the multi-scale representation or Fourier transform approach, which often uses an input/output transformation that varies with the regional feature of a dronogram","Firstly, multi-scale representation-based schemes decompose an image into a level of multi-scale sub-bands by using the contourlet, discrete dyadic wavelet, complex wavelets or shearlet  ","However, most wavelet based schemes are unable to preserve both the contour and geometry of edges in images   whereas contourlet and shearlet are better for preserving image details","Moreover, traditional transformation-domain techniques can produce artefacts such as undesirable blocking effects  , or enhance image uniformly, but incompletely enhance all regional image details/regions   ","Still, implementation and run-time costs for contourlet or shearlet are complicated issues in real-time applications  ","Since uncertainty and vagueness are undoubtedly created during the acquisition or transmission of images, a reliable model interpreting such images should use the personal experience to determine heuristically","Although, this can be construed by the classical mathematical modeling  ","Hence, fuzzy sets (FSs), logical FSs, type-I FSs, type-II FSs, or intuitionistic FSs (IFSs), have been used to improve the contrast and visual quality of images because they are knowledge-based systems",These fuzzy techniques efficiently process faulty data collected from imprecision and vagueness  ,"In most of the cases, the grey-level range of a contrast enhanced image using type-I FSs is relatively consistent, and inadequate to enhance the corrupted images with short grey levels and low-intensity values  ",Type-II FSs are tough to practice and insufficient to address the exact hesitancy in ambiguity  ,"Consequently, image enhancement is basically a challenging task in the domain of image processing",Lots of researchers have suggested various methods to enhance low-contrasted images and most of them perform quite efficiently  ,Most of the existing methods cannot achieve the desired enhancement result for underexposed and low-illuminated images and typically suffer from deficiency with robustness and accuracy,"Here, we propose an appropriate and fittest model to correct the intensity distribution of pixels in the image, which is suitable for human perception","To plan a dronogram (drone image) enhancement scheme, it is motivating to select intuitionistic fuzzy hesitant sets (IFHSs) theory because IFHS take into account more uncertainties in the form of membership function that is more bound to the aspects of human decision-making, in comparison to traditional FSs  ",The IFHS is well-known for its power to measure hesitant quantity while addressing uncertainty in image information  ,"Inspired by literature study, a novel dronogram enhancement algorithm motivated by the IFHS theory has been developed in this paper",The proposed scheme is designed by using the hyperbolic regularization approach in the intuitionistic fuzzy hesitant set,We have formulated a different membership grades generator to construct the intuitionistic fuzzy hesitant set to measure the hesitant score in fuzzy membership grades under certain fuzzy enhancing criterion,The proposed framework has been divided into two main stages,"First, we have separated the background-foreground areas utilizing the global threshold and constructed membership functions in the fuzzy domain","Secondly, a nonlinear real-valued hyperbolic function is applied to modify and adjust both the memberships in background-foreground areas to enhance source drone images","To increase the clarity of the dim image for object detection task, we have utilized adaptive threshold instead of direct or fixed threshold for the separation of foreground/ background of the image","Since low illuminated images are ambiguous in nature, we have applied hesitant score through intuitionistic fuzzy set for the enhancement task",The designed algorithm involves few parameters and thus the suggested scheme is automated and does not require any expert knowledge,Experimental results verify that the designed method is an efficient and simple way to improve the contrast and visual clarity in the dronogram,"More detailed contributions are given below: 
                   The rest of this paper is structured as follows: In  , we present the preliminaries to the related work","In  , we illustrate the designed dronogram enhancement algorithm in details","In  , we give experimental results and discussions","Conclusions and perspectives are presented in  . 2 Preliminaries This section concisely discusses the theories of intuitionistic fuzzy sets, hesitant fuzzy sets, and the basic framework of fuzzy image enhancement for drone imaging","The novel enhancement scheme is developed on the basis of the following concepts. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   2.1 Fuzzy sets According to mathematical definition in fuzzy theory  , let   is a reference of point set",The fuzzy set   is defined on   by a membership function  ( ) for each generic points of  ,"The membership function  ( ) is real-valued function such that   → [0, 1],   ∈   mapping each point of   into real line  .   is a crisp set when  ( ) takes only 1 or 0  ","In fuzzy set, a digital image is defined by an array of singletons as membership grades of each pixel","Let   be a 2D image with size   containing   pixels with intensity levels in the dynamic range   thus a fuzzy set   is written as follows  : where  ( ) denotes the membership grades of intensity levels of each pixel  ( ) ∈   in image   with size  .   and   are respectively the highest intensity levels (for 8-bit,  ), width, height and total count of pixels in  ","Due to problem definition in image processing, the membership function  ( ) ∈   for an image can be different","Several functions such as  -function are used as the membership function in literature  . 2.2 Intuitionistic fuzzy sets and its construction According to Atanassov  , typically a fuzzy set considers only membership function  ( ) ∈   whereas an intuitionistic fuzzy set (IFS) deals with a membership function as ( ( ):   → [0, 1],   ∈  ) with another special real-valued function known as non-membership function such as ( ( ):   → [0, 1],   ∈  ) on  ","Thus, an IFS for a point set   under discourse   is defined as follows  : where  ( ) and  ( ) are membership and non-membership grades of a point   belonging to the set   under the following conditions  : If   for   then   becomes a fuzzy set (FS)","For all intuitionistic fuzzy sets (IFSs), due to lack of knowledge to determine the membership grade of each point   a hesitation can been considered","Now, the hesitation is defined by a real-valued function   for   as follows  : where,  ( ) represents the hesitation degree of   under interval 
                      According to Atanassov  ,   is a continuous monotone function and monotone IFS constructor if  ( ) satisfies the following conditions  : 
                      We have devised an IFS generator for the proposed model of the non-membership grade   for IFS   through  ’s intuitionistic fuzzy complements   and  ’s   negation function","The IFS construction functions as IFS generator  ( ) for IFS   in terms of intuitionistic fuzzy complements and negation functions which have been determined by modification of intuitionistic fuzzy complements, negation functions by the following expressions  : 
                         Both  
                         ( ),  
                         ( ) satisfy  ","Hence, we define our IFS generator  ( ) by using   and   as follows: 
                         where,  ( ) always satisfies   and   > 0,   > 0, respectively","By using  , IFS   in   is given by  and its hesitation degree   becomes 
                      On the other hand, we can measure the non-membership function   by using Yager’s generator as follows  : 
                      Based on the aforesaid description, the IFS for an image  , notionally,   is defined by   and    as follows: where,  ( ),  ( ), and  ( ) are denoted as membership, non-membership and hesitant grades of the  th pixel of image  ","It is noted that, for different values of parameter   in fuzzy plane, the contrast of an image changes from high to dim or vice versa","In the proposed method, the value of   is related to the image enhancement task, i.e. the contrast of an image can be controlled by adjusting the parameter  ","Before fuzzification, first, we have used the gray-level normalization procedure to normalize each source image","On the other hand, for defuzzification (viz. the inverse fuzzy map), we have utilized inverse gray-level normalization procedure  ","In this paper, the fuzzy membership  ( ) for IFS   is defined in terms of normalized intensity levels of image   by following expression  : where,   quantifies the total count of pixels,   and   specify the maximum and minimum gray level ( ) of the image  , respectively.   is applied to transfer the pixel intensity of image to the fuzzy domain i.e., the membership grades   of the corresponding gray levels",Intuitionistic fuzzy set theory mainly deals with two uncertainties-membership and non-membership degrees,"As the choice of the membership function is dependent on the types of problems and differs from problem to problem, often a distinct function is used as hesitant when defining the membership function","Mostly, the membership function may be triangular, trapezoid, Gaussian, Gamma, Cauchy etc","In the intuitionistic fuzzy set, the non-membership degree is equal to the complement of the membership degree due to the existence of hesitant property. 2.3 Hesitant fuzzy sets and its construction A hesitant fuzzy set (HFS)   in   is defined as  : where,   is a set of different values in [0, 1] denoting membership degrees of point   to   and is known as the hesitant fuzzy element (HFE)  ",HFS   can be defined as a fuzzy set (FS) if there is only one element belonging to   or a intuitionistic fuzzy set (IFS) if there are two elements present in    ,"According to Xia    , the hesitant normalized Hamming distance measure function is typically used to determine the distance between two HFSs","To measure on HFSs, given two HFSs   and   
                          and   represent   values in   and   which are defined on   such that  ; (1 ≤   ≤  )","The hesitant normalized Hamming distance (HNHD) is then given as   : 
                          where,   
                          are the cardinal numbers and   
                          denote the  th highest value in   
                          respectively",The Hesitant Score (HS) [ ] usually measures the amount of hesitance in a hesitant fuzzy set (HFS),"The hesitant score for two HFSs such as   and   are given as  : 
                         where,   denotes the number of elements in   and   respectively","In order to construct the hesitant fuzzy set (HFS) in terms of intuitionistic fuzzy set (IFS), we adopt  – ",The membership degrees in a HFS may be different  ,"For exact measure, the cardinality  
                         ( · ) of both the two HFS   and   should be equal",If elements in   are fewer than in   then   is enlarged by repeating its maximum element until its cardinality   becomes equal to that of   of    ,"If   and   then the two HFSs such as   
                          using   are considered as intuitionistic fuzzy hesitant sets (IFHSs) which are given as follows: 
                         
                      2.4 Outline of fuzzy image enhancement In fuzzy image processing, there are three steps associated with the process of handling spatial image data, mostly  : (i)   Λ, i.e., the input data   ⊂   (histograms, gray levels, features, etc.) is converted into a membership plane. (ii)   Φ, i.e., few algebraic as well as a logical operator (addition, multiplication, AND/OR) are used for proper modifications of the membership grades in a fuzzy plane for the enhancement and threshold. (iii)   Ξ, i.e., if necessary, the modified membership in the fuzzy plane has to be inversely mapped into the characteristic plane (crisp set) without loss of generality","The output   of the fuzzy system for an input   ∈   is given by the following processing stage as: 
                      A fresh fuzzy membership function can be defined from the membership function of the input image","For an intuitionistic fuzzy set, Sugenoâs and Yagerâs intuitionistic fuzzy generators are used to find the non-membership function and the hesitant function",The hesitant score is utilized to find the optimum divergence value,"In case of an intuitionistic fuzzy hesitant set, hyperbolized operators are used to reform a new membership function from the two membership levels of background/foreground areas",The proposed hyperbolized modifier is used to find the fittest shape of membership grades,"The main distinction from other systems is that in image processing, the input data   is managed in the membership plane using the variety of FSs, IFSs, fuzzy logical, or fuzzy measure theories to modify and/or aggregate membership values, or to classify data, decision making, logical inferences, etc.   ","The new membership values are re-transformed into the pixel plane to form new features in terms of histograms and gray levels  . 3 Proposed methodology Combining the hesitant information with nonlinear hyperbolic operators, we have introduced a novel fuzzy enhancement scheme in this study, which is based on intuitionistic fuzzy hesitant sets (IFHSs), called as DIEM for drone image enhancement","The proposed DIEM model is devised in terms of intuitionistic fuzzy set, hesitant measure and hyperbolic method which are described below. 
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                   3.1 DIEM scheme In order to enhance an image, it is essential to separate the image into foreground and background portions because of correlation in both spatial and frequency domains",Foreground/background areas are used to quantify the amount of brightness to the blackness of an image,The division of an image into a foreground/background is illustrated in  (a)–(c),"Initially, a given original dronogram  ( ) is divided into foreground region  
                         ( ) and background area  
                         ( ) via a threshold  ","Then, rearranged areas ( 
                         ( ) and  
                         ( )) are processed through several fuzzy operations","The processed dronogram  ( ) is assembled with the source image via different orders of fuzzy transforms to obtain a final enhanced dronogram  
                         ( ) using fuzzy regularization","The schematic diagram of the DIEM scheme is shown in  
                         ","The different stages of dronogram enhancement by an intuitionistic fuzzy hesitant approach are scheduled as follows: 
                      3.2 Separation of dronogram The foreground/background separation is widely used in object detection task for contrast enhancement methods",This approach is adopted in order to measure whiteness/blackness of an image,Deng   have suggested an efficient utilization of the foreground/background separation in the dronogram enhancement task  ,"Typically, the foreground portion of an image includes the image details (viz. edge, corner, etc.) whereas the background represents smooth image information","Besides, the overall pixel intensity in the background area is shorter than the pixel intensity in the background area of an image","Because of different illuminations in foreground/background areas, the foreground/background areas play a key role in the image enhancement task","To separate the foreground/background of an image, we calculate an optimal adaptive threshold instead of a direct or a fixed threshold",The threshold value plays a crucial role for the foreground/background seperation task to quantify the image pixels and assures the convergence of the algorithm,Our aim to choose an optimal thresholds which reduce the number of iterations as well as execution time,"We have introduced an iterative approach to divide a dronogram ( ) into foreground ( 
                         ) and background ( 
                         ) portions automatically","For a given source dronogram ( ), the following strategy has been utilized to determine a global threshold ( ) based on the mean   of ( ). 
                      Practically, for the original low-contrast (under-exposure) image, the pixel intensity distribution (illumination) in the background is darker","On the other hand, the pixel intensity distribution (illumination) in the foreground is brighter","But for an enhanced image, the black-white ratio (pixel intensity distribution) in background and foreground is stable and normalized  .  (a)–(c) show the original under-exposure drone image, the background image, the foreground image, respectively. 3.3 Image fuzzification To determine the exact fuzzy membership for foreground and background images, we use restricted equivalent relation (REF) with fuzzy decision theory  ","Let   be an objective function, then the membership function is defined by two real valued functions such as   and   given below: To constrain  ( ) function on the basis of  , it is characterized by the membership function as where,  
                         ,  
                          and their inverse   can be written as follows: 
                         
                         Using   in  , after simplification, we have Then, the membership function   of the decision is given as 
                      By using   and  , we have 
                         
                      Applying   and  , the adjusted fuzzification for the foreground area   can be expressed as, Similarly, the adjusted fuzzification for the background area   is given by where,   and   are the mean adjusted membership grades of foreground and background areas, respectively",The fuzzy membership value of a pixel quantifies the grade of belongingness to the foreground or background area in an image,"Hence, it is meaningful to split an image into foreground and background areas for contrast enhancement","For a given pixel belonging to a foreground or background area in an image with threshold  , its membership grade is given as (from   and  ) 
                         
                      
                         
                         
                         (c) shows the histogram of an image with a local area of grey levels","For source image, we have separated the image area into components   and  , where component   be related to component   in the fuzzy membership plane","The proposed model measures the divergence between components   and   by the hesitant score i.e., 0.3231","Typically, we have found that the intuitionistic fuzzy divergence is considered as 0.85  ","After that, we use the pixel value   at point ( ) in the foreground/background fuzzy hesitant image   as defined by   and   to replace the variable   by  ( ), and the average gray value of the block (viz.,  / ) in   and ","Then, the final fuzzy membership grades for the foreground area   can be expressed as, 
                      Similarly, the final fuzzy membership grades for the background area at space   are, 
                      According to IFS  , we construct two sets such as   
                          from   
                          by applying   individually.   and   are formed by fuzzy membership degree  , hesitation degree   and non-membership degree   of   and   by applying  ","To determine the membership degree of   for each pixel to foreground region, we formulate following expression: 
                      Hence, we define the membership degree of   for each pixel to background region, by the following expression where,  ( ) and  ( ) denote the intuitionistic fuzzy membership of sets   and   for image  ",The hesitation degree and non-membership degree are initially estimated by using   and   for both of them,"Finally, the intuitionistic fuzzy membership grades for   and   are achieved as follows: 
                         
                      The membership grades of the foreground intuitionistic fuzzy hesitant set belongs to bright pixels in the foreground area, whereas the membership grades of background belongs to dim pixels in foreground area","The relationship of intuitionistic fuzzy membership grade  ( ) and hesitant grade  ( ) with two parameters   and   are shown in  (a) and (b). 3.4 Hyperbolic regularization scheme In this work, we generate suitable membership grades for better contrast using the following hyperbolic regularization function  : where,   denotes the Euclidean distance in   between two points   and   (fixed point)","In addition,   if   and   with 0 ≤   ≤ 1,  ( ) always fulfils 0 ≤  ( ) ≤ 1  ",The hyperbolic regularization procedure is adopted to expand the liking of the pixels whose intensity levels are closer to the average gray of the foreground (object) to the background of an image,"Similarly, the hyperbolic regularization is used to reduce the liking of the pixels to the average gray of the foreground (object) or background area for those pixels whose gray levels are wider from the average gray of the foreground (object) or the background area of the image  ","The hyperbolic regularization for a foreground area ( ) using   is given as: 
                      Similarly, the hyperbolic regularization for a background area ( ) by applying   is determined as: where,   and   denote the hyperbolic regularized membership grades of each point in   and   at the fuzzy hyperbolic space","The threshold   is calculated by an iterative approach from,   and   which represent the minimum and maximum membership grades of the foreground area ( ).   and   stand for the minimum and maximum membership grades of the background area in the fuzzy plane.   is the normalized Hamming hesitant fuzzy distance between two IHFSs   and  . 
                         (c) shows the relationship between a linear and a hyperbolic function where the red, blue and green color denote the linear curve, normal hyperbolic function and the suggested hyperboloid function (the crossover point on reference line is set as 0.5)",The vertical line is the partition plot between the foreground and background areas,"For example, we take two point such   and reference cross point  ","According to  , we achieve new hyperbolic regularized membership ( )",It is obvious that the new point is more close to a reference point ( ) than a previous point ( ),"Similarly, for two-points   in   , we have found a new membership value of   that is less than the proposed approach. 3.5 Image defuzzification We improve and modify each membership degree in terms of hyperbolic regularization approach for foreground ( ) and background ( ) via the transformation of hesitant intuitionistic fuzzy image (HIFI) ( ) in the fuzzy plane","In the defuzzification stage, the hyperbolic regularized membership grades at the fuzzy plane are converted into the image plane through defuzzification operation","According to  –  and   and  along with mathematical simplification, the defuzzification for the foreground area is achieved as follows: On the other hand, the defuzzification for the background area is achieved as follows: where,  
                          and  
                          represent the fresh gray value at all the pixels in foreground and background areas, and   and   indicate the minimum and maximum gray values   of the original dronogram  , respectively","For example,  (d)–(f) depicts the histogram before and after enhancement. 3.6 Enhanced image reconstruction from image partitions To achieve enhanced results, we combine the normalized foreground ( 
                         ), background ( 
                         ) and original ( ) images in terms of certain arithmetic operators (such as division, dot product, etc.)",We apply the multiplication ( · ) and addition ( + ) arithmetic operators for integrating fuzzy data in   to achieve the desired enhancement,"In this work, we use point wise computation by the operator addition ( + ) and multiplication ( · ) in  ","After that, the enhanced result ( 
                         ) is obtained the by following formula:  where,   and   are the scaling parameters for original, background and foreground images, respectively.  ( ) denotes the input dronogram","On the other hand, the functions Λ, Φ, Ξ, and   represent the fuzzification, hyperbolic regularization, defuzzification, and normalization operations applied on  ( ) orderly, and  ·  denotes the simple multiplication operation","For a given original under-exposed (dim) image, typically the histogram curve is nearly close to the left of the histogram abscissa and for a over-exposed (high-contrast) image, the corresponding histogram curve is more close to the right of the histogram abscissa","However, for an enhanced image, the histogram curve mainly fits in left to right of the histogram abscissa ( ).  (d)–(f) show the histograms of the under-exposed, enhanced drone image, and  (a) show the source image, respectively","From  (d)–(f), we can observe that the peak of the histogram curve for the enhanced image is flatter from left to right abscissa in the histogram","The pseudo code of DIEM is described in  . 
                      4 Experiments results and analysis In this section, we discuss the performance measures and efficiency of the proposed   method in comparison to several performance measure metrics and baseline methods. 
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   4.1 Benchmark data The validation of the suggested scheme is tested on a large drone image database and the test data sets are taken from authorized websites  ","In simulation, we have selected a set of RGB colour images with size 512 × 512 pixels with 8 bits-per-pixel",The images selected are not clear and are either lowly illuminated or blurred,We have used more than 100 images to validate the enhancement performance of the proposed DIEM method,"As an example, some original sample images (viz. âGarfield Parkâ, âToledoâ, âLewisâ) are shown in  
                          (1st column). 4.2 Performance metrics The quantitative evaluation of contrast enhancement techniques is a challenging task",There exists no universally accepted quantitative and qualitative evaluation methods for the exact measure of enhancement of individual resultant images,All performance measuring schemes are not able to determine the quality of visual contrast of an enhanced image in uniform situations,"However, to validate the effectiveness and quality of image enhancement, we have used different performance metrics such as mean absolute error (MAE)  , linear index of fuzziness (LIF)  , Weber-law-based contrast measure (EMEE)  , Michelson law measure of enhancement (AME)  , Michelson law measure of enhancement by entropy (AMEE)  , universal quality index (UQI)  , Structural Similarity index measure (SSIM)  , fuzzy quality index (Q/FS)   and Intuitionistic fuzzy quality index (Q/IFS)   to assess the efficiency of DIEM",The higher values of all the aforementioned metrics indicate better enhancement except MAE and LIF  ,"MAE, EMEE, AME and AMEE are conventional statistical based metrics whereas LI, UQI, SSIM, Q/FS and Q/IFS are image quality measurement metrics. 4.3 Enhancement analysis In this section, we discuss the performance of the enhancement task","Both the quantitative assessment as well as qualitative analysis with comparative study have been explored in this treatment. 
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      4.3.1 Quantitative evaluation In quantitative evaluation, we have tested the DIEM method on one hundred drone images taken from ‘OpenDroneMapâ (ODM)    and compared with four state-of-the-art enhancement methods in this study","The selected four baseline methods are ‘Parameterized logarithmic framework for image enhancement’ ( ) by Panetta    , ‘A novel image enhancement method using fuzzy Sure entropy’( ) by Li et al.  , ‘A novel reversible data hiding method with image contrast enhancement’ ( ) by H.T","Wu    , and ‘Mammogram Enhancement using Intuitionistic Fuzzy Sets’ ( ) by Deng et al.  ","FSE and MEIF are scheduled for completion with default parameters, as recommended in their own papers",The quantitative and visual assessment of the enhanced images by the different baseline methods are executed to analyze the overall performance,The   method is devised on   programming language on 64-bit   with Intel Core i3 CPU-3.5 GHz and 16-GB RAM,"For the purpose of quantitative description,  
                             and  
                             show the metrics for each enhanced results by all the baseline methods as shown in  
                            – , respectively",It is pointed out that the proposed DIEM method produces the lowest MAE and LIF than other baseline methods which indicate that DIEM provides better-enhanced images,"Performance is further computed by using UQI, SSIM, Q/FS, Q/IFS for each test.   and   provide the MAE, UQI, and SSIM, Q/FS, and Q/IFS indices for all the methods","Lower MAE, LIF and a higher UQI, SSIM, Q/FS, and Q/IFS indicate the best contrast",It is important to use more quantitative assessment for exact performance evaluation of DIEM,"Hence, we have used some popular statistical metrics such as EMEE, AME and AMEE in this work","The estimated results of EMEE, AME and AMEE are provided in  ","DIEM has achieved higher EMEE, AME and AMEE values than the baseline methods which indicates that DIEM more efficient to enhance drone image","Similarly, the universal quality index (UQI) and structural similarity (SSIM) as well Q/FS, and Q/IFS are measured for all the baseline methods and DIEM",Higher UQI values specify that the contrast is improved,"On the other hand, for better structural similarity between the source and enhanced images, the SSIM value is very close to 1","The estimated values of all the methods are listed in  , where metric values achieved by DIEM are listed in boldface","It is important to note that baseline methods and DIEM yield different results for the enhanced drone images.   and   suggest that DIEM for Garfield drone image gives the best visual quality score with 0.1245, 0.2098, 1.5039, 114.7139, 0.4039, 0.99953, 0.98809, 0.84911, and 0.88737","However, PLF obtains slightly poor visual quality score with 0.1315, 0.2136,1.1538, 113.2538, 0.3818, 0.97856, 0.96456, 0.83942 and 0.83389, respectively","We have observed from   and   that DIEM is better than other baseline methods with respect to MAE, LIF, EMEE, AME, AMEE, UQI, SSIM, Q/FS, and Q/IFS","Furthermore,   and   represent the contrast measures for ‘Toledo’ data as shown in  
                            
                             and   obtained with the baseline methods","For two groups of ‘Toledo’ drone images, the DIEM method obtains MAE, LIF, EMEE, AME, AMEE, UQI, SSIM, FSQI, and IFSQI values with scores of 0.1265, 0.2108, 1.5107, 116.9181, 0.4135, 0.99985, 0.99758, 0.86033, and 0.88908 for first Toledo ( (f)) and scores of 0.1295, 0.2031, 1.5918, 114.8107, 0.4006, 0.99651, 0.99758, 0.84972, and 0.87058 for last Toledo ( (f)), respectively","The above objective measures (MAE, LIF, EMEE, AME, AMEE, UQI, SSIM, FSQI, and IFSQI) indicate DIEM has improved the pictorial quality compared to the baseline methods","So, the enhanced results obtained using the proposed DIEM method are almost accurate and useful in recognizing different types of scanning target objects accurately",This will provide assistance to user to precept the target objects clearly. 4.3.2 Qualitative analysis Various enhanced approaches have been used to assess and analyze the enhancement results in the literature,"To visually improve an underexposed (very low-light) image for exact discrimination of the object boundaries in ROIs, we have suggested DIEM in this study",We have simulated the designed method with a set of raw drone data in this experiment,"For example,  (a) demonstrates dronograms with car parking region, where few cars are found present",The boundaries between the green region and park along the parking road are much blurry in the original test data   of  ,"From the enhanced results obtained by DIEM, it is clear that dim regions are more clear and easy to be discernible without any effort","The car parking region is highlighted in the   data by square and its clear appearance assists for better understanding and visualization of the target objects like cars, trees under the green field compared with the original image","Four ground truth dronograms of   such as   ( (a)),   ( )(b), ( )(a) and   ( (a) contain different objects (car, trees, roads houses and etc.) which are visually slightly unclear as well as the locations of target objects and their morphological details seem to be obscure in the original images","However, after application of the proposed DIEM technique, those differences are improved in the enhanced results as shown in  (b)–(f)",It is observed that deformities in ROI can be easily identified by DIEM,"As a result, we can use DIEM for object recognition and to discriminate the proper location of object positions under scanned dronograms","Also, it is helpful to differentiate and highlight the deformities of ROI in dronograms. 4.4 Regions of interest When a user observes a digitized dronogram, he or she usually observes the spatial positions of the target objects and describes their morphological details with a ROI","If an enhanced version of ROI is present alongside the original image, the user can switch between the original dronogram and the automatically enhanced views for comparison","Accordingly, he/she can easily describe any new details or features which are manifested to him or her that can even identify some activity findings in dronograms","Three dronograms (viz. ‘Garfield Park, ‘Toledo’, ‘Lewis’) with target and/or ROI are displayed in  , where the color box covers the locations of the ROI","The red rectangles indicate the ROIs and highlighting target objects as shown in  (b),(d),(f)",We can see that the contrast of target object is distinct in the enhanced results achieved by the DIEM method,It can be found that both the visual quality and contrast of the enhanced ROIs are much better than that of the original ones,"To synchronously show enhanced target objects as well as normal object boundary, the enhanced ROIs are superimposed with the corresponding original dronograms, as shown in  (a)–(f), where the red areas consisting of certain pixels denote the detected region and/or ROIs",These pixels are visually clear in enhanced ROIs and the edges are also refined well,The detected image objects and/or ROIs are assessed by a drone expert majoring in the monitoring of a drone,He/She considers that the detected results are accurate reflections of target regions (viz. scanned object when drone monitoring) in dronograms,"Moreover, these results facilitate to recognize and analysis further tasks","In results, the DIEM algorithm shows the improvement of the contrast as well as the visual quality of target regions",This proves that DIEM has potential for understanding the target object details and to identify the target ROIs by enhancing fine details in dronograms inducing the capability of the object detection task. 4.5 Comparisons with baseline methods We have conducted three experiments to get considerably a more detailed qualitative assessment to investigate the effectiveness of DIEM to enhance drone raw images,"At the end of each experiment, the objective measures are evaluated to ensure that the tracing of image objects can be easily achieved in the presence of image enhancement as compared to other baseline methods",The proposed DIEM method has been compared with recent state-of-the-art enhancement algorithms with large dronogram data sets  ,"As mentioned above, we use four well-known baseline methods for performance evaluation notationally expressed as PLF, FSE, MEIF and RDH, respectively",We have used the baseline methods with their suggested parameters in literature,"In the simulation, we have used benchmark color dronograms of size 512 × 512 pixels with 8 bits-per-pixel","To show enhancement performance of the DIEM algorithm,   shows the results for all the images, respectively","For all the test images, the enhanced results achieved by the baseline methods PLF, FSE, MEIF, RDH, and DIEM are shown in  – , respectively","For instance, the first selected ‘Garfield park’ drone image and the enhanced results obtained by the baseline methods are shown in   (a) (original drone image) and the remaining shown in   (b)–(f) are those obtained with PLF, FSE, MEIF, RDH, and DIEM methods, respectively","For the case of a low illuminated image, testing of color contrast improvement methods is a hard and challenging task","For example, the enhanced results obtained by the baseline methods and the proposed method are visually different","So, the purpose of defining an efficient algorithm in this study is to ensure highly informative visual content and significant illuminated images for better interpretation","From the results, we have observed that the visual improvement after the proposed DIEM enhancement method shown in  (f) is almost better than the baseline methods in  (b)–(e)","In the second experiment, we have tested all the methods with the ‘Lewis’ drone image with size 512 × 512 × 3","We have noted that all the baseline algorithms except PLF have slightly improved the visual quality in the resultant images as shown in  (a)–(e), respectively","MEIF, RDH and DIEM have given better effect and thereby assist to distinguish the image objects for better understanding","However, MEIF and RDH produce over enhanced images than DIEM on the boundary regions of the image objects in the ‘Lewis’ dronogram as shown in  (d) and (e)","Perceptibly, PLF and FSE have generated artifacts at the edges of objects and provide a blurry view than the original ones","In contrast, DIEM not only improves the background of the image but also successfully achieves prominent boundary regions and sharp edges depicted in   (f)","From  (b)–(f), for all baseline methods, we have observed that the proposed DIEM method is better than the baseline methods due to its improved capability in producing clarity of distinct regions and better exposure on object details","In the last test, we have selected another sample of the ‘Lewis’ drone image in this study","The enhanced results of all the baseline methods including DIEM are shown in  (b)–(f), respectively","Compared to  (a)–(f), the proposed method DIEM successfully improves the local contrast and visually sharpen boundary regions","We have also noted that the image details generated by PLF and FSE shown in  (b),(c) fail to preserve the colour details","Moreover, the boundary regions are slightly blurred","The existing MEIF and RDH methods produce much brighter images but slight hazy effects on object boundary especially trees, cars and roads","In addition, the inner structures are not clearly visible.  (a) is just an original image of the ‘Lewis’ drone dataset where the object regions (viz. houses, cars and roads) are not well visible",It is clear that the enhanced image after application of the proposed DIEM method ( (e)) contains almost higher object details and is visually prominent to easily distinguish the scanned objects more accurately,"The computed performance metrics (viz., MAE, LIF, EMEE, AME, AMEE, UQI, SSIM, Q/FS, and Q/IFS) obtained with DIEM are presented in   and   (listed in bold)","These are also illustrated graphically in  
                         (b)",It is seen that DIEM is well suited for the task of contrast enhancement,It observed that DIEM is statistically more distinct from all the baseline techniques in subjective and/or objective evaluation,"Consequently, the quantitative and qualitative comparisons demonstrate that the proposed DIEM method provides better performance than the baseline methods","Thus it is beneficial for differentiating and highlighting objects in dronograms and assist in further operations in advanced image processing (e.g. object detection, classification and etc.)","A lot of images in ‘OpenDroneMap’ (ODM) are complex enough for enhancement purposes as they contain small objects (viz., men, small trees.) in drone images",These images show better enhancements when considered for DIEM technique,This shows that the DIEM algorithm is good enough for such challenging cases in drone image enhancement,"In summary, the proposed DIEM method outperforms the baseline enhancement techniques as specified by the subjective and objective evaluations",The proposed DIEM method offers better contrast and visually true results mainly for tough-to-enhance drone images such as those underexposed with obscure images,"Moreover, the stability and consistency of an enhancement algorithm are more important to perform a constant enhancement for all images, especially for the low illuminated cases","Our experiments can be summarized with the following observations: 
                      4.6 Time complexity analysis To compute the execution time, we carried out an extensive assessment with a larger drone database with image size 512 × 512","The average run time of the different baseline methods for each image is listed in  
                         ",We use   on   systems with core  3 processor 3.20   and 16  ,"Almost, the computational time varies depending on the image resolution and the number of bins existing in the histogram of the processed image","Thus, if the resolution of the image increases, then the computational time for the histogram and grey-level transformation will be raised","From the experiments, we have observed that MIEF takes little time as it involves unidirectional fuzzy mathematical function using the hyperbolic criterion","Moreover, MIEF takes slightly more time than the proposed algorithms as MIEF is primarily concerned with restricted equivalence functions and hyperbolic membership degrees for foreground and background areas over the image pixels in ROI computation","FSE is a slow method because of the exhausted search, histogram computation and the need for computation of entropy","From the runtime analysis, the PLE computes the bi-histogram equalization, which takes considerable high execution time than MIEF","On the other hands, the RDH method takes more computation time due to its histogram computations via a reversible data hiding approach","Similarly, PLE and FSE methods require extra time because of their reiterative nature","In comparison, the proposed DIEM algorithm takes less time than MIEF, RDH, PLE, and FSE","Besides, RDH, PLE, and FSE only perform global contrast enhancement, while MIEF and DIEM carry out both local and global contrast enhancements",The average execution times of different baseline methods are shown in   for the all the test images with size 512 × 512 and it is obvious the   is very fast,"Therefore, the proposed algorithm more suited in real-time especially for low light drone imaging applications with regard to its accuracy and efficiency. 5 Conclusion I n this paper, a novel intuitionistic hesitant fuzzy set based image enhancement scheme is presented for lowly illuminated dronograms where a hesitant score is used as a new way to measure image uncertainty",The proposed method initially separates a dronogram into foreground/background areas based on a global threshold and determines the membership functions by intuitionistic hesitant fuzzification approach via hyperbolic operations for membership modification,"Finally, we achieve a highly informative and improved dronogram obtained by defuzzification",Results are compared with privileged methods and it is perceived that the proposed intuitionistic hesitant fuzzy set based method performs well and provides improved contrast images with better clarity and sharpness,"For the lowly illuminated drone images, the proposed enhancement scheme works better because of its capability of dealing with image uncertainties by taking recourse to the intuitionistic hesitant fuzzy set","In comparison to the state-of-the-art methods, the proposed method exhibits better performance to increase both the visual quality and contrast for dronograms","In future, the present algorithm can be improved from different directions for noisy and heavily blurred images with respect to speed up execution so that it can be applied to real-time environments","As a part of further investigation, an image enhancement model can be developed for several types of color combination drone images",The authors are currently working in these directions,"Supplementary material Supplementary material associated with this article can be found, in the online version, at doi: 
                   Appendix A Supplementary materials 
                      
                  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0142061518304319, 1 Introduction Partial discharge (PD) measurement is a powerful tool for monitoring and diagnostics of the insulation of high-voltage equipment,"Over the decades the technique has evolved significantly but the main challenge still remains the same: to classify, identify and recognize each of the active PD sources in the insulation that might be seen mixed or overlapped in the conventional phase-resolved PD patterns, causing identification problems","With modern digital PD measuring systems, a collection of features from the pulse shape can be extracted and then used for classification (or separation) and identification purposes",The main challenge is the selection of suitable features from the shape of the PD pulses,"In  , current peak I , charge Q and energy E were proposed and tested as suitable clustering parameters under the rationing that those parameters are correlated with the physics of the discharge and that they are more resilient to variations in the acquisition parameters of the digital recording card used for the measurements  ","Techniques such as the spectral power cluster  , equivalent time and bandwidth cluster  , frequency-energy characteristics of the pulses by Wavelets  , principal components analysis and t-Distributed stochastic neighbor embedding technique and morphological gradient of cumulative energy   have been widely researched for feature extraction and classification of PD sources","Pairwise plotting of these features, if properly tailored, results in data sets that can be clustered and in turn each cluster linked to a PD source","Literature commonly classifies the clustering techniques into five classes  : distribution-based algorithms, hierarchical-based algorithms, density-based algorithms and grid-based algorithms, the particular choice of one family or another depending greatly on the shape, density, anomalies and priori knowledge of the data sets  ","Density-based algorithms stand out as the most suitable family of algorithms for clustering PD data sets due to their remarkable ability of discovering clusters of arbitrary shape, without the previous knowledge of the existing number of clusters","Examples of density-based algorithms are DENCLUE (DENsity ClUstEring)  , OPTICS (Ordering Points to Identify the Clustering Structure)  , DBSCAN (Density Based Spatial Clustering of Applications with Noise)   and DPC (the density peak clustering)  ","DBSCAN and DPC are widely known and has been extensively researched to overcome their main drawbacks  : reduced accuracy with data sets having varied densities and high computational cost with large data sets; a minimum complexity of O( 
                      )","On the other hand, DPC has been prized for being fast and efficient since it finds high-density peaks in a non-iterative   manner and only one parameter has to be tuned  ","Joining others efforts to find improvements for the DPC method  , this paper takes the aim at tackling the issue derived from the assumption that the cluster centers in the data sets are separated by a relatively large distance",The knowledge of real world data sets extracted from partial discharge measurements brings up awareness of closeness between clusters,"Therefore, a method is proposed in which the PD data sets are first split into subsets by means of the smoothed density method (SD)  , and then each subset is passed to the DPC method",This partition of data sets is done based on the spatial density ρ,"Unlike DPC method, the SD method computes ρ from a two-dimensional histogram which limits the complexity of the whole algorithm",A final step involves undoing the partition of the data sets for which a solve-a-puzzle-like (SPL) routine is defined,"As the subsets are being put together back again, a criterion for distances between subset contours is applied resulting on the ability of the algorithm to discover clusters that are close to each other","In addition, the results that will be discussed in this paper show that this technique proves effective not only to discover clusters even when they have very dissimilar densities and scatters, but also our SPL algorithm makes the whole algorithm little affected by pseudo cluster centers found by the DPC method, also referred to as the “decision graph fraud”  ",The following parts of this paper are defined as follows:   describes the characteristics of the six data sets used to test the algorithms,These data sets as well as the Matlab implementation of the algorithm can be found in  .   provides a description of DPC and SD methods and the adjustments to the SD method to split the data sets into subsets.   defines the SPL routine and the concept of data contour,"Finally, the performance of the proposed algorithm is evaluated, and further comments are given in  . 2 Data sets For the purpose of this paper, the data sets were brought about from the values of charge (abscissa axis) and energy (ordinate axis) computed from actual PD measurements","The measuring set-up described in   was used to record the PD pulses and the values of charge and energy were computed by means of the software tool   
                       applying the methods described in  ","The data sets are shown in  
                      ","Since the source of the data sets was known, it was possible beforehand to identify and circle (when more than one) the clusters that exist in each of the data set","In addition, the data sets exhibit some particular characteristics such as: 
                   3 Density-based clustering methods The density peak clustering technique (DPC) developed by Alex Rodriguez and Alessandro Laio   is the method used in this paper","Since 2014 when the method was first published it has received tremendous attention among other reasons because of its relatively simplicity, moderate computation cost and large availability of implementation codes in several programming languages that makes it easy to try out the algorithm. 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   3.1 DPC method DPC is an approach based on the idea that cluster centers are characterized by a higher density than their surrounding neighbors and by a relatively large distance from points with higher densities  ","In this description, the density refers to the spatial density   and it is defined as the number of data points whose Euclidian distance   to the point   is not larger than a certain threshold  . 
                      In  , χ is the kernel density function or the weighting function for estimating the influence of a neighboring point","In this definition χ is of square type, meaning that all points within a radius   exert the same influence on the point  ",The study on the performance of the DPC method reported in   has suggested a Gaussian kernel   as a better density function,"As a result   becomes  . 
                      The second main step in the algorithm is to compute   that is the minimum distance between the point   and any other point of higher density. 
                         
                      Eq.   only applies when   is the point with the highest density","After solving  ,   the decision graph is constructed with the values of   as the abscissas and the values of   as the ordinates",In the decision graph only those points that have relatively large values of   and   turn out to be the cluster centers,The last step is to assign each remaining point to the same cluster as its nearest neighbor of higher density,"In the decision graph, the cluster centers are easily identified when the data set fits well the condition that the cluster centers are far away from each other","Besides, if the number of points in the data set is large, the results of the analysis are robust with respect to the choice of  ",Both the authors and others   have experimentally set the free parameter   in such a way that 1.6–2% of the data are contained in the resulting radius,"On the other hand, given the characteristics of PD data sets, especially the ones listed in (a) and (b) in  , it might be difficult to determine the cluster centers from the decision graph by its own","Just to illustrate this difficulty, let’s compute the decision graph for Set 2 and compare it to the case when the left cluster in Set 2 (circled in  ) is arbitrarily moved away from the right cluster as shown in  ","The color 
                          map in  
                          is the representation of the density  ","The original data in Set 2 led to the decision graph of  
                         a where only one points (square) is a cluster center candidate","When the Set 2 was modified, the second cluster center (circle), hidden in  a, appeared clearly as a second cluster center in  b",This occurs because the data set exhibits one of the bottleneck for the correct finding of cluster centers by the DPC method: clusters having large differences of densities and a relative short distance between them,The color map in   highlights that the density of the right cluster is so high that masks the density peak of left cluster,"In addition, if it happens that the two clusters are very close to each other, then it might be difficult to find the correct number of cluster centers","Since the characteristic (e) implies that clusters very close to each other will be common in PD data sets, then it is necessary to find a way to overcome the limitations of the decision graph illustrated in   and  ","In this paper, a method to achieve this task is proposed based on the SD method as described next. 3.2 SD method To overcome the intrinsic difficulty of the DPC algorithm to separate clusters with large density differences and close distances, a solution is proposed in which the data set is split into subsets, and then each subsets is passed to the DPC algorithm",Each subset comprises data points with density   within a certain range,"For example, in  , subset 1 might be the data points with   between 0.8 and 1",The limits of the range of   for each subset will be discussed later in  ,The calculation of   as per DPC method is costly as it requires the pairwise computation of a distance matrix,"In order to improve performance, the smoothed density method proposed by Paul H",Goeman in   is rather preferred to compute the density  ,This technique relies on a two-dimensional histogram instead of a distance matrix and therefore it improves computing speed with large data sets,The two-dimensional histogram approach means that the cluster plot is divided into a grid or bins and the number of points in each bin is the density of the points within such a bin,"Although fast, simply diving into bins may result in a non-optimal solution as that of in  
                         ","In other words, the mere histogram approach does not lead to the smoothed color map shown in  ","Let’s consider  
                          that corresponds to one column of the two-dimension density histogram shown as the color map in  ",It can be noticed that the choppy color map is rendered by the broken spline line ( ),Applying a smoothening function would render the more optimal density color map of  ,The SD method proposes to apply a smoothening function first to all the columns of the two-dimension density histogram individually,The resulting histogram is transposed and again the smoothening function is applied column-wise,This results in a smoothening in the two dimensions of the histogram,"In this method, there are two free parameters: the number of bins in the histogram and a factor λ ",The latter is a parameter of the smoothing function,The higher the λ the higher the smoothening and therefore the worse the fit to the original data,The authors in   have suggested 200 bins for the histogram and a value of 20 for λ ,"Our objective of splitting the data set into subsets makes the analysis little dependent on these parameters and therefore we also have set these values in our algorithms 3.3 Grouping data set based on density Once the smoothed density has been computed, then the points in the data set are split into subsets",Let X be the vector containing the data set and D the vector whose elements represent the density ρ of each point in X as per the SD method,"In order to split into n subsets, a pair of simple steps has to be followed: 
                      Now D  represents the density of each point in X with the particularity that there are only n discrete values","In  
                          it can be seen that grouping the data based on the now discrete values of density splits the data set into subsets",Splitting too many can result in subsets having a very low number of data points and a large scatter,Too few subsets and the wanted goal of having subsets far away from each other might not be achieved,"Extensive trial-and-error tests led to a suitable value of 4 for the number of subsets n. 4 Methodology for unsupervised cluster separation As described earlier, the DPC algorithm applied to data sets from partial discharge measurements not always leads to satisfactory results","The proposed workaround for this limitation is a series of steps as described next: 
                   Examples of splitting into subsets and then applying the DPC method are shown in  
                       and  
                      , where each sub cluster is rendered in different color in every decomposition level","For instance, sub clusters A, B and C are the output of DPC method applied to the first decomposition level (subset 1 out of 4) of data of Set 2 (see  )",Step IV involves determining if the two sub clusters (e.g,A and B in  ) belong to one same parent cluster,This is an iterative method that finds the points in A that are the closest points to all the points in B,This procedure is repeated but this time finding the closest points in B to all the points in A,If it happens that A and B belong to one same parent cluster then the two sub groups of closest points form contours that are close to each other,The former procedure may be compared to putting together a puzzle where the player evaluates first if the contours of two pieces match to determine whether they are to put together or not,"Here, the distance between two contours has to be lower than a certain threshold so that the respective two sub clusters are considered that belong to one same parent cluster","Since data is normalized, the proximity threshold has been set to 0.025","The contours for the sub clusters labelled A, B and C in   is illustrated in  
                      ","From   it can be seen that A and B belong to one same cluster, but not A and C","However, B and C do belong to one same cluster","Therefore, D = (A ∪ B) ∪ C",This new sub cluster D is added to the sub clusters in the decomposition of level 2 and the routine to determine the contours is applied again,"This iterative method is applied up to the decomposition of level 4. 
                       is a good example to show how the method can lead to find two clusters as the final result","In the decomposition level 1, both clusters belong to one parent cluster, then C = A ∪ B","In the decomposition level 2, C (brought from decomposition level 1), D and E belong to one same parent clusters, then F = C ∪ D ∪ E, but G does not form contours with any other sub cluster","This way, each sub cluster in the decomposition level 3 will form contours with F or G that are the cores of the two existing clusters in data set 3. 5 Results The results of applying the methodology described in the previous sections are shown in  
                       and the accuracy of our method and the DPC method is contrasted by visual inspection given that the definition of clusters in the data sets is known from  ","The methodology in this paper successfully found the two clusters in the cases of sets 1, 2 and 3 and the single cluster in the Set 5","In the case of set 4 despite the existence of two clusters, our methodology only detected one and two clusters in set 6 where there only existed one","Looking closely at the results of the decomposition levels of Set 4 in  
                       it is observed that all the sub clusters up to decomposition level 3 form one single parent cluster, and the DPC algorithm fails to identify the two sub clusters necessary for the expected result highlighted in  : one cluster being the data points within the ellipse and the second cluster all the remaining data points","However, in this case, this second cluster is formed by much less data points and with much larger scatter compared to the data points in the first cluster, which explains why the DPC algorithm identifies only one cluster",The scatter and the number of points in the data set also hinder the detection of interfaces between sub clusters,"For instance, cluster B found in Set 6 is clearly part of the main cluster A, but it is found as a second cluster because of the large scatter","Increasing the threshold for the distance between two sub clusters eventually will result in assigning clusters A and B to the same parent cluster, but also might result in assigning other sub clusters that do not belong to the same parent cluster reducing accuracy. 6 Conclusion The problem of source separation from charge-energy partial discharge plots was approached by combining the smoothed density method (SD) and the density peak clustering method (DPC), followed by a routine to group sub cluster based on a threshold proximity","Six data sets, from actual measurements of partial discharges, were used to test our methods leading to successful separation of the actual sources in most of the cases",The splitting of the data by means of the SD method presented the advantage that it limits computational load by computing the density ρ with help of a two-dimensional histogram and not by calculating the Euclidean distances matrix as is required by the DPC method,"The grouping of sub clusters by defining a threshold for the proximity between sub cluster contours proved effective to discover cluster that are close to each other because the partition of data sets inherently has the effect of increasing the distance between subsets, increasing, in turn, the likelihood of the DPC method to find the correct number of clusters","However, the proposed methodology contains an extra free parameter: the threshold for the contours between two sub clusters that adds to the uncertainty of choosing cluster centers by the decision graph in the DPC method","Nevertheless, the methodology becomes stronger against this free parameter as the number of points in the data set increases",The “solve-a-puzzle” strategy of our method with threshold proximity as criterion in fact downplays the issue of finding pseudo cluster centers because they can be merged back together if they share contours closely.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0933365716302950, 1 Introduction In 1960 the United States spent 5.2% of gross domestic product (GDP) on healthcare and by 2004 that number has risen to 16%,Current predictions have healthcare expenditures exceeding 18.7% of GDP today and forecast to increase to over 20% by 2025,"There is no doubt of a healthcare crisis unraveling in the United States and in other parts of the world, driven by the rapid increase in the prevalence of chronic diseases such as diabetes and kidney disease","In addition to increasing prevalence of certain diseases, this increase in health care costs can be attributed to new and prohibitively expensive therapeutics for cancer and other disorders, and ever escalating costs of patient mismanagement and malpractice",It is apparent that the returns from the current paradigm of innovation in healthcare have diminished dramatically,Traditional approaches of hypothesis driven clinical research are becoming less effective in translating empirical data to successful health outcomes,"Given the rapid growth in the rates of accumulation of healthcare data, every vertical of the global healthcare industry from therapeutics research and development to epidemiology and precision medicine would benefit from data driven, unbiased approaches",Mathematical and statistical tools developed in the field of artificial intelligence (AI) and machine learning are well poised to assist clinical researchers in deciphering complex predictive patterns in healthcare data,"With government incentives offered to clinical organizations to transition from paper based patient information to well-structured and managed digital form, there has been a tremendous explosion in the availability of patient-centric healthcare data",Such data can be leveraged to open up new avenues in advancing healthcare by improving patient care and creating new efficiencies in delivering care  ,Understanding variation in treatment outcomes due to patient specific molecular and clinical factors   is essential to the practical implementation of and adoption of precision medicine in clinical practice,The predominant focus of the analytical efforts in health care has been structured management and compilation of disparate data modalities and data mining with the objective of testing hypotheses generated through reasoning,"These efforts are limited by the current thinking in medicine and biology, and often ignore unbiased, observational and “unknown biological” evidence",A data-driven hypothesis generation approach creates a paradigm shift in healthcare by leading to discovery of new and often surprising trends in clinical outcomes,"Identification of non-obvious correlations and causal relationships may lead to a significant mitigation of side effects, need for additional medications, reduction of hospitalizations, decrease in unnecessary care, loss of income for patients, and eventually in the overall change in clinical practice","This study demonstrates how high-level, publicly available healthcare data, in particular, data coming from the United States Centers for Medicare and Medicaid Services (CMS) can be leveraged to generate profound and surprisingly powerful hypotheses in patient management and disease outcome by employing advanced learning methods such as Bayesian networks   (BNs)",BNs or Bayesian artificial intelligence is a mathematical framework for learning probabilistic cause and effect relationships directly from data,The factor or variable relationships learned as directed acyclic graphs,Learning BNs in large data sets is typically an NP-hard optimization problem where an enormous number of possible graphs are attempted in a heuristically driven process of finding locally optimal structures,"In this case study, the Diagnosis Related Group (DRG) codes are the factors and the provider specific discharge numbers are the observations",The approach presented here is a data analytics pipeline developed to work with population claims data on the   scale,A network of cause and effect relationships between medical diagnoses was learned in a purely data-driven manner from previously published billing data from CMS,"A follow up network analysis revealed novel comorbidities, disease progression factors and the sequence of disease diagnoses for a number of chronic diseases",The findings were confirmed by subsequent association-based statistical analysis using logistic regression models,BNs are a powerful tool that can be readily used in the analysis of big data and presents a dynamic approach to the hypothesis driven epidemiological research,These newly discovered non-trivial relationships might have a significant impact on clinical disease management,"The increased utilization of technologies such as AI, in healthcare research, is likely to have a significant impact on clinical disease management and global healthcare efficiency. 2 Methods The methods overview and the data analysis workflow are shown in  
                      ",Data was obtained from publicly released files from the CMS website  ,"Top 100 most frequently billed diagnosis codes in the year of 2011 for a majority of healthcare providers in the United States were assessed in the data along with the high level provider information, amount charged and amount paid to the provider by the Medicare insurance plan","The wide discrepancy of the provider charges across the United States had been analyzed, presented, and discussed by the media  ",The focus of this analysis was the relationship between the annual incidences of diagnoses across the largest hospitals in the United States,"In this data set, the diagnosis of the discharged patients was represented by the DRG codes","Fields containing information about the DRG codes, total number of discharges in 2011 for each diagnosis and the provider code were extracted from the dataset",The initial step in the data processing workflow was to organize the discharge count information as a data matrix of DRG codes across provider codes,"Next, the DRG codes missing in more than 70% of the providers were removed from further analysis along with the provider codes not reporting any patient discharges for more than 25% of the DRG codes in the data set","After the filtering procedure, the dataset consisted of 100 DRG code discharges across 1618 hospitals",The total number of discharges for each provider was used to normalize the number of discharges per a DRG code,"In the follow up analysis, it was assumed in the case of a missing data point that no patients were discharged with that diagnosis code from the facility in 2011","Therefore, the remaining missing data were imputed as zeros",Median polish normalization was performed on the data matrix and the data was transformed to the logarithmic scale,All data processing steps up to this point were performed using R ( )  ,Details of data processing are discussed in Supplementary file 1,"The processed and normalized data matrix was analyzed using the BN learner, bAIcis™ (Berg LLC, Framingham, Massachusetts, U.S.A.)",An ensemble of 20 BNs was learned directly from the data using stochastic optimization methods: Each network in the ensemble was created by optimizing the global network structure with respect to the network Bayesian information criterion score from graph families represented by a single linear regression model with up to four regressors,Each DRG code was modeled as a node in the network with the normalized number of discharges across the provider data treated as the variable observations,A final summarized BN was created by performing in silico interventions in the ensemble for all learned node relationships  ,Such simulations are helpful in determining the strength of the cause and effect link and reducing the number of spurious connections and cross-correlations,"In the final step, a confidence metric was assigned to each reported structural relationship in the learned BN ensemble based on the c-statistic measuring the difference in the posterior distributions of the baseline and intervention data","Namely, each driver node in a structural relationship was perturbed 10 fold relative to its baseline mean a 1000 times for each of the ensemble networks",The two posterior distributions for the dependent node (the baseline and the perturbed) were compared using the c-statistics or the area under the receiver operating characteristic curve,Relationships in the resulting DRG code network ensemble were retained if the strength of the connection measured by the c-statistic was larger than or equal to 0.8,Nodes representing related DRG codes were combined,For example: DRG codes that only vary based on presence of complicating conditions,The resulting network ensemble was visualized using Cytoscape ( )  ,"Table with results of simulation for the selected sub network are shown in supplementary table A. 3 Results and discussion The DRG code network learned from the CMS data ( 
                      a) connected 61 DRG codes in a network of 178 connections","If two nodes are connected in the network, it means that a change in the number of discharges for one DRG code affects the number of discharges for the second DRG code with the arrows indicating the directionality of the cause and effect relationship",Interpretation of the meaning of these connections depends on specific diagnoses involved,"For example, when the source node is diabetes and the target node is hypertension, the connection likely represents comorbidities at a population level","When the source is diabetes and the target is neuropathy, the connection likely represents complications from disease",Subnetworks centered on ‘heart failure & shock’ and ‘renal failure’ were selected ( b) for further exploration,These diagnoses were selected based on the cause of death index compiled by the Centers for Disease Control and Prevention  ,"According to this report, heart disease was the top cause of death in 2011","Kidney related conditions were ranked 9, 12, and 13",All renal conditions combined were noted as major causes of death,"Therefore, the ‘heart failure’ and the ‘renal failure’ subnetworks were analyzed in detail","The goal of the subnetwork analysis was to classify the connections in the subnetwork as known or predicated on information in support of the cause and effect already well established in the clinic, or novel with little to no reference in the scientific or medical literature",Presence of known connections in the networks demonstrates that relationships inferred statistically in a BN structure are valid and can be confirmed by clinical or epidemiological research,"Thus, novel connections can be considered to be the basis for formulation of new hypotheses generated in a strictly data driven manner","Based on the analysis it was found that many connections in the ‘heart failure’ and ‘renal failure’ subnetworks were known while novel interactions were also identified, in particularly, in the ‘renal failure’ subnetwork. 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   3.1 Associations correlated by medical literature Heart failure is a plurality of conditions that reduce the ability of the heart to pump blood efficiently","These conditions include congenital heart defects, arrhythmias, coronary artery disease that narrow arteries over time and high blood pressure that in combination with heart muscle unable to effectively pump blood can compromise the blood flow in the body",The interaction between ‘heart failure and shock’ and ‘simple pneumonia and pleurisy’ was removed from the subnetwork analysis because of inconsistencies in the interaction between these diagnosis codes in the ensemble of networks,"Diagnosis relationships discovered from data in this study within the ‘heart failure’ subnetwork are explored below: 
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                         
                      3.1.1 Chronic obstructive pulmonary disease Pulmonary hypertension is a well-established complication observed in patients with chronic obstructive pulmonary disease","In situations when the lungs try to compensate for low oxygen concentration in blood due to inadequate lung ventilation, the compensatory effect is the observed increase in blood pressure inside the lungs",Increase in blood pressure inside the lungs (pulmonary hypertension) leads to increased stress on the right ventricle that can eventually cause heart failure. 3.1.2 Cardiac arrhythmia and conduction disorders These disorders could result in heart failure,Therefore diagnosis of heart failure correlates to diagnosis of causal conditions discovered through the network,"Based on output from the data analysis, it can be surmised that potential increase in the number of heart failure diagnoses will correspond to the increase in the number of diagnoses of cardiac arrhythmias and conduction disorders in the same patient population. 3.1.3 Gastrointestinal hemorrhage Heart failure can be a result of hypovolemic shock",Hypovolemic shock occurs due to rapid loss of large blood volume that can result in the failure of multiple organs including the heart,"The most frequent causes of hemorrhagic (hypovolemic) shock are trauma, gastrointestinal hemorrhage and organ injury. 3.1.4 Renal failure The link between anemia, cardiac problems and renal disease is well known  and the challenges of treating patients with cardio-renal insufficiencies are well documented  ","About one-fourth of the patients with renal disease have congestive heart problems and as renal disease worsens, the fraction of patients with heart disease increases to about 65–70%  ",Large studies have shown that worsening kidney disease is associated with higher mortality and hospitalization rates in patients with previous diagnosis of heart failure  ,"Thus, there is significant evidence supporting the relationship between heart failure and renal failure. 3.1.5 Acute myocardial infarction Heart failure is known to be a frequent complication of myocardial infarction   and hence is a well-established connection",This interaction has been well studied through epidemiological studies and it is also known that the rate of heart failure after myocardial infarction decreases with time  ,The diagnosis code corresponding to ‘renal failure’ in this analysis encompassed chronic/acute kidney failure and other renal disorders,Renal failure/insufficiency refers to reduction in the kidney’s ability to remove waste products from blood,More than 10% of adults 20 years or older have chronic kidney disease (CKD)   The cost of treating CKD is very high because of the wide range of comorbidities and quality of life factors  ,One study showed that the cost for treating end stage renal disease is continually increasing and CMS costs for this condition have reached 30 billion dollars for 2009  ,"Known medical relationships within the renal failure subnetwork are explored below: 3.1.6 Kidney and urinary tract infections This diagnosis code encompasses many types of renal infections including cystitis, abscess, lower and upper urinary tract infections","Recurrent or untreated urinary tract and kidney infections are known to be linked with kidney scarring which contributes to lowered kidney function and renal failure  . 3.1.7 Disorders of nutrition, metabolism, and fluids/electrolytes Kidneys play an important role in maintaining the fluid and electrolyte balance","Therefore it is possible that a diagnosis of renal failure would lead to follow up tests and diagnosis of nutrition, metabolism, fluid and electrolyte balance problems. 3.1.8 Simple pneumonia and pleurisy It is established that chronic kidney disease increases susceptibility to infections and that incidence of pneumonia in chronic kidney disease is a well documented infectious complication associated with kidney disease  . 3.1.9 Hypertension Hypertension or high blood pressure is known to be the second leading cause of kidney failure",It is thought that high blood pressure damages blood vessels in the kidneys hence reducing the ability of the kidneys to remove waste products from the blood  ,The above investigation of the medical literature vis-à-vis the cause and effect relationships discovered by the BN learner confirms the validity of application of the AI based mathematical methodology to data driven discovery of medical outcome patterns from high level billing data. 3.2 Discovery of a novel relationship in a patient population: linking asthma to renal failure The ‘renal failure’ DRG code was inferred to interact with the ‘bronchitis and asthma’ diagnoses,There is limited information in the medical literature that supports a direct link between incidence of bronchitis/asthma and renal failure,"Thus, the asthma and renal failure relationship inferred from this study may be considered a novel discovery that can be linked to medication effects, disease comorbidities or the order of diagnosis based on the dataset used in the analysis",Specific diagnosis codes for this interaction were identified and a linear regression model was built for these variables to confirm the BN finding and identify the significance of the interaction,Significant statistical association between the number of renal failure diagnoses and the number of bronchitis/asthma diagnoses was demonstrated by conventional statistical analysis in the same data set with  -value of less than 1e-5 and the adjusted r  of 25%,"Moreover, the bronchitis/asthma factor was significant in the multivariate linear regression model controlling for the other predictors in  b with  -value 0.0014","In summary, a novel hypothesis was generated in an entirely data-driven manner: The incidence of renal failure may potentially be associated with asthma or bronchitis, or perhaps the asthma treatment strategies leading to a direct correlation of the number of asthma/bronchitis discharges to the number of renal failure discharges","Below is a summary description of the exploratory findings in the scientific and medical literature potentially linking asthma and renal failure. 3.3 Scientific evidence in support of the novel linkage between asthma and renal failure A clinical hypothesis and likely corresponding molecular mechanism is summarized in  
                         a and b summarizes potential molecular mechanisms linking the effect of drugs used in the treatment of bronchitis/asthma to their ability to potentially cause renal dysfunction and/or renal failure",Below is a detailed description highlighting the direct and indirect effect of β -adrenergic agonists on renal function and its potential to cause renal dysfunction and/or renal failure,"Bronchitis and asthma are respiratory diseases where narrowing of airways results in cough, shortness of breath, tightness in the chest and wheezing","There are many choices of medication currently available to treat the symptoms as well as key pathways involved in the etiology of asthma ( 
                         )",One of the oldest and most widely used drug classes for symptomatic treatment of asthma is β -adrenergic agonists that are referred to as bronchodilators  ,"As shown in  , 7 out of top 10 medications used to treat asthma contain either a long acting or a short acting β -adrenergic agonists. β - adrenergic agonists interact with the β  sub-type of the β-adrenoceptors","Most β -adrenergic agonist bronchodilators in asthmatic patients are primarily administered via use of a nebulizer to ensure maximal retention of the drug within the lung environment, the primary site of action","However, the rich vasculature within the lungs enables systemic absorption of the β -adrenergic agonists  ","Thus, the physiological and pharmacological effect of β -adrenergic agonists can be manifested systemically in multiple organ systems including heart, skeletal muscle, kidneys expressing receptors  ",This study explores direct and indirect effects of β -adrenergic agonists that might potentially lead to renal dysfunction,Most of the β -adrenergic agonists used in treatment of asthma are primarily removed from the circulation in the kidneys  ,The kidneys have widespread expression of the β-adrenergic receptors within the glomerulus and tubular epithelium influencing the hemodynamics and electrolyte balance  ,Electrolyte imbalances have been noted in asthma patients treated with β -adrenergic agonists  ,The β -adrenergic receptor has been demonstrated to regulate the sodium linked transporter expression   and it has been suggested that it is involved in salt sensitive hypertension  ,"Furthermore, β-adrenergic receptors are known to influence the synthesis and secretion of renin  ",The fact that β -adrenergic agonists do not undergo significant biotransformation prior to excretion suggests that the drugs could exert pharmacological effects in the kidneys during the process of excretion,One of the side effects associated with circulating β -adrenergic agonists such as albuterol is its ability to reduce circulating potassium levels and lead to hypokalemia  ,"Indeed, hypokalemia is a well-defined and often reported electrolyte imbalance observed in asthmatic patients on β -adrenergic agonists  ",Persistent hypokalemia can lead to renal dysfunction and tubulointerstitial diease,"It has been shown that in rats, hypokalemia induces renal injury   and, in humans, causes renal failure  ","In a study of 55 patients, it was shown that chronic hypokalemia was accompanied by renal cystogenesis that resulted in scarring and other kidney damage leading to renal insufficiency  ",Other studies have also shown that hypokalemia in patients with renal disease increased the rate of progression to end stage renal disease and also increased the rate of mortality  ,Other β-adrenergic agonist effects on renal function include their ability to influence renin release,Studies have demonstrated that β -adrenergic agonists tend to activate the renin-angiotensin system (RAS) leading to increases in both the circulating plasma renin and angiotensin II  ,Literature documenting the role of RAS inhibitor in conferring protection against chronic kidney disease and renal failure is extensive  ,"In addition to its ability to induce hypokalemia, β -agonists mediate increase in aldosterone, and consequently angiotensin II",Aldosterone is a key component of RAS and leads to the adrenal gland secretion of angiotensin II generated within the RAS pathway,Activation of RAS and its components such as aldosterone and angiotensin II has been linked to renal dysfunction  ,This hypothesis is further substantiated by the observation that blocking of aldosterone function is associated with improvement in kidney function  ,The above evidence supports a role for β  adrenergic agonists in the potential etiology of renal failure in patients with asthma,"Indeed, there is recent evidence in the literature identifying higher incidence of chronic kidney disease in patients with asthma although the cause is unknown  . 3.4 Impact on healthcare economics and challenges If the novel link between asthma medication and renal dysfunction identified from this study is confirmed through rigorous epidemiological research, approximately 25% of the diagnoses of asthma/bronchitis may be associated with potentially undesirable drug side effects in certain individuals","Further investigation and modification of treatment guidelines for asthma will not only lead to better patient care but also to considerable cost savings given the cost of annual treatment for a patient with end stage renal disease is estimated at around United States Dollar (USD) 70,000  ","Furthermore, clinical intervention will result in significant saving to the CMS that reimbursed USD 2.3 billion for the renal disease treatment shown in the studied data across 3000 hospitals in the United States enrolled in the Inpatient Prospective Payment System","Given that healthcare costs have been skyrocketing in the recent past and containing costs is one of the major concerns in public health, new knowledge generated using methods such as those presented in this paper are invaluable",Novel insights into treatment complications using AI based methods will contribute to rapid expansion of medical knowledge and improvements in patient care. 3.5 Future direction The correlation identified in this study would not be readily identified in the current clinical research settings or analyses from literature,This is because the interaction between a treatment of asthma and renal failure is non-obvious and is not currently subject of widespread studies; there has been little evidence until recently  ,"Herein, we propose some future directions for this research",The first step would be to use patient level diagnosis and pharmacy data to further test this hypothesis,"With patient level diagnosis information, it will be possible to strengthen the link between diagnosis of asthma and diagnosis of renal disease",Use of pharmacy data in addition to diagnosis information will enable gathering more evidence that β -adrenergic agonist use is linked to loss of renal function,A combination of diagnosis and pharmacy data should be to study the effect of blood pressure medication such as non-specific β-blockers and angiotensin converting enzyme (ACE) inhibitors,This interaction will provide another pathway to strengthen the link between asthma drugs and renal complications since non-specific β-blockers and ACE inhibitors have the reverse effect of β -adrenergic agonists,Validation of this hypothesis provides the foundation for incorporating routine checks on kidney function into the clinical practice for the treatment of asthmatic patients,"Using longitudinal patient level data and genetic predisposition, one could develop a risk model to identify patients at high risk of renal damage and divert those individuals to other treatment options such as corticosteroids, leukotriene modifiers, mast cell stabilizers, anticholinergics, and others","Such treatment strategies can be incorporated into the clinic, through clinical decision support systems, by integrating patient electronic health records with a knowledge base, to provide truly personalized patient care. 4 Conclusions AI mathematical methodology is a powerful set of tools in data science","The application of BNs to high level billing data from CMS has demonstrated its utility in uncovering non-obvious relationships in the data, in particular, a potentially critical interaction between β -adrenergic agonist asthma medications and renal dysfunction",This hypothesis generated purely from data gained more validity after a thorough review of the relevant medical literature,The review showed it is likely that the newly discovered causal link is real,A confirmatory epidemiological study is necessary to implement this finding in clinical practice,"Results presented in this study lend strong support for the use of hypothesis free, data-driven methodology in “big data” approaches to healthcare research and management",Such methods are complementary to classical epidemiological research and may be able to provide the researchers with non-obvious and unconventional leads that are challenging to focus on in a hypothesis driven experimental design and analysis,"The new perspective presented here creates an opportunity to make significant progress in personalization of the treatment pathways for some of the most prevalent diseases such as diabetes, hypertension, congestive heart failure, and cancer","In conclusion, the work presented here provides a rationale for employing use of Bayesian artificial intelligence algorithms for the analyses of disparate healthcare, socioeconomic, demographic, genetic, and even data from wearables to advance medical research thereby improving patient outcomes and reducing treatment costs","Appendix A Supplementary data Supplementary data associated with this article can be found, in the online version, at  ","Appendix A Supplementary data The following are Supplementary data to this article: 
                      
                      
                  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0306437915000459, 1 Introduction Business process compliance emerged as hot topic in research during the last few years,"In essence, several approaches have been developed to formally and (semi-) automatically prove that business processes comply with relevant constraints such as regulations, laws, or guidelines",An example constraint from the medical domain would be “The patient has to be informed about the risks of a surgery before the surgery takes place”,"In practice, compliance checks are often conducted manually and hence perceived as a burden  , although their importance is undoubted",The need to check for compliance of business processes based on a set of constraints may emerge in different phases of the process life cycle  ,"During design time, the compliance of a process model with a set of constraints is checked","At runtime, the progress of a potentially large number of process instances is monitored to detect or even predict compliance violations","For this, typically, terms such as compliance monitoring or online auditing are used","Finally, processes can be diagnosed for compliance violations in a   or offline manner, i.e., after process instance execution has been finished",This paper is dedicated to compliance monitoring as this is crucial for the timely detection and prediction of compliance violations as well as for the provision of reactive and pro-active countermeasures on compliance violations  ,"Further, in realistic settings, the existence of a complete process model for compliance checks cannot always be assumed","In fact, business processes are often implemented in a rather implicit manner and executed over different information systems (e.g., Enterprise Resource Planning (ERP) or Customer Relationship Management (CRM) tools) as depicted in  
                      ","Although there are similarities between design time/  analysis and compliance monitoring (see also  ), this paper will focus on the latter in order to provide a clear scope","Typically, compliance requirements on business processes stem from different sources such as laws, regulations, or guidelines that are often available as textual descriptions","An important task towards compliance monitoring is the interpretation of these requirements as compliance objectives and the subsequent specification as compliance rules or constraints (note that, in this paper, we will use both terms interchangeably)","As shown in  , the specified compliance rules will be verified over the process execution events","The results of compliance monitoring can be visualized and reported back to users in different ways, ranging from notifications on violations to fine-grained feedback on reasons for violations, or even the prediction of possible and unavoidable future violations","In general, compliance monitoring approaches are driven by two factors: (1) the   that is used to specify the compliance requirements and (2) the   the compliance checks are based on","Due to the possible heterogeneity of the data sources employed, an integrated target event format is desirable. 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   1.1 Problem statement There is a overwhelming body of literature on business process compliance",The approaches address different phases of the process life cycle and often propose different languages,These are used to formally represent the constraints to be checked on the business processes,"Overall, it is hard to oversee and compare the already existing approaches and hence, the decision of which approach could be utilized for which kind of problem is hampered","Hence, the main challenge tackled in this paper is to provide proper means for comparing approaches for compliance monitoring in business processes in a systematic way",This challenge will be addressed by the following four research questions,"The first one refers to the challenge of identifying approaches for compliance monitoring and to distinguish them from approaches that provide design time compliance checks,   conformance checking, compliance checking architectures, or mention compliance monitoring as an important building block (  
                          1 ( 1):  )",This is important to provide the study with a clear focus,"The second research question fosters the derivation of a set of typical functionalities required in compliance monitoring approaches and practice (  
                          2 (RQ2):  )",RQ2 meets the challenge of granularity and coverage,"The challenge of how to demonstrate the appropriateness of the identified functionalities is picked up by the third research question (  
                          3 ( ):  ?)","The fourth research question asks for the applicability of the functionalities (  
                          4 ( ):  ?). 
                          discusses the applied methodology and gives an overview on how RQ1 to RQ4 will be tackled in this paper. 1.2 Research methodology The goal of this paper is to define a framework for Compliance Monitoring Functionalities (CMF) that enables the systematic comparison of existing and new approaches for monitoring compliance rules over business processes during runtime",Specific challenges for eliciting the CMFs are the multitude of existing approaches in the area of business process compliance and the decision of which functionalities are required in real-world scenarios,"In order to address these challenges, we apply the methodology depicted in  
                         ","The methodology consists of three phases, i.e.,  ,  , and   of the CMF framework. 
                         : The elicitation phase follows the research methodology described in the context of elicitation of process change and time patterns  ","First of all,   are defined that scope the research done in this paper (  RQ1 and RQ2)","As overarching selection criteria, we focus on: 
                      The elicitation phase includes a systematic literature review described in   and an analysis of five case studies from different domains introduced in  ","The CMF identification is based on the results of the systematic literature review and the case study analysis (  RQ2) and possibly illustrated by additional examples. 
                          2— : The CMF design itself is presented in  ","Each CMF is described using a CMF template and illustrated by examples taken from literature or case studies (  RQ2). 
                          3— : In order to demonstrate the appropriateness of the CMF design, existing approaches are classified along their support for the CMFs (cf.  )","Moreover, constraint patterns as suggested by the literature are compared to the CMFs proposed in this paper (  RQ3)","Finally, to illustrate the application of the CMF framework, compliance rules are extracted from two realistic and publicly available data sets",Based on the extracted rules and the data sets we showcase the application of compliance monitoring for business processes in existing tools (cf.  ) (  RQ4),"Note that the data sets for CMF application are different from the case studies utilized for CMF elicitation. 1.3 Contribution In this paper, we address research questions RQ1 to RQ4 as stated  ",The core of the approach is a framework of functionalities that are relevant in the context of compliance monitoring,These   are denoted as   for short,The   (  for short) shall enable a systematic comparison of existing as well as new approaches on compliance monitoring in business processes,"In summary, the main contributions of this paper are 
                      The   and   annotations of selected contributions highlight the extensions made on the previous EDOC 2013 conference paper  ","Here,   means that the contribution is based on   and   means an extension of the particular contribution when compared to  ",Contributions without annotations are entirely new,The remainder of the paper is structured as follows,"In  , a short introduction to compliance monitoring in the context of process mining is provided.   describes the research methodology followed and discusses approaches that are closely related to compliance monitoring.   describes our CMFF composed of ten CMFs","In  , we focus approaches to concretely support CMFs","First of all, a comparison of the CMFF with pattern-based approaches is presented","Furthermore, existing compliance monitoring approaches are classified using the CMFs","Finally, the application of the CMFF based on two realistic data sets within three selected tools is showcased.   concludes the paper. 2 Compliance monitoring in the context of process mining The basic idea behind process mining is to discover, monitor and improve processes by extracting knowledge from data that is available in today׳s systems  ",The starting point for process mining is an  ,"XES (eXtensible Event Stream)   has been developed as the standard for storing, exchanging and analyzing event logs","Each event in a log refers to an activity (i.e., a well-defined step in some process) and is related to a particular case (i.e., a process instance)",The events belonging to a case are ordered,"Hence, a case can be viewed as a sequence of events (i.e., a trace)","Event logs may store additional information about events such as the resource (i.e., person or device) executing or initiating the activity, the timestamp of the event, or data elements recorded with the event",The reference framework presented in   gives an overview of the process mining spectrum,The event logs are partitioned into two kinds:   and   logs refer to current process instances that are ongoing;   refer to historical process instances that have completed,The framework also distinguishes two types of models:   and  ,"A   model is normative, i.e., it specifies how things should be done or handled","A   model is descriptive and its goal is not to steer or control reality; instead,   aim at capturing reality","Moreover, ten process mining related activities are identified in  , which can be grouped into three categories:   including discover, enhance, and diagnose;   including detect, check, compare, and promote;   including explore, predict, and recommend",Compliance monitoring corresponds to the   activity as defined in  : the   models would serve as representations of the compliance constraints and are hence used to analyze the   logs,"Diagnostics are provided to the user if the behavior in the logs is in some way different from the one specified in the models, i.e., detects deviations at runtime (auditing)  ",An enabling technology for the   activity is  ,Conformance checking and related techniques are introduced in   together with a discussion on their differences to compliance monitoring approaches. 3 Compliance Monitoring Functionality (CMF) elicitation The elicitation of the CMFs corresponds to   depicted in  ,"To this end, we systematically reviewed literature ( ) and analyzed five case studies ( ). 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   3.1 Literature review After defining the selection criteria, the first input for the elicitation of CMFs is compiled from a systematic literature review","With some adaptations, we follow the procedures for systematic literature reviews as proposed in  . 
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                      3.1.1 Research identification The search for the primary literature is driven by   (cf.  ).   can be met by defining the following selection criteria for the search: (a) compliance monitoring on business processes and (b) constraints that refer to process activities",This will be reflected in the keywords searches as well as in processing the literature found. 3.1.2 Selection of primary works (horizontal search): The search was conducted using   (last access 20 February 2014),"In a first step, keywords were searched in the titles of the papers, excluding patents and citations.  
                             summarizes the results of the horizontal search","It states the searched keywords in the first column, the number of hits in the second, the number of selected papers of the primary search in the third, and the criteria for selecting these papers in the fourth column","If, for example, a paper containing keywords   in its title was found, we checked whether this paper refers to business processes as well","Overall, we aimed at finding all papers that combine the aspects  ,  , and  ",This was important to guide the horizontal literature search while keeping a clear focus,"Note that more keywords and combinations were checked than stated in  , however, these keywords and the resulting references are only displayed if at least one paper was found and selected","Overall, the horizontal literature search resulted in 70 references out of 1605 hits","The respective list of references is available at  . 3.1.3 Processing of primary literature list The list of references resulting from the horizontal search was evaluated in a first round excluding papers that Obviously, the first filter criterion led us to loose design-time approaches that can be potentially lifted to runtime analysis","This is the case, for example, of  ","On one hand, we wanted to isolate as sources for our investigation only those approaches that are natively developed for dealing with compliance monitoring","On the other hand, we believe that the CMF framework here presented can serve as the basis for assessing in which design-time approaches are indeed apt to be used at runtime","For the backward search, the reference lists of the papers presenting a compliance monitoring approach for business processes were analyzed","Moreover, we identified papers that provide surveys on business process compliance approaches such as Becker et al.   and validated our search results against these articles by comparing the references","This resulted in adding one reference, namely  ","Moreover, the backward search led to interesting references in the context of Web Services, i.e.,  ","Here it is important to distinguish approaches that concentrate on SLAs (and are not further considered) and approaches that apply compliance monitoring at a process level (i.e., based on a Web Service orchestration) that should be considered","Finally, the backward search resulted in replacing primary paper   by more specific papers, i.e.,   as well as primary paper   by  ",This step was conducted using the name of the provided tools (MONPOLY in the case of   and Dynamo in case  ) and by going through the papers of the authors,"In summary, we extended the primary list by   references",The systematic literature analysis resulted in 60 papers,"They were analyzed and synthesized as described in the next section. 3.1.4 Data synthesis The results from the first and second round of the literature review were analyzed in two rounds; first of all, by assigning each paper to a researcher, followed by a group discussion on all 60 papers","For each paper, it was checked whether it 
                         Categories 1–5 are processed in the course of the paper as follows (if none of the categories applied then the paper was discarded from further processing): 
                         In the remainder we first discuss Categories 3–5, before describing the CFMs","This helps to position our work. 3.1.5 Enabling technologies and related techniques We briefly survey research approaches that, although not directly focused on compliance monitoring, can be used either as enabling technologies or as techniques for tackling this problem","At first, relevant line of research is concerned with  ","Although the usage of the two terms of   and   is not homogeneous in the literature, in the Business Process Management (BPM) setting conformance checking is typically understood as the problem of comparing an existing process model with an event log, so as to understand how far the event log reflects the behavior set out by the process model and, in the case of discrepancies, to measure to what extent they diverge","A number of approaches have been proposed to tackle this problem, see, e.g.,  ","The two main differences between conformance checking and compliance monitoring are the kind of model used to analyze the logs (conformance checking usually involves a complete model of the process), and the tackled phase in the process lifecycle (conformance checking is typically applied post mortem)  ","Despite these two differences, there is a lot of potential in the interaction between these two areas","In particular, observe that many of the conformance checking techniques could be actually lifted to runtime","Furthermore, the fine-grained comparison metrics used in conformance checking to assess how much the input model and log deviate from each other have the potential to extensively contribute to CMF 10 (cf.  )",A second group of approaches that are related to compliance monitoring is that of  ,"In broad terms, stream data management focuses on the management of data received as a continuous, real-time sequence of items  ",The relevance of stream data management for compliance monitoring covers both the querying and the event gathering aspects,"As for querying, part of stream data management deals with query languages, techniques, and tools for stream data, to suitably mediate between the expressiveness of queries and the efficiency of answering","Since such queries are posed against dynamically acquired data, they usually involve temporal operators that can be used to compare and correlate data across time","In this light, those approaches that tackle compliance monitoring by just analyzing the trace of events accumulated so far, without reasoning on the possible future outcomes, can be seen as a special form of query answering over stream data, where the data stream delivers data about the monitored cases, while compliance rules are formulated as special queries",We consider the investigation of these synergies as one of the most interesting lines of research for the future,"As for event gathering, stream data management comes with principles, techniques, and tools for processing a stream of (raw) data produced by multiple, possibly heterogeneous sources, so as to extract, analyze, and infer meaningful events from it",This specific area of research is called   (CEP)  ,"CEP frameworks are able to iteratively clean, refine, correlate, and combine low-level events into abstract, higher-level events","Since compliance monitoring focuses on business-level events, CEP can be considered as an enabling technology for compliance monitoring in all those situations where business-level events are not directly generated by the monitored system, but can be obtained by suitably aggregating low-level events","In addition, CEP can support compliance monitoring in all those large-scale systems where an extremely large amount of events must be analyzed with tight real-time requirements  . 3.1.6 Frameworks for compliance monitoring infrastructure In this category, we find works that do not propose a specific technical approach for compliance monitoring, but address the problem of implementing a general architecture or infrastructure for compliance monitoring in the literature",These approaches particularly address the challenge of bringing different perspectives of compliance management together,The development of a compliance management architecture is a focus of the COMPAS project,"In  , Mulo et al. propose a systematic method of realizing a compliance monitoring infrastructure in a process-driven SOA",Compliance of a business process instance is determined by monitoring controls applied to the activities of the process,It provides a domain-specific language that enables the definition of single or groups of activities to be monitored,"For such activities, conditions for monitoring directives can be defined","The conditions may comprise   to narrow down the amount of particular activity instances that need to be considered by a monitoring component (e.g., only credit worthiness checks with a loan amount exceeding a threshold are to be monitored)","Further, conditions are associated with   that specify expected values of monitored data (e.g., a certain role is expected for credit worthiness checks with a loan amount exceeding a threshold)","If assertions are not fulfilled, a compliance control is violated","Being model-driven, the framework further foresees patterns for translating compliance monitoring statements specified in the DSL into code, such as queries, that can be processed by specific compliance monitoring engines","In their prototype implementation, Mulo et al. provide templates for generating queries for the Esper event processing engine",This separates the SOA concerns from the technical compliance monitor and ensures the replaceability of the CEP engine employed,"In  , Awad et al. introduce a framework for implementing an approach addressing compliance monitoring",It is exemplified for Separation of Duty (SoD) requirements how the framework provides support along the process of implementing compliance monitoring from the definition of compliance requirements in controlled natural language to the translation into checkable constraints,"The framework relies on CEP for aggregating significant process events (e.g., a completed travel request)",Such events may trigger constraints,"Constraints are associated with conditions referring to data, resources or roles to be checked (e.g., checking whether SoD is ensured) and actions to be scheduled when conditions apply (e.g., blocking the process execution). 3.1.7 Domain-specific approaches Middleton et al.   and Stevovic et al.   address compliance monitoring in the health care domain","In  , the authors present requirements on providing compliance monitoring functionalities, but more at a technical level such as a common and extensible data model","In  , the authors utilize business processes as a means to define, implement, and monitor security and privacy policies in sharing Electronic Health Records (EHR)","Domain-specific compliance monitoring approaches are not further investigated, but analyzed for examples to illustrate the CMFs. 3.2 Harvesting compliance functionalities from selected case studies Next to our literature review, we used various case studies to assist in the elicitation of CMFs","For harvesting compliance functionalities, in a first step, we analyzed the compliance constraints relevant in five case studies","The case studies were chosen because they cover a diverse set of different domains and we had access to the project data.  
                          summarizes the details",The number of harvested compliance constraints might seem to be low for some projects at first sight,"However, some of the compliance constraints are very complex","For example, the European skin cancer guideline in the “EBMC ” project requires a textual description of more than ten pages and entails different CMF functionalities in a single guideline. 4 Compliance Monitoring Functionality (CMF) design This section presents our CMFF, i.e., the framework of   (CMFs)","Following the methodology set out in  , we derived CMF candidates from a systematic literature review and five case studies","Based on several rounds of discussions, these candidates were then cleaned and aggregated into the ten CMFs proposed in this paper","Each CMF is described using the following template listing the  , a brief   on the CMF, a  , guidelines about the  ,  , and   hints of compliance rules illustrating their functionality",Whenever possible we directly borrow the examples from literature or the projects,Sometimes we also provide new examples to highlight specific features of the CMFs,"Moreover, the following requirements for CMFs were identified that also serve as basis for classifying the ten presented CMFs in the following (cf.  
                      )","Such requirements tackle the three main dimensions of any CMFF: (a) modeling of compliance constraints, (b) analyzing the raw data at runtime, and (c) generating compliance monitoring results to be returned to the end users. 
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   4.1 Modeling requirements The following three CMFs refer to the ability of a compliance monitoring approach to deal with constraints that address aspects beyond control flow: time, data, and resources. 
                          1:  
                      
                         : The bulk of real-world process compliance rules involves a combination of multiple activities or events in time","Hence, time is obviously one of the most important dimensions that a compliance rule language must tackle. 
                         : Time-related conditions within compliance monitoring constraints may be   or   (i.e., metric time)",This determines how temporal entities can be related to each other,A qualitative notion of time supports the comparison between temporal entities without referring to their actual distance,Typical qualitative temporal patterns are “before” and “after”,"Such temporal relations are utilized, for example, to capture the fundamental ordering between events constrained by a compliance rule","In contrast to qualitative time constraints, metric (or quantitative) time constraints specify the distance between time entities","Metric constraints typically refer to deadlines, delays and latency constraints in compliance rules  . 
                         : To fully support this functionality, the approach must be able to monitor qualitative   quantitative time-related conditions. 
                      
                         : We briefly discuss the case of atomic timestamps, which are associated to a point-based algebra (see CMF 4 for a discussion on durative time entities)","Temporal logics such as LTL, CTL* and  -calculus   all adopt an inherent qualitative notion of time","Thus, they easily capture qualitative temporal relations such as “before” or “after”","If not already inherent, such temporal relations can be introduced to the compliance rule language as the semantics of these relations can be defined over execution traces (and their linear/branching future)","When metric times come into play, two approaches are typically followed for their representation: an   approach, embedding them inside temporal operators (like in real-time logics such as MTL and TLTL  ) or an   approach, where explicit time variables are introduced and subject to arithmetic constraints (like in extensions of logic programming such as the Event Calculus  ). 
                          2:  
                      
                         : Compliance rules often not only define constraints on activities or events but also contain conditions on data processed in a business process. 
                         : Data refer to the ability of the compliance rule language to not only target the control-flow aspect, but also the  ","This leads to data-aware compliance rules that can include constraints, requirements and expectations about data objects and their values","For what concerns the constraints׳ shape, a major distinction can be drawn between   that just involve a single data object and   that possibly relate multiple data objects at the same time","Unary data conditions take the form  , where   is some data object,   is a comparison operator and   is some value of  ׳s domain","Extended data conditions express comparisons between multiple data objects, e.g., comparing the values of data element   measured at two different activities within a business process","According to the classical data-related workflow patterns  , we can further distinguish between different sources of data, namely  , i.e., data taken as input or produced by the activities of a business process and  , namely data that are associated to a whole process instance and can be accessed/manipulated by all activity instances executed inside the case. 
                         
                      
                         : To fully support this functionality, the approach must be able to monitor unary data conditions   extended data conditions over activity   case data. 
                      
                         : Data-aware compliance rule languages typically employ variables to denote data objects and conditions to pose constraints over them","The main difference lies then in the domains of the data objects, as well as in the “shape” of such constraints","In order to support data-aware compliance rules, the corresponding compliance monitoring approach must be able to evaluate the truth of data conditions","This necessitates access to respective data sources within the process runtime environment. 
                          3: Constraints referring to resources 
                         : Compliance constraints often relate to organizational resources involved in the business process. 
                         : Compliance rules often involve not only the control-flow and the data perspective but also the organizational perspective of a business process",This is particularly true for compliance rules stemming from legal sources,Resource-related conditions in compliance rules can be considered a special case of data-related constraints where the data refers to the resources involved,This is because resource-related information is often represented as case or activity data,"Resource-aware compliance rules include constraints, requirements and expectations on resources (e.g., agents or roles) ass;ociated with activities or events","Similar to data-related constraints, we can distinguish between   expressing expectations on specific resource properties in isolation and   relating multiple resources. 
                         : To support this functionality, the approach must be able to monitor unary resource conditions   extended resource conditions. 
                      
                         : Depending on the particular event model and the process runtime environment, constraints on resources may be dealt with in a similar manner as data-related constraints","Clearly, the evaluation of resource-related constraints requires access to resource information (such as originators, roles, groups) during process execution",This is supported by the XES organizational extension   (cf.  ). 4.2 Execution requirements There are several requirements imposed on compliance monitoring approaches by the domain,"The following CMFs enable the assessment whether or not a compliance monitoring approach meets these requirements. 
                          4:  
                      
                         : Activities in a process may be non-atomic, i.e., may have a duration","Hence, compliance rule languages must also support non-atomic activities. 
                         :   are durative activities whose execution spans across a time interval","While the execution of an atomic activity is associated to just a single event attesting that an instance of the activity has been “done”, non-atomic activities are associated to multiple events and to a lifecycle that disciplines the allowed orderings among such events",The lifecycle contains at least the two event types   and  ,"Moreover, often additional event types such as   are possible  ","Compliance rules dealing with non-atomic activities follow either an   approach, talking about their multiple, atomic constitutive events, or an   approach, where the activities are mentioned as such without referring to their events","If the approach is explicit, the definition of compliance rules is similar to what can be done with atomic activities (now mentioning the atomic constitutive events of each activity)","However, if the approach is implicit it is necessary to match the implicit semantics of the rules with the information provided in the actual data where activities are distributed over multiple events. 
                         : To fully support this functionality, the approach must be able to monitor explicit   implicit conditions on non-atomic activities. 
                      
                         : Implementations differ depending on whether the explicit or implicit approach is adopted","With the explicit approach, the monitoring framework must be able to handle at least two types of information about each event: the activity it refers to and its type, which must be one of the event types constituting the activity lifecycle (start, complete, suspend, resume, abort, etc.)","Since the language directly tackles these constitutive atomic events, it typically relies on a   to relate their relative position in time",The implicit approach directly targets activities and assumes that the time windows corresponding to their (non-atomic) executions can be reconstructed from the monitored event stream,"Since the compliance rule language predicates in this case over durative temporal entities, it relies on an   (such as the one by Allen  ) to relate the execution of different activities over time","See   for a survey on temporal reasoning. 
                          5:  
                      
                         : Non-atomic activities are associated with a lifecycle defining the allowed orderings of the constitutive events","Suitable monitoring mechanisms should be provided to check whether this lifecycle is indeed followed. 
                         : The activity lifecycle describes the allowed executions of atomic, correlated events that together describe the execution of non-atomic activities over time","In particular, the lifecycle lists the states in which an (instance of a) non-atomic activity can be at a given time, the constitutive events that mark a step in the execution, as well as in which states such events may happen, and to which state they lead",This latter aspect implicitly defines the allowed orderings of the constitutive events,The lifecycle is therefore mostly captured by a state chart (cf,ADEPT   or iUPC  ),"In general, multiple, independent executions of the same activity (i.e., activity instances) can occur inside a case",Each such instance corresponds to an instance of the activity lifecycle,"A proper   mechanism is required to correctly manage the progressions of each lifecycle instance and, in particular, to associate a given event to the right corresponding lifecycle instance","For example, if two starts of some activity and two completions of the same activity occur during a case, it is necessary to identify to which start event each completion event refers","From the monitoring point of view, (meta-)rules capturing the activity lifecycle and its instances can be used to check whether the activity executions contained in a given trace indeed comply with the expected lifecycle constraints. 
                         : To fully support this functionality, the approach under study must capture the activity lifecycle   implement a correlation mechanism between events",Note that the correlation of activity instances builds on the correlation of process instances (cases),"Within the same process instance, there may be multiple instances of the same activity. 
                      
                         : Implementations of this CMF are possible if the compliance rule language supports: Out-of-order events can either be ignored, or managed by putting the corresponding activity instance into a special “error” state, pointing out that a deviation from the expected lifecycle has been detected",Correlation can be realized by providing a special parameter used to identify the corresponding activity instance,"This way, two events carrying the same identifier are recognized to be part of the same lifecycle","Events carrying different identifiers but referring to the same activity correspond to potentially parallel lifecycle instances. 
                          6:  
                      
                         : There may be multiple instances of the same compliance rule in a trace due to multiple, possibly parallel occurrences of the involved activities","Monitoring at the constraint instance level allows for tracking fine-grained compliance rules. 
                         : When compliance rules are able to express requirements about time (CMF 1), data (CMF 2), and/or resources (CMF 3), the same compliance rule can be activated multiple times, as multiple events referring to the activities targeted by the rule occur, each with its own timestamp, data and resource information","In fact, each of such events provides a specific “context” for the compliance rule",This context is then used to instantiate the temporal/data/resource conditions possibly associated with the compliance rule,"Consider, for example, the rule stating that  ","Clearly, the constraint is instantiated for each specific closed order and each instance has its own evolution depending on events specific for this order","For example, it could happen that two orders are closed but only one is delivered","In this case, two instances of the compliance rule should be generated by the monitoring framework, then judging one of them as satisfied and the other one as violated","A more detailed discussion on multiple instances handling can be found in  . 
                         : To support this functionality, the approach must be able to monitor multiple instances, where the notion of instance is substantiated using temporal   data   resource-based conditions. 
                      
                         : Supporting multiple instances of a compliance rule requires mechanisms to discriminate between different rule activations","This can be achieved by precisely characterizing which information (time, data, resources) contributes to define the “context” of the rule (see the examples below) and which are the events that create separate instances of the rule by filling this context with specific values","Each observed rule instance has to be associated with a separate compliance state in order to assess compliance at the rule-instance level. 4.3 User requirements The CMFs described in this section refer to the ability of a compliance monitoring approach to address user requirements. 
                          7:  
                      
                         : If a violation is detected by the compliance monitoring approach, it could simply report it and provide no further support","Once the behavior is non-compliant and this is irreversible, the monitor may take the viewpoint that no further support is needed","However, a compliance monitoring approach may accommodate a variety of additional advanced features (besides detection) to continue the monitoring after a violation takes place, give feedback to the user and suggest compensation actions. 
                         : In the context of reactive detection and management of compliance violations, these factors can be exploited to characterize the degree of support provided by a compliance monitoring approach: 
                      
                         : To support this functionality, first of all, the approach must be able to detect a compliance violation and provide an intelligible compliance report","In addition, the approach should be able to guarantee continuous monitoring, i.e., it should be able to continue the compliance monitoring after the violation has happened","This can be supported, for example, by implementing recovery and compensation mechanisms in the case of compliance violations","This can comprise automatic and semi-automatic treatment of violations. 
                      
                         : As for recovery and compensation, an added feature of the compliance rule language is the ability of dealing with violations","An event violating a rule can be used to contextualize it, making the rule active only when some violation takes place","This kind of rule represents a form of recovery or compensation, which introduces further constraints/requirements upon a violation","This can be realized, for example, by introducing notions like   operators   or   
                          in the compliance rule language","Approaches that query the partial execution trace for certain event patterns, such as  , typically do not have difficulties with continuing after detecting a violation","However, continuous monitoring can be a challenge for logic-based approaches (e.g.,  ) as the approach must be able to tolerate inconsistencies to continue monitoring after a violation occurred","In  , the authors introduce some recovery capabilities to realize different strategies for continuous monitoring showing that automata-based approach are also able to accommodate sophisticated recovery mechanisms. 
                          8:  
                      
                         : While recovery and compensation measures may be applied when detecting a violation, the violation itself cannot be undone","To prevent possibly costly compensation on non-compliance, a compliance monitoring approach should be able to provide support to pro-actively detect and manage possible compliance violations. 
                         : Pro-active support includes detecting possible and unavoidable future violations and mechanisms for preventing violations",Future violations are violations whose source is not yet explicitly contained in the trace,They can be detected by implicit violations caused by currently conflicting rules,"The presence of conflicting rules identifies violations that cannot be revealed by considering each compliance rule in isolation, but only by merging the contribution of two or more compliance rules",The early detection of such future compliance violations enables timely preparation of recovery and compensation actions,Support for preventing violations refers to the ability of a compliance-monitoring framework to provide assistance for complying with imposed rules before compliance violations become manifest,"This comprises, for example, predictions and recommendations of activities to be executed next in order to preserve compliance. 
                         : To support this functionality, the approach must implement mechanisms for the early detection of conflicting conditions   provide the user with recommendations about what to do next to avoid violations. 
                      
                         : Future violations as described can be detected when considering the interaction of all imposed compliance rules","A typical task is evaluating whether the compliance rules are not conflicting a priori, i.e., that the whole set of rules admits at least one compliant execution trace","However, compliance rules that are not conflicting in general may still become conflicting at some point during the process execution","Thus, checking of compliance rules at design-time or per individual constraint, is not sufficient for detecting all types of future violations",It should be noted that supporting such implicit violations can become quite costly and the cost grows with the amount of rules involved,"For expressive compliance rule languages, this becomes even undecidable. 
                          The identification of suitable decidable compliance rule patterns for data- and time-aware compliance rules is still an open challenge","To avoid violations in a running process instance, it is also possible to give recommendations about what to do next by exploiting complete cases stored in event logs (e.g., using process mining techniques  ) or by analyzing the prevailing obligations to satisfy compliance rules. 
                          9:  
                      
                         : Key to the practical application of a compliance monitoring approach is its ability to pinpoint the root cause of a compliance violation beyond providing the counterexample that resulted in the violation","This is particularly true when a compliance rule can be violated in multiple ways or multiple rules are involved in a violation. 
                         :   enables to diagnose the root cause of a compliance violation, e.g., by isolating the responsible event occurrences or the involved compliance rules",Note that this kind of analysis is far from trivial and sometimes could lead to multiple possible explanations or to no explanation at all,"Consider, for example, the case of a sequence of events that culminates in the expiration of a deadline: isolating the responsible events in this case is impossible in general","Similarly, as discussed in   there can be multiple sets of compliance rules that are involved in a violation at the same time and therefore fine-grained analysis is needed to identify the minimal set(s) of conflicting rules","Beside the root cause analysis itself, it is also of utmost importance to provide suitable ways for communicating the result of the analysis to the end users in a comprehensible and intuitive manner  . 
                         : To support this functionality, the approach must implement mechanisms for root cause analysis and support their effective communication. 
                      
                         : For future research, efforts should be taken to provide diagnostics and pro-active recommendations based on the identification of the root cause of a violation","So far this is only supported by few approaches  . 
                          10:  
                      
                         : Compliance metrics and indicators should be employed by a monitoring framework to provide aggregated feedback to the users, summarizing the detailed information computed for each compliance rule. 
                         : The practical feasibility of a compliance monitoring approach also relies on its ability to give practitioners a sense of the compliance situation","For that, crisp approaches associating two possible truth values to each compliance rule, representing whether it is satisfied or violated, is not sufficient","In contrast to crisp compliance characterization, fuzzy approaches allow for a range of values to capture the “degree of compliance” of the running trace with respect to a compliance rule","In this respect, we differentiate between approaches that   the possible truth values from approaches that adopt   distributions between 0 (violation) and 1 (satisfaction). 
                         : To support this functionality, the approach must be able to characterize the “healthiness” of a running trace through metrics","Advanced support is provided by an approach that is able to derive statements about the health of the whole process/system by evaluating multiple running traces, i.e., by determining the fraction of deviating traces. 
                      
                         : A typical approach to quantify the degree of compliance is to “count” the number of violations and devise meaningful metrics that give a measure about the   of a running process instance",This is particularly effective when multiple instances are managed (cf,More fine-grained metrics can be devised by using detailed information about individual violations,"Approaches using a continuous scale need to calculate a “degree” of compliance, rather than simply providing a yes/no answer","For example, in the case of a deadline, a matching function could assign different noncompliance weights to traces missing the deadline, depending on the amount of time that passed between the deadline and the (late) event occurrence. 5 Compliance Monitoring Functionality (CMF) realization This section is concerned with   of the methodology depicted in  ","Phase 3 consists of three building blocks, i.e., a pattern-based comparison of CMFs related to language aspects with compliance patterns set out in the literature (cf.  ), a classification of existing monitoring approaches using the CMF framework (cf.  ), and the application of the CMFF in selected tools (cf.  ). 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   5.1 Common compliance rule patterns and the CMF framework We provide a brief, comparative survey about some of the most typical compliance rule patterns found in the literature, so as to concretely substantiate the relevance of the language-related CMFs, namely CMFs 1–4","It is worth noting that the vast majority of the literature takes inspiration, for such patterns, on a catalog of temporal logic specifications typically employed in model checking  ","The semantics of such patterns are not exactly the same, though: while   follows the standard infinite-trace semantics for dynamic systems, compliance rule patterns are typically meant to be checked against a partial trace whose continuation will be finite. 
                         
                      Specifically,  
                         
                          summarize typical patterns related to the control-flow dimension of compliance rules","The tables respectively tackle the (co-)occurrence and relative orderings between activity executions, substantiating the need for qualitative time constraints (cf","In fact, each of the listed patterns express requirements on the expected/forbidden execution of atomic activities over time, but without expressing metric constraints over the corresponding timestamps","Extensions of patterns in   with metric time have been studied in  , supporting the need for quantitative time constraints as well (cf","Furthermore, it is not surprising that such patterns have been also extended with non-atomic activities (cf","CMF 4), given how much widespread they are in BPM  ","CMF 2) have been also considered in combination with patterns of the forms shown in   and  , see  ","Differently from data and non-atomic activities, dedicated resource-related patterns (cf",CMF 3) are mentioned in  ,"It is interesting to notice that, as argued in CMF 3, not only  , but also   are present","Extended conditions are in fact necessary to relate and compare performers of different activities in a compliance rule. 
                         
                      Finally, observe that, from the expressiveness point of view, the resource patterns in  
                          can be re-expressed using the data-aware extensions of  ","This can be done by introducing special data slots tracking activity originators, and adding specific conditions on them","However, from the modeling perspective data and resources have a different nature",This is why they are separately tackled by two different CMFs. 5.2 Classification of compliance monitoring approaches We classified compliance monitoring approaches using the ten CMFs presented in this paper,We focused on compliance monitoring approaches that mainly address compliance checks during the process execution,"These approaches are different from other approaches that can be used in other phases of the process lifecycle such as process design (e.g.,  ) or compliance constraint modeling (e.g.,  ) and trigger specific questions","For example, monitoring is carried out with actual data and by considering finite, evolving prefixes of event traces","Furthermore, we selected the approaches to be classified based on the degree of detail on concepts provided in publications","In fact, a certain degree of detail (for example, in the used compliance rule languages) is necessary to properly classify the approaches through our framework","The results of the classification are shown in  
                         , where “−”, “+” and “+/−” indicate functionalities that are not supported, supported and partially supported from the conceptual viewpoint, respectively",A rating of “n.a.” indicates that the CMF cannot be assessed based on the analyzed literature,The scores shown in bold refer to approaches where the implementation is   available,The first ten approaches as summarized in   are discussed in this section,"The approaches proposed by the authors, namely  ,  , and  , will be described in detail in   in order to showcase the implementation of CMFs along two data sets",The detailed description of the classification results starts with the framework described in   which is based on Supervisory Control Theory,"This approach allows for the definition of constraints on resources but, in general, it does not support data conditions","Through this approach, it is possible to supervise the process-aware information system by “blocking” those events that would lead to a violation","This can be considered as a very sophisticated form of pro-active violation management, which is applicable only when the process-aware information system can be (at least partially) controlled by the monitor","Since violations are prevented, the framework does not directly consider the problem of reactive management nor violation explanation",ECE rules   are a domain-independent approach that was not specifically tailored for business process monitoring,"Therefore, functionalities like support for case data and activity life cycle were simply not investigated (this is matter of ongoing work)","ECE rules can deal with both atomic and non-atomic temporal entities, capturing qualitative and metric time constraints, as well as point-based and interval-based ones",Two key features characterize ECE rules,"First, they support an imperfect (i.e., fuzzy and probabilistic) matching between expected and occurred events and hence deal with several fine-grained degrees of compliance","Second, expected events can be decorated with countermeasures to be taken in the case of a violation, hence providing first-class support for compensation mechanisms","With BPath  , Sebahi proposes an approach for querying execution traces based on XPath",BPath implements a fragment of first-order hybrid logic and enables data-aware and resource-aware constraints,Quantitative time is supported by referring to and comparing timestamps of events,This enables sophisticated time constraints,"Due to the querying nature of this approach, it is able to distinguish between multiple activations of the same compliance rule","However, the approach still lacks support for advanced diagnostics and pro-active compliance management",These issues do not seem to be in the focus of the work,A compliance degree could be calculated from the results provided by this approach,The prototypical implementation of BPath is also presented in  ,An approach based on constraint programming is provided in  ,"As time aspects play a crucial role in this approach, CMF 1 is a focal issue in this approach",The approach also explicitly addresses the duration of activities,The main goal of   is to pro-actively detect compliance violations and to explain the root cause of the violation,The constraint satisfaction problem for the example provided in the paper has been implemented,"However, there is no prototypical implementation of a compliance-monitoring framework","Giblin et al.   present an approach based on Timed Propositional Temporal Logic for transforming high level regulatory constraints into REALM constraints that can be monitored during process runtime.   explicitly elaborates on temporal constraints, hence addressing CMF 1","As the main focus of the paper is on the transformation and correlation, the other CMFs cannot be evaluated upon this paper",The paper presents architectural considerations and an implementation of the transformation on the basis of a case study,"Narendra et al.   address the problem of continuous compliance monitoring, i.e., they evaluate the compliance of a process execution with respect to a set of policies at runtime","In particular, the policies are checked when some specific tasks are executed, which are called control points",The authors define an optimization problem to find the right balance between the accuracy of the compliance checking and the number of control points (each control point has a verification cost),They define policies in terms of first order clauses and their framework can support data- and resource-based rules,There is no notion of time,The framework supports a reactive violation management and not a pro-active violation management,The ability of the framework to detect the root cause of a violation cannot be assessed based on this paper,"The authors, however, define metrics to evaluate the degree of compliance of a process instance",Thullner et al.   define a framework for compliance monitoring able to detect violation and to suggest possible recovery actions after the violation has occurred,The framework is focused on the detection of different types of time constraints,"A violation is handled after it has occurred and, in this sense, the framework support CMF 7 but not CMF 8",The other CMFs cannot be assessed based on this paper,"MONPOLY   is a runtime verification framework for security policies, specified in the logic MFOTL, a variant of LTL-FO with metric time","Monitorable formulas are those of the form “Always  ”, where   is a so-called   MFOTL formula, i.e., a MFOTL formula that can be evaluated within a bounded number of steps in the future","The high expressiveness of MFOTL allows one to express sophisticated compliance rules involving advanced temporal constraints, as well as data- and resource-related conditions","Consequently, even though non-atomic activities and their lifecycle are not explicitly tackled by the approach, they could be properly accommodated","The monitoring algorithm, described in   and implemented in the publicly available MONPOLY tool  , supports continuous monitoring, in that it fetches all the time points at which a violation is detected","Due to the high expressiveness of MFOTL, no advanced features related to pro-active management of violations can be supported. 
                         
                      Halle et al.   provide an approach based on LTL-FO . “LTL-FO  is a linear temporal logic augmented with full first-order quantification over the data inside a trace of XML messages”   that focuses on monitoring data-aware constraints over business processes",The data is part of the messages that are exchanged between the process activities,The approach can handle unary and binary data conditions,"As time and resources can be handled based on data, the approach is also able to deal with CMF 1 and CMF 3, although these CMFs are not explicitly mentioned in the paper","Based on  , CMF 4–10 cannot be assessed","The work by Baresi et al. on Dynamo   constitutes one of the few techniques and tools dealing with monitoring (BPEL) Web Services against complex rules, going beyond the analysis of quantitative KPIs for service-level agreement","In  , we use (Timed) Dynamo to identify the combination between Dynamo and the following languages: timed WSCoL   to specify the rules to be monitored, and WSReL to specify the recovery mechanisms to be put in place when given violations are detected  . (Timed) Dynamo allows one to model ECA-like rules that mix information about the location (i.e., the target Web Services and operations), the involved data, and qualitative/quantitative temporal constraints","Location-related aspects can be considered as referring to resources in a business context, but since the focus is on interacting Web Services, there is no support for more advanced resource-related aspects such as groups and roles","Timed WSCoL supports the correlation between messages, and is thus able to deal with non-atomic activities, but being focused on Web Service message exchange, no notion of activity lifecycle is considered","Dynamo provides continuous, reactive monitoring facilities based on the messages fetched so far, with sophisticated recovery and compensation mechanisms based on WSReL","Furthermore, the monitoring results can be aggregated and reported to the user, providing useful insights that go beyond a yes/no answer","In  , Namiri et al. describe an approach that is based on the patterns proposed by Dwyer and Corbett  ","In particular, the authors introduce a set of control patterns",Control patterns are triggered by events (such as the execution of a controlled activity),"When being trigged, conditions associated with the control are evaluated","In order to provide data for evaluating the associated conditions, Namiri et al. introduce a semantic mirror that is filled with runtime data of a process instance","For each control, actions to be carried out if a control fails may be specified (CMF 7)","However, root cause analysis (CMF 9) and pro-active support (CMF 8) are not addressed","Being based on  , the approach is restricted to a predefined set of patterns",Data (CMF 2) and resource (CMF 3) constraints are supported as conditions to be evaluated once a control becomes triggered,This corresponds to evaluating a query on process data,"Metric time (CMF 1) is, however, not addressed","Furthermore, the approach does not incorporate a notion of activity lifecycle (CMF 5)","As events may occur multiple times within a process execution, there may be multiple activations of a control (CMF 6)","Using the semantic mirror, non-atomic events (CMF 4) can be supported using this approach",An implementation of the approach is also described in  ,"Specifically, controls are implemented as ECA rules, which are evaluated by Drools","As can be seen from  , the majority of approaches focus on time aspects (CMF 1) and reactive management of compliance violations (CMF 7)",This is not astonishing since time constitutes an important requirement in many application domains,"In this paper, we have looked at constraints from the health care, the financial, and the maritime safety domain","However, in many other domains the adherence of time constraints and their violation is an important matter, e.g., in logistics","Intuitively, each approach should support at least one of CMF 7 and 8, i.e., provide reactive and / or proactive management of compliance violations","Interestingly, Supervisory Control Theory   is the only approach that supports the proactive management, but not the reactive management of compliance violations",This is the case since events that will lead to violations (proactive) are blocked before they actually occur,All other approaches enable the detection of violations when they occur (reactive),"Overall, it can be stated that there are approaches that can deal with modeling requirements, i.e., incorporate time, data, and resource aspects","When it comes to execution requirements such as supporting multiple constraint instances, less approaches are available that support all of the related CMFs, i.e., CMF 4–6","However, the least number of approaches addresses user requirements such as CMF 7–10","For example, only few approaches enable the explanation of the root cause of a compliance violation","As the user requirements are often of particular importance, e.g., if medical staff has to react on compliance violations in stressful situations, it would be desirable to put more research effort on user feedback and support in dealing with compliance violations during run time. 5.3 Application of CMFs in selected tools In  , we classified existing compliance monitoring approaches using the CMFF",This was done based on the information in publications,"For approaches assigned a “+” or “+/−” for a CMF, the concrete implementation of this CMF may still vary","Thus, it will be interesting to also have a look at the application of CMFs in the tool implementations","The authors of this paper proposed three approaches, namely  ,  , and  , that make use of different techniques to enable compliance monitoring","In the following, we showcase the application of the ten CMFs in these three tools","As these are the tools we know best, it is ensured that we are able to correctly apply the tools in a case study for discussing the implementation of CMFs","In order to analyze tools using the CMFs, a data set consisting of compliance rules covering the CMFs to be investigated and process instances (or process logs for replaying process instances) are necessary",We selected the Business Process Intelligence Challenge (BPIC) data sets from 2011 and 2012,"Using the process-mining tool ProM, 
                          we were able to derive, from these data sets, a set of compliance rules that covers all language-related CMFs","To apply the CMFs, we applied the tools to check compliance of the process instance logs with the set of rules derived the BPIC data","Since the BPIC data sets contain post-mortem data about complete execution traces, we used a “log replay” component so as to feed the tools with an evolving stream of data that recreate the real executions",This technique makes post-mortem data indistinguishable from truly runtime data,"Notice that our aim is to assess the tool functionalities in relationship with the CMFs, and not to go into the details of non-functional aspects such as reaction time and performance-related insights","For such details, please refer to  . 
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      5.3.1 Data set Since 2011, the BPI Workshop features an initiative called   (BPIC)",The idea is that an event log is provided with some background information and points of interest,"Researchers and practitioners participate in a competition in which they are asked to test, apply or validate whatever technique or tool they developed using this log","In 2010, the three universities of technology in The Netherlands joined forces in erecting the 3TU Datacenter",This initiative aimed at publicly sharing datasets such that other researchers can benefit from whatever data can be collected,"The BPIC aims at making the research community aware of the existence of these datasets. 
                            
                         The constraints used for analyzing the tools were extracted from the logs provided for the BPIC 2011   and 2012  ",The first event log pertains to the treatment of patients diagnosed with cancer in a large Dutch academic hospital,"It contains 1143 cases and 150,291 events distributed across 623 activities",Each case in this event log is related to a different patient treated in the hospital,"The event log contains domain specific attributes, e.g.,  ,  ,  ,  , and   in addition to the standard XES attributes for events:  ,  ,  , and   
                            ",The second event log was recorded for an application process for personal loans or overdrafts in a Dutch financial institute,It merges three intertwined sub-processes,"Therefore, in each case, events belonging to different sub-processes can occur","The log contains 262,200 events distributed across 36 activities and includes 13,087 cases",The amount requested by the customer for a loan is indicated in the case attribute  _ ,"In addition, the log contains the standard XES attributes. 5.3.2 Methodology In this section, we illustrate the methodology we have followed to show how the CMFs have been implemented in the selected tools","We started from the data sets described in  .  
                             illustrates the methodology",We have split the logs in two parts (Log Part 1 and Log Part 2),"For the hospital log, in the first part, the first 571 cases are considered","In the second part, the rest of the cases (572 cases) are included","For the financial institute log, in the first part, the first 6543 cases and, in the second part, the rest of the cases (6544 cases) were considered","Then, we have mined Log Part 1 (training log) to extract a set of compliance rules","To do this, we have used the   component of the process-mining tool ProM  ","This allows us to automatically discover compliance rules related to control-flow, data and resources based on the mainstream observed behavior, i.e., frequent behavior is converted into a collection of   constraints.   is a declarative language based on an extensible set of constraints  .   supports most of the modeling constructs mentioned in   and hence a good candidate to evaluate and illustrate the CFMs",The compliance rules have been used as a reference model to monitor a stream of events coming from the replay of Log Part 2 (testing log),The compliance rules from the BPIC 2011 and 2012 logs are provided next,They are well suitable for a case study as they nicely cover the CMFs,"Specifically, they involve quantitative and qualitative time, conditions on both case and activity data as well as on resources",The compliance rules from BPIC 2012 also refer to non-atomic activities and activity lifecycle,"Furthermore, some of the presented rules can become conflicting for some specific cases. 
                            : Through the Declare Maps Miner plug-in of ProM, we have extracted, from the training log derived from the BPIC 2011, the following compliance rules: 
                         Using the Timed Declare Miner plug-in of ProM, we have derived: 
                         Using the Data-Aware Declare Miner plug-in of ProM, we have derived: 
                         The same plug-in was used to extract additional resources rules: 
                         
                             Using the training log derived from the BPIC 2012, through the Declare Maps Miner, we have extracted the following compliance rules: 
                         Using the Timed Declare Miner plug-in of ProM, we have derived: 
                         Additional rules about resources were derived using the same plug-in: 
                         5.3.3 Compliance monitoring with MobuconEC 
                             is a compliance-monitoring framework based on a reactive version   of the Event Calculus  ","As described in  , the approach has been exploited to formalize the extension of Declare described in  ","This extensions support: In fact,   is able to express all compliance rules from R1 to R28; the last three rules do not correspond to any data-aware Declare pattern, so they are not directly supported, but could be seamlessly modeled as Event Calculus constraints as well","Due to the presence of data and metric time-related conditions,   does not directly monitor the modeled constraints, but expands them in their different instances, each of which grounds the constraint on a particular context  ","Consequently, it provides fine-grained compliance checking by analyzing the evolution of each constraint instance (CMF 6: +)","In particular, at a given time each active instance could be either satisfied, violated, or pending, the last state meaning that the constraint instance is currently violated, but can still be satisfied by properly continuing the execution of the monitored process (CMF 7: +)","In the graphical feedback provided to the end-user, two visualization modes are correspondingly provided for each constraint: a “summary view” in which all instances are packed together, or an “expanded view” in which each single instance is shown separately","These two possibilities are shown in  
                            ",This fine-grained analysis of violations at the constraint-instance level constitutes also the basis for quantifying the degree of compliance,"For the time being, only simple aggregation metrics are provided to give an indicator about the “global health” of the system (CMF 10: +/−)","The high expressiveness of compliance rules in   has the main drawback that only reactive management of violations can be tackled: proactive management would require to reason on the possible future continuations of the current, partial trace by considering also data and metric timestamps, which is undecidable (CMF 8: -)",Another limitation of the approach is that violations are reported to the user without any additional inference about the corresponding root causes (CMF 9: -),"The latest version of the core   reasoner with case-data support, implemented in Prolog, is publicly available. 
                             The integration with the operational support backbone of ProM is still ongoing. 5.3.4 Compliance Monitoring with MobuconLTL 
                             is a compliance-monitoring tool implemented as a provider of the operational support in the process-mining tool ProM",It takes as input a reference model expressed in the form of Declare rules,"More generally, every business constraint that can be expressed as an LTL formula can be monitored using  ","A stream of events encoded using XES can be monitored with respect to the given LTL specification. 
                             
                             deals with a qualitative notion of time (being based on LTL) but it does not support constraints concerning metric time","In this sense,   partially supports CMF 1 (CMF 1: +/−)","Therefore, rules expressing qualitative time information (e.g., R1–R5, R17–R26) can be monitored using  ","On the other hand, rules based on a quantitative notion of time (used to define, e.g., deadlines) cannot be monitored (e.g., R6–R9, R27–R28). 
                             monitors finite-trace LTL constraints through deterministic finite state automata","Therefore, it does not tackle constraints referring to data and resources (ranging over finite state) because of the state space explosion problem (CMF 2: - and CMF 3: -)","Therefore, rules involving data and resources cannot be monitored (e.g., R10–R16, R29–R31)","With this approach, it is possible to express rules on non-atomic activities (e.g., R17–R26) but it does not fully support the monitoring of activity lifecycle","Indeed, with this approach it is possible to associate an event type to each occurrence of an activity","However, a correlation mechanism to link different events belonging to the lifecycle of the same activity cannot be defined (CMF 5: −)","This is, again, related to the impossibility for an automata-based approach of monitoring constraints referring to data","Indeed, the most natural way of implementing such a correlation mechanism would be to connect events with the same value for a certain data (e.g., an activity ID). 
                            
                             represents how rules R1–R5 can be used to monitor with   a case replayed from the BPIC 2011 testing log.  
                             shows the monitoring results obtained for a case from the testing log derived from the BPIC 2012 log using the compliance rules R17–R19, R21, R22, and R24",Events are displayed on the horizontal axis,The vertical axis shows the constraints,"In particular, each line is labeled with the Declare constraints used to encode the rules mentioned before","Five states are possible for compliance rules:  , and  ",The first state attests that the monitored case is currently compliant with the rule but can violate the rule in the future,"The second state indicates that the compliance rule is currently violated, but it is possible to bring it back to a satisfied state by executing some activity in the future",The third and the fourth states model permanent violations and satisfactions of the rule,"In this way,   implements reactive monitoring (CMF 7: +)","It is possible, at runtime, to detect that a state of affairs is reached such that two or more compliance rules become conflicting (conflict state); the presence of a conflict means that no possible future course of execution exists such that all the involved constraints are satisfied","In this sense,   also supports a pro-active management of violations and root cause detection (CMF 8: + and CMF 9: +)","For example, in  , when   occurs some of the constraints move to a conflict state since some of them require the execution of   to be satisfied and for others the execution of this activity is forbidden. 
                             supports continuous monitoring and allows a case to be monitored also after a violation or a conflict has occurred as shown in  ",One trivial way to implement continuous monitoring is to reset all the automata needed for the monitoring when a violation occurs,Other more sophisticated ways to implement continuous monitoring are presented in  ,"Note that, in the visualization in  , different instances of the same rule are condensed in only one line",After every violation and conflict a new instance of the violated rule is started in the same line,The automata-based approach allows   to provide the user with detailed diagnostics about which activities can be executed and which ones are forbidden at any point in time during the process execution,This is possible by evaluating which transitions can be fired from the current state of the automaton and which ones bring the automaton in an inconsistent state,"The tool only supports simple metrics for quantifying the degree of compliance of a case   (CMF 10: +/−). 5.3.5 Compliance monitoring with SeaFlows 
                             is a compliance checking framework that addresses design and runtime checking",It aims at encoding compliance states in an easily interpretable manner to provide advanced compliance diagnosis,The core concepts described in   were implemented within the prototype named  ,"With  , compliance rules are modeled as   (CRG).   enables to monitor a stream of events encoded in a predefined event format",It further enables the import of logs in XES standard format,Qualitative time constraints are well-supported by  ,"In particular, the CRG approach is not restricted to predefined compliance constraint patterns but allows for constraints that are more complex","However,   does not address quantitative time (CMF 1: +/−)","Hence, constraints using metric time (e.g., R6–R9 and R27–R28) are not supported. 
                             only partially supports CMF 2 (CMF 2: +/−)",This is because it provides only limited support for constraints with non-unary data conditions,Resource-aware compliance rules are only supported if the resource conditions can be expressed via the supported data conditions (CMF 3: +/−),"Hence, conditions as required for the resource-related constraints R14–R16 and R29–R30 are not supported.   supports both atomic as well as non-atomic activities (CMF 4: +) and constraints on the lifecycle of activities like R21–R23 (CMF 5: +)","For this case study, the logs were automatically replayed and checked against all modeled rules","For each violation, a violation file was created.  
                             illustrates how compliance with R5 is monitored by replaying the log of a specific case","As compliance with a CRG is checked by executing it against the event trace using well-defined rules, the compliance state is represented through markings of the CRG","When a violation is observed, it is reflected in the markings of the CRG. 
                             supports fine-grained compliance checking by analyzing the individual constraint instances that occur during the process execution (CMF 6: +)","In  , two instances of R5 were identified by the monitor (state   in the   panel in the screenshot)",The states of these instances are represented by the marked R5 shown in the main panel in  ,"From this state representation, it is possible to derive information for providing pro-active support in terms of guiding the process execution to avoid violations (CMF 8: +)",How this can be done is described in  ,"In the example, activity   is associated with an absence node that is next to being executed","Hence, this activity is prohibited and must not be executed as its execution would lead to a violation","When the last event of the case, i.e.,  , is executed, both instances become violated as shown in  
                            ",The marked CRG serves as basis for deriving explanations for violations (CMF 9: +),"In the example, clearly R5 is violated because   was executed even though it was prohibited after  ",That is the reason why the corresponding node is marked with red color,"Once violated, the instance of a compliance rule cannot become satisfied in the further process execution",This is because the violation is already manifest in the log,"However, monitoring can still be continued for the compliance rule (i.e., future possible violations of the rule in the process instance can also be detected)","Thus,   supports reactive compliance management (CMF 7: +)","As the compliance monitor of   was implemented as a proof-of-concept for the concepts described in  , it does not put emphasis on sophisticated visualization and reporting features for compliance monitoring","Although the compliance states of multiple compliance rule instances can be aggregated to provide an overall compliance level, the framework does not support a more detailed analysis of the individual metrics (CMF 10: +/−)","In addition,   does not detect violations caused by the interplay of two or more constraints","Hence, the conflicts among some of the rules in the case study (cf.  ) remain undetected. 6 Conclusion This paper presents a framework for the systematic comparison of compliance monitoring approaches in the business process management area","The framework consists of   (CMFs) and includes requirements for the constraint modeling notation (e.g., supporting time and data), requirements with respect to the execution (e.g., supporting multiple constraint instances), and user requirements (e.g., providing fine-grained feedback)","The CMFs are harvested based on a systematic literature review as well as from five case studies from different domains (health care, manufacturing, and maritime safety)",The appropriateness of the CMFF is shown in two ways,"First of all, the CMF framework is compared with existing compliance patterns in the business process management area","Secondly, existing compliance monitoring approaches are classified based on their support for the CMFs","The comparison with compliance patterns supports the importance of the four constraint-related CMFs, i.e., those CMFs that relate to language and expressiveness aspects in the constraint specification",The classification of existing approaches pointed out that none of them supports more than seven CMFs and most approaches are not supported by publicly available software tools,Here we have to note that for some approaches several of the CMFs could not be evaluated based on the literature,It seems that several approaches focus on a specific language aspect rather than integral monitoring support,"Less attention has been devoted to user requirements, i.e., the provision of fine-grained feedback or even the proactive indication of compliance violations","Nevertheless, it is crystal clear that users play an important role in compliance monitoring, e.g., to interpret deviations","In order to demonstrate the applicability of our CMF framework, two realistic data sets from the BPIC 2011 and 2012 were applied using three compliance monitoring tools, i.e.,  ,  , and  ",The data sets consisting of process execution logs from patient treatment and the financial domain were divided into training and testing data sets,"Compliance constraints were harvested based on the training set, implemented within the three tools, and monitored over the testing data set",The application of concrete tools to these data sets nicely illustrates how compliance monitoring works in practice and shows what the interaction with users looks like,"Let us now reflect on the four research questions described in the introduction. 
                   The work can be further extended in several directions, e.g., to cross-organizational or configurable processes","An interesting area of adaptation/extension of the CMF framework here presented naturally arises when there is the need of a comprehensive compliance evaluation not just within, but also across process cases","Traditionally, business processes are monitored in a case-by-case manner","However, compliance rules may span across cases, e.g., because they focus on resources independently from the specific case in which they operate, or because they need to compare and combine data produced inside different cases.",,,,,,,,,,
s0921889015003000," 1 Introduction Autonomous systems are increasingly required in various practical applications, including unmanned aircraft, driver-less cars, healthcare robots, manufacturing robots, etc","In all of these cases it is easy to imagine a situation where an autonomous system causes harm to people or property, as a result of an error in its engineering, or an unfortunate combination of circumstances","Therefore, if such autonomous systems are to operate within society, we must be able to trust that their behaviour complies with the legal, social, and ethical norms of that society","Determining the trustworthiness of technology in this respect is usually delegated to a regulatory body, such as the Federal Aviation Administration (for aircraft in the USA) or the Vehicle Certification Authority (for road vehicles in the UK)","The process is known as  , and is used to determine the safety and reliability of safety-critical technology, including aircraft, road vehicles, nuclear reactors, pharmaceuticals, etc","For non-autonomous systems, such as cars or manned aircraft, it is assumed that the operator of the system will satisfy the ethical standards of society,  , the pilot of a civilian aircraft does not intend to use the aircraft to commit murder, and will, if necessary, disregard legal restrictions for ethical reasons,  , the pilot will disregard the Rules of the Air in order to preserve human life",These assumptions are an unavoidable result of the opacity of human behaviour; it is extremely difficult to pre-determine the behaviour of a human being,"However, autonomous systems are far more transparent, and can be engineered to meet requirements","Typically these requirements are technical (“an aircraft must be able to fly at 10,000 feet”) or legal (“a car must have visible registration markings”), but in the case of autonomous systems some requirements may be ethical ( , “an autonomous unmanned aircraft will never choose to do something dangerous unless it has no other option”)","Such ethical requirements may prove essential for an autonomous system to be certified by a regulatory body, since ethical autonomy is obviously desirable",Machine ethics is an emerging discipline concerned with ensuring that the behaviour of machines towards humans and other machines they interact with is ethical   ,"It is an open question whether machines are, or will ever be, moral agents,  , in possession of an innate ability to distinguish between right or wrong","However, it is necessary to enable them to adhere to our human understanding of morality, despite there exists no obvious or easy way to accomplish this   ","If we assume that an autonomous system can be capable of moral agency, and possibly even be a better moral agent than a person, the goal of machine ethics is to enable machines to  ",Notable works in this area are   ,Within this sub-area of machine ethics a lot of the questions traditionally studied in moral philosophy are reiterated but now from a computational perspective,"The focus of research lies on automated extraction and identification of ethical guidelines for conduct, as well as on automated solving of ethical ambiguities and problems",These systems are often developed with the intention to be used to aid ethical decision-making by people,"If we assume that an autonomous system is   capable of moral agency, then the goal of machine ethics is to ensure that machines  ",This is done by developing methods for ethically constraining the actions of machines   ,"Within this subarea of machine ethics, research focuses on identifying ethical principles that a system should not violate during its operation and developing methods for embedding consideration of these ethical principles in the decision-making process of the machine",Examples of work in this area are   ,We are interested in representing and embedding consideration for ethical principles in the decision-making process of an autonomous system in a way that is amenable to certification,"The work on ethically constraining actions of autonomous machines in    focuses on machines used in military operations and methods for stopping the autonomous machine from performing any action that is deemed unethical, but it does not consider circumstances where no ethical action is possible",Our focus in this paper is on civil applications,"We propose a method for selecting among unethical actions, when no ethical action is possible, and for proving that a machine only behaves unethically, by choosing a minimally unethical course of action, if it has no ethical choice. 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   1.1 Formal verification It appears increasingly the case, particularly in autonomous vehicles, that the autonomous control architecture is of   form comprising discrete and continuous parts",Traditionally such systems have been engineered using the concept of a   (in which continuous aspects are encapsulated within a single state of an automaton while discrete jumps are represented as transitions between these states),"However, as these systems have become more complex, combining discrete decision-making and continuous control in this way has created challenges for understandability and reuse of design and code","Since we are particularly interested in the issue of decision-making, rather than control we have focused here on an alternative architecture, referred to as a   in which a distinguished agent is responsible for decision-making",This is motivated by evidence that hybrid automata based implementations scale poorly with the complexity of decision-making when compared to agent-based control   ,"A typical such architecture is shown in  
                         ","The discrete part is often represented by a   taking the high-level decisions, providing explanations of its choices, and invoking lower-level continuous procedures   ",In this kind of hybrid autonomous system the continuous control and the higher-order decision-making components can be separated clearly,"The lower-level procedures appear in non-autonomous systems as well, and are familiar to certification authorities","As such, we can focus analysis on the decisions the rational agent makes, given the beliefs and goals it has   ","In an autonomous system we cannot show that an agent always does the right thing, but only that its actions are taken for the right reasons","Following this premise,  , more precisely  , has been used in    for providing formal evidence for the certification of autonomous unmanned aircraft",Formal verification    involves proving or disproving that a system is compliant with a “formally specified property”: a requirement specified in a mathematical language,Formal verification is an application of Formal Methods to the challenge of system verification,Model checking is a variety of formal verification in which   possible executions of a system are examined automatically based on a model of the real world,Model checking takes place relative to some requirement specified in a formal language   ,In        is used to assess whether or not an autonomous system for an unmanned aircraft (UA) follows the specified “Rules of the Air” (ROA) that a pilot   follow   ,"The stated aim in these papers is to provide evidence that the autonomous system in control of an unmanned aircraft is safe and reliable, therefore providing supporting evidence for the potential certification of such an aircraft","The rationale behind using the Rules of the Air is that they provide a codified, statutory set of behaviours which human (and machine) pilots should satisfy","However, there are many circumstances that are not covered by the Rules of the Air","Indeed, the Rules of the Air are not intended to be exhaustive, but rather to provide a set of guidelines for pilot behaviour","It is anticipated that the Rules of the Air will be implemented by a skilled and experienced pilot whose responsibility is to ensure the safe passage of the aircraft through airspace (in this case, civil airspace)","In those circumstances which are not covered by explicit Rules of the Air, it is the responsibility of the autonomous system in control of an unmanned aircraft to make sensible, rational, safe and   decisions at all times","So, while the formal verification of safe and legal decision-making has been covered in previous papers, we now focus on the formal verification of ethical decision-making within autonomous systems controlling autonomous aircraft. 1.2 Overview This paper is organised as follows","In Section    we cover relevant background material on autonomous systems, machine ethics and verification",In Section    we outline our formal theoretical framework for the implementation and verification of ethically constrained behaviour in autonomous systems and also point to some relationships of our framework to deontic logic,In Section    we discuss our prototype implementation of this framework,"In Section    we consider three simple examples of ethical reasoning implemented in our prototype, while, in Section   , we present our conclusions and discuss further work. 2 Background 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   2.1 Agent architectures for autonomous systems Webster et al.    discuss the analysis of an autonomous unmanned aircraft controller as a  , with an architecture such as the one given in  ",The rational agent-based autonomous system in control of the unmanned aircraft was separated from lower level control systems (such as the autopilot),This enables the decision space of the agent to be analysed separately from the lower level control systems,"Model checking (which is best applied to discrete problems) is then used to analyse the high level decision-making and methods, such as testing (or indeed theorem proving—see    for an example of where program model checking of an agent is combined with a theorem proving approach based on hybrid automata), are used to verify 
                          the behaviour of the lower level control systems",These two approaches can be used together to provide a higher level of assurance that the system would behave as expected than either can provide individually,"For example, it is possible to verify that an agent in control of an unmanned aircraft will always attempt an emergency landing if it runs out of fuel","Then the lower-level control systems can be analysed separately to ensure that, once this decision has been taken by the agent, the auto-land systems will correctly implement the emergency landing","The predominant view of rational agency is that encapsulated within the BDI model   . “BDI” stands for  ,  , and  : beliefs represent the agent’s (possibly incomplete, possibly incorrect) information about itself, other agents, and its environment; desires represent the agent’s long-term aims; while intentions represent the aims that the agent is actively pursuing","There are   different agent programming languages and agent platforms based, at least in part, on the BDI approach",An overview of particular languages for programming   agents in a BDI-like way can be found in   ,"Agents programmed in these languages commonly contain a set of  , a set of   ( , desires), and a set of  ",Plans determine how an agent acts based on its beliefs and goals,"As a result of executing a plan, the beliefs and goals of an agent may change as the agent performs actions in its environment","It is important to note that, in a typical BDI programming language, plans are supplied by a programmer not by an independent planning mechanism. 2.2 Model checking While the conventional hardware and software used within an autonomous machine can be certified as any complex tool, the decision-making component of an agent-based autonomous machine needs to be certified in a way that has more in common with the way persons in a position of responsibility are certified",The manufacturer needs to offer evidence that the machine will make decisions for the right reasons,"To accomplish this, we can use formal verification   ",Formal verification involves proving or disproving that a system is compliant with a “formally specified property”: a requirement specified in a mathematical language,Formal verification is an application of Formal Methods to the challenge of system verification,Model checking is a variety of formal verification in which all possible executions of a system can be examined automatically based on a model of the real world,Model checking takes place relative to some requirement specified in a formal language   ,"Typically, the formal requirement, or  , is expressed within a linear temporal logic which allows us to specify what should happen at some specific moment, at some point in the future, or at all points in the future (or some more complex combination of these)","Computer programs for autonomous systems are typically deterministic in nature, but when placed within the real world the behaviour of deterministic systems becomes unpredictable as the real-world is non-deterministic","In other words, a variety of things can happen to the system at any given time",Therefore there is a range of possible things that a system can do within a given environment,This can be handled within model-checking by analysing the environment to determine the finite set of input classes that effect the agents’ decision-making,This approach is described in   ,We adapt this approach when we perform verification in our case studies in Section    by exploring different ways the possible ethical consequences of an agent’s choices can be represented,It should be noted that an important alternative approach to the verification of hybrid systems is that of       in which the system is designed and/or implemented as a hybrid automaton and well-developed techniques are then used to model check that automaton,There is also a theorem proving approach to the verification of hybrid automata   ,"As discussed above, hybrid automata are not ideal when complex decision-making is involved and we consider ethical choice to be an example of complex decision-making","It is certainly not ideal if we wish to verify the ethical sub-component of some system, separately to the verification of the system as a whole which, as in our case, may involve planning or learning sub-systems which operate as “black boxes” and have not been verified",Model checking has been used in    for providing formal evidence for the certification of autonomous unmanned aircraft,"Specifically, an autonomous system for an unmanned aircraft (consisting of a  ) was developed and formally verified using Agent Java PathFinder, a program model checker for rational agents   ",The properties verified were based on the “Rules of the Air” (ROA) and notions of Airmanship,"The latter refer to requirements which do not necessarily appear in the Rules of the Air (which is a statutory document), but nevertheless constitute good practice for “airmen”",Clearly Airmanship can apply to an autonomous system in control of an aircraft as well as to a pilot,"For example, it was verified that in all cases the rational agent in control of the unmanned aircraft would request clearance before taxiing onto the manoeuvring area of an aerodrome (a specific Rule of the Air), and that in all cases the agent would check its current fuel level before take off (Airmanship), amongst other things",We use this work as the basis for our case studies in Section   . 3 A framework for constraining ethical behaviour and its formal verification Our concern in this paper is with high-level decision-making in autonomous systems that   takes ethics into account when reasoning   can be proved to do so,"This gives us three tasks to resolve:   In order to show that an autonomous system has the property of making the right decisions for the right reasons, we need to first define and formally specify what the “right decisions” are, not only in the operational, but also in the moral sense of the word","In this paper we mainly focus on this problem of formal specification of moral machine decisions for the purpose of verification. 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   3.1 Professional ethics for agents Ethics is a sub-field of philosophy that studies moral values and moral reasoning",Normative theories of ethics are concerned with designing ethical principles that constrain the behaviour of people within ethically desirable boundaries,An overview of the most popular normative theories can be found in   ,Ethical principles are rules of conduct that should guide the behaviour of moral agents when making decisions,Typically they are   or abstract by design allowing for applicability in a wide range of specific situations that arise,"These formal principles are intended to be additionally made concrete, or  , constraints by applying the facts of the decision-making context","For example, the formal principle of doing no harm is violated by a specific action of moving ten metres to the left when an aircraft is on the ground, therefore the aircraft should be ethically constrained from performing this action in this circumstance",The same action should not be ethically constrained when the aircraft is airborne,"How abstract principles are transformed into substantive rules that constrain behaviour is a difficult problem, even for people","An abstract ethical principle can be specified by narrowing the scope in which it should be applied, namely by specifying “what, where, why, how, by what means, by whom, or to whom an action is to be, is not to be, or may be done” (p. 295,   )","Within machine ethics, a growing body of research has been devoted to making these transformations,  ,   ","We are interested in using ethical principles to constrain the behaviour of an agent, therefore we must answer two questions: where do we get these ethical principles and how do we transform them into precise context-dependent constraints? Autonomous systems, and robots in particular, are being developed as specialised assistants and tools with a pre-designed domain of application","We suggest, therefore, that we should think of autonomous systems as members of a profession and use this insight when developing ethically behaving machines","For example, medical robots can be seen as members of the medical profession, while autonomous systems for unmanned aircraft can be seen as aircrew personnel","If machines are seen as special members of a profession, then special ethics that apply solely to machines is not necessary",We can focus not on developing ethical principles but on ensuring that an autonomous system’s high-level decision-making is subject to a code of ethics that has already been decided upon by relevant authorities and associations,We make some assumptions regarding the availability of abstract ethical principles and rules,"We assume that each professional domain has abstract ethical principles developed, which express what is considered right and wrong within that domain","For example, in the biomedical domain, the principles of respect of autonomy, nonmaleficence, beneficence and justice, as summarised on (pp. 13–14   ) are considered to be the core abstract ethical principles","Given a set of formal principles, we still need the substantive principles to actually evaluate how ethical an intended conduct is","For frequently occurring situations, the relevant institutions and governing bodies can, and do, instantiate the abstract principles into substantive rules","For example, each country or biomedical institution would further instantiate the mentioned abstract ethical principles into rules that should be followed under given conditions","Thus, for instance, the abstract principle of respect for autonomy is instantiated into a rule that precisely describes when a patient’s desire to refuse treatment must be observed by not forcing treatment","We distinguish between anticipated contexts, for which substantive principles can be determined directly and unanticipated contexts","For anticipated contexts, we can reasonably assume that substantive principles will also be defined","Since machine ethical reasoning is outside of the scope of our work, for unanticipated contexts we shall assume that the agent is additionally informed, by a human attendant or by the context itself, of what constitutes a breach of an abstract principle in that context","Having established where the ethical principles for an autonomous agent come from, we now elaborate on how we intend to ethically constrain the reasoning process of an agent using these principles","Arkin et al.    propose an addition to the reasoning process, a so called  , effectively embedded within the agent",The ethical governor evaluates each plan with respect to given ethical constraints and removes those plans that violate at least one of the constraints,"However, in civilian operations, stopping all actions that violate at least one ethical principle is not always a satisfactory method of ensuring ethical behaviour",Assume that an agent is choosing which plan to execute and the ethical governor vetoes all available plans as unethical,"Should the agent just wait for the world to change? What if taking no action is also unethical? Situations such as these, in which each course of action leads to violating one or more ethical principles, are called “ethical dilemmas”",In an ethical dilemma a person that behaves ethically would still be considered to behave ethically when she violates the lightest of the ethical principles that she could under the circumstances,It is not difficult to illustrate this point with an example from medicine,A core principle in medical ethics is that of not doing harm,"A substantive rule that ensues is to distinguish, during childbirth, between the life of an unborn child and that of its mother,  , both need to be preserved","However, circumstances arise in which preserving both lives is not possible and the principle of doing no harm must be violated at least once by making a choice of whose life to save",If a choice is not made the principle of doing no harm would be violated twice,Which life to save is no longer an issue of medical ethics but an issue of private moral choice made by the mother or her next of kin,"Arguments can be found to justify either choosing to save the mother or the baby, and some mothers would choose differently to others","However, neither choice can be considered universally moral or immoral","On the other hand, doing nothing, letting both mother and child die is morally reprehensible",We want an autonomous system to be capable of minimising the unethical outcomes under the circumstances where there is no ethical option available,To accomplish this we need to enable the autonomous system to distinguish the available choices in terms of how badly they violate ethical principles and choose the “least unethical” one,"Now the question we need to resolve is: how can we constrain the unethical actions of autonomous systems but allow them to make justifiably unethical choices under certain circumstances? We propose that instead of treating the ethical principles as a veto on actions, we view them instead as  ","With ethical principles as soft constraints an autonomous system would be allowed to violate an ethical principle, but only under the condition that there is no ethical option available and under the assurance that the unethical option chosen is the “least of all evils”",To represent ethical principles and rules as soft constraints we need an  ,"An ethical policy is an order over the rules that are applicable in the same situation, in terms of which rule it is better to violate when no ethical option is possible","The principles and rules are obtained from the ethical code of conduct within a given society, but where does the policy come from? Some professional codes of conduct do offer guidance as to the priority among principles in the case of conflict","When such guidance is unavailable or inapplicable, people are expected to follow their own moral judgments","Assuming that machines, certainly as is the case at present, are devoid of a moral judgment capability, we advance that the person who is the most influenced by the immediate consequences of the machines decision-making should be the source of the ethical policy",To justify our stance we return to the example of mother and child in danger during delivery,"When the decision is needed about whether the life of the mother or the life of the unborn infant should be given precedence, the decision-maker makes an ethical preference and that decision maker is the person with decision-making capacity that is most influenced by the choice,  , the mother (the infant is not in capacity) or if the mother is not in capacity, her next of kin",It is advantageous that most legal systems often clearly define both the decision-making capacity property and the next of kin relation between persons,An autonomous system making ethical decisions should thus be equipped with the policy of the person or persons who would be most afflicted if a bad decision is made by the system,Within our framework we cannot express how gravely an ethical principle is violated,For example making a scratch on another UA counts as damaging property to the same extent as obliterating that UA is damaging property,"However, a plan that incorporates two actions each of which violating a principle separately is considered less ethical than a plan that has only one action that violates the same principle (assuming this second plan does not violate some other, lower ranked principle as well). 3.2 Ethical plans and planning To construct an ethical reasoning process for machines we need to represent abstract ethical principles and specific ethical rules, paired with the context in which they apply and then invoke these at an appropriate moment in an agent’s reasoning process","As noted in Section   , the reasoning of a rational agent is controlled by its beliefs, its goals and its plans",The plans are presumed to have been supplied by a programmer,"Therefore, on a trivial level, the task of ensuring that the agent acts in an ethical fashion, should lie with the programmer who should ensure that the plans are ethical","Given our assumption of a pre-existing set of ethical rules, showing that such plans always preserve the rules can then be tackled using pre-existing verification approaches","However, in many realistic scenarios, it is either not possible to provide an exhaustive set of plans that will cover all situations or, at least, not possible to provide such plans in full detail","So, for instance, a plan may need additional complex information such as the calculation of a route or schedule that is based on the situation in which it finds itself",For simplicity we will treat all such situations as the acquisition of entirely new plans,"There are long traditions of AI research into specialised route planners and schedulers as well as more general plan generation systems such as   , and such planners are good candidates for integration with BDI-style languages; indeed many BDI researchers are interested in such integrations",In our case we were particularly interested in a route planner such as that implemented in    which can generate different routes for a UA to follow,"The construction of an appropriate planner is not the focus of this work, which looks at how a typical BDI agent would work with the output of such a planner",We do assume that such a planner can inform an agent about the relevant   of any plan generated—in our cases the nature of anything with which the UA might collide,"We do not discuss here how some system reasons to determine the side effects of such a plan, obvious candidates include simulations ( , as discussed in   ) or specialised algorithms for, for instance, reasoning about collision avoidance (indeed a verified version of such an algorithm exists   )",See also   ,We assume therefore that there are two modes of operation for our rational agent,In one mode the agent uses its pre-existing (potentially pre-verified) set of plans in situations which are within its anticipated parameters,In these cases it is assumed that the programmer has taken responsibility for any ethical issues that may arise,In the other mode of operation the rational agent is working outside of these parameters but has various planning resources available to it which allow it to continue to act,In this situation it must use ethical reasoning to govern its actions,Thus a software agent needs to apply ethical reasoning when new options become available,New options are needed when:   We assume that the agent has access to some external planning mechanism from which it can obtain new plans,"The new plans supplied by the external planner can then be associated with substantive ethical rules,  , the sets of ethical principles which are violated by that plan (this work of association can be performed either by the agent, the planner or some other system)","The job of the agent, then, is to determine which of the available plans are the most ethical to follow","Let us begin by defining an abstract ethical principle and an ethical policy.  
                      
                          We now need to represent the ethical rules which are a specification of an abstract ethical principle by a context",Anderson and Anderson    show how cased-based reasoning and abduction can be used to identify ethical rules,"This approach not only instantiates an ethical principle into a context dependent rule, but also resolves conflicts among rules by specification. (See, for example,    for an overview of conflict resolution approaches.) We propose that this process of specification, or  , 
                          is externalised altogether","When the context is some societal construct such as, for example, an institution, a state, or a company, it is not unusual to expect that the context informs the agent of what counts as a violation of the laws and principles by which the context is governed",Within normative reasoning these are encoded as statements of the form “X counts as Y in context C”   ,"The counts-as statements and their generation can be seen as a mechanism for implementing context instantiation, such as the one we need to transform abstract ethical principles into rules","In general, an agent’s context can be identified by space, time, goals, and/or a variety of other factors","However, an agent can be constructed to manage a context regardless of how abstract it is",We start with the assumption that an agent’s environment is “intelligent”,"Instead of considering the contexts to be a passive collection of properties, we consider them to be an agent extended to facilitate structures as described in   ","The agent of    is extended with two sets:   and  , describing the sets of agents it contains, and is contained within, respectively",This agent can implement the approach of    to derive the rules that should be observed by the agent it contains,We can now define ethical rules to be context-dependent statements pairing actions with ethical principles.   For simplicity we can consider that   represents not doing an action,"This formalisation is needed to represent cases when abstaining from action is an ethical infringement, for example not calling an ambulance when witnessing a person having a heart attack",To be able to reason about plans in terms of ethics we need a plan selection procedure that uses the substantive policy implied by the abstract policy,"We favour plans that violate the fewest concerns, both in number and in gravity",We propose that the plans are ordered using   which results in a total order over plans,"The agent can be in multiple contexts while determining which plan to choose, so we need to consider the rules from all the contexts that apply to the plan.   The first property above ensures that the ethical plans will always be preferred to the unethical ones","The second property states that when the principles violated by both plans are disregarded, the plan that violates the worst principle is considered less ethical","The third property guarantees that when the worst principles that each plan violate are different, but equally bad, the plan which violates more such principles is less ethical",Reasoning about plans and preference-based planning has been considered before in the BDI agent literature,"However, to the best of our knowledge, preference-based planning has not been applied to ethical reasoning","For example, in    plan selection is considered in terms of agents’ desires","However, the desires are not ranked, so selecting the most desirable plan is done by summing up the number of desires each plan satisfies",In    the agent can reason about plans by selecting the plan that can satisfy the most goals,Goals are ranked and the plan selection functions much as our plan ordering above,For an overview of preference-based planning in BDI agents one can consider   ,"Similarly, planning with priorities is considered in the literature","The difference between planning with priorities and preference-based planing is that in the first case one considers how a planner might choose which plan to develop given a ranking on the goals the agent wants to achieve, while in the second case the ranking is on the plans themselves","In   , a planner is proposed that develops those plans which would accomplish the highest ranked goals of the agent","Only after attempting to find a plan for the most preferred goals, the planner would attempt to construct a plan for reaching a less desirable goal","In   , the issue of deadlines is considered in combination with the desirability of goals",The main idea is that an agent would only be interested in pursuing a goal if it can be feasibly reached within a certain deadline,"Thus the plan selection and generation is influenced by the deadline by which the goal should be reached, not only by the desirability of the goal itself","Preference-based planning, as well as planing with priorities and planing in general is outside of the scope of this work",For now the above-described plan order is sufficient for plan selection,"Inevitably, in real-world implementations, the question of critical deadlines can be expected to rise","Namely, the most ethical decision is not always the least unethical decision that can be made under the circumstances, but the least unethical course of action that can be   accomplished under a deadline","The impact of deadlines on the ethical policy, as well context-dependent policy updating in general, is an issue we intend to explore in our future work. 3.3 
                          and deontic logic Deontic logic is the sub-field of logic and reasoning most concerned with representing and reasoning about obligations","Counts-as-statements, the paradigm of which we use to represent our ethical rules, and reasoning with them are considered to belong to the field of deontological reasoning   ","Since we build an ethical reasoning method on abstract ethical principles and ethical rules, the natural question to ask is why introduce a new representation   instead of relying on a formalisation developed in deontic logic, even more so since the abstract ethical principles have already been linked to   obligations","However, deontic logics, in particular the standard deontic logic, are notoriously inadequate for specifying   duties, as illustrated with numerous paradoxes   ","For this reason we choose to introduce the construct   to denote ethical principles, instead of using the deontological obligation operator  ","Nevertheless, a distinction has to be made between our “defeasible” ethical principles and a very similar concept developed in deontic logic, that of   (CTD) imperatives   ","CTD imperatives are ordered conditional pairs or lists of obligations, for example   represents that the agent is obliged to (do)  , but if the agent violates  , or for whatever reason   is the case, then the agent is obliged to (do)   and if   is not observed then the agent is obliged to (do)  ","Similarly   represents that under the condition  ,   is an obligation","The difference between a CTD imperative and   is that all ethical principles are in force at the same time, while a CTD imperative is in force only if a higher ranked one has failed","The CTD imperatives do not indicate how to choose between   and  , but rather generate a new principle when the existing principle is failed, they act as a form of cascading “damage containment”","An ethical policy differs in this manner from CTD imperatives, as it orders ethical principles in terms of importance but all ethical principles are in force at all times","Thus a policy  , denotes “priority”, namely ideally all   are in force, but if one has to be violated, it is better to violate   before violating either   or  ","With CTD imperatives, the ranking is no indication of priority, but it implies that if   activation of   implies violation of   and  ","With ethical principles, a violation of a lower ranked principle according to   does not imply a violation of the higher ranked, ethically better principles","For example, consider the principle to not violate people’s privacy  , which is breached for instance when making low flights over private property, and the principle to do no harm  ",Our UA can violate   by crashing atop a person on an open road without violating anyone’s privacy,"Within the study of CTD imperatives, there are works considering how to extend an ordering over imperatives into an ordering over sets of imperatives, which identify states of the world and could be taken to correspond to our plans   , however the ordering of the sets hinges upon the inherent properties of the ranking of CTD imperatives","We have constructed the structure   deliberately to have   allude to a modal logic box (necessitation) operator, but we have given no specific syntax for this operator","Furthermore, there exists a precise semantical and syntactical formalisation of a counts-as operator as a modal logic operator in   , but we have not included the semantics here","The precise semantics of an   modal operator captures the whole meaning of an abstract ethical principle is worthy object of study on its own, but outside of the scope of our interests at present work",We are here interested in constructing an agent programming language and the logic we use is primarily for specification purposes. 3.4 Verifying decision-making is ethical We can now begin to define a logical property which specifies what it means for an implemented agent to reason ethically,"Informally we mean that whenever an agent selects a plan,  , then all other applicable plans,  , are ethically worse,  , that  ",We could frame this in linear temporal logic (where ‘ ’ means “at all future moments”) as   Unfortunately this property is first order involving quantification over plans which are unknown when the verification process starts since they are calculated during the course of execution,In general model checking systems cannot handle this kind of quantification,Insofar as quantification can appear at all in properties it should ideally be a shorthand for enumeration over a finite set whose members are known before model checking starts,"In our case, the ethical policy forms such a set where the possible plans do not",Fortunately it is comparatively straightforward to relate the plans and the ethical principles,We will use   to indicate that ethical rule   is violated by the currently selected plan,"This can be checked as model checking proceeds, when plans are known, but the particular plan referred to does not need to be stated at the outset of checking","We will also need to refer to the set of ethical principles,  , that are more important than some principle  — , it is preferable to violate   than the principles in  .   We use   to indicate that there is no applicable but unselected plan that does not violate some principle in the set  ","As with   this property can be checked as model checking proceeds, but can also be stated before model checking starts",We can then formulate our general property as   This can be instantiated for particular agent instances by enumerating the ethical concerns   and the set  ,We provide examples of such instantiations in Section   . 4 Implementation We developed a BDI agent language called   for a prototype implementation of our approach.   was based on the    agent programming language,"A full operational semantics for    is presented in   , but its key components are, for each agent, a set,  , of beliefs which are ground first order formulae and a set,  , of intentions that are stacks of   associated with some event","Deeds can be the addition or deletion of beliefs, the adoption of new goals, and the execution of primitive actions","A    agent may have several concurrent intentions and will, by default, execute the first deed on each intention stack in turn.    is event-driven and events include the acquisition of new beliefs (typically via perception), messages and goals","A programmer supplies plans that describe how an agent should react to events by extending the deed stack of the relevant intention. 
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   4.1 Implementation of ethical reasoning In our prototype, ethical reasoning was integrated into a BDI agent programming language via the agents’ plan selection mechanism","In accordance with our theory, we assumed that the agent’s existing plans are ethical   and, indeed, had been formally verified as such",In the scenarios we consider below we assume the verification of the formal “Rules of the Air” and notions of Airmanship as discussed in    satisfied this requirement,Obviously we were assisted by the existence of these rules,"As discussed above we needed to ensure that an   agent:   For our prototype we made the simplifying assumption that the agent could only ever be in one context at a time and that, therefore, we could reason with the substantive, rather than the abstract ethical principles","Among other things, this allowed us to avoid reasoning about ethical consequences within the agent","Instead of inspecting a plan,  , for the actions,  , it contained (explicitly or implicitly) we were able to list the unacceptable outcomes and send these to the external planner which could evaluate the side effects of its plans for these outcomes","It is important to observe that even with additional contexts, the verification of ethical reasoning would unfold in the same manner as in our one-context-at-a-time prototype, because the choices the agent makes are still determined only by a unique ethical order over available plans","However, when multiple context influences are in play, verification can be used more “deeply” and explore when and why a particular policy or rule was introduced","We extended the    language as follows:  
                      In normal operation    agents cycle through the deeds in their intentions","When a deed requires the generation of a new plan all applicable plans are extracted from the plan library, one is selected and converted into an intention, then the system returns to cycling though the deeds in the intentions interleaved with checking perception and messages for new beliefs etc",For    we added the recording of selected plans,This was done by storing an identifier for the plan together with the unifier that was used to match it to the current agent state; this information was linked to the particular goal the plan was expected to achieve,We extended the plan selection mechanism to select the most ethical plan from those applicable according to  ,"The most significant change for    was altering the reasoning cycle so that, if no existing plan were applicable, an external planner would be queried for new plans","This query involved sending the planner the current goal, and the list of ethical rules relevant to the current situation in order that the planner might note any ethical rules that could be violated by a plan’s execution",We did   implement a generic planning mechanism for our investigation but relied upon hard-coded pseudo-planners customised to the scenarios studied,"The    reasoning cycle is shown in  
                         . 4.2 Implementation of verification of ethical decision-making One of the reasons for selecting    as the basis for our implementation language,  , was that it provided the potential for formally verifying ethical decision-making.    is implemented in the    framework for model checking agent programming languages   .    comes with a property specification language based on   extended with modalities for describing the beliefs of an agent",This property specification language did not explicitly reference   and   from  ,In order to circumvent this we made further adaptations to    in order to reason about ethics in  ,We enhanced   with a special set of beliefs which would allow us to reason about these particular beliefs in lieu of reasoning directly about   and  ,"We enhanced    to store, as explicit beliefs, currently applicable plans, plans that had been attempted on a particular goal, and the ethical concerns violated by any selected plan",These were stored in specialised belief bases,"These beliefs are shown in  
                         ","Immediately a check that an   agent believes   in  ’s property specification language,  , corresponds to a check that  ","In the above,   conventions are used, and so capitalised names inside terms indicate free variables which are instantiated by unification (typically against the agent’s beliefs).   programs may also perform deductive reasoning on their atomic beliefs as described in their  -style  ,  : 
                         
                      indicates that the program deduces that all is well if it is not the case ( , “ ”) that the brakes have failed (the closed world assumption is used to deduce this negation)",In some cases an atom in such a   rule needs to specify that deduction should be applied to a specialised belief base rather than the default one,"In these cases the notation,  , is used","This allowed us to use   style rules to deduce further beliefs allowing us to construct  ; these are shown in Code Fragment 4.1. 
                         
                      The predicate “ ” is deduced if all untried applicable plans violate a concern contained in the list  ","The currently selected plan is marked as already tried and so this corresponds to  — , that there is no applicable but unselected plan that does not violate some principle in the set  ","The beliefs about plan applicability ( ), plans already tried ( ) and the ethical concerns of particular plans ( ) were all inserted into the agent’s belief base during execution of the    reasoning cycle","It should be noted that while the restriction to untried plans prevents “thrashing” where the system alternates rapidly between two possible plans, it does preclude the system from changing its behaviour should the ethical evaluation of the situation change— , should a previously occupied field become vacant",We discuss this in further work,With these adaptations and the rules in Fragment 4.1 we were able to formally verify properties of the form   where   are the set of ethical rules in  ,This work on model checking ethical choices is preliminary,"It is undesirable to have constructs, such as beliefs and belief rules, which can potentially affect program execution, used for verification purposes alone",However adapting    with a more expressive property specification language was outside the scope of this research,The issue of how the approach scales remains open,The work here does demonstrate that an ethical policy can be incorporated within a BDI agent in such a way that adherence to the policy can be   verified and so we can be   the agent will always make the most ethical choices. 5 Scenarios We examined three ethical aviation scenarios for unmanned aircraft derived from discussions with domain experts: a retired Royal Air Force fast-jet navigator and a current UK private pilot licence holder,We created an    program for each scenario and then verified that program,"It should be noted that model checking is, inherently, a technique based on an idea of exhaustive testing and, in the case where a system interacts with the real world, it is necessary to supply a computational abstraction of the world as a model","In many cases it is also necessary to provide a model of the program to be verified, rather than the program itself but in the case of    this is not necessary though we do have to provide a model of the planner",In each of our scenarios we have constructed our models of the planner and the real world in a slightly different way in order to demonstrate the different ways verification may be used to establish facts about the system,"In each of our scenarios we first tested our agent in one specific situation and then verified it in a more general model. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                   5.1 Ethical principles for civilian UAs We assume that the UA agent operates only in civilian contexts",We establish a (small) list of relevant   ethical concerns as examples in order to show the method in action,"The list contains:   ( ),   ( ),   ( ), and   ( )",The (formal) ethical policy is given by comparing the concerns in terms of how unethical it is to violate them,"We propose the order  , with   meaning that it is more ethical to violate   than  ",Our substantive ethical policies were context-dependent refinements of the formal ethical policy,"In our prototype, each flight phase ( , landing, taxiing, take-off) of a UA constitutes one context  ","Since all contexts are known, and the UA can only be in one context at a time, the substantive concerns can be represented directly, omitting the formal-substantive relations. 5.2 Brake failure during line up In this scenario we examine a program for a UA to line up on a runway prior to take-off which includes plans for reacting to brake failure",We tested this in a simple simulation: Ahead of the aircraft is a second manned aircraft crossing the runway on a taxiway,To the left and right of the runway are runway lights (which can be damaged by aircraft taxiing over them),To the right of the runway is an airport staff member who has erroneously moved onto the manoeuvring area of the aerodrome,"The ethical concerns for this example, with the rank of each concern marked in parentheses, are:   When the agent determines that its brakes have failed it requests new routes from the ethical planner since its current route to line-up is no longer valid",The ethical planner quickly produces three potential routes:   Code Fragment 5.1 shows abridged    code for this example,"We use many syntactic conventions from BDI agent languages:   indicates the addition of a goal,  ;   indicates the addition of a belief,  ; and   indicates the removal of a belief","Plans consist of three parts, with the pattern   The “ ” is typically the addition of a goal or a belief (beliefs may be acquired thanks to the operation of perception and as a result of internal deliberation); the “ ” states conditions about the agent (in this example its beliefs) which must be true before the plan can become active; and the “ ” is a stack of “deeds” the agent performs in order to execute the plan","These deeds typically involve the addition and deletion of goals and beliefs as well as   ( ,  ) which indicate code that is delegated to non-rational parts of the systems (in this case, the route planning system)","In Fragment 5.1, during normal operation, the agent polls the vehicle’s sensors and, if all is well, it requests that the planner supplies routes for a normal take-off",The planner does this by sending predicates naming the routes to the agent which detects them via perception,Once the agent has a route (line 20) it then delegates the actual following of the route to the underlying control system ( ),"If the brakes fail after the vehicle’s sensors are polled, all these plans become unavailable (since   ceases to be true)",In this case the “external planner” returns a set of routes as plans shown in Code Fragment 5.2,We use the notation   to indicate the substantive ethical concerns that are violated by each plan,"On receiving these plans, and assessing the ethical policy, the agent elects to turn left. 
                         
                      
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                      5.2.1 Formal verification of brake failure on line-up example Following on from ideas in    we replaced the ethical planner we used in the case study with a model that contained a random component",This model assumed that plans could potentially be available that violated any combination of the ethical concerns in the policy,"An example of such a plan is given in Code fragment 5.3. 
                            
                         This “random” planner then selected a random subset of these plans and returned them to the agent","This meant we were no longer “testing” the agent in the simple simulation where we assumed the existence of aircraft and airport infrastructure in specific places in relation to the agent, but instead in a random environment where aircraft or infrastructure could potentially appear on any of the alternative routes",When executed in combination with a model-checking algorithm the random choice caused the search space to branch and so the model-checking examined every possible set of plans that might be returned by the ethical planner,This allowed us to show that the most ethical plan was always chosen no matter what set of plans were available,"In particular we formally verified the following properties, where the   formulae refer to the substantive ethical concerns used in the example. (Here ‘ ’ means “always in the future” and ‘ ’ means “agent believes”.)   Collectively these properties show that if the plan chosen violates some substantive ethical concern,  , then the other available plan choices all violated some concern that was equal to, or more severe than,  ",Further similar properties can be used to establish that the “most ethical” option is always chosen,"In effect this verification demonstrates that, on this example at least, the underlying implementation of ethical choice was correct","It took our system over 21 h to verify each of 65,534 combinations of the 15 possible plans giving a total verification time of nearly four days for the four properties. 5.3 Erratic intruder aircraft This example is based on a program for avoiding other aircraft in accordance with the Rules of the Air","This program takes into account the possibility that some other aircraft, possibly a malicious intruder, but potentially also some ill-trained new pilot, appears on a collision course with the UA and fails to take the anticipated evasive actions","In the environment we used for testing, the UA is cruising through civil airspace when it encounters an intruder aircraft approaching head on","Here the ROA (Rules of the Air) say that the UA should turn right, so the agent requests a route for turning right","However, this plan fails and the detect/avoid sensor (DAS) continues to indicate that the intruder aircraft is approaching",At this point the agent knows that it has already tried to turn to the right in order to avoid the intruder,Since the intruder is still approaching its first plan has failed,The agent has no more routes (or    plans) that apply since its only plans obey the ROA and would cause the agent to turn right again,At this point the ethical planner is invoked,The relevant substantive ethical concerns and their ranks are as follows:   The planner returns the plans shown in Code Fragment 5.4,The agent initially chooses to turn left,"In our test environment the oncoming aircraft once again matches the course change and so the agent then chooses to return to base. 
                         
                      An abridged version of the code for this example is shown in Code Fragment 5.5","Here,   causes the intention to suspend execution until the agent believes it has a route for turning right","The action   suspends the intention for a set time to allow the effects of actions to manifest. 
                         
                      Lines 14–15 are triggered when information arrives from the DAS that there is an intruder",As a result the flight phase changes from   to   and a new goal is set up to avoid a collision,"The existing, ROA-compliant, plan for this goal is to get a route for turning right, enact that route and wait a short period to see if a collision will now be avoided","If the plan succeeds the belief that there is an intruder will vanish, the flight phase can be changed back to cruise, and the goal will be achieved since the agent now believes a collision has been avoided (see the belief rule in line 8)","When the existing plan fails, the plans in Fragment 5.4 are added to the agent’s plan library",The first of these ( ) is attempted first,"This also fails and the agent then attempts the third plan ( ), which succeeds. 
                         
                         
                         
                         
                         
                      5.3.1 Formal verification of the erratic intruder example For this example we chose to leave the planner unchanged, so the ethical outcomes of turning left, emergency landing and returning to base remain fixed, but instead had random behaviour from the oncoming  ",Essentially we are examining the success and/or failure of all the possible plans in the scenario,"So rather than verifying every possible ethical annotation on a plan, we verified that the system continued to reason ethically as the external situation changed","Obviously it would have been possible to combine random behaviour on the part of the intruder, with random annotations on the available plans to increase the scope of the verification result","As in the previous example, we verified the following properties, where the   formulae refer to the substantive ethical concerns used in Example 2. (Here ‘ ’ means “always in the future” and ‘ ’ means “agent believes”.)   Collectively these properties show that if the plan chosen violates some substantive ethical concern,  , then the other available plan choices all violated some concern that was equal to, or more severe than,  ",The verification of each property took between 21 and 25 s and explored 54 model states on 3.06 GHz iMac with 4 GB of memory. 5.4 Fuel low Our final program was one for handling “fuel low” alerts from the Fuel subsystem causing the UA to attempt to land,"In our test environment, if the agent cannot locate a safe landing site the ethical planner is invoked and returns three options (shown with ethical concerns violated and their ranks):   The agent then chooses the most ethical–the third plan–although both the first and third plans violate an ethical concern of severity 4, the first plan also violates a concern of severity 3 while the third plan does not. 
                         
                         
                         
                         
                      5.4.1 Formal verification of the fuel low example For the verification of this example we chose not to examine all possible plans that could be returned, based on their ethical outcomes, but instead defined four possible plans (the three listed above and landing in an empty field which had no adverse ethical outcomes) and returned some non-empty subset of these plans at random","Effectively, in this case, the assumption in our model is that there are only ever four possible options: empty fields, fields with power lines, fields with people and public roads (and that the ethical concerns involved with each choice are fixed)",What may vary is which of these four options are available at any given time,"Depending on which plans were available we also allowed the agent to perceive the presence of the empty field, road, etc",The model-checking then explored all possible combinations of these plans that could be returned and we verified that the UA would always chose the most ethical of the possible landing sites,"In this case, instead of verifying that the most ethical choice was always made at an abstract level we verified based on the specifics of the options,  , we examined three properties:   where   indicates that   holds true in the environment ( , it is potentially  )","The verification of each property took between 7 and 10 s and explored 64 model states on 3.06 GHz iMac with 4 GB of memory. 5.5 Remark on the examples Our example programs are really only fragments of some larger program for control of a UA and in each case we have chosen to verify only a single property, demonstrating different ways models of the behaviour of the real world and the planning system can be created in order to allow verification","Obviously a full formal verification of an ethical UA would want to examine the full program, verify against several properties and use the model most appropriate to the full system— , a model based on the construction of the planner, ethical annotation system, and a detailed understanding of the operational environment (all aspects outside the scope of this paper)","Our aim has not been to present a verified ethical UA but to demonstrate how our system for reasoning about ethical concerns, can be combined with an existing system in order to verify properties relating to the ethical operation of an autonomous system. 6 Summary and future work Before an autonomous system is allowed to operate in a shared environment with people or other autonomous systems, sufficient assurances have to be provided that it will always behave within acceptable legal, ethical, and social boundaries","We propose a method for, and have implemented a working prototype of, an ethical extension to a rational agent governing an unmanned aircraft (UA)",The agent can be provided with a particular ethical policy it uses to distinguish among possible plans and to select the most ethical plan for execution,We are able to   formally that the prototype   performs an unethical action if the rest of the actions available to it are even less ethical,"Obviously there are limitations to what formal verification can tell us, particularly since many simplifications are involved",In our case we assume that the plans the agent receives have been correctly annotated with ethical consequences,Integration with assertion-based simulation and hardware-based testing can help in defining the limitations of formal verification and in developing a truly reliable system,A methodology for an integrated approach is currently being developed    and we would be interested in developing a fuller set of examples for our system and investigating them in such a framework,"The ethically enhanced agent is autonomous in the choice of actions, but not in the choice of ethical concerns and policies it will follow","These are constructed externally, but nonetheless agent-specific and can be private to the agent","The implemented agent follows only one ethical policy at any decision-making moment, because we assumed it can be in only one context at a time",We also assumed that all the contexts are known to the system designer,Our theoretical framework however is more general and does not involve these assumption,"The ethical governor of Arkin et al.   , and similar solutions to ensure ethical behaviour    “transforms” ethical machine behaviour into a constraint satisfaction problem, namely find a plan that is ethical","A list of precise ethical constraints is compiled, purified of ambiguities and inconsistencies, stopping the autonomous system from performing any activity that violates these constraints","This approach is simple, elegant, but foremost does away with all the complications of ethical reasoning","We consider situations in which no ethical course of action is possible, which is an issue not addressed in   ","Unfortunately, the moment we consider that an unethical action has to occur, the necessity for some form of ethical reasoning creeps in",We focus on establishing a minimal system for ethical decision-making that “transform” ethical machine behaviour into a   constraint satisfaction problem,The main contribution of our paper is a verifiable ethical decision-making framework that implements a specified ethical decision policy,In our approach we do not develop our own planner or method for generating plans,Rather we assume annotated plans are supplied to the agent,"For BDI agents that have access to such a planner we construct a method for selecting among unethical plans, when no ethical plan is available","Our method is verifiable, namely we can prove that if an agent chooses to execute a plan that is in any way unethical, it does so only when it believes that this is the minimally unethical course of action it has available",This way the ethical decision-making is done by the agent and one is able to use agent verification techniques to prove correct behaviour,"An alternative approach for engineering ethical behaviour of an agent is to develop a planner that only develops a plan that is ethical,  , “push” the ethical decision-making on the side of the planner",There are two immediate problems with this approach that we avoid in ours,"First, the possible ethical consequences of a plan cannot be known until the plan is developed","Although the goal of the plan may in itself be ethical, the means to reach it might not","Second, a planner is a tool that can be used by several agents","When the ethical decision-making is on the planner side, all the agents that use the same planner would be subject to the same ethical policy which may or may not be known to the agent",This infringes on the autonomy of the agent to a certain extent,"The ethically enhanced agent we described follows only one ethical policy at any decision-making moment, the agent can only be in one context at a time and all the contexts are known (the contexts being the flight phases of the UA)",The advantage yielded by this approach is in removing the need for additional calculation within the agent to relate specific plan outcomes to ethical principles via ethical rules,"However, it cannot always be the case that the contexts in which an autonomous system operates are predicted or non-overlapping",We would like to extend our implementation to consider the full framework of ethical reasoning as outlined in Section   ,Our implemented agent is also limited to only attempting a plan once for any given goal,"In a highly dynamic environment, where the predicted ethical outcomes of plans might be rapidly changing, this is clearly unsatisfactory and we would be interested in extending the system so that plans could be re-evaluated if their ethical outcomes had changed, while making sure that the system must, at some point, commit to some plan",Our general theory assumes that ethical rules can be expressed in terms of   where   is an action involved in some plan  ,"However in our examples we already see that the relevant actions ( , “collide with airport hardware”) may not be explicitly referenced by a plan ( , “turn right”)",More work is therefore needed to understand the nature of actions that are implicit or side effects of some plan in order to better enable the calculation of ethical consequences either within agents or planning systems,We would also like to extend our system so that it could account for uncertainty in the evaluation of ethical outcomes,There are two ways in which a context can influence the ethical constraining of an agent: (a) by making abstract principles substantive; and (b) by indicating which ethical policy should be used,"So far, we use one ethical policy and contexts influence ethical reasoning by specifying what counts as an ethical violation in them","However, it is not difficult to envision situations in which the agent would need to use a different policy","Consider for example, the principles of doing no harm and autonomy (in medicine)","During a minor surgery, the patient might prefer a local anaesthetic over a total one, however if the surgery turns out to be more complicated than predicted, the patient’s wishes for local anaesthetic will be disregarded, violating the autonomy principle, in the interest of not harming him/her, preserving the no-harm principle","However, a patient might express the desire to not be resuscitated if his/her heart stops","Under these special circumstances (heart failure), observing the principle of autonomy should be placed as more ethical in the policy than the principle of doing no harm",There are many ways in which a policy can be   with respect to a special context; we mention two,The most simple way would be to provide a replacement policy,"This method is only possible if the context is predetermined, and it also represents significant effort, since all principles have to be re-ordered",Another method is to only alter a part of the policy concerning specific principles,"For this method, update procedures need to be developed","One way to develop policy update procedures is to consider them as a special case of   and use a belief update operators, see for example    for such operators","Clearly, whether the requirements for belief update operators are fully suited for operators used to update ethical policies is an issue that merits further exploration","A further hindrance to using belief update to update ethical policies could arise from research in belief update being normative, studying which properties a belief update operator should satisfy, rather than operational,  , designing belief update operators","Finally, while our examples have been from the UA domain the approach and principles are general enough to be relevant across autonomous systems","Consequently, we aim to extend this work to the formal verification of domestic/healthcare robotics and driver-less cars in the future.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s1071581916300866, 1 Introduction Computer systems nowadays support humans in many different tasks,"Their superior abilities in searching, book-keeping and summarisation render them irreplaceable in many complex situations",However computer systems in general only address the informational needs of a human being,"For a stressful task, a human supporter will not only provide information but also attempt to alleviate the undesirable emotions being experienced by the task performer","This is a feature that is only beginning to be addressed in computer systems, typically in the context of virtual agents","Virtual agents are computer-generated virtual characters that interact intelligently with users typically taking on roles that normally performed by humans such as coaches, tutors or customer representatives",By   we refer to communications from a supporter to a task performer that do not provide concrete help with the details of the task but attempt to address the emotions that are being invoked by the task,We focus on emotions arising from the stressful nature of the task,"For instance, a supporter might reassure “Don't worry”, show empathy “I understand that you are feeling frustrated”, praise “you are doing a great job” or encourage “you can do this”",Human beings seem to be remarkably successful at giving emotional support,"At least, we trust them to give emotional support in key situations, often with very little training","Moreover, they are able to adapt their support to the type of situation being experienced, which is important as support provided in the wrong context can have a detrimental effect (cf.  , as reported in  )","This paper is about initial attempts to produce a computer algorithm able to capture some aspects of this human behaviour, in particular able to adapt emotional support to different stressors","In order to ask the question “what emotional support should be given in this situation?”, we start by addressing some more fundamental questions: The task is then that of modelling the human behaviour as a computer algorithm 
                       in particular deciding: 
                   The work of this paper was inspired by the MIME project ( ), which investigated the development of a computer aid for Community First Responders (CFRs) attending medical emergencies","CFRs are volunteers with limited medical training who attend medical emergencies, particularly in remote and rural areas, while an ambulance is en route","The computer aid enabled CFRs to measure and monitor the key medical parameters of the casualty, enter information about their observations and actions, and generated a handover report for the ambulance personnel when they arrived","Although the task of a CFR is known to be stressful in a number of ways, the MIME system only addressed the provision of factual information to the CFRs","Inspired by this, the research described in this paper asked the question “what sort of emotional support might a computer provide to people experiencing the kinds of stressors CFRs experience?”","Following a review of related work in  , in   we produce and validate textual scenarios depicting individual stressors, crowd source a corpus of emotional support statements, and reliably categorise these statements into emotional support categories","In  , we use the statements and scenarios to investigate what emotional support people offer to other people experiencing different stressors","Based on this, we develop three emotional support algorithms and evaluate these, leading to a refined algorithm and a further evaluation","An overview of this process is illustrated in  
                      .   concludes the paper and provides indications for future work. 2 Background and related work 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   2.1 Types of stress We assume in this paper that emotional support to a person is relevant when there is one or more   in the environment","A stressor is here just a cause of  , where we use Selye's definition of stress as “the non specific (that is, common) result of any demand upon the body [‥] be it a mental or somatic demand for survival and the accomplishments of our aims” ( )","The APA categorizes stress into the following three categories ( ): (1)   occurs over short durations of time and comes from pressures from the recent past or anticipated near future, (2)   is when an individual experiences one episode of acute stress which is then followed shortly and frequently by another episode, (3)   is a long term experience which is continuous over a long duration of time, such as months and years",We only consider emotional support for acute stress,"In previous work, to identify different stressors for CFRs, we started from the NASA-TLX ( ), a multi-dimensional subjective scale for measuring task workload developed by the American National Aeronautics and Space Administration","The NASA-TLX had been used to record participants' perception of task demands, such as those experienced by firefighters ( ) and had been used in various domains due to its high reported levels of internal validity ( ) and simplicity","The NASA-TLX measures six workload facets: mental demand, temporal demand, physical demand, frustration, effort, and performance","We excluded effort and performance, as it was assumed that CFRs would always give their maximum effort and that to measure performance would be inappropriate","Based on focus groups with CFRs, we added the emotional aspects of what CFRs may experience while fulfilling their duties, namely ‘worries about whether you were doing the right thing’, ‘irritation by external factors such as interruptions’, ‘feeling alone and isolated’, and ‘being upset about (anticipated) outcome’",The resulting set was used to investigate which stressors CFRs experience the most and during what moment of the call-outs (see   for the results of the survey),"The current work takes this list of stressors as a starting point, and considers mental demand, temporal demand, physical demand, emotional demand, frustration, interruption and isolation (see  
                          for informal glosses of these terms). 2.2 Emotional support Emotional support is an important social construct and a skill which people develop from an early age","However, it is not entirely straightforward and without complications",The same emotional support may not be equally suitable in different situations,"Some people, such as counselors, might be considered as being better at providing appropriate emotional support than others","As different stressors are present in different stressful events, it seems plausible that supportive messages should also vary, with some being considered better suited and more beneficial for one stressor than another ( )",Emotionally supportive actions are those aimed at increasing positive emotional states in conjunction with helping people overcome negative emotional states ( ),Emotional support can be considered as a synonym for comforting support,Comforting support can be defined as being “messages having the intended function of alleviating or lessening emotional distress…” ( ),We applied the same definition to emotional support and when the term   is used in this research it refers to this definition,"When received, emotional support has been shown to impact a person's mental and physical wellbeing as well as reduce negative affect such as stress ( )",Even the perception that emotional support is available to a person has been shown to be beneficial and can have important implications for stressors such as loneliness ( ),"Furthermore, as well as reducing stress, appropriate and sensitive support also strengthens the bond between the support provider and the receiver ( )",This could be potentially important in strengthening the bond between a user and a virtual agent,"However,   also warn that inappropriate support can exacerbate the recipients' stress and have detrimental effects on the relationship between provider and receiver",This is regardless of how good the intentions of the support provider are,This again promotes the need for further investigation into the different types of emotional support that a virtual agent should provide in different stressful circumstances,"The benefits of emotional support alongside its potential pitfalls accentuate the need for further exploration into the differing types of emotional support which could be provided by a system, such as a virtual agent, for different stressors","As discussed in more depth in  , we will base our emotional support categories on those proposed by  , namely Emotional Reflection, Praise, Emotional Advice, Reassurance, and Directed Action",There is at present no literature on what types of emotional support would be most appropriate for the stressors investigated in this paper,"In terms of the emotional support that embodied agents have provided in numerous studies to participants experiencing stress, there remains little to no distinction between the types of emotional support that should be given for different stressors",Researchers working on computer-generated emotional support have tended to focus only on the stressor frustration,"The emotional support provided in these studies (such as in  ) has also often considered support to be more of a ‘ ’ approach, with it being considered emotional support as long as it contains either empathy and or encouragement/praise",We will explore the related work more in the next subsection. 2.3 Agents providing emotional support to alleviate stress Several researchers have designed agents to alleviate negative moods experienced by users in stressful situations,"In this section, we explore which emotional support algorithms these agents used","We focus here on agents specifically targetting a stressor, that have been implemented, and the effectiveness of which has been evaluated (more work on computer-generated emotional support will be referred to when discussing emotional support categories in more detail in  ).  
                          provides a summary. 
                          used a frustrating computer game and compared the user's frustration levels between a condition that allowed the user to vent their emotions and a condition that ignored the user's emotions completely",The system provided affective support adapted from the participants' input to a questionnaire,"This ranged from, “ ” when the participant was least stressed, to “ ” for when they were most stressed","However, it is not clear from the paper which categories of emotional support were expressed","Although these statements were not validated or categorized into emotional support categories, we believe statements of this sentiment could be mapped to our emotional support category  , which aims to acknowledge how the recipient is feeling",Klein et al.'s study showed that the affect-support agent could reduce negative feelings after the users had been frustrated by network delays,"The studies by  , which were a partial replication of Klein et al.'s study, used the same method of delivering affective support as Klein et al. did (though using a virtual agent rather than just text messages)","Although Hone stated that “participants interacted with an affective agent with similar behaviour to that in [the study by]  ”, we do not know if the same emotional support messages where applied","Again, as the statements which Hone applied were not categorized, we considered them to belong to the   support category, similar to those applied in the study by Klein et al.",Hone's studies also found that affect-support agents reduce negative feelings after users have been frustrated,"In   and a number of similar studies, the emotional feedback that was provided to the user was given promptly after the user experienced frustration and this feedback was also delivered via a lifelike character and not just via text",The affective agent described in the study conveyed that it was either ‘happy for’ or ‘sad for’ the participant and expressed emotion through the use of body gesture and text,The agent would either smile to express happiness or hang its shoulders to express sorriness,"To express empathy to the participant (who had been subjected to system delays), the agent would utter “ .”","However, unlike in the aforementioned studies, we do not categorize this emotional support as belonging to any of our categories defined below","Although the statement is given in recognition that there was a delay which could have been a source of frustration to the participant and therefore could be categorized as  , it does not explicitly portray this and as such could either be categorized as commiseration or as simply stating a fact",Klein et al.'s study showed that the empathic agent reduced participants' stress level (as measured through skin conductance),"In a further study by  , the emotional support provided by the affective agent is more clearly defined",The different categories of support which were used by the agent in this study are shown in  ,"From the categories of support shown in  , it is clear that there are parallels in our defined emotional support categories and those that were applied by the agent in Prendinger and Ishizuka's study","Prendinger and Ishizuka's agent used four support categories, three of which can be compared to our five defined emotional support categories. ‘Show empathy’ can be compared to our emotional support category of  , where a sense of understanding for the user's emotions is conveyed. ‘Encouragement’ can be compared to our emotional support category of  , where the user is reassured about their abilities","And ‘Congratulate’ can be compared to our emotional support category of  , where the user's abilities are positively reinforced","The ‘Ignore’ response was a category which lacks any emotional support, in which the user's feeling are not recognized by the agent",We do not have a similar emotional support category to Prendinger and Ishizuka's ‘Ignore’ response,"While their study did not find an overall effect of the emphatic agent, there was some evidence that emphatic feedback helped lower stress levels while the participant listened to interviewer questions (in a virtual job interview setting). 
                          studied a virtual agent that reacted to frustration caused by a pop-up error message that provided an obstacle to answering survey questions, comparing between an emphatic agent and an apologetic one","In their work, the agent was part of the system that caused the frustration, hence the potential appropriateness of an apology",We do not have a similar emotional support category to Baylor et al.'s ‘Apology’ response,Their ‘Empathy’ response is similar to our   category,"Both versions of the agent also encouraged the user to provide feedback, so they could vent their emotions","In contrast to the other studies mentioned, Baylor et al.'s study actually found that both conditions increased user frustration, though users felt the emphatic agent was more believable","It is possible the venting option combined with the type (and amount) of frustration may have caused this, as this frustration was clearly the system's fault (more clearly than the network delay used by Klein et al)","In our studies, described later, we had two additional emotional support categories:   (where the person is told how to feel) and   (where the person is told what to do, or the manner in which to do it)",Neither of these emotional support categories were applied in the aforementioned studies,The presented studies have demonstrated that emotional support when provided by virtual agents can affect a person's emotional state,"However, these studies have only focused on the stressor of  , and the source of this frustration has been from the system itself (i.e. the system is to blame)","In the domain of pre-hospital care, there may be various different stressors (e.g. from the weather, the seriousness of the patient's condition) and the source of stress would not come from the system providing the support",We have already argued that people provide different emotional support depending on the stressor that the recipient is experiencing and that when the inappropriate emotional support is provided it may have detrimental effects ( ),This strongly promotes the need for an emotional support generating algorithm which an embodied affective support agent can use to tailor its support accordingly,"With this in mind, this paper presents several studies which investigate the development of an emotional support algorithm for the generation of effective and appropriate emotional support for different individual stressors. 3 Phase 1: stressor scenarios and emotional support categories This section summarizes and extends our previous work ( ) that lays the foundation for the remainder of this paper. 
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   3.1 Scenarios that describe individual stressors To investigate how different types of emotional support are provided for different stressors, we were first required to generate conditions in which participants would be able to empathize with somebody's stressful circumstances",One method for achieving this might be to show participants videos of people experiencing stressful situations or to have participants observe people experience stress in real life,"However such an approach would have considerable drawbacks, most notably the ethical issues involved with exposing participants to witnessing potentially distressing scenes and the practicality of observing people experiencing stress life","An alternative and much more viable option would be to devise a series of simple, textual statements in which a person and a stressor are described",We used the CFR role as an inspiration to produce these,These statements would then allow participants to take the place of a virtual agent and provide what they consider to be appropriate support,"This approach is referred to as the ‘User as Wizard’ method ( ). 
                          To simply provide a short sentence stating “John is a CFR and is feeling lonely” however is unlikely to elicit a strong enough feeling within participants so that they would empathize with John and provide true emotional support","As a solution to this we developed a set of stories where each describes a scenario involving a CFR and one particular stressor, using seven from nine stressors previously explored via a survey to CFRs ( ): temporal demand, mental demand, frustration, interruption, emotional demand, isolation, and physical demand",Glosses of what these terms were intended to mean are shown in   for our later validation study,The stressors ‘worries about whether you are doing the right thing’ and ‘being upset about the (anticipated) outcome’ were excluded as it was deemed that these stressors (which indicate how an individual cognitively interprets an event) would be challenging to accurately describe to someone not in the same situation and if described in a scenario are likely to resemble emotional demand,"The hand-constructed scenarios and the stressors they intended to describe are presented in  
                         . 3.2 Study 1: validation of scenarios that exhibit specific stressors We validated that the scenarios only described the stressor for which they were designed and not those intended to be described by the other scenarios. 
                         
                         
                         
                      
                         
                         
                         
                      3.2.1 Method 
                            
                            
                            
                            
                            
                            
                            
                         3.2.1.1 Participants Participants were recruited through Amazon's Mechanical Turk service ( ), a crowd-sourcing tool","On Mechanical Turk, adult participants (called  ) complete small tasks made available by   and are paid a small sum for completing the task successfully","For this study, participants had to be based in the US and have an acceptance rate of at least 90% (meaning that 90% of the work they have previously completed has been accepted by other requesters and graded as being of good quality)",Participants were paid $0.50 upon successful completion of the study,"Due to the language based nature of the study, potential participants first had to complete an English fluency test, namely the Cloze Test ( )",Workers who failed the test were excluded,"Thirty participants completed the study with 73% being male and 27% being female. 46.7% of participants were aged between 18 and 25, 46.7% were between 26 and 40 and 7.6% were between 41 and 65",The average duration of time taken by participants to complete the experiment was approximately 2.5 min,"Ethical approval was obtained for this study, and all other studies in this paper, from the University of Aberdeen College of Physical Sciences ethics committee","Participants provided informed consent. 
                               : Participants were introduced to the stress categories and their definitions (which are also shown in  )","Next, participants were presented with a scenario (as presented in  ) and asked to place it into one of the scenario categories (while still seeing the definitions)","All the scenario texts were preceded by a statement saying “[ ] is a Community First Responder, and is attending a call.”","At the start of the study, participants were informed that a CFR was a first aider",It was deemed unnecessary at this stage to inform participants fully of the role of a CFR,A screen capture of the study is shown in  ,"This process was repeated for each scenario, and the order of the scenarios was randomized. 
                               : The   ( ) was used as a metric for establishing how well categorized the scenarios were and therefore how well they described each unique stressor","The kappa value describes agreement amongst the ratings of participants, with 1 indicating unanimous agreement, 0.7 excellent and 0.4 moderate agreement. 
                                To be reliably categorized, the kappa score for the scenario had to be  . 3.2.2 Results 
                             shows the results of validation, as ordered by   ( )","The results presented in   show most scenarios strongly depict their intended stressor independently, as indicated by the Kappa scores","However, the scenario describing the stressor of   had only a moderate Kappa (0.4), as some participants identified the stressor as being either   or  ",A classification as temporal demand may have happened because it mentioned a time: “at least half an hour”,A classification as frustrating may have happened because it mentioned that the CFR had to wait for an ambulance to arrive,"Despite this scenario not being as strongly categorised as the others, the Kappa score was still adequate for it to be deemed as portraying  . 3.3 Study 2: generation of emotional support statements The previous study provided a set of validated scenarios each expressing a unique stressor",This allowed us to investigate what type of emotional support people would provide to individuals experiencing each individual stressor,"In this study, we were interested in gathering a corpus of emotional support statements which could be used to support CFRs experiencing such stressors","To achieve this, we ran a simple data gathering experiment which asked participants to provide emotional support to the CFR portrayed in each of the seven validated scenarios. 
                         
                         
                         
                         
                      
                         
                         
                         
                      3.3.1 Method 
                            : The study was run on Mechanical Turk, with the same conditions as in  , including the English fluency test","Twenty participants took part and were 60% male and 40% female. 20% of participants were aged between 18 and 25 years, 45% between 26 and 40 and 35% between 41 and 65",The average time to complete the study was approximately 8 min,"Participants were paid $0.50. 
                            : Participants were shown each of the seven scenarios from   and shown some examples of potential emotional support statements (taken from a different domain)","Participants were asked to provide three examples of emotional support for each scenario. 
                             The scenarios were presented in a randomized order","Participants were reminded that they were not expected to give any medical advice (decision support), only emotional support",Participants were asked at the end of the study if they had any comments,"A screen capture of this study is shown in  
                            . 3.3.2 Results Each participant gave three support statements per scenario, resulting in a total of 420 statements",The statements were processed during a focus group of researchers to filter out statements which were not considered to be emotional support (e.g. “Call 911”),"Inappropriate statements were those that were considered to be decision support, unsuitable to a general population (e.g. religious sentiment), or would not be suitable when provided by a virtual agent (e.g. “I will handle the passers-by so you do not need to deal with them”)","Of the remaining statements, duplicates and semantically similar statements were removed","This process resulted in 85 unique statements. 3.4 Deciding upon emotional support categories To decide upon emotional support categories for the generated statements, we used two approaches","Firstly, we ran an open card sorting session with three other members of our research department, where related statements were grouped together into categories","Secondly, we considered emotional support types used in the literature to try to make sense of the groupings resulting from the card sorting. 
                         
                          shows emotional support types used in the literature related to our initial categories from the card sorting","We decided that the existing classifications of empathy, active listening and to an extent sympathy portrayed the sense of understanding how a person was feeling",We therefore decided to use the category Emotional Reflection,We used the category Praise as the support types mentioned under Praise in   all include statements that make people feel better about themselves,"Following much discussion, it was decided that there was no clear distinction between Reassurance and Encouragement, with both seeking to motivate a person when things might not be going as well as expected",We therefore decided to merge these categories and use Reassurance,"We decided that Advice was too broad, and similarly to   divided this into Directed Action (informing a person what to do, or the manner in which to do it) and Emotional Advice (telling a person how to feel).  
                          shows the five resulting categories and their definitions",These categories were also applied to motivate learners by  ,We also considered the literature in sociology and psychology to see how well these categories cover existing support strategies,"Within sociology, social support researchers tend to distinguish five types of social support, namely informational, emotional, tangible, esteem and social network support ( )",Tangible support (e.g. offers to provide goods and services) and social network support (e.g. the sense of belonging to a group with similar interests) are outside the scope this paper,"Our support category Directed Action provides informational support, defined by Cutrona and Suhr as advice, factual input and feedback on actions","Our category Praise provides esteem support, defined by Cutrona and Suhr as expressions of regard for somebody's abilities, skills and intrinsic value","Our category Emotional Reflections provides what Cutrona and Suhr call emotional support, which they define as expressions of caring, empathy, sympathy and concern","Cutrona and Suhr do not explicitly mention Reassurance or Emotional Advice, but these seem to fit best within their overaching concept of nurturant support (efforts to comfort or console), and from its subcategories fit better with emotional support than with esteem or network support","In this paper, we call all our support categories emotional support, as they can aid users to regulate their emotions","Within psychology, there are two interrelated strands of work that are particularly relevant: (1) coping and (2) emotion regulation",Coping is defined as the “cognitive and behavioural efforts to manage specific external and/or internal demands that are appraised as taxing or exceeding the resources of the person” ( ),"Distinctions have been made between problem-focused and emotion-focused coping ( ), between active-cognitive, active-behavioural and avoidance coping ( ), and between approach and avoidance coping ( )","Many coping strategies exist; we will consider the 14 strategies distinguished by  .  
                          shows three types of coping (combining the categorisations mentioned above), the coping strategies from   that belong to these, and which of our emotional support categories are appropriate to support these strategies",We have not mapped our categories onto the Avoidance coping and Turning to religion strategies,"It has been argued that the Avoidance coping strategies are maladaptive (i.e., not having good effects in the long-term) ( ) and they are outside our research scope (e.g., we do not intend for the agent to prescribe drugs)","More positive results have been found for Turning to religion ( ), but this strategy is also outside our research scope","Emotion regulation is defined as the “processes by which individuals influence which emotions they have, when they have them, and how they experience and express these emotions” ( ).   distinguishes five types of strategies to regulate emotions: situation selection (approaching or avoiding certain situations to regulate emotions), situation modification (tailoring a situation to modify its emotional impact), attentional deployment (selecting which aspects of the situation to focus on), cognitive change (selecting which meaning to attach to a situation's aspect, e.g. reappraisal), and response modulation (influencing emotional response tendencies once they have been elicited, e.g. suppression).   shows which of our emotional support categories are appropriate to support the five emotion regulation strategy types","In summary, our emotional support categories cover support types in the literature, cover the social science categories of informational, emotional and esteem support, cover the problem-based and emotion-based coping strategies (with the exception of turning to religion), and cover the emotion-regulation strategies","It may be possible to produce more detailed subcategories in future in particular within Directed Action, to distinguish between Directed Action aimed to support each of the problem-based coping strategies, and/or to support each of the situation modification, situation selection and attentional deployment emotion regulation strategies. 3.5 Study 3: validation of emotional support statements' categorization As outlined in the previous section, a corpus of 85 unique emotional support statements for CFRs was obtained and categorized into a preliminary set of five categories",It was required that these statements were validated as belonging to these categories,"To achieve this, we ran another validation experiment similar to the study outlined in  . 
                         
                         
                         
                         
                      
                         
                         
                         
                      3.5.1 Method 
                            : As before, the validation experiment took the form of an online questionnaire administered on Mechanical Turk, with the same participation criteria as used previously",Forty participants partook in the study and were paid $0.50,"Participants were asked to indicate their gender, their age from a range and indicate if they were a health professional. 55% of participants were female and 45% were male. 22.5% were aged between 15 and 25, 45% between 26 and 40, 27.5% between 41 and 65 and 5% were aged over 65",Out of the participants 5% regarded themselves as being healthcare professionals,"The average completion time for the experiment was approximately 8 min. 
                            : Participants were shown an explanation of what being a CFR entailed, the five emotional support categories and their definitions","Next, they were shown each of the 85 statements in turn, and asked to select the category which they felt best fitted the statement",The order in which the statements were presented was randomised,The interface for this study is shown in  ,Participants could also categorize a statement into an ‘Other’ category if they felt that it did not fit any of our defined categories,"At the end of the study, participants were asked if they had any comments. 3.5.2 Results Of the 85 statements, 52 were categorized with at least moderate agreement (  
                            )","These are reported in  
                            ","Of the remaining statements, 29 were weakly categorized (Kappa  ), 2 statements had no clear decision (equal rating in more than one category) and 1 was strongly categorized as ‘Other’","Out of the five defined emotional support categories,   and   were the most reliably categorized, with at least one statement in each having complete agreement (Kappa=1) amongst participants","When reviewing the total number of statements,   had the most statements, with   having the least",This may indicate that our corpus simply had less examples of   to begin with,"However, this does not necessarily indicate that participants wanted to use less of this type of statement, as there may have been multiple examples of the same statement being used in the corpus, 
                             which would only result in one statement required to be validated.   also shows the top four statements for each category","These were the statements that we took forward into our studies about emotional support generation. 4 Phase 2: emotional support algorithms that tailor support to specific stressors 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   4.1 Study 4: initial emotional support algorithms generation In this study we investigated which categories of emotional support (if any), people would use when providing support to other people in scenarios exhibiting varying stressors. 
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                      4.1.1 Method 
                            : Participants were recruited using Amazon's Mechanical Turk service, similarly as in the previous studies",They were paid $0.50 upon completion of the study,"One hundred participants completed the study. 54% were male and 46% female. 26% were aged between 18 and 25, 53% between 26 and 40 and 21% between 41 and 65",The average time for completion was just under 7 min,Participants were also asked to indicate their level of medical expertise,This was to gain an insight into the potential expertise that the participants may have with the scenarios presented and how similar they may be to the medical expertise of CFRs,"Only 1% considered their expertise to be advanced, whereas 13% considered it to be intermediate and 53% as novice","Of the participants, 33% considered themselves to have no medical expertise at all. 
                            : The top four most strongly correlated statements from each support category (those with the highest Kappa score) were used (the italicised statements in  )",The decision to only select the top four statements was taken for two reasons,"Firstly, in the   category only four statements were strongly validated","As previously stated, this does not necessarily indicate that   is a weak emotional support category, only that in the corpus gathering process people may have simply provided very similar statements for this category","Secondly, it was deemed undesirable to overwhelm participants with a vast list of statements from which they would have to choose","If a participant was overwhelmed with a large list of options, it was hypothesized that they may simply pick the first suitable option instead of reviewing the full list and selecting the one they considered to be the most appropriate. 
                            : Participants were presented with the seven scenarios (as shown in  ) which had been validated to describe a unique source of stress",The scenarios were presented in a randomised order,"For the purposes of this study, CFRs (who were originally described in the scenarios during their validation) were referred to simply as first aiders",Participants were asked to provide support to the first aider in the scenario using a set of statements made up from the top four previously validated statements from each emotional support category,These 20 statements were presented in a randomised order,"Participants could provide multiple statements of support for each scenario. 
                             Participants could string together these statements using a list of conjunctions and the system would automatically display their proposed support message",The system also allowed participants to edit their support by removing previously added statements and conjunctions,"When participants were happy with their support, they progressed to the next scenario","A screen capture of the system is shown in  
                            . 
                            : We hypothesise that the categories of emotional support used will differ depending on the stressor. 4.1.2 Results 
                            : We investigated the categories of emotional support which participants used for each stressor.  
                             shows the results if multiples of the same emotional support category were only counted once (e.g., if a participants gave two   statements, this was only counted as one)",People provided different categories of emotional support depending on the cause of the stress,"For example,   which had been applied in previous related work for stress caused by frustration, as shown in  , was indeed the most commonly used emotional support category in our study for frustration, but used less for say mental demand. 
                            
                             shows the results if multiples of the same emotional support category were taken into account. 
                             This took into consideration that participants were allowed to provide up to four of the same emotional support category in the feedback that they provided",The most frequently used emotional support categories for all stressors have not changed but rather have only been accentuated,"For example for the scenario describing Temporal Demand, the emotional support category   has extended its advantage over the other categories","To investigate how the use of different support categories compared between different stressors we performed Kruskal–Wallis tests, with stressor scenario as the independent variable, and a numerical dependent variable for each emotional support category, counting how often a participant used that category for that stressor",Multiples that were applied by participants of the same emotional support category were counted in this analysis,This analysis revealed that there were statistically significant differences in the categories of emotional support which people provided based on the stressor that was currently being experienced by the recipient,"A statistically significant effect was shown for the following support categories between the different stressors described in our scenarios:   
                            ,   
                            ,   
                            , and   
                            ",These results have been Bonferroni corrected,A statistically significant result was not found for the emotional support category  ,"Next, we explored how the different categories of emotional support provided by our participants varied for each stressor","For this analysis we identified the most frequently used support category for each scenario (stressor), as shown in  , and compared it with the other support categories using Wilcoxon Signed Rank tests 
                             (Bonferroni corrected)","Multiples of the same emotional support category used by participants were included in this analysis (see  ).   was shown to have been provided a significantly larger number of times by participants than any other category for the scenarios for which it was the most frequently used:  ,  , and   
                            .   was also shown to be significantly higher than all others for the scenarios for which it was the most frequently used:   and   
                            .   was found to be significantly higher than only   for   
                            , as was   for   
                            ",This is expected from the data represented in  ,These results indicate that people do alter the category of emotional support that they provide depending on the cause of the stress that is being experienced by the recipient of that support,"This confirms our hypothesis and supports the differences in the data presented in  . 
                            : Next, we investigated what were the most frequent combinations of emotional support category that people used for each stressor","As the average number of statements used by participants was two (ranging from 2.03 to 2.24), we performed this analysis to a depth of two",To aid readability we only report the combinations for the most frequently used emotional support category per scenario,We began with analysis of the number of occurrences for when the top emotional support category was given solely by itself,"Next, we calculated the number of times the leading emotional support category was given multiple times or in combination with other emotional support categories (in any order)","These results are reported in  
                            . 4.2 Algorithm generation From the results of the study, we developed three algorithms for providing emotional support to people experiencing different stressors","These algorithms are presented in  
                         ","The following rationale was used for the generation of these algorithms: 
                      
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      4.2.1 Order of support statements chosen by our algorithms To determine the order in which our algorithms should present the emotional support statements, we analysed the order that the chosen statements had been given by our participants","As a result, a support category that was more commonly applied before another, when given in conjunction with that category, was also provided first by our algorithms","For example, for   the most commonly provided categories were   and  ","Participants provided   before   38 times in their support, compared to only 3 for the vice versa","The results of this analysis have been applied to our algorithms, as depicted in  . 4.2.2 Algorithm accuracy and validation To investigate how well our proposed algorithms describe the collected data we calculated the Dice coefficient ( ) for each algorithm. 
                             The Dice coefficient is a well-accepted distance metric for computing the degree of similarity between two sets",We use it here to determine how similar the emotional support generated by our algorithms is to the emotional support provided by the participants,"The Dice coefficient was computed by multiplying the number of emotional support categories that the two support messages had in common by two, divided by the overall number of statements in each: where  
                             is the set of emotional support categories that our algorithm used and  
                             the set of emotional support categories that the participant selected","As the formula uses sets, duplicates of the same support category are ignored (i.e. a message with two praise statements and a message with one praise statement are treated the same)","For example, if our algorithm picked a   and a   statement and a participant chose to offer  ,   as well as  , the Dice coefficient score would be: 2 2/(2+3)=0.8","Dice coefficient scores range from 0 to 1, where 1 indicates a perfect match with the data",The Dice coefficient scores for our algorithms are shown in  ,"From the scores, we can see that Algorithm 2 and Algorithm 3 match the data gathered in this study slightly better than Algorithm 1","Although the scores for these algorithms were promising, we could not determine from this data which was the more effective algorithm for generating emotional support. 4.2.3 Selecting individual statements The algorithms so far only prescribe which emotional support categories to use and in which order","To apply the algorithms, and be able to evaluate their effectiveness in future studies, a selection also needs to be made of which statement to use within a category",It is the intention that a virtual agent in future would be able to choose a random statement from a set of statements for each category,"However, while we have the statements produced by the crowd-sourcing, we do not know whether they would all be as effective","We decided to use participants' frequency of use as an estimation of effectiveness, assuming that statements that had been used more frequently would be more effective","To determine whether certain statements were better suited to particular stressors than others within the categories, we analysed which statements within each category were more commonly used.  
                             shows that certain statements were far more frequently used than others","For example, for Mental Demand, “ ” was never used while two other   statements were used very frequently",We decided that the algorithms would use the most frequently used statement in the category for that stressor (as indicated in  ),"For example, for Isolation, the reassurance used by the algorithms will be “ ' ”","This choice was made to ensure the comparison between algorithms would be fair, with each algorithm using statements that had been frequently used by participants for each stressor","To determine which conjunctions to use to connect the statements selected by our algorithms, we investigated the most commonly used conjunctions when participants had used the same combination of categories","These were used to produce the combined statements in  
                            . 4.3 Study 5: evaluation and refinement of algorithms In this study we evaluated the emotional support generated by the algorithms for the stressors described in the scenarios",This was to determine which algorithm generated the most appropriate emotional support for each individual stressor,The most appropriate strategy for each of the stressors would then be applied to create a new and improved algorithm,"To achieve this, a different approach to how participants evaluated the generated emotional support statements was taken",Participants were asked to imagine that they were the person described in the presented scenario and to rate the generated emotional support messages presented to them,"This signifies an important change as to how participants evaluated the statements, as the emotional support that one person may give to a stressed individual may differ from the emotional support that they would wish to receive if they were that stressed individual. 
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      4.3.1 Method 
                            : As before, we used Amazon's Mechanical Turk and the same methodology and necessary requirements for the recruitment of participants","One hundred participants completed the study. 51% were female and 49% male. 20% were aged between 18 and 25, 49% aged between 26 and 40 and 31% aged between 41 and 65",The average completion time was 6.5 min,"Participants were paid $1. 
                            : Participants were shown the seven previously validated scenarios (in randomised order), but with a slight variation","For this study, instead of describing another person in the scenario (i.e. ‘Mark is a first aider’), participants were told to imagine themselves in the scenario in place of the first aider","For each scenario, participants were shown the unique statements generated by the algorithms for that scenario (as shown in  ) in randomized order","For example, if an ‘algorithm A’ and an ‘algorithm B’ produced the same emotional support for the same stressor then only one message would be shown for participants to rate","They rated each statement on four scales measuring  ,  ,  , and  ",These scales have been previously applied to validate emotional support cf. ( ),Responses for each scale were recorded on a Likert scale ranging from 1 to 9,"A screen capture is shown in  
                            . 4.3.2 Results The independent variables in the analysis are scenario and emotional support message (as selected by the algorithms)",The results of the ratings which participants gave for each message are presented in  ,"There was a clear winning (received the highest score) support message for each scenario, as reported on each of the four scales","The highest rated message for each scenario is presented in  
                            ","For the scenario describing  , a MANOVA showed there was a significant effect of emotional support message on the four dependent variables (our scales), Pillai's trace F(8, 590)=7.58,  , partial  
                            =0.09",Each dependent variable was subjected to a further ANOVA analysis in order to show whether this trend was the same for each of the separate dependent variables,"For the measure of difference between the scale   and the other three scales, an ANOVA showed there was an overall significant difference between the means, F(2, 297)=18.18,  , partial  
                            =0.1","Similarly, significant differences were found for:   (F(2, 297)=29.31,  , partial  
                            =0.16),   (F(2, 297)=30.5,  , partial  
                            =0.17) and   (F(2, 297)=22.88,  , partial  
                            =0.13)","Similar conclusions could be drawn for the following stressors which were described in our scenarios:  
                         Despite a significant effect on emotional support message being reported for both Emotional Demand and Isolation  , no significant comparisons were reported as to whether the trends for each scale were similar",No comparisons could be made for the stressor Frustration as it considered only one unique emotional support message,"The results indicate that Algorithm 2 was marginally better at generating emotional support than Algorithm 3, with five of the highest rated messages being generated by Algorithm 2 compared to four being generated by Algorithm 3. 
                             This is shown in  ",Algorithm 1 was shown to perform relatively poorly with only one of the highest rated emotional support messages being generated by it,"The emotional support generated in that instance was also generated by Algorithm 2 and 3, therefore rendering Algorithm 1 unnecessary. 
                            
                            
                            
                         4.3.2.1 Scale correlation Interestingly, the winning support strategy for each scenario was ranked as having the highest mean values on all four scales",This posed the question as to whether participants perceived there to be any distinction between the measurements of each individual scale,"Using Cronbach's Alpha analysis the alpha reliability of the four scales was 0.94, indicating the scales to have a very strong reliability",An alpha of .70 or above is considered satisfactory,"This indicated that when a participant rated a statement highly (or lowly) on one scale, they also rated it similarly on the other scales",The scale measuring ‘ ’ was the most strongly correlated with the others,"As the alpha value is very high, it also suggests that there is a high level of item redundancy; that is, participants are not making a clear distinction between the scale measures and are therefore rating them similarly","This would suggest that not all of the four scales are regarded in further studies, and the scale could be reduced to three items","These results are presented in  
                               . 4.3.3 Optimized algorithm Based on the study results, a refined algorithm has been designed which produces emotional support considered the best of those produced by our previous algorithms","The emotional support categories which the ‘ ’ algorithm applies, and the messages used to instantiate these categories (using the same rationale as above for using the most frequently used messages) are shown in  ","The results of the study (as presented in  ) showed that the messages applied by our optimized algorithm were considered to perform well (above average on the scales for Appropriateness, Effectiveness, Helpfulness and Sensitivity)",The refined algorithm uses four unique emotional support strategies as being the most suitable for our identified stressors (instantiated using seven unique emotional support messages),"For Isolation, it uses support comprised of   and  ","For Physical Demand and Interruption, it uses support comprised of   and  ","For Temporal Demand and Mental Demand, it uses support comprised of   and  ","For Frustration and Emotional Demand, it uses support comprised of   and  ",Three out of the four unique emotional support strategies include the emotional support category  ,This suggests that   is a suitable form of emotional support for most stressors,"Between all of the emotional support strategies, each one of our defined emotional support categories was used. 4.4 Study 6: evaluation of the final emotional support algorithm The previous studies have led to the development of an emotional support algorithm that has been evaluated to provide the most suitable emotional support messages from a corpus of statements which a human might provide for individual stressors","Following the development of our algorithm, this study aimed to investigate whether the generated emotional support by our algorithm was uniquely suitable to individual stressors (depicted by our scenarios) or if there are more dominant and general emotional support paradigms which people choose to apply","In other words, if the emotional support generated by our algorithm was solely appropriate for the stressor that it was specifically tailored for or whether it was considered to be appropriate for other stressors",This study presented participants with multiple scenarios each validated to describe a unique main stressor,Participants were then presented with each of the unique emotional support messages generated by our algorithm and were asked to rate how effective they considered the message to be for that each scenario,This study acted as an evaluation of our algorithm and investigated whether there are genuine differences between the emotional support strategies used by our algorithm when given for different stressors,"This study sought to provide evidence as to how well our emotional support algorithm performs. 
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      4.4.1 Method 
                            : As before, we used Amazon's Mechanical Turk and the same methodology and necessary requirements for the recruitment of participants","One hundred participants completed the study. 54% were male and 46% female. 13% were aged between 18 and 25, 55% aged between 26 and 40, 30% aged between 41 and 65, and 2% aged over 65",The average completion time was 7.5 min,"Participants were paid $0.70. 
                            : Participants were shown the seven scenarios, each having been previously validated to depict a unique stressor, in randomised order","They were presented with the seven emotional support messages generated by our refined algorithm (see  ) in randomised order and rated how effective they considered each message to be for the current scenario. 
                             To reduce the time needed to complete the study (given participants needed to provide 49 judgements), we have only applied the most strongly correlated scale 
                             from the four scale measures we used previously for the measurement of participants' ratings during this study. 
                             This was the scale which measured ‘ ’","As in the previous studies, this scale ranged from 1 ( ) to 9 ( )",Participants were again asked to imagine that they were the person depicted in the scenario and were the receiver of the support rather than the provider,"The seven emotional support messages were presented one after the other for each scenario, in a randomised order","A screen shot is shown in  
                            . 4.4.2 Results The median effectiveness rating for each message for each scenario is presented in  
                            ",The   presented in   indicates which emotional support message was intended for that stressor,"From the data presented in this chart, it is clear that the message originally considered to be the most suitable for   has been universally considered as being the most (or equivalently as) effective across all scenarios by our participants",This message consisted of categories   and   and is shown in  ,"However, as can be observed from the chart in  , other emotional support messages have been found to be as equally effective for their intended stressors","This is true for the emotional support strategies designed for   and  , with the strategy designed for   also having been considered to be the most effective for its intended stressor. 
                            : There was a clear distinction between emotional support messages in terms of   for all of the scenarios",A one-way unrelated analysis of variance was performed for each scenario,"An overall significant effect between the rating of messages was found for all scenarios: Physical Demand (F(6, 693)=42.23,  ), Temporal Demand (F(6, 693)=24.40,  ), Mental Demand (F(6, 693)=13.86,  ), Emotional Demand (F(6, 693)=31.50,  ), Interruption (F(6, 693)=39.37,  ), Frustration (F(6, 693)=36.40,  ), and Isolation (F(6, 693)=53.58,  ).  
                             presents the results from Scheffé's test 
                             for each scenario showing the significant differences between the generated emotional support messages in each scenario. 
                            : Out of the seven emotional support messages provided by our algorithm, three were rated highest for their corresponding scenario (the scenario for which they were designed), namely the messages intended for Physical Demand, Interruption and Isolation","The three messages intended for Emotional Demand, Mental Demand and Frustration performed sufficiently well; they were not found to be significantly worse than other messages in their corresponding scenario",Only the message intended for Temporal Demand performed badly; significantly worse than several other messages in the Temporal Demand scenario,"Interestingly, the message intended for when the recipient experienced Physical Demand was rated consistently highly across all scenarios",This would suggest that the emotional support message generation by our algorithm for Physical Demand is applicable for all of our described stressful scenarios,"Despite not having been rated as the most effective emotional support message for its intended stressor (though not significantly worse than any other), the message designed for Mental Demand was still rated as more effective for that stressor than any other one",The reason for it not being rated higher may have been due to the wording of the message rather than the emotional support categories that it was comprised of,"Similar results were obtained for the message intended for Frustration, with the message's highest rating coming in the scenario describing Frustration as well as in the scenario describing Isolation",These results may indicate a problem with the Isolation scenario describing not solely isolation but also frustration (this scenario only validated with marginal kappa),"Interestingly, the emotional support message intended for Emotional Demand had a similarly high effectiveness across all scenarios",This may indicate that emotional support intended for Emotional Demand is universally appropriate and effective for all types of stress (if not the most effective),"No interesting trends were observed for the emotional support message intended for Temporal Demand, with it being rated consistently lowly across scenarios",Rather than just focusing on the individual emotional support messages (i.e. the wording used) generated by our algorithm we also investigate the differences found between strategies (i.e. the emotional support categories used to obtain the statements for the message) comprised of the same categories,"Despite our messages all being generated with validated emotional support statements, there may still be statement bias for certain stressors, where one keyword may be considered more suitable than another","With this in mind, we then looked to explore how the different emotional support strategies were rated by participants. 
                            : While the results above indicate that the algorithm is performing well (except for Temporal Demand), this result is linked to the messages used to instantiate the support categories, and other messages may have had worse results","To investigate this further, we also considered the results per emotional support strategy","As discussed above, the algorithm generated messages using four unique emotional support strategies, as listed below and indicated by colour in  .  
                         The results of a statistical analysis (ANOVA) for the difference of user rated   for these strategies are reported in  
                            ","To aid readability, only statistically significant results are shown. 
                            : The comparison between the different emotional support strategies used by our algorithm for different stressors provided some interesting conclusions","Firstly, reflecting on the effectiveness of the algorithm,   performed well for both stressors it had been designed for, namely Physical Demand and Interruption, independent of the message used","It performed better for those stressors than   and  , and as well as  .   performed well for both stressors it had been designed for, namely Frustration and Emotional Demand","It performed better than  , as good as  , as good as   on Frustration, but not as good on Emotional Demand where one of its messages performed better than the other","Only one message was used for  , so we cannot reflect on the influence of its message wording.   performed similarly badly on all stressors with the exception of Mental Demand independent of the message used","However, for Mental Demand, the   message designed for Temporal Demand was clearly less effective than the message designed for Mental Demand","So, overall, we see some influence of message choice, but also evidence that different messages within a category can perform very similarly","The significant difference between   and all other strategies for all stressors except for Mental Demand for  , shows that   was considered relatively ineffective for all stressors expect for Mental Demand (one of the two stressors for which it was designed, the other being Temporal Demand).   was the only strategy including   suggesting that although people consider it appropriate to give   to those experiencing stress (as validated through our algorithm generation process), they may not necessarily wish to receive   when experiencing stress","This hypothesis has some support from the literature as studies have suggested that the emotional support that people provide to a stressed individual may sometimes be considered insensitive, unhelpful or inappropriate by the receiver ( )",The exception found for Mental Demand may indicate that people only wish to receive   when the stress they are experiencing is down to their perceived abilities to cope,"Although the strategy was rated as performing relatively well for this stressor, the other strategies were as well, suggesting that those are equally appropriate","Therefore, we can conclude that participants have rated this emotional support strategy on the whole as ineffective",The lack of significant differences between   and   for any stressor may indicate that people find   and   equally effective in stressful situations,"In contrast, the significant differences for every stressor between   and   on the one hand and   on the other hand shows people perceived the effectiveness of   as lower compared to   and  , when used with  . 
                             2 is interesting as it is the only support strategy that does not include  ","Based on the results in   and  ,  , which consisted of   and  , was considered to be reasonably effective across all scenarios","Finally, it needs to be considered that when participants generated emotional support messages, they were presented with many possibilities to choose from","This enabled them to create plausible messages, but we cannot assume that they would always have created the most optimal message","However, when we asked participants to make a decision between a much small number of possibilities, as in this algorithm evaluation study, this became a much easier task for people to achieve reliably. 4.5 Limitations A limitation with the final study and with research of this nature is that people tend to rate strategies on the exact wording and phrasing of the message, not just on the categories of support that it contains","Despite our attempts to ensure that each strategy was comprised of previously validated statements, categorized into specific emotional support categories, it remains hard to guarantee that the wording or phrasing of one statement when used in conjunction with another might not have unintentionally made the message become applicable to other stressors",Whereas other generated messages using different statements from our emotional support categories may have been inadvertently considered as being better suited as support for the stressor which another message was intended for,This emphasizes the potential limitation of a using a limited set of examples for each emotional support strategy,Another limitation of this study and the previous ones is that each stressor has only been depicted through one scenario,"It is possible that the support strategies participants used, and their ratings of support strategies and messages may have been influenced by the wordings of the specific scenarios","We do not expect that it would influence the choice of strategies (though this warrants investigation), but it may well influence the choice of particular messages within the category","For example, one message, namely the ‘be glad that you can help’ one used for  , seems linked to the scenario (where the CFR is helping a patient) and may not be appropriate for other frustration situations",The others used seem quite generic,Caution should also be taken when drawing conclusions about how appropriate the emotional support generated for Frustration is by our algorithm,This is because the same emotional support strategy has been applied throughout the algorithm refinement process as each of our three original algorithms applied the same strategy,"Similarly, when drawing conclusions about how appropriate the emotional support strategies are, it should be noted that the strategy   only has one instance (as it was only intended for Isolation) and so was not subjected to comparisons",With only one instance the potential for participants not liking the specific use of language employed by the strategy increases,A clear limitation of all studies described in this section is that they took place in artificial situations,This meant that participants were under no stress when rating the emotional support messages and may have had difficulty imagining the described scenarios,"How one thinks one would react to support in a situation may differ from how one actually reacts if that situation were a reality. 4.6 Summary In this section, we investigated how people provide emotional support for recipients who are experiencing different stressors",This led to the development of an algorithm which could be used by an embodied agent to provide appropriate emotional support depending on the source of the recipient's stress,"This process began with the generation of three algorithms, as detailed in  ",These algorithms were then refined to produce one algorithm through the process of applying what was considered as the best emotional support strategy used by the algorithms for each of our identified stressors,This process was fulfilled by the study detailed in  ,The performance of this algorithm was then evaluated in the study presented in  ,"The emotional support messages which were produced by our algorithm and the emotional support strategies which it used, were assessed in the study","We assessed whether the emotional support messages and strategies that had been produced for each stressor were unique to that individual stressor, or if some messages and strategies where equally suitable for those which they were not designed for",The results of the study revealed that some of the generated messages were appropriate for more than the stressor that they had been designed for,Equally the results demonstrated that certain emotional support messages and strategies are more appropriate for individual stressors,These results have provided empirical evidence to support the answer to the research question: What are the different strategies for providing emotional support to people experiencing different stressors? The emotional support strategy comprising of   and   was rated highly not only for its intended stressor for which it was designed (Interruption and Physical Demand) but was also rated highly for all the other identified stressors,"Similarly, the emotional support category designed for Isolation (comprised of   and  ) was rated, not only as performing very well for its intended stressors but as performing consistently well across all the other identified stressors","Although not as comprehensive, the emotional support categories designed for Emotional Demand and Frustration (comprised of   and  ) and for Mental Demand (comprised of   and  ) also performed adequately for the stressors for which they were designed","Importantly the emotional support strategy designed for Temporal Demand was not considered to have performed well, for its intended stressor or for any other stressor (despite being comprised of the same emotional support categories as applied for Mental Demand)","These conclusions signify an important step for the development of a virtual agent (potentially perceived as a teammate) which seeks to provide appropriate and effective emotional support. 5 Conclusions and future work This paper presents the empirically-led development and the evaluation of an emotional support algorithm that tailors its support to the stressor experienced. 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   5.1 Main results and lessons learned 
                         
                          summarizes the six studies presented and their main results","In Phase 1 of the research, we first developed and validated seven scenarios, each of which describes a situation that exhibits an individual stressor","These validated scenarios enabled a study into how people provide emotional support to individuals experiencing different stressors, resulting in a corpus of emotional support statements",We identified emotional support categories and validated a subset of statements as strongly belonging to a particular category,"In Phase 1, we learned that crowd-sourcing is a rapid and effective way to (1) rapidly generate a corpus of emotional support statements for scenarios, (2) validate that a scenario depicts a particular stressor for the general public, and (3) validate that an emotional support statement belongs to a particular emotional support category","As we have shown in this paper, the scenarios, categories, and statements enable research into how humans provide emotional support, and the development and evaluation of emotional support algorithms","We expect that the validated statements in particular will be useful for others' research, as will the method used here to obtain them","Next, in Phase 2 of the research, three studies inspired the development of and evaluated the effectiveness of an algorithm that generates emotional support tailored to a specific stressor","Study 4 showed that people use differing categories of emotional support when supporting others, depending on the stressor which that person is experiencing","Interestingly, we also found that despite all emotional support statements having been reliably classified into categories some statements within each category were more favourably used","From the results, three emotional support algorithms were developed",These algorithms were then refined by Study 5 resulting in the development of one ‘optimized’ algorithm,This process revealed four unique emotional support strategies for providing support for specific stressors,Each identified emotional support category was used by at least one of these strategies,Study 6 evaluated our algorithm from the point of view of the receiver of the support,"The results suggested that the algorithm performed very well for Isolation, Interruption and Physical Demand, as the tailored support generated for those stressors was rated as most effective for those stressors","The messages designed for Emotional Demand, Mental Demand and Frustration all performed adequately, with no other messages being rated significantly better for those stressors","Only the support designed for Temporal Demand performed badly, with messages designed for other stressors being rated significantly higher for this scenario",This was not unexpected as the initial ratings of the messages designed for Temporal Demand during the development of our algorithm received a lower rating of effectiveness than the messages for other stressors,"The results also showed that certain emotional support strategies were better received than others with the emotional support strategy consisting of ‘Emotional Reflection’ and ‘Praise’ performing well for all stressors, while the strategy consisting of ‘Praise’ and ‘Directed Action’ performed poorly for all scenarios except for that describing mental demand","In Phase 2, we learned that emotional support needs to be tailored to the stressor experienced, and that crowd-sourcing can be used to (1) inspire emotional support algorithms, (2) evaluate the perceived effectiveness of emotional support algorithms",We learned about the relative effectiveness of different strategies for different stressors,"We also learned that even when using emotional support categories, one still needs to be careful with the design of individual messages within a category","We expect that the method we followed here can be of use to other researchers, and that the resulting algorithm (with an adaptation for Temporal Demand where it did not perform well) will be implemented in virtual agents and further investigated","Our recommendations to practitioners can be seen in  
                         . 5.2 Directions for future work We validated scenarios which each described one stressor","Research is needed to design scenarios that describe combinations of stressors, as multiple stressors can be present in a situation simultaneously (as also indicated in discussions we have had with CFRs)","Such scenarios would enable an investigation into whether the emotional support that people provide or wish to receive varies depending on combinations of stressors, and the development of a more sophisticated emotional support algorithm","As discussed above, there is a possible limitation due to the use of only one scenario to depict a stressor, and it would be good to develop additional scenarios, to be able to investigate whether the wordings of the scenarios have impacted the strategies used",We validated a corpus of categorised emotional support statements.This corpus could be further populated by future research allowing for a greater depth and richness of statements from which the algorithm can generate emotional support,This may also provide the opportunity to apply techniques such as Natural Language Generation allowing the algorithm to ‘go beyond’ the restrictions of concatenating statements together to generate support,"The algorithm produced adapts to stressors, but not yet to user characteristics","We want to investigate how user characteristics such as personality, gender, age, and cultural background may impact what support people would provide, would like to receive, and how they would react to support provided by a virtual agent",Tailoring support to these characteristics may lead to a more effective algorithm,"For example, when we investigated the use of individual support categories (see  ), we found that   was not the most appropriate strategy for any of the scenarios","However, praise is related to Esteem support (see  ), and it can be hypothesized that esteem support is more needed for people who lack self-esteem","Within the domain of e-learning, there has been some work showing that emotional support needs adapting to learner personality ( )",We have not yet implemented the algorithm into a virtual agent,"Before the studies presented here, we had already produced a virtual agent that attempted to alleviate user stress (see  ) and measured its impact on stress through sensors and self-reporting questionnaires","While performing that work, we realized that we needed to more thoroughly understand how best to provide support which formed the rationale for this research",The next step would be to implement our algorithm into this virtual agent,Our studies did not evaluate how effective the emotional support was considered to be by participants when they were experiencing actual stress,"In future research we could seek to investigate how effective the support was considered to be in a stressful situation, using a methodology similar to the one we used in   or immersing participants in a controlled stressful scenario in a virtual reality setting","To achieve this, task based scenarios would need to be developed that could be undertaken within a controlled environment, which would induce a specific stressor","Additionally, a longitudinal study could be carried out with the agent providing emotional support over a longer period",The relationship between the virtual agent and the user may impact the effectiveness of the emotional support,"In our previous research we portrayed the virtual agent as having a relationship that is   to the participant, i.e. the agent was portrayed as an expert","Alternatively, the relationship could be portrayed as  , i.e. the agent being portrayed as a peer",The effect of this on emotional support needs to be investigated,The agent's appearance is also likely to impact on the effectiveness of its emotional support,"The development of intelligent agents that interact with humans in a human like manner, with dynamic and rich interactions such as appearance and movement has been the focus of innumerable studies (e.g.  )",Research in to how virtual agent can effectively portray emotions (cf.  ) may also reinforce the emotional support which a virtual agent is trying to convey,"As well as appearance, there are several experimental approaches and aspects of interaction surrounding embodied agents that could be explored (cf.  )","People may be reluctant to believe that they can receive emotional support from a computer (as raised in our discussions with CFRs), and an investigation is needed into how best to provide support to somebody while they are in a stressful situation (e.g., audibly or visually), and how to best incorporate this in systems used","Finally, the virtual agent needs to be able to determine the affective state of the user and the stressor experienced","Substantial advances have been made within the affective computing research field allowing automatic detection of a person's emotional states, for example based on posture ( ) or facial and vocal expressions ( )","However, being able to determine these within a real-world environment and determining the stressor experienced remains a challenge.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s1071581918300016," 1 Introduction In a scenario of digital content delivery for the cultural heritage sector—either online or onsite—to adjust what is presented to the visitor is seen as essential to accommodate different visit motivations, expectations, and needs ( )","Within the meSch project, 
                       we addressed the challenges of supporting a personally meaningful, sensorily rich, and socially expanded visitor experience through tangible, embedded and embodied interaction ( )","We envisage a cultural space filled with smart objects, each with their own stories embedded therein","Content will be revealed if and when conditions are right, e.g. visitors have picked up an object on display to inspect it, or a group has reached a certain location, or another smart object is close by",Visitors can continue their visit online—via a personalised interaction—to experience heritage in a novel way that combines the material and the digital,To create such a hybrid experience requires a personalisation infrastructure able to span the digital-physical divide,"This in turn requires reconsidering how personalisation is done, which features should be applied and when—e.g. on-site or on-line—and, overall, how multiple contact points of the same institution can be orchestrated in a seamless extended and memorable experience. ‘Personalisation’ is a broad term that encompasses three types of system behaviour ( ):   (also called  , the term we use hereafter) offers users a number of options to set up the application/system the way they like it;   is the ability of the system to sense the current state of the environment and to respond accordingly  implies the system maintains a dynamic model of the on-going interaction and dynamically changes its own behaviour to adapt to the changing situation","When applied to a scenario of tangible interaction, the concept of personalisation widens, as the interaction between the user and the system expands to include smart objects and networks of sensors, e.g. visitors hold smart objects or move in reactive spaces","The meaning of customisation, context-awareness and adaptivity must then be extended to include physical aspects",A visitor choosing a smart replica that holds one of many stories makes a choice of customisation—the visit is shaped by the replica that triggers specific content,A system that senses the presence of the visitor and their current visit preferences shows context-awareness that combines the physical and the digital,"Finally, a system that offers tangible interaction shows an adaptive behaviour when it uses the dynamic model of the visit to craft a personalised souvenir tailored to what that specific visitor did",These few examples show how personalisation must be reinterpreted when the physical aspects become part of the experience,This paper presents a multilayer framework to support personalisation across the physical and the digital,"In collaboration with curators and museum experts, we set out to understand personalisation in a way that is meaningful to heritage and its visitors, that is sustainable for curators to implement, and effective in managing the complexity of hybrid experiences","To deliver such a complex personalisation service the overarching framework has: (i) to reuse the main functionalities in different contexts (e.g. onsite vs. online interaction); (ii) to facilitate porting applications to different sites, hardware devices, and heritage domains; and (iii) to implement personalisation for both content and interaction","It has to be an easy-to-use tool for curators who can, autonomously, create new stories and interpretations, as well as modify current exhibitions ( )",The paper is structured as follows.   gives an overview of the field of personalisation for cultural heritage and the new opportunities offered by tangible and embodied interaction,Interventions in museums and outdoor cultural heritage sites developed as part of the meSch project are illustrated in  ; they show a breath of multisensorial personalised experiences in both content and interaction.   reports a collaborative study with curators that questions the meaning of personalisation and the different features that must be taken into account.   discusses the personalisation framework and how complementary approaches allow for content creation to be controlled by curators while the delivery in context is controlled by the system,"We also discuss how exhibition design choices that grant visitors some control on tailoring their experience (customisation) can be more effective than automatic logging and complex events processing (adaptivity).   concludes the paper with reflections on how different forms of customisation, context-awareness and adaptivity are supported by the proposed framework and their effect on the user experience. 2 Personalisation in cultural heritage: looking for new opportunities The call for personalisation for cultural heritage has mostly been applied to content adaptation, i.e. to dynamically change the amount or type of information conveyed to the single visitor to fit what they like or know, and how they behave","However, an analysis of personalisation in cultural heritage over the past 25 years opens up possibilities and offers new challenges ( )","From mobiles ( ) to the most recent augmented reality ( ), personalised interaction with digital information has been designed for individual use","But personal devices do not really immerse people in the space and the social context ( ): strong personalisation might end up isolating the visitor within a hyper-individualised experience, which is somehow unnatural in a museum context where it is most common to visit with family and friends ( )","In this way, personalisation misses out on the fundamental fact that the context affects the experience more than the visitor's cognitive and psychological status","Tangible, embedded and embodied interaction ( ), in which digital content is revealed in synergy with the sensorial experience, has the potential to keep the exhibition at the centre of visitors’ attention and strengthen the sense of “being here” ( )",In our research we investigate how tangible interaction combined with personalisation can support new forms of personal engagement where visitors are offered tailored experiences (both in content and interaction) “in place” ( ),"The visit is generally done in self-organised groups (family, group of friends, class, couple) or as a casual group (guided visits or in-place activities), but even when visiting alone, individuals move in a shared space and compete for the same exhibition resources","Personalisation of interaction according to proxemics and social context has recently gained attention, with some solutions taking advantage of projection facilities or situated public displays ( ) as well as screens and portable devices ( )","Research that directly addresses the social dimension is still limited, e.g. group conversations around a context-aware table ( ), sharing partially missing content to foster discussion on exhibition topics ( ), or sharing tablets among family members ( )","Exhibitions designed to engage visitors into shared interactions have proved very effective, even between strangers that just happened to be close to the installation at the same time ( )",These interactive pieces build upon the surprise triggered by the unexpected and the physical engagement that follows when trying to understand what happens,"However, most of the time these interventions are individual artistic expressions not intended to bring the visitors closer to and engaged with the heritage and its stories",They are limited and understood as performances,Design can be used to amplify the physical engagement with the artefacts on display and foster social interactions ( ),"Within this articulated research domain, we investigate how different personalisation techniques can be integrated to support a variety of experience patterns (e.g., very energetic and interactive vs. contemplative and emotional) that fit different social dynamics","The aim is to accommodate different motivations, emotional attitudes and expectations","Finally, sustainability should be a founding principle for personalisation in cultural heritage ( )","For personalisation to become the norm it is essential that the system is conceived for: reusing the same functionalities in different contexts (e.g. onsite vs. online interaction); porting an application to different physical sites and to different heritage domains; supporting the preparation of content and the definition of adaptivity strategies; and enable easy maintenance. 3 Case studies This section briefly describes the installations and prototypes based on tangible, embedded and embodied interaction used later in the paper to illustrate the multilayer personalisation approach",It intends to give a sense of the type of experiences enabled by the new Internet of Things (IoT) technologies as well as to show some of the design choices relevant to the discussion that follows,"While these examples were developed to different degrees of refinement (some were just prototype, some one-point installation, some spanned across several stations, some moved from the physical exhibition to online content), they were all fully developed and were evaluated with participants in a series of studies","All the examples were created in co-design, that is to say museum professionals, computer scientists and designers collaborated in the concept ideation while later each expert focussed on their own specific area","When the concept was agreed, then work split: the content was always selected and curated by the museum while the designers refined the interaction and the computer scientists developed the hardware and the software ( )","The examples are given to support the discussion of the personalisation framework, readers interested in the single case studies could refer to published papers. 
                       was an exploratory prototype designed to test, in the wild, the concept of the place itself telling the many stories of the people who lived there (Nagià Grom, Trentino, Italy). 
                       It is composed of a set of Bluetooth-enabled loudspeakers encased in wooden lanterns positioned at points of interest in the trenches and fortified camp of WWI on the Italian Alps; the lanterns are paired with an interactive belt that hosts an NFC reader and a set of 4 cards (NFC tagged)","Each card is a perspective on the war: “Order of the day” is the commanders’ voice and the more factual of the themes; “My dear wife” are personal accounts of the soldiers; “Women in the war” is the story, rarely told, of the civilians during war times; and “Poetry in the trench” is an evocative collection of poems written during the war","The interaction is triggered by presence: when a visitor wearing the belt enters the area of a loudspeaker, a loud sound attracts the visitor closer; when the visitor gets closer and is about 5 m away the loudspeaker plays the story of the theme (the card) currently inserted in the belt","The visitor can then change the card and listen to a different story or walk away and continue the visit. ( 
                      ) This prototype was evaluated with 9 volunteers who visited in small groups of 2 or 3; interaction logs were collected, the visit observed and the participants interviewed","They appreciated the automatic starting of the stories when they approached the points of interest, the quality of the stories and the variety","The logs showed every group took an individual visiting path and listened to more than one story in every place, but none listened to all content in every place","Finally, participants appreciated the possibility of choosing what to listen to and were observed discussing the content with their companions or commenting on the surroundings","The choices of topics seem idiosyncratic although the “Order of the day” was the most listened to. ( ). 
                       complements a permanent collection of WWI artillery at the Museo della Guerra 
                       (Rovereto, Italy) with the human aspect of the war and the stories of witnesses who had their life affected by the presence of the fort ( ). 
                       Besides the soldiers from the opposite armies who fought each other, the stories are those of the engineer who designed the fort, the army chaplain, the commanders, and the villagers before, during and after the war",Four thematic stations are positioned along the visiting route and feature several personal accounts (as an array of slots); each slot maps a personal story; the short stories (less than 3 min. each) and the many slots invite visitors to choose more than one content per station,"At the entrance the visitor receives a smart object, a ‘pebble’, that conceals a NFC tag and that, when placed on a slot, activates multimedia content","When leaving, the visitor returns the pebble; its NFC is read by another slot that prints a personalised postcard with text automatically generated on the basis of the personal visiting path ( ) ( 
                      ). 
                       was a temporary exhibition held at MUSEON 
                       (The Hague, The Netherlands) on the effect the construction of the Nazi costal defence system had on the city of The Hague ( ). 
                       The same events were told by contrasting voices: the German soldier; the Dutch civilian; and the Officer who had to do the bidding of the occupiers against the population",Smart replicas of historical objects represented a voice and concealed an NFC,"Ten display cases had an interactive ring (with NFC reader) on which visitors placed a replica, watched a video projected on the case and listened to the story via a hear piece","A final station with the same interactive ring printed a personalised postcard that gave access, via a unique code, to a personalised website where the visitors could contribute personal or family memories and explore those left by others","The exhibition was designed as a map of the city with every interactive station standing for a neighbourhood; similarly, the online interaction was shown as a map of The Hague with content from the exhibition shown with a meSch logo (grey or coloured depending if the content had been consumed during the visit or not); the visitors contribution was displayed as a pin on the map","At the entrance visitors chose one (or more) replicas as if they were following a character during the visit, they used it at every station and finally printed their personalised postcard ( 
                      )","The exhibition was open from April to November 2015; the logs show the replicas were used over 14.800 times but only 1557 (∼10%) printed their souvenir and of those only 39 accessed the online system and added 62 pieces of content to the map ( ). 
                       was an exploratory prototype tested in two different museums","Shaped as a magnifying glass, the Loupe conceals a mobile phone that uses Augmented Reality to trigger the display of content","In the version tested at the Allard Pierson Museum 
                       (Amsterdam, The Netherlands), the “Children of Zeus” ( 
                      ), the Loupe was used to highlight engaging stories within a traditional display","It is implemented as a trail: the outline of an object is displayed on the Loupe, the visitor has then to find the object and overlap the outline onto the object itself",This triggers the display of specific content,"The visitor could ask for more content by tilting the Loupe, this would display another snippet of text, or could move on by shaking the loupe to delete the current object and display the outline of the next one in the trail","The content was mostly text with the addition of an audio, two animations and three images","The text was broken into short snippets with a short description of the myth, a longer description, and an invitation to look at the object to answer a question before moving on","In the evaluation 22 participants were observed using the Loupe, individually or in pairs, completed a questionnaire, and some were then interviewed","Findings show visitors appreciated the novel interaction and were reading more than they would normally do, slowing down their pace to pay more attention to the objects on display ( )","In the implementation of the Loupe for the Hunt Museum 
                       (Limerick, Ireland), two different trails were available for the visitor","Objects belonging to different trails had different visual markers, e.g. a shamrock for “The History of Ireland in 10 Objects” and an icon of the Hunt Museum building for “Architectural Perspectives” ( 
                      )","Here the visitor does not follow a trail by seeking the next exhibit on the screen, but chooses what to explore next by browsing the space to find the objects marked with their trail symbol: using the Loupe to focus at the symbol triggers the content","As in the example before, tilting gets new content while shaking deletes it. 
                       ( 
                      ) was deployed at Museo della Guerra in January 2017 to encourage visitors to touch and learn about original objects",The station has two distinct areas: one area to showcase the objects and an active area,A message on the screen invites visitors to pick up one object and place it in the marked activity area that fits its profile,"An audio track that describes the object, how it was made and what it was used for plays",The audio narration has a simple matching (white-on-black) graphical animation; captions on the screen highlight points in the audio,"The visitor is invited to pick up and touch the object, observe it closely, possibly handing it to visit companions: these actions do not interrupt the presentation","When the audio is over, the visitor is offered with additional stories",If the object is held or left on the activity area the story starts automatically; if the object is placed back in its showcase position the presentation ends,"When no more content is available, a message invites the visitor to explore a second object",Two buttons allow selecting the output language (Italian or English); pressing a button restarts the latest presentation in the newly selected language,All the case studies presented offered several opportunities for personalisation both in content and interaction,"In the following of the paper we illustrate how we came to the definition of the overarching personalisation framework that was used for the prototypes. 4 Shaping personalisation that is meaningful to professionals Personalisation for cultural heritage has been a topic of research for many years ( ), however no common understanding is shared across the community on which personalisation features (e.g. age, interest, visiting style, location, etc.) should be used and for which aim","In addition to this limited specification, tangible interaction opens up new opportunities to experience personalisation",To gain a better understanding of the most relevant features to consider and how these match the goals of cultural heritage professionals in providing meaningful experiences for their audiences we conducted two complementary studies,The outcome was used to define a personalisation framework suitable to model articulated tangible interaction scenarios,The first study is a meta-analysis of the literature that classifies the features used in different personalisation systems,The second is a user-centred qualitative study of what personalisation means to cultural heritage professionals,"The results of the two complementary studies were then compared to produce an overarching framework and a set of guidelines for implementing personalisation for cultural heritage. 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   4.1 Personalisation features in the literature and their use In order to determine which features have driven research so far and the computational approaches adopted, an extended survey of the existing literature was undertaken",The selection of work was broad and included both seminal works in the field of visitor studies rooted in extensive ethnographic investigations as well as technical papers describing implemented solutions evaluated with final users in onsite settings,Therefore our survey covered both museum scholarship as well as computer science research,"Overall 41 features were classified according to the subject they refer to, such as the visitor or the environment, and to the static/dynamic nature of the information","Examples of the most significant features, grouped by information model, are shown in  
                         ","The features from the literature were then clustered by theme resulting in the following categories:
 
                      The literature survey showed that implemented systems usually concentrate on the modelling and evaluation of one specific complex feature (e.g., visiting style) or on a subset of easy to model features (e.g., age, stereotypes, location), possibly leaving out other personalisation dimensions highly valued by curators but much more complex to identify and capture automatically (e.g., motivation for the visit)","A strategy for prioritising the many possible features when deciding which personalisation to implement is still missing and, we believe, much needed. 4.2 The perspective of cultural heritage professionals To have a fresh look at personalisation and to incorporate curators’ views, we conducted a user-centred qualitative study aiming at understanding what personalisation means for cultural heritage professionals","Research from museum studies acknowledges that there is not such a thing as “the public” or “the visitors”: the same person can visit many times, alone or with others, and each time have different motivations and therefore different needs ( )","During a co-design workshop that brought together curators, designers and computer scientists, we asked the 25 participants to contribute their thoughts on what must be changed in a visit to achieve personalisation",The group was composed by a core of researchers involved in meSch plus a number of invited guests with experience in heritage,It included: curators—10 participants from different museums; computer scientists and engineers—8 participants (only 2 with experience in personalisation); 7 designers (all with some experience in interaction design and experience in the cultural heritage domain),We briefed our participants and explained our aim as to collect the broadest set of personalisation features that could be used to personalise “content” in “context”; we used these two terms to broadly direct participants’ thoughts towards two distinct clusters,"We invited participants to write their features on post-it notes, read what others have contributed already and stick their post-it close to similar concepts under one of the two labels “content” or “context” or somewhere in between the two if they felt neither would do",Clusters of similar concepts were created in this organic way,A total of 176 post-its were collected,The content of the post-its was at different levels of granularity with some very precise features such as ‘age’ and other much more open such as ‘no information but emotion’,A thematic analysis ( ) was used to systematically classify the post-its and create an affinity diagram: similar features were aggregated under a single label and a question was used to make the interpretation clearer; groups of labels were then aggregated under the same theme,"In this way from a large number of small clusters a total of 20 classes (or themes) were created (8 entries were not classified as they were unique, such as ‘hermeneutics’ or ‘intended educational goal’); the 20 classes were further aggregated in 3 larger sets that map the Content, the Context and the Visitor, as shown in  
                         ","When comparing the two sets of features, literature vs. user-generated, we can see that some occur in both sets such as ‘age’ or ‘short time’ or ‘interest’, but overall there are many more differences than similarities","We explain this by having in the group of 25 people only 2 with experience in personalisation systems; for the other 23 participants it was an exercise of imagination, on “what could it be?”",The result is an unexpected and exciting range of challenges and opportunities,"As it could be expected, the larger sets of entries refer to ‘me’ and the ‘visit history’, however features generally considered worth implementing in the personalisation literature such as ‘learning style’, ‘visiting style’ and ‘personality’ have not been mentioned at all in our sample","Intriguing is the large number of terms generated that is novel and has never been addressed by implemented personalisation. ‘Unexpected’ (7) and ‘mood’ (8) clearly indicate an interest in interactions that are different from what is generally provided by technology designed for cultural heritage, that is to say they point toward emotion rather than information",A similar call for affective engagement is found in other entries such as ‘how is this content related to my life’ classified as ‘me’,"From an implementation point of view this affective dimension is a serious challenge that, we believe, must be addressed by other means than computation; in meSch we used design","In other words, we designed experiences that enabled visitors to choose among a wide range of content that best matched their moods, including poetry, songs and music, jokes, personal accounts, news and propaganda, historical images and videos",We then provided visitors with physical objects to be used to select their preferred content and shape their visit in the way that best suites them,"To use the terminology of personalisation, we used customisation as a way to match emotional needs",The user-generated features also show the importance given to the content and the direct engagement of visitors with objects,"For the content, the heritage is seen as a complex canvas on which multiple narrative threads can be weaved over the physical space",Our participants acknowledged that many layers of knowledge exist (the ‘background’) that can feed different perspectives,"The direct engagement with interactive objects and spaces is, instead, a new and different take on personalisation for cultural heritage seen as a challenge ( )","The three sets above Content, Context and Visitor, point at three major ingredients that shape the visit experience",We use these as the building blocks of a personalisation framework that envisages: the curator-supervised preparation of the content and of the overall visitor experience; the system-controlled adaptivity of the content to the context; and forms of visitors’ driven customisation. 4.3 Principles for personalisation in cultural heritage The co-design study carried out with curators and museum experts showed a very different set of features than those used in implemented systems,"Taken together the two complementary sets provide a comprehensive range that allows us to rethink personalisation as a combination of customisation, context-awareness and adaptivity actions performed by different actors at different stages, i.e. from the design of the interactive intervention, to the preparation of the content and the final delivery to visitors.  
                          shows how the features have been classified respect to the type of personalisation performed and the actor responsible for it; different shades of grey aggregate features that impact different stages of personalisation preparation, coding and execution",In   some entries summarise features that occur under different labels in the literature ( ) and in different keywords mentioned by the curators ( ),"For example “interest in topics and narrative threads” in   aggregates features such as “general interests, background knowledge” from the literature ( ) as well as “me”, “personal interest” and “how is content related to my life?” identified by curators","What curators call “attention”, “take away”, “social interaction”, “mood/emotion” and “unexpected” is grouped in   under “type of visit”, i.e. different types of experiences the visitors may wish to have",What in the literature of implemented systems is referred to as “visiting style” ( ) and “acquired knowledge and interest level” ( ) is in   the general category of features that require a modelling of “history of individual interaction with objects/places and content delivery”,"Although this is an attempt to be as exhaustive as possible in classifying personalisation features, the features do not all have to be supported by every system","Instead each cultural heritage setting has to be considered in its own right: the most suitable combination of customisation, context-awareness and adaptivity depends on design choices made on the bases of the curators’ objectives and the specific visiting audience","For example, many science museums are visited regularly by families with pre-school children while this type of visitors is rare in war museums that instead welcome many school visits",Therefore each intervention should choose the most appropriate combination of features and how to implement them,"Below we offer four principles that should be considered when deciding on a specific design; they derive from our understanding of both the literature and the needs of the professionals. 
                          Not all the features for personalisation produce the same benefit, or are easily portable across different settings","Features that are simple to acquire and to model (like age) can in principle be taken as the basis to infer automatically what might be interesting for that visitor, but the risk is that the corresponding stereotypes oversimplify the current visitor needs and preferences ( )","For example, the stereotype that a child (young age) has limited knowledge (respect to an older person) may be very wrong as some children can be so passionate about a subject (e.g. dinosaurs, ancient Rome) to become domain experts","Therefore, to rely on easy features may be counterproductive","By contrast more complex features such as visitor motivations or personal interests for current visit are highly valued by curators and are more effective in representing the visitor's expectations, behaviour, and visiting style","Indeed such complex features can help model various aspects of personalisation simultaneously, e.g. personal interest implies spending more time and going more in depth. 
                          Techniques at different levels of computational complexity can be used to model the features that relate to visitors, their evolving experience, the social context, and the environment","Possible personalisation approaches include: the sophisticated processing of logs and events for deriving inferences on what is going on (e.g.,   the current focus of attention or the interest level); the request for the visitors’ collaboration and input (e.g.,   for preferred thematic threads); and the import of known data from external services (e.g.,   the weather forecast)","What computational approach is selected for which personalisation feature depends on a sustainable trade-off between: the reliability of the modelling, the time readiness of information, the portability of the approach to different domains and hardware settings, and the computational cost",The research we conducted in meSch showed that visitors who have a role in shaping their experience feel a strong personal engagement,"Thus, instead of asking visitors to fill in questionnaires and match the answers to a profile that delivers content to a passive audience, the visitors are offered the active role of choosing and controlling some aspects of the visit while the system monitors these actions to fine tune the experience","To be requested to take an active role empowers the visitors and evokes a sense of appropriation; this in itself is an advantage over forms of transparent personalisation based on live-data of the visit, where the visitor is not requested to engage directly as the system dynamically adapts to visitors’ behaviour","An active role makes the visit “my visit” and therefore reinforces the personalisation effort carried out by the system in the background. 
                          Personalisation as intended by computer scientists often implies curators have a very limited role to play in the creation of the visitors’ experience, for example when the personalisation system uses a knowledge representation and rules ( )","However, what content is delivered to visitors and how is part of the museum mission, and curators feel certain aspects of personalisation must be under their control, such as the provenance and the type of content used, the interpretations and perspectives available","Curators then should be those in charge of the stories told by the system; they should have tools for searching and uploading content tokens (i.e., portions of text, audio snippets, images); they should be able to create alternative perspectives and thematic threads, and different levels of detail","Support should be given to match the many variables that influence the final delivery of the intended experience, specifically the variability in content with the desired interactions ( )","In this way the curators will have the confidence that the system will deliver what they want it to. 
                          When the museum curates the narratives and the visitor has an active role, the personalisation system then can exploit this information on the delivery of content (curated by the museum) in the specific context of the visit (accommodating visitors’ choices)","In other words, content and context are kept distinct although tightly connected","By keeping the rules for runtime context-aware instantiation of the experience separated from the description of narratives, it is possible to decouple the curator authoring task from the physical architecture, facilitating the reuse of exhibition templates with different hardware setups","Thus, the heritage professionals will focus on the personalisation they are already familiar with (different stories for different visitors) and on the exhibit objects they want visitors to interact with, and leave the system to deal with the sensing and modelling of a dynamic context that determine how the story is delivered","This separation is key in the personalisation framework presented below. 5 A multilayer framework for customisation, context awareness and adaptivity The study above underpins our approach to experience creation and personalisation that is based on a clear separation of content from interaction, and aims at facilitating the preparation and the reuse of (i) the narrative threads that can be adapted to different visitors and types of experience and (ii) the interaction strategies that describe how content should be released in a specific context","To assist the creation of interactive experiences that feature different forms of personalisation, a multilayer framework ( 
                      ) was defined and implemented to separate the retrieval and preparation of the content (Layers 1 and 2), the preparation of the expected type of visitors’ interaction (Layer 3) and the rules for context-aware adaptive experience instantiation (Layer 4) ( )",These layers map the different clusters of personalisation features identified in  ,Layer 1 enables the selection and curation of the content items used to compose the experience,"It is at this stage that cultural heritage professionals access data archives of their institutions or other open access resources, such as Europeana, to look for existing suitable media items","During this phase, curators may benefit from contextual search that uses information about the institution or the task in hand (e.g. the subject-matter domain, frequently used search strings, content already selected) to suggest a filtered list of results potentially more relevant for the current authoring task ( )",Personalisation here maps system-led customisation as the suggestion of new content derives from static settings such as the type of museums (archaeology vs. science) or its settings (indoor vs. outdoor),Layer 2 extends the approach proposed in   of composing content in a narrative network where nodes are controlled by if-then-else rules with conditions over customisation features,"The outcome of this stage is a set of content data annotated with customisation choices related to: the features of the visitor in the static user profile (e.g., spoken language) or modelled by design (e.g. interest in topics and thematic threads); the structure of the story (e.g. whether parts of the story are to be delivered in different steps or the story is narrated all at once); the history of content delivery (e.g., whether a certain content has already been delivered or not)",Templates can be used to speed up the editing: curators upload new content on a given node via a simple graphical user interface ( ),"Templates can also be modified to create different experiences with minimal effort, for example “The Hague and the Atlantic Wall” scales up “Narratives in the Trenches of WWI” from 7 to 11 points of interest and reduces the number of alternative stories from 4 to 3","While Layers 1 and 2 deal with the content and the narratives, Layer 3 moves the focus to the context of the visit and introduces additional personalisation options that shape the interaction experience with the augmented objects and the space","The outcome is a richer set of annotations related to: the features of the interaction context (e.g. the visitor's position or his actions over the objects); the features of the social context (e.g. proximity of visitors belonging to the same group); the features of the environment (e.g., the noise level)","An interaction script describes how the annotated narrative network should be traversed by means of context-awareness and adaptivity rules based on those features (e.g. “If the visitor is at about 20 m from a point of interest, then play a content item associated to that point of interest and annotated as content for attracting visitors” is one of the rules for “Narratives in the Trenches of WWI”,  )",More complex rules can be written to map the content annotation in Layer 2 with the input/output interaction capabilities and the actual interaction design,"Visual paradigms could be adopted at this stage to support non-technical people in writing rules, as in the approaches recently suggested by   and  ","Within the meSch project, research was conducted on visual aids for composing the if-then-else rules as an extension of the Google Blockly library ( )","However, if cultural heritage professionals would feel confident enough to edit the rules by themselves is not certain","Indeed our co-design work points in the opposite direction: while they are comfortable with preparing content, curators prefer more technically skilled interaction designers to take care of the rule editing",The key issue seems to be the ability to generate the logic of the scripts,"In a series of events targeted to heritage professionals, the meSch interface based on file tagging was used with ease by participants to extend existing interactions to, for example, support a new output language or to add new smart objects",In our view this demonstrates confidence in modifying an existing and proved interaction; creating interactive behaviours from scratch is instead a step too far that calls for a collaboration between heritage professionals and interaction designers to manage the activities in Layer 3,"Layer 4 implements the intelligence for contextual instantiation and execution of the adaptive networks and interaction scripts prepared at the previous stages, including low-level mediation strategies for playback",This service supports the monitoring of the interaction events during the onsite/online visit and performs the execution of the adaptive rules that instantiate the actual experience delivered to the visitors,This includes resolving conflicts when alternative system behaviours are possible,An example is “Narratives in the Trenches of WWI” ( ) where the content delivered depends on the precise position of the visitors (close/far from a point of interest) and their specific choice of topic,"Overall this framework brings together in a coherent way different personalisation features and enables bespoke installations to use the combination of customisation, context-awareness and adaptivity that best fits the specific heritage","By separating the content from the context this multilayer framework enables reuse: the same structured content (defined at Layers 1 and 2) can be loaded onto different smart exhibits so the content will be activated in different context, i.e. different interactions (specified at Layers 3 and 4)","For example, the same content describing the trenches of WWI was used both with an augmented belt with thematic cards ( ) as well as with a digitally augmented book whose thematic pages can be selected by positioning a magnetic bookmark ( )","Similarly, the same smart exhibit can hold different structured content so that new presentations will be offered for the same interaction: for example, the digitally augmented book tested at the WWI trenches, was originally used with different content to help visitors discover the Sheffield Hallam General Cemetery ( )",This leads to sustainability as the initial investment for the smart exhibits covers a number of different exhibitions each one with a different content,"Moreover, the same exhibition could travel to different institutions that will change the content to suite their visitors. 6 Evaluation of the multilayer personalisation The proposed framework splits personalisation into multiple layers of complexity that involve a blend of (i) customisation on visitor initiative or according to the visitor profile, (ii) system context-awareness, and (iii) automatic adaptivity computed by the system based on visitor behaviour models","This framework and the design principles outlined in   have been used to develop the installations described in   demonstrating the framework is optimal to support onsite experiences where:
 
                   The points above distil our understanding from a number of studies and evaluations","Below we go more in depth and discuss the most significant findings related to personalisation that emerged from 6 evaluation studies that involved 279 visitors using the 5 prototypes described in  , all based on the multilayer personalisation framework","We believe that a good personalisation is invisible, i.e. it is not perceived as a “special effect” or something exceptional, but smoothly accommodates visitors’ expectations and needs",The aim of personalisation is to facilitate the dialogue between visitors and complex forms of heritage comprised of many stories and many options,"But then, how do we evaluate something that, if everything goes well, the visitors is not aware of? How can we measure the effectiveness of the synergistic work of the curator and the system to adjust the content and the interaction for the visitors? In meSch we evaluated the outcome of personalised interactions within a naturalistic setting: instead of a deductive approach based on rigid user evaluations (e.g. assigning tasks to participants and monitoring their execution) we used an inductive approach and looked at the individual enjoyment of the visit exhibition/interaction",A combination of qualitative and quantitative methods was used to identify critical points without disrupting the visit,"Below we bring together results from several evaluation studies and organise them around emerging themes across the different installations. 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   6.1 Personal engagement through content First and foremost, high quality content is pivotal to deeply engage visitors at the cognitive and emotional level and involve them personally","Even though deep engagement provoked by content cannot be directly classified as one of the forms of personalisation in its traditional definition (i.e. customisation, context-awareness, adaptivity), it was explicitly mentioned by cultural heritage professionals during their workshop",The category “me” (i.e. the personal engagement of the visitor) is one of the most frequent in reply to the question “what must be changed in a visit to achieve personalisation?” (see  ),"For curators, content design should aim at creating an evocative experience by intertwining the sense of being in place, the narratives and the bodily interaction","Therefore, the content creation phase (Layers 1 and 2 of the framework) should encompass different types of emotions curators may want to evoke, a clear connection with what visitors will see (objects, places), and whether the content can provoke comments among group members","In meSch we have experimented with different media (audio only, video, still images and slideshows, graphical animations, displayed text, textual summaries, graphical summaries), languages, genre (poems, military journals, personal diaries, object descriptions, historical newspapers, satire, songs, descriptions of historical events, anecdotes, play writing style, question answering as well as more traditional curated text), effects (surrounding sounds, evocative sounds, music, theatrical recitation, e.g. formal commanding voice vs. intimate and reflective voice) and we have studied their impact in shaping immersive and personally involving experiences ( )","The importance of content quality was in focus in the evaluation of two case studies, “Narratives in the Trenches” ( ) and “Voices from the Past” ( )","The stories were drawn from the museum archives, specifically from diaries or memories written during wartime or from oral history collected by the museum over time",The evocative effect of personal narrations was amplified by the theatrical rendering of actors reciting or performing and complemented by music and sounds,"In both cases the play occurred in very evocative surroundings, in a beautiful landscape (the trenches) or in caverns (the Artillery gallery)","Participants much appreciated the high-quality historic research of curators, the selection of the contents from original documents, the evocative acting of the narrated stories, which provide credibility to the multimedia content, and a delivery in place that did not make use of technological devices",Most of them felt engaged by the narrated stories (in “Voices from the Past” of the 143 participants who filled in the questionnaire 30% agreed and 58% strongly agreed),"A specific interest in the personal stories clearly emerged during the interviews:   Different participants favoured different content and felt free to discover and hear what they wanted, while at the same time felt motivated to listen to most or all the available stories (25% agreed and 65% strongly agreed)",This confirms that the design of a content structure with multiple stories and depth combined with an engaging interaction mode empowers visitors to explore and discover what they like most and fosters appropriation,"Offering options for the visitors to choose from is, in our view, a better strategy than for the system to second-guess what the visitor may want","We also see benefit in proposing variety including what one would not generally pick, an invitation to experience something different: “ ” (from the interviews of the “Narratives in the Trenches” case,  )",This last point is radically different from what personalisation systems generally do that is to propose more of what is known or liked,"For a personalisation system to diverge from the known is possible, but there is the risk of irritating the visitor: to offer alternatives that a visitor can willingly pick allows starting a new exploration path in a self-directed and natural way",A different strategy to engage visitors with content was adopted in the implementation of the Loupe at the Allard Pierson Museum ( ),Here text provided by the curators was elaborated on by a playwright to create a narrative with “cliff-hangers” that invited the audience to continue the physical-digital exploration,"For example, the story snippet shown on the Loupe “Heracles is the son of the mortal Alcmene and king of the gods Zeus","What is he hunting for?” invites the visitor to look at a vase and introduces background information about Heracles and his famous 12 labours before diving into another exploration of the object with two consecutive snippets: “First, he had to kill the Nemean lion",The skin of this animal could not be penetrated by weapons,Heracles strangled the lion and tied its skin around his shoulders.” then “How is Heracles using the lion skin to protect himself on this drinking cup?”,The evaluation showed visitors engaged with the exhibition switching their attention from the Loupe to the object and claiming they were reading more than they would do in a normal visit. 6.2 System customisation over visitor initiative The study on personalisation as seen by the cultural heritage professionals (discussed in  ) points toward features such as “mood/emotion” or “unexpected” that are difficult to anticipate and extremely challenging (or impossible) to model,"Instead of attempting to give the system more intelligence by second guessing visitor's temporary disposition and then offer a single option chosen by the system, in meSch we have taken the decision to exploit the new opportunities opened up by tangible interaction and offer visitors a range of different experiences they can choose from, together with the opportunity to change their mind or to select more options at the same time","For this to be possible, the framework supports the preparation of multiple content options (Layers 1 and 2), the definition of the interaction behaviour script (Layer 3) and the combination of visitors’ choice with autonomous decisions of the system (Layer 4), this last possibly focussed for example on the movement in the exhibition space or on visit history","We call this approach   as it requires a substantial design phase of the intended experience, the careful preparation of content and of different interaction options that are offered to visitors in an intuitive and tangible way","The case studies in   show several examples: the cards, the smart replicas, the pebble, and the loupe","This approach was used and evaluated in the case studies to measure visitors’ acceptance and willingness to customise their visit and the impact this had on their experience. 
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      6.2.1 Theme selection A well-known way to collect preferences for the initial system setting is to implement a questionnaire-filling first step followed by stereotype matching ( )","However, when visitors can easily select the places, themes and contents they feel are most interesting, the (boring) questionnaire filling step can be skipped",We designed experiences where visitors get straight into the visit and make their choices in context: the system benefits from this accurate bootstrap as it can be used for a more precise personalisation in the following interaction,Tangible interaction also offers intuitive means for small groups to negotiate choices on preferred themes that would otherwise be difficult for the system to automatically compute,"In Summer 2014, for the immersive auditory experience “Narratives in the Trenches” ( ), we evaluated the visitors’ appreciation for an autonomous selection of themes and their willingness to explore all the available content via tangible means ( )","Visitors were free to choose their own visiting path, stop at points of interest and choose what to listen to by selecting a card; on this basis the system then personalised the play of the audio files taking into account the position (both point of interest and distance), the current thematic choice and if a narrative has been played last","The theme selection was via four illustrated NFC-augmented cards and visitors were observed discussing themes and what to listen next ( 
                            )",The visitor study conducted with 9 participants showed that variety was the norm in the sequence of visit ( ),"Visitors demonstrated a clear appreciation for self-customisation, i.e. for the possibility of freely choosing which themes to explore, in which order, and how much content to experience at each hotspot",Empowering visitors with content choices based on a material interaction also fostered collaboration within small visiting groups and the sharing of the experience ( ),These results were confirmed by the more extensive evaluation study carried out for the Atlantic Wall exhibition ( ); at the start visitors choose a perspective to follow by taking a smart replica that represents it and placing it on active hotspots ( ),"The evaluation showed that visitors selected the smart replica on the basis of the perspective they wanted to listen to; sometime they carried two smart replicas when they were interested in contrasting stories, most frequently the Dutch civilian and the German soldier",When visiting in groups they were often choosing different perspectives (i.e. they carried different objects) and used the fact that they listened to different stories as a way to stimulate discussion and share their experience The “pebble” in the “Voices from the Past” ( ) used to select a specific story at each station had the same effect: “ .” and “ ” To hold a tangible object was key to make visitors aware of their thematic choice and that they are building a personal visit,"Making choices had a much stronger effect than we expected in terms of emotional involvement and engagement with the exhibition: “ ” and (from Atlantic Wall) “ ” Although we did not probe for a different, passive setting in which the visitor receives content chosen for by the system, the wording used seems to tightly bond the emotional experience to the choice of the object and the holding of it during the visit",The Loupe tested at the Hunt Museum ( ) offers another example of how thematic choices can be offered,"Although the organisation of the content network into multiple narrative threads is the same as in the previous examples, here visitors look for and select alternative visual markers to be framed with the Loupe to activate the corresponding content","The content structure is the same (Layers 1 and 2) but the interaction is different (Layers 3 and 4), thus demonstrating how the framework flexibly supports the combination and reuse of content and interaction strategies to create a rich variety of experiences. 6.2.2 Profile specification If multilingual material is available, the preferred output language is a choice for visitors done generally at the entrance as part of an initial profiling",Initial profiling can be facilitated by smart objects: a brooch augmented with a Bluetooth Low Energy device was used in a lab demonstration to store language (Italian vs,English) and content type (for adult vs. child) to automatically adapt the content when the visitor reaches a display case,At the entrance visitors received a brooch for their profile (a unique combination of adult vs. child and English vs,Italian) and a set of augmented cards to be used at each station: the interactive display case sensed the approaching visitor and used the information in the brooch to select the correct content when the augmented card was placed on the interactive case,"A similar approach was used in “Voices from the Past”, pebbles in different colours were for different languages, and in “the Atlantic Wall” where different objects combined language and perspective","These are examples of combining customisation, context-awareness and adaptivity to different degrees",Embedding profiles within objects that the visitors select at the beginning of the visit opens up new possibilities,For example one can imagine different visiting experiences to be designed to map Falk's motivations for the visit ( ) and offered as different objects; on this visitor's choice the dynamic personalisation is then rooted. 6.2.3 Request for additional information To automatically adapt the amount of information delivered is a form of personalisation that has often been investigated in the literature ( ),"However, it may be difficult for the system to precisely determine the visitor interests and to adjust its verbosity accordingly","If tangible interaction is properly designed, we can replace the system's automatic guessing (that would be implemented in Layer 4 of the framework) with visitors’ explicit actions, thus empowering people with more control over the presentations","When using the Loupe prototype, evaluated with visitors both at the Allard Pierson Museum in Amsterdam (22 participants,  ) and at the Hunt Museum in Limerick (17 participants), visitors have to explicitly request additional information about an object on display by tilting the Loupe",The results of the user study conducted at the Allard Pierson Museum confirmed the positive attitude of visitors to seek additional information according to their personal interest and level of expertise ( ),"Visitors mentioned being driven by curiosity to read more after each short section of text, a feeling induced by the creative writing style of the narrative that alternated reading text on the Loupe with observing the object on display in response to provocative questions","Interviews confirmed that different visitors experienced different quantities of information, with nine out of fifteen visitors who had read all the text ( )","These findings were confirmed by the user study conducted at the Hunt Museum where most visitors wanted more, rather than less, information ( )","People who decided to use the Loupe at the beginning of the visit were interested in learning more and were actively using the Loupe throughout the visit as a tool to help them receive more information, though the tilting gestures seemed to require some time to get used to","The two studies showed that when visitors are empowered with tools with an intuitive design and physical affordance, they seek more information","However, asking for more may not be limited to “more of the same”",Experiences like “Narratives in the Trenches” and “Voices from the Past” invite visitors to deepen their interest by listening to contrasting voices and the evaluations showed that they do,"Indeed all the participants in both studies listened at least to two pieces of content at every point of interest or interactive station, sometimes they listened to all the content available showing an interesting form of self-regulation depending on what content was provided","What is common across these experiences is that the content is split in separate small chunks. “The Atlantic Wall”, instead, had longer pieces of content with a video lasting for 14 min; the logs showed only a few visitors listened to the content in full further supporting the conclusion that how much content is delivered should be left to the visitor to decide. 6.3 System context-awareness Some forms of personalisation depend on the ability of the system to monitor the state of the environment, of the interactive objects and the actions of the user","Whether these forms of personalisation are used or not depends on the design of the interaction rules in Layer 3 of the framework and on the sensing mechanisms implemented at the lower level in Layer 4. 
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      6.3.1 Awareness of presence and proximity The possibility of personalising the visitor experience according to users’ location and proximity to hotspots has a long tradition in the field of personalisation for cultural heritage ( ). “Narratives in the Trenches” ( ), is the prototype that better represents our effort in meSch",The scenario can be experienced by simply walking around: the system tracks the visitors’ movements and reacts accordingly playing the right piece (of music or story) relevant for this place and the choices made by this visitor,"The analysis of the interviews, combined with the observations, confirmed that key to the appreciation of the experience was the seamless interaction",The attraction sounds had the double effect of rising surprise and increase awareness: “[the attraction sound]  ”,"When hearing the sound coming from a point of interest nearby, visitors were observed changing direction and move towards the sound source so detouring from their path to reach the location","The automatic start of the narrative then induced people to stop and listen, while visually exploring the environment: “ ”; “ ”","There was also a theatrical effect in some of the locations such as inside the caverns where the sound lanterns were not visible, and audio resonated all around ( )","It is clear that, although technically not complicated to implement, location awareness can significantly improve the visitor experience when it is coupled with a careful interaction design sensible to the context and the environment of use. 6.3.2 Awareness of visitor actions and state of objects Similarly to location-awareness, the awareness of what actions are performed by visitors and of the state of objects (e.g. position, internal state, proximity to other objects, time spent in place, etc.) can be used by the system to decide how to react coherently to users’ behaviour","For example in the “Atlantic Wall” exhibition, by monitoring the placement and removal of smart replicas onto the active areas of interactive cases ( ), the system controlled the presentations start and stop and cumulated a model of which stations the visitors spent the most time at","This monitoring was meaningful as the videos were quite long, up to 14 min; if the visitor decided to move on and took the replica before the video was over, the play stopped","Each time a replica is used an event is added to a log; this enables the system to know where this replica has been used (the sequence of the stations) and for how long (if it was removed before the video was over) enabling the personalisation (adaptivity) of the souvenir postcard, discussed below","Context-awareness might also involve complex reasoning, as in the case of the new interactive plinth installed at Museo della Guerra ( )","Here the interaction is not regulated by a simple play/stop schema based on the presence of the object, but depends on a sequence of actions made by the visitors—who are encouraged to touch, lift and put down original exhibit objects while the presentations are playing—and on the different positions that objects can take—at the centre of the plinth, in the hands of visitors, or in their showcase position","The implementation of this form of context-awareness required the definition of a finite state automaton that models the internal state of the system (i.e. idle, presentation playing, waiting for specific users’ actions)",The visitors’ interaction behaviour was modelled to decide when to automatically start additional information: further stories about an object are presented to those visitors who have completed the physical exploration of the object and heard its description until the end and still have it in their hands,A heuristic evaluation noted that the first video felt long and the automatic start of the second layer of information created a very long presentation,In this version the visitor was not aware that the content was actually composed by two parts and the combined listening could trigger a sense of information overload,"In the new version, the system's decision to present additional information is feedback to visitors with an explicit message displayed on the screen that explains what would happen next (an additional story would start if the object is held) and what can alternatively be done (put the object back in its showcase position and choose a different object)","Within the ongoing evaluation of the interactive plinth with visitors at Museo della Guerra we will investigate whether this decision of balancing the automatic personalisation decisions of the system with an explicit notice to visitors on what comes next provides the necessary support for a smooth interaction. 6.4 Automatic adaptivity based on visitor behaviour models In a scenario of tangible interaction, adaptivity gains new opportunities such as to react to the current contextual situation by changing the physical settings (e.g. triggering the vibration of an object or turning on lights), or to change objects created on demand (e.g. through 3D printing)","This can be obtained with interaction rules (in Layer 3) that instruct how to shape the physical elements. 
                         
                         
                         
                         
                         
                         
                         
                         
                      6.4.1 Exploiting the visit history to generate personalised souvenirs We use the personal interaction history collected during the visit to generate tangible souvenirs that capture what visitors have experienced onsite",This is an advanced form of material adaptivity as the post-visit artefact represents the physical output tailored in a personal way,The effect of this type of personalisation was evaluated in “Voices from the Past in Forte Pozzacchio”,"The logs collected during the visit are used at the check-out station (located at the exit of the exhibition): a narrative strategy for adaptive storytelling composes a personalised text that reflects the order of visit, mentions the names of the voices that the visitor has heard, contains optional phrases depending on what the visitor has actually experienced, and guarantees proper syntactic and lexical cohesion of the text after the dynamic assembling",A final sentence invites visitors to connect to the museum website to find the stories and the bibliographic references of the original documents from which the narrations were extracted,A stamp with the current date completes the souvenir,Thus visitors who have experienced the installations in different ways receive different postcards,Evaluation results showed much appreciation for the personalised postcards ( ),"Visitors liked the familiarity of the design concept and the format of the souvenir (a postcard to be retained for personal memory or to be shown to others), the image (the theme of the exhibition), the narrative summary (that recalls the actual experience), the opportunity to find more online","We also asked participants whether they would prefer the personalised souvenir with alternative types of texts and layouts, e.g. a postcard to send","The idea of transcribing the story item that they liked most or a booklet were discarded by all 61 interviewed visitors who preferred the personalised visit summary shown in  
                            ","So developing complex techniques, e.g. log-based reasoning, for estimating the visitors’ top interest would not be justified in this case","In the “Atlantic Wall” exhibition we experimented with a different layout for the souvenir, based entirely on graphics ( )",Here the system keeps track of which stations the smart replica has been used at and for how long,As some videos were long (up to 14 min) the length of play was meaningful as many stopped it midway through and this information was used as an estimate of the interest,"Each station in the exhibition was associated with a neighbourhood in The Hague and represented by a stamp: the postcard then shows the stamps of the three places where the visitors spent the longest time ( 
                            )","The postcard also shows the language and the perspective; it also gives a unique code (three letters and three numbers, top right) to be used online to enter a personalised web experience",The reverse of the postcard shows the map of the city with the numbered spots of the neighbourhood in the exhibition that correspond to the different stamps,We wanted the postcard to be an invitation to go out and explore the city but also a way into an online system that enabled visitors to contribute their own or family memories,"An interactive table-top in the exhibition allowed exploration of the visitors’ contributions; the same map with pinpoints showing added content was available online, but one needed to login with a postcard code to contribute ( 
                            )","When the visitor logs in, the interaction log recorded during the visit for that code (modelled by Layer 4 of the framework) is used to generate a personalised page that shows the content of the exhibition on the city map as meSch logos",A further distinction is between the content that has been seen in the exhibition (displayed with a coloured logo) and what was not seen (displayed with a grey logo),So at a glance visitors see their visit as well as the content they missed in the exhibition and other visitors’ contribution ( ),The map-based website was developed for the Atlantic Wall as the exhibition was highly connected with the city,However we have experimented with other ways of using the logs collected onsite to personalise online experiences,"In particular we have developed a generic approach that uses a tile-style layout to display the experience via different facets and enable the visitors to explore it in different ways.  
                             shows the two personalised webpages automatically generated by combining content from the exhibition (prepared in Layers 1 and 2), the personal log (modelled by the system in Layer 4), and additional online material available in public repositories, such as Europeana or online databases the museum wants to use to further engage visitors in an online exploration (suggested by the services in Layer 1)","The two examples show many different ways to represent the visit, e.g. “everything you have missed” shows the content of the exhibition this visitor did not consume, while the “favourite” displays the single element on which the visitor spent the most time","Starting from content items of different exhibitions and from different visit traces, each visitor is shown their personalised view","A timeline shows the visit against the progress of historical facts while the exhibition personality represents in a fun way the visiting behaviour applying generic rules to the log, i.e. the dominating colour is the theme chosen, the size of the eye maps the overall engagement etc",It is worth underlining that no personal information was asked from the visitor: they just have to keep the postcard to access their entire experience online,"This choice for anonymity should not be underestimated: the public is becoming more aware of the implications of giving away personal information therefore alternative ways of offering personalisation without intruding visitors’ privacy are worth exploring. 7 Discussion and conclusions Shaping personalisation in a scenario of tangible, embedded and embodied interaction for cultural heritage involves challenges that go well beyond the implementation of content personalisation for portable mobile guides","Content is coupled with the material dimension of experiencing the objects and the spaces, thus the facets of the context are more relevant than in a situation where digital content is consumed on a mobile","The context itself combines multiple aspects, personal, social, and the state of objects and space","The endeavour of determining which features should be used to drive adaptivity has to first acknowledge what forms of personalisation curators value as most meaningful, irrespective of the complexity in modelling and implementing them","Indeed fully automatic adaptivity, where the system takes all the decisions on what to present to which visitor, when and how, may not be the best solution","Through an inspiring co-design process involving curators and museum experts, we discovered the meaning personalisation has for museum professionals and identified aspects of personalisation that curators explicitly wish to be in control of and that have been overlooked by a technology-centred perspective","Features such as “mood/emotion”, “unexpected” and “me” challenge the traditional thinking of personalisation as that of an intelligent system taking decisions on behalf of the visitor in favour of a more open approach that intertwines system intelligence with visitor's choice and curated content prepared for specific aims",This requires a radical rethinking of how personalisation in cultural heritage manifests itself and the role curators and visitors play,The personalisation framework we developed works at different levels; it decouples content and context allowing the curators to compose different media into multiple stories delivered to the visitors in a specific context,"The visitors, in turn, are not just receivers of information; they are called upon to make choices and contribute to the shaping of their personalised experience","Interaction design can become a powerful means to get the visitor into the personalisation loop: purposefully designed interactions can grant to visitor control of the adaptation of the experience, bootstrapping multiple personalisation features at the same time and relieving the system from complex log-based guessing",Personalisation is no more solely a matter of adjusting the type and the amount of content,"A synergy can be created with tangible and embodied interactions to increase visitors’ awareness they are building their own visit path, to deeply involve them through multiple senses and at the emotional level, to foster the sharing of the experience with visit companions",The accurate preparation of content that uses different strategies to connect with the place and to convey the stories in an engaging way is pivotal,"The proposed framework then allows us to flexibly reuse the same content with alternative interaction experiences (e.g. in guided visits, self-directed explorations, group games), with alternative types of devices (e.g. smart activating replicas, hotspots reacting to proximity, postcards and online), and for different purposes (e.g. informing, rising surprise, fostering reflection, stimulating social interaction and discussion, favouring fun, creating a link to post-visit activities)","The system takes on the burden of monitoring the state of the context, updating the information models, and applying automatic adaptivity whenever multiple options apply","By decoupling the low-level management of the context from the higher level task of structuring the narratives, we support a more sustainable porting to different hardware configurations and a reduction of complexity: by means of a bespoke authoring interface cultural heritage professionals focus on the preparation of the content according to the narrative dimensions planned for the experience, ignoring all the details related to technology","For curators, the rules for putting content in context are transparent, although they can be edited by experienced interaction designers ( )",Personalisation services can also help building a long-lasting relationship with visitors by favouring new opportunities for the visitor to get in touch with the heritage (e.g. a second visit to the same place; the visit to a partner site; a follow-up online exploration),"The proposed multilayer personalisation framework supports the transition between different heritage touch points, by exploiting the logs of visitors at one touch point to bootstrap the experience at the following touch point, using the information on what has already been experienced to suggest what to experience next or other interesting paths for content discovery","We explored in particular how the generation of personalised post-visit souvenirs with their own materiality can reinforce visitors’ positive attitude towards the experience, support memory and sharing, foster further curiosity and exploration of online personalised resources","Curators are keen to invest effort on providing different visitors with the right information at the right time and with the most effective type of interaction. meSch developed a platform where personalisation technology helps curators to tailor aspects of a digitally enhanced visiting experience, the interaction modalities through which the content is disclosed, and the pace of the visit both for individuals and for groups",We believe that the direct involvement of cultural heritage professionals in the co-design of meSch technology as well as the extensive evaluation with visitors in field studies was instrumental in shaping a holistic approach to personalisation that exploits in full the new opportunities offered by the tangible and embodied interaction.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s1071581916301380," 1 Introduction Many socio-technical systems, including search engines, news aggregators, and social media sites, employ personalization algorithms to rank and filter the content displayed to users","The Facebook News Feed Algorithm is one example, designed to “deliver the right content to the right people at the right time, so they don't miss the stories that are important to them” ( )","The algorithm acts as a constraint on the information available for users to pay attention to, by generating a personalized ranking of posts for each user","The ranking is based on quantified signals such as how often Facebook Friends interact with each others’ posts, overall post-level engagement, and the type of content in the post","For example, a post can be “surfaced” by the algorithm for a given user—moved higher up in the ranking—based on how much other users are interacting with it ( )","Personalization algorithms are designed to reduce information overload and improve the user experience by connecting users with the information the system predicts they are likely to want to see, based on their past interactions with the system","The Facebook News Feed presents a unique opportunity to study the potential effects of a filtering algorithm for end users in an extremely popular 
                       socio-technical system in which complex interactions between user and algorithm behavior determine the constraints on content visibility ( )","Algorithms are often thought of as neutral gatekeepers because they are computer code, assumed to be free from human bias ( )","However, rules designed to promote some information necessarily make other information less visible, imparting a very real “threat of invisibility” upon those whose contributions are not evaluated favorably by the algorithm ( )","Personalized content filtering may restrict the subset of Friends whose posts appear in a user's News Feed to only those who they interact with frequently, or who post information similar to what they have read in the past ( ).   defined algorithmic bias as systematically and inappropriately denying opportunities or assigning undesirable outcomes","By constraining which posts and thereby which Friends users can most easily interact with, choices made by the system which are invisible to users may be just as important to study and understand as the visible aspects, especially for the identification of possible bias. 
                       recently argued that the work an algorithm does looks very different to the end user experiencing the effects of this work than it does to the system designers and operators responsible for creating, maintaining, and updating the algorithm","It is therefore important to investigate users’ experiences interacting with algorithmic filtering, to understand systematic, unexpected patterns of system behavior","This paper presents a study in which Facebook users engaged in a task where they might experience algorithm effects firsthand, by visiting a Friend's Timeline and noticing posts that they did not remember seeing in their own News Feeds","Because Facebook users can navigate directly to their Friends’ Timelines and view a reverse-chronological list of another user's past posts, they can use the system to find posts that may not have appeared in their News Feeds","This task triggered an “infrastructural inversion” ( ), making invisible aspects of the way the system works visible to participants ( ), in order to measure their expectations for what they should see in their News Feeds","The study investigated whether the prevalence of missed posts might vary according to the closeness of participants’ relationship with each Friend, and whether that would have any bearing on how surprised participants were about missing posts from particular people","Surprise is evidence of an unmet expectation ( ), indicating that a particular missed post was one the user would expect to see in the News Feed","Results show that individuals are likely to have missed posts from Friends in their News Feeds regardless of how close the relationship is, after controlling for factors like participants’ memories of their Friends’ past posting behavior and their perceptions of how recently the Friends were active","Even frequent Facebook users who accurately remembered when Friends’ latest posts were created still encountered missed posts, indicating that some of the missed posts participants identified were likely due to the algorithm and not to their own attention and memory","Participants were more surprised about missed posts from close Friends, and from Friends they felt like they saw often in their News Feeds","In addition, believing that the system caused missed posts, as opposed to their own behavior, was related to more surprise",The infrastructural inversion method used in this study created an opportunity for users to notice aspects of the system's behavior that would have been invisible otherwise,"The conditions under which participants experienced surprise reveal that participants believed the system would prioritize posts from close friends, and these beliefs were strongest for those who thought the system took an active role in choosing which posts to display","This study highlights a possible consequence of offloading the work of choosing which posts are attention-worthy onto the algorithm, by identifying a pattern of opportunities for interaction that users did not know they were missing","Because even passive consumption of posts on Facebook can strengthen ties between Friends ( ), these results suggest that choices the system makes regarding visibility and invisibility of posts could have consequences for real relationships. 2 Related work 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   2.1 Measuring algorithm effects Systems like the Facebook News Feed that use personalization algorithms to filter content are conceptually and technically similar to recommender systems, with one important difference","In a social media system, there is no recommendation, and no obvious moment of choice or evaluation by the user","This means that biased performance and impact of a filtering algorithm is difficult to measure.   analyzed data from 1405 MovieLens users to look for evidence of a “filter bubble” ( ) effect of the recommendation algorithm over time, operationalized as decreasing diversity in the set of movies either recommended to a user, or rated by the user","They found a reduction in content diversity of the recommended and rated movies over the length of a user's participation in the site, although the change was small",This supports the idea that filter bubbles do exist in some recommender systems.   also looked for evidence of decreasing diversity of music consumption over time in a large dataset from a music recommender system that worked as a plugin to Apple's iTunes software,"They found that people who used the plugin became more similar to each other in terms of the artists they listened to, than people who did not use the plugin","Despite this, they found no evidence of clusters or fragmentation of music interests among the users of the recommendation system, indicating that filter bubbles did not seem to be forming among the system's users","However, it isn't clear whether the effect these researchers identified is because of the algorithm, or because the users themselves narrowed their actual preferences, or some combination of both","In addition, because these studies used log data only, it is not possible to know what the users were thinking when they chose movies to watch or artists to listen to, or how they reacted to the recommendations when they received them","There are few studies of the effects of the News Feed Algorithm, because outsiders cannot obtain access to the necessary data to conduct this type of research ( ).   recruited 40 Facebook users for a study that was designed to explore the effects of users’ awareness of the algorithm on their satisfaction and preferences for what they wanted to see in their News Feeds","They created a tool that used the Facebook Graph API to access and compare the output of the /user-id/home 
                          function, which returns the participant's News Feed posts, against the/friend-id/feed 
                          function, which they used to pull all posts created in the past week by the participant's Friends","However, this illustrates one of the difficulties of trying to operationalize the extent of the influence of the algorithm: the Facebook API does not provide information about which posts were actually seen by participants in their News Feeds","In fact, the documentation for Graph API 1.0 used by Eslami et al. for /user-id/home includes a cautionary note: “The posts returned by this API may not be identical to the posts a person would see on   or in Facebook's mobile apps”","To verify whether the posts had appeared or been filtered by the algorithm, Eslami et al. “asked participants if they remembered seeing randomly selected stories”","Despite variability introduced by the API, this manipulation was accurate enough to trigger awareness of the presence of the filtering algorithm by showing users missed posts, but not accurate enough to measure and estimate the magnitude of its effects","Studies like this one are unfortunately no longer possible for researchers unaffiliated with Facebook, due to changes to the Graph API that removed the ability to request the  _  permission necessary to access users’ News Feed posts. 
                         
                      
                          undertook an analysis of a dataset consisting of data from 10.1 million active US Facebook users, to measure the impact of individual choice on what “hard news” links users click on","As employees of Facebook, they had access to information not available to other researchers via the API: a measure of how long posts have displayed on the user's screen",This allowed them to determine which posts users were “exposed” to,"They reported that ideological conservatives are exposed to 5% fewer cross-cutting links (links that tend to be shared by people who hold an opposing political ideology) than are actually posted by their Friends, whereas liberals are exposed to 8% fewer","However, this metric cannot be used to determine the extent to which the user noticed and attended to the posts that were displayed, or whether viewing the content had other user-level effects, like attitude change","This illustrates that even with access to system logs and behavioral trace data, it is still challenging to conclusively measure the impact of filtering algorithms. 2.2 Missed posts, memory, and expectations The only way for a Facebook user to find out via the system about missed posts—those posts the algorithm has assigned a low rank so that they are unlikely to be seen by the user—is to visit each Friend's Timeline individually and look for them ( )","Identification of a missed post requires two kinds of information: whether or not the post was displayed to the user, and whether the user read the post","At this time, information about post exposure is only available internally to Facebook, or by building client-side software that automatically records which posts are displayed, which violates the Facebook terms of Service. 
                          Information about whether or not the user read the post depends on the user's memory for posts he or she has seen before, and can be obtained by asking users directly","Asking users to identify missed posts is an approximate measure, likely to be correlated with the truth but also biased in predictable ways","Upon encountering a missed post, users might experience a surprised reaction which would indicate that their expectations for the system's behavior had not been met","Measuring users’ reactions to encountering system behavior that does not meet their expectations is one way to characterize the impact of a filtering algorithm. 
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      2.2.1 Memory People sometimes feel a sense of recognition for items or events that they have not actually seen or experienced.   has been widely studied using cognitive psychology lab experiments, and is more likely when people are shown a false cue that is very similar to the item that was actually seen or studied, than when they see a cue that is different from the real item ( )","However, even if people are given an incomplete but accurate cue (e.g., fragments of memorized words ( )) people can accurately recognize and distinguish between items they have previously seen and items they have not ( )",There are many properties of both the remembered item itself and the conditions surrounding participants’ exposure to the item that affect whether or not the participant recognizes it at all,"For example, the emotional valence of the content ( ) and how deeply the participant processes the information ( ) are both important factors","In general, the more time a person spends processing a piece of information, more likely they are to accurately recognize it ( ); but, the more distracted a person is during the initial exposure, the less able they are to remember the information later ( ); this might cause   errors","In addition, memory for past events decays and becomes less accurate over time ( )","And, the farther into the past something happened, the more uncertain people are about when it happened ( )","Memory for social media posts is likely to be different than for cues studied in cognitive psychology lab experiments, because the content is more personally relevant to users","This could influence things like emotional valence and depth of processing, and make memory for social media posts more accurate than for cues in the lab","False recognition might be more likely if, for example, a particular Facebook Friend creates posts that are very similar to each other","However, participants in cognitive psychology memory experiments are able to accurately distinguish between falsely similar cues and actual items when they see all of them at once ( )","This suggests that seeing a Friend's posts all together in the Timeline would lessen the occurrence of false recognition, as compared to seeing past posts one at a time, out of context and not in reverse chronological order","There has been little research on memory accuracy for social media posts.   studied recognition memory for tweets in a lab experiment, in which they created a Twitter app which pulled 100 tweets from participants’ own feeds and then showed them half of the tweets","In the recall phase of the experiment they presented participants with all 100 tweets in random order, and asked them which tweets they remembered seeing",Participants were 69% accurate at identifying which they had seen before and which they had not,"They were also more accurate at recognizing the tweets they rated as interesting, and the ones they spent more time looking at","Unfortunately, this paper did not report the false recognition rate","Based on what is known about recognition memory, it is reasonable to expect that some   and   errors will occur when visiting Friends’ Timelines, but also that participants should be able to identify some missed posts accurately. 2.2.2 Unmet expectations The cognitive function of surprise is to help people “develop and maintain accurate representations of the world” ( )","Surprise is an automatic mechanism ( ) that provides feedback essential for recognizing when expectations do not match reality, so people can learn to better comprehend and predict future events ( )","Expectations are powerful influences on interaction patterns and behavior, and experiencing surprise is a signal that an expectation has not been met, and that there is something new to be learned about the situation at hand ( )","Recent work suggests that surprise is initially a slightly negative experience ( ), but this reaction is quickly followed by an emotional response that depends on the interpretation of the event in relation to other possible outcomes ( )","Surprise is different from other emotions like happiness or sadness, because the same surprising event can bring about positive or negative emotion depending on the situational context ( )","Many users are not aware of the News Feed algorithm ( ), which makes it hard to directly ask them about which posts they expect to see in their News Feeds","However, when a user notices a missed post, this presents an opportunity to assess their reaction and thereby indirectly learn about their expectations for system behavior","Previous research has shown that if a Facebook user notices a missed post from a Friend, they frequently react with surprise, which can sometimes lead to frustration, and anger ( ,  )",Points of unexpected or surprising behavior like this within a system often occur in conjunction with aspects of the system's structure or behavior that people do not understand ( ),"In this case, surprise is an indication that the user's expectations for what the News Feed would show them have not been met. 2.3 Facebook and relationships Online social networks like Facebook support “pervasive awareness” of one's social ties which preserves a continuing connection between people, even through life events like moving to a new city or graduating from college ( ).   found that Facebook status update posts provide visibility and opportunities for interaction with Friends, and creating posts was positively associated with receiving more social support even after controlling for the size and diversity of a user's network","They suggest that for post creators, monitoring the feedback they receive on their posts helps them to know who is paying attention to them on Facebook, and therefore who is available to provide future social support","Research by   also found that communicating with others on Facebook is related to increases in reported relationship closeness, even when controlling for other kinds of non-Facebook communication, such as face-to-face interaction, phone calls, and email","Text-based, written communication on Facebook, such as direct posts, had a larger impact than other kinds of signals such as “Likes”.   conducted a survey which measured Facebook relationship maintenance behaviors, focusing on participants’ responses to posts from their Friends sharing good or bad news, or asking for advice",Engaging in more relationship maintenance behaviors was positively related to increased bridging and bonding social capital,"Other recent research has also found that more frequent communication on Facebook, as well as the use of more kinds of Facebook communication modes, predicts “relational escalation,” or an increase in tie strength ( )","And   reported that after being made aware of particular missed posts, some participants indicated that the post would have triggered a supportive response, had they seen it","People's perceptions and memories of interactions with members of their social networks are important aspects relationship maintenance, because they constrain their awareness of others and their understanding of the social environment ( )","A filtering algorithm that ranks posts for display provides a solution to problems of information overload, but may also constrain the user's active, visible social network by affecting which posts are available for feedback and comments from other users","This happens for users as consumers of posts, in that if they do not see posts from certain friends they miss opportunities to stay informed about those Friends’ lives and to engage with their posts","It also affects users as producers of their own posts, because they will not receive social support from people who do not see the posts they create","In other words, an algorithm that ranks posts for display might create opportunities for giving and receiving social support between certain people while systematically removing those opportunities from others. 2.4 Research questions In most systems that use algorithmic filtering, users are not aware of the bounds of the set of possible items that might be presented to them so they do not have a way to know what items they're not seeing","However, Facebook users typically have an existing relationship with the people they are Friends with ( ) and may have a general sense of what and who should be represented in their News Feeds",This study triggered an infrastructural inversion by exposing participants to potential invisible effects of the News Feed Algorithm: Friends’ posts that they may have missed in their News Feeds,"This made hidden aspects of the filtering work done by the algorithm visible to users, in order to identify patterns in the circumstances under which such missed posts are evident, and unexpected","Missed posts are a socio-technical outcome that arise from the combined influences of user and algorithm behavior, and participants’ reactions to the infrastructural inversion may illustrate areas in which the system is biased","In addition, because posts from Friends are opportunities for relationship maintenance that are important for feedback and social support, there could be implications for users’ real-world relationships","To investigate this possibility, I asked the following research questions: 
                      3 Method 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   3.1 The survey I used the Qualtrics platform to conduct a survey in April 2014 that asked participants questions about themselves and their experiences on Facebook, their relationships with eight of their Facebook Friends, and general questions about those Friends’ Facebook activity","The survey required participants to select four Facebook Friends they felt closest to 
                         ; another four Friends were automatically selected at random from each participant's Friend List","Because people typically have many more weak ties in their social networks than strong ties ( ), I explicitly sampled for close Friends to ensure that relationships of this type were included in the survey","After consenting to the study, participants were prompted to log in to Facebook (if they were not logged in already) and authorize an app developed for the survey","The app used Facebook's Graph API v1.0 to access the names, profile photos and user ids of the participant's Facebook Friends",This data was all classified in v1.0 of the API as “basic information” available by default once the user authorized the app,"Using only this data was an intentional research design choice; if the app asked for additional permissions, this would increase the dropout rate and exacerbate selection effects in ways that would negatively impact the sampling frame","The names and profile photos of all eight selected Friends, as well as links to the Friends’ Timelines, were incorporated into questions in the survey","All Friend information was discarded automatically when each participant finished the survey, and participants were provided with instructions for de-authorizing the app upon completion","The survey started by displaying a “Friend selector” that allowed participants to scroll through the names and profile photos of all of their Facebook Friends, and select the initial four close Friends","They were instructed not to select Friends under the age of 18 (per the IRB), anyone they would feel uncomfortable answering questions about (participants were allowed to opt out of visiting particular Timelines that might cause them distress), and any “Friends” that were not actually people (e.g. pets, organizations, etc.)","After selecting close Friends, the Friend selector appeared again, this time populated with 20 people randomly selected from the participant's Friend list",The 20 randomly selected Friends did not include the close Friends they had already selected,"Participants de-selected any of these Friends per the instructions described above, and then four of the randomly selected Friends were retained by survey",They were not allowed to proceed with the survey until they had completed the Friend selection process,"After Friend selection, the survey ordered all eight Friends randomly, and the same random Friend order was used throughout the survey","Participants first answered questions about what they recalled about each Friend's recent activity on Facebook, how close they felt to each Friend, and how often in the past month they had used various media to communicate with each Friend","Then, they were asked to visit each Friend's Facebook Timeline via a customized profile link for each Friend, generated by the Facebook app and embedded in the survey question along with a clickable thumbnail of the Friend's profile photo","They were instructed to skim the posts each Friend made just within the past week, and answer questions about each Friends’ actual posting behaviors","The survey specifically instructed participants to:  '  The date was automatically customized by the survey for each participant to be one week prior to when the participant started the survey, and inserted into the question",This was intended to constrain participants’ responses to a timeframe they would reasonably be able to remember,"Because this was an online survey, direct evidence that each participant visited all eight Timelines was not available; however, if more participants in the dataset were cheaters than honest participants, the data would be too noisy to see any results","After visiting each Friend's Timeline, participants were asked questions that make up the two main dependent variables in this study, about whether they had noticed a missed post, and if they had, how surprised they felt by it","The final section of the survey asked about participants’ own Facebook activity, their beliefs and inferences about how the News Feed selects content to display (including whether they are aware of the News Feed algorithm), and demographics. 3.2 Participants Participants were recruited from a panel provided by Qualtrics","Eligible participants lived in the United States, were 18 or older, had more than 20 Facebook Friends, and reported visiting Facebook once per week or more often",The sample included an age quota: 30% of participants had to be older than 50. 1576 participants started the survey; 530 completed it,"Participants could complete the survey using multiple platforms, including mobile devices",Participants received an incentive specified by Qualtrics equivalent to $1.50 for completing the entire survey,"The median completion time was 23 minutes ( =35,  =106)","To ensure data quality, potential participants who answered attention-check questions incorrectly were not allowed to complete the survey","Of those who completed the survey, I excluded cases that reported “Good” or “Full” familiarity with a fake word that was part of the Internet Literacy index variable. 
                          I also used an edit distance metric to identify cases that used the same response for many questions (e.g., choosing the middle category for every question), and excluded cases with low edit distance scores across multiple sections of the survey",An issue with the way the Qualtrics survey platform's client-side JavaScript works in Internet Explorer resulted in some otherwise eligible participants being unable to complete the survey correctly,These cases were excluded from the analysis,"Finally, three participants selected four Friends as “close”, but later in the survey reported that they did not remember who the Friend was","These Friends were excluded from the analysis, but the remaining Friends for these three participants were retained","The final number of participants was 410, which is 26% of those who began the survey","Online survey panels from Qualtrics tend to be more female than male, and this sample was no exception","The sample had 260 women and 149 men, with one person reporting “other”","Participants were mostly white, with a median age range of 35–50","A majority reported some education after high school, and 55% of the sample reported that they had attended or graduated from college","The median participant reported visiting Facebook several times per day, posting less than once per week, and having 300 or fewer Facebook Friends",The mean Internet Literacy score was 2.64 out of 5 ( =0.87),The sample resembles the population of US Facebook users in several ways,"According to a report released in January 2015 by the Pew Research Center, the majority of American Facebook users visit the site at least once a day (70%) with at least 45% checking the site several times per day ( )","Also, an analysis of Nielsen data from a nationally representative US household audience panel collected in March 2011 that involved measured computer usage (not self-report) found that Facebook users are more likely than non-users to be female, young (13–17 years old) , white, and to have at least a high school diploma ( ) ","This sample is a reasonable approximation of these characteristics, with the caveat that users younger than 18 were ineligible for the survey. 3.3 Measures As with any survey, the measures described below are self-report","Some questions asked about participants’ perceptions and reactions, which cannot be objectively measured","Other questions asked them to retrospectively estimate the frequency of their own past actions or experiences, or to report what they remembered about the Facebook activity of their Friends. 
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                      3.3.1 Noticing a missed post A user who reads a Facebook Friend's Timeline to identify whether there are any past posts that he or she did not see in the News Feed is engaged in a recognition task","A recognition task was included in the survey to measure the prevalence of missed posts, and characterize aspects of the circumstances in which they occur","The dependent variable   is a Friend-level variable, meaning that participants answered the same question once for each Friend:  '  Seventy-six percent of participants (311 of 410) had at least one Friend for whom they noticed a missed post; 99 participants reported no missed posts, 120 reported one Friend with a missed post, and 14 participants reported missed posts from all 8 Friends","The frequency histogram of number of Friends with at least one missed post, ranging from 0 (none) to 8 (all) can be found in  
                            ","Each participant was asked, “Approximately how many posts did you see on (Friend Name)'s Timeline that you don't recall seeing in your News Feed?” [1 post, 2–3 posts, 4–5 posts, More than 5 posts]","This is a Friend-level measure that asked participants to estimate how many missed posts they saw, and choose a categorical response","Out of all 3276 Friends included in the survey, participants noticed at least one missed post for 871 of them (27%)","Looking at just the Friends for whom participants noticed missed posts, for 25% participants reported only one missed post, 44% estimated they had seen 2–3 missed posts, 14% had 4–5, and participants reported more than 5 missed posts for 16% of the Friends for whom there were missed posts. 3.3.2 Surprise about missed posts If a participant reported that she had missed a post on a Friend's Timeline, the survey presented a question asking her to indicate how surprised she was about this","The question was,   Responses on the dependent variable   ranged from Strongly Disagree (1) to Strongly Agree (7)",The mean level of surprise was 4.43 ( =1.7),One hundred ninety-two participants (47%) reported Somewhat Agree or higher for at least one Friend,"The survey question did not ask participants to assign an emotional valence to surprise (e.g., positive or negative)","The experience of surprise is evidence of a reaction from each participant about whether, for this particular relationship, the system behavior has met their expectations",Whether a single missed post is a net positive or negative experience would be different for each user based on their own unique circumstances,"However, independent of valence, a relationship between participants’ surprise about noticing a missed post on a Friend's Timeline and other variables, like relationship closeness, identification of patterns across users in their expectations about system behavior. 3.3.3 Relationship closeness The survey measured relationship closeness using the “Inclusion of Other in the Self Scale” ( ), which is a 7-point scale measuring emotional closeness that uses images of circles that do not overlap on the low end (coded as 1) and move closer to each other until they almost completely overlap on the high end (coded as 7)","The mean closeness for the “close” Friends participants had specified via the Facebook app at the beginning of the survey was 4.57 ( =2.11), and for the “randomly selected” Friends was 2.31 ( =1.76)",The sample may over-represent the proportion of closer relationships to distant ones because the distribution of Friend closeness was not sampled to be representative of the proportion of close and distant relationships for the participants,"However, participants answered questions about fewer close than distant relationships which is an approximation in the same direction as these effects in the real world","In the sample, the median number of high closeness Friends (5 or higher on the relational closeness scale) was 3 Friends out of the 8 they answered questions about. 3.3.4 Frequency of communication on facebook A survey question from   was included to measure communication frequency between the participant and each of their Friends selected for the survey, using four different channels:  …  
                         This question was used to assess whether frequency of recent communication with each Friend was related to the likelihood of noticing missed posts, or surprise","Previous research has found that stronger social network ties use more different types of media to communicate than weaker ties ( ), and if participants used Facebook less often than other channels for closer relationships, this could have an impact on how the News Feed Algorithm prioritizes posts from those Friends","If close Friends do not interact on Facebook, the system could infer that they are weak ties rather than strong ties. 
                            
                             displays communication frequency in the past month broken down by whether the Friend was selected by the participant (close) or chosen at random by the Facebook app, and communication channel","Communication frequency via all channels, including Facebook, is positively correlated with relationship closeness. 
                             This means that close Friends do not communicate less on Facebook than they do using other channels",The data also confirm that closer Friends communicate through more channels; the bars in   are all around the same height for the close Friends,"Talking on Facebook is the only channel included in the survey that would generate metadata that the system could incorporate into the News Feed algorithm, providing relationship-level information that might impact post ranking; therefore, Facebook communication frequency was included as a variable in the analyses","In the sample, 88% of participants (359) had talked on Facebook a few times or more in the last month with at least one Friend they were asked about in the study","Overall, this was 47% percent of Friends (1540). 3.3.5 Recall accuracy The accuracy of participants’ recall about their Facebook Friends’ posting activity could be a source of variability in their responses about whether or not they noticed or were surprised by missed posts","For example, if a participant's recall for a particular Friend's behavior is inaccurate, it is reasonable to expect that her judgment about whether the Friend's posts had appeared in her News Feed would be less reliable than if she had remembered her Friend's posting behavior accurately","To assess the accuracy of participants’ recall for details about the Friend's posting activity, the survey asked participants   visiting each Friend's Timeline to recall how long ago they thought each Friend had created a post:   Then,   visiting each Friend's Timeline, they reported how recently each Friend had actually created a post:  ' 
                         The responses to both questions were categorical, and asked participants about relative time intervals   The recall accuracy measure was calculated by taking the difference between the before and after responses for each Friend, and centering them at the median","I then coded the differences as a categorical variable with two levels: “Yes”, meaning that there was a match between participant's memory and when the Friend was most recently active and therefore the participant's recall was accurate (62% of cases); or “No”, the answers to these questions were not the same, and therefore the participant's recall was inaccurate (38% of cases)",This measure is used in the analyses as a control for memory accuracy. 3.3.6 Recency of friend activity The survey instructions directed participants to visit each Friend's Timeline and scroll back one week into the past,It is possible that some of the Friends that participants were asked about had not created any posts in the past week,"If so, it is reasonable to expect that if participants followed instructions they would not have noticed any posts created in the past week from inactive Friends","To account for this variability, I used the responses to the question about how recently the Friend had created a post, asked after participants visited their Friends’ Timelines, to create a binary variable representing whether or not the Friend had created a post in the past week","Responses were coded as “No” if the participant reported that the Friend's most recent activity was “Not at all within the past week”, and “Yes” otherwise","Sixty-seven percent of all Friends in the study (2197) were active, and 407 participants (99%) reported having at least one Friend that had created post(s) in the week before they took part in the study. 3.3.7 Friend visibility Posts from some Friends may appear more often in a user's News Feed than posts from others","This could represent a prioritization of a particular Friend's posts by the News Feed Algorithm, or that a particular Friend simply creates more posts",Both of these explanations could result in some Friends appearing more often—being more visible—in a user's News Feed than their other Friends,"Regardless of the cause, the more a user sees a particular person represented in his or her News Feed, the more the user might believe they are seeing most or all of the Friend's posts","I anticipated that for more frequently-seen, high visibility Friends, missed posts might be more noticeable and also more surprising","However, participants’ Friends were not participants in the study themselves, and the survey did not have permission to access their posting histories for a direct measure of their posting frequency","Although this information could have been accessed via the Facebook API, the survey did not do so because it would have required additional permissions beyond “basic information,” which would have discouraged privacy-sensitive users from participating","I also determined through experimentation with the Facebook API that it was not possible to re-create an accurate representation of which posts had actually displayed in a user's News Feed over the past week, or the precise order in which they appeared as determined by the News Feed Algorithm",So it was not possible to construct an objective measure of how often posts from a particular Friend appeared in participants’ News Feeds,"For more frequently-seen, high salience Friends, missed posts might be more noticeable and also more surprising","Therefore, to measure the overall visibility of each Friend in a participant's News Feed relative to the other Friends, participants were asked the following question:  '  Seventy-six percent of participants answered this question “Probably Yes” or “Definitely Yes” for at least one Friend that they were asked about (25% of Friends)","This self-reported measure is a proxy for Friends’ overall News Feed visibility as it is experienced by the participant, and captures the participants’ subjective impression of how much they see a particular Friend in their News Feed in relation to their other Friends. 3.3.8 Facebook activity, number of friends, and demographics Three variables describe participants’ self-reported level of activity on Facebook in terms of how often they visit, post, and how many Facebook Friends they have",These variables are important controls,"Users who visit Facebook multiple times per day are likely to have seen more different Facebook posts than those who visit only about once per week, and users who post to Facebook often may attend to the content of others’ posts differently than those users who do not create posts","Visit frequency was measured with the question,   Participants were also asked,   Two additional variables are demographic controls in the analyses: internet literacy and participant age","Age is particularly important as a control for memory performance differences across the lifespan ( ). 4 Results I used two multi-level regression analyses to answer questions about how important closeness with a Friend is for whether or not a participant noticed a missed post when visiting the Timeline of a Facebook Friend ( ), and how much the missed post surprised him or her ( )","Both models use the same predictors, including self-report measures controlling for subject-level differences like individual Facebook activity and demographics, and Friend-level differences like frequency of Facebook communication between the participant and the Friend, and visibility of the Friend in the participant's News Feed","The subject-level questions were answered once by each participant, and the Friend-level measures were answered eight times per participant (once for each Friend)","Because the dataset consists of multiple responses from each participant, both models include a random effects control for participant","Both models are specified as follows; (F) denotes a Friend-level variable, and (P) denotes a participant-level variable: 
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   4.1 Closeness did not matter for missed posts I expected that participants would be less likely to notice missed posts from close Friends, because the News Feed Algorithm prioritizes posts it predicts users will want to see, and because participants should be most interested in posts from people they feel close to",I used a mixed-effects binary logistic model to identify effects related to the likelihood of noticing a missed post when visiting the Timelines of specific Friends,I compared this model with a null model containing only the intercept and random effects term  ,"This is similar to the F-test in an OLS regression, and statistical significance indicates that the more fully specified model does a better job of explaining the data than the null model","The regression results are presented in  
                          under  ","The intercept (−2.33,  =0.24) represents the log odds of noticing a missed post from a Friend who was not active on Facebook in the past week and whose activity the participant does not remember accurately, with closeness, frequency of communication on Facebook in the last month, Facebook visit and post frequency, age, and number of Facebook Friends centered at the median, and internet literacy centered at the mean","In other words, the model predicts that there is a 9% probability that a participant with 101–300 Friends who is 35–50 years old and visits Facebook a few times per day and posts a few times per week would notice a missed post from a Friend that has not created a post in the past week, when the participant is not that close to the person and doesn't remember the Friend's past posting activity very clearly","In the model, closeness had a slightly negative relationship with the likelihood of noticing a missed post, but was not statistically significant ( =0.968,  =0.30; an odds ratio of 1 means the likelihood of both outcomes is the same)","However, two other measures in the model had statistically significant relationships with the likelihood of noticing a missed post: whether the participant's recall was accurate, and if the Friend had posted on Facebook in the past week","Participants were   likely to notice missed posts from Friends who were active in the past week ( =3.66,  ), and less likely to notice them when their memory was accurate ( =0.46,  )","Two variables that might have an impact on noticing a missed post on a Friend's Timeline both had very small effect sizes and were not statistically significant: how often the participant reported he or she had talked on Facebook with the Friend in the past month ( =1.06,  =0.31), and the perceived visibility of that Friend in the participant's News Feed ( =1.03,  =0.58)","I included an interaction between closeness and frequency of communication on Facebook in the model, because previous research has shown that communicating on Facebook with strong ties has less of an impact on tie strength than communicating with weak ties ( )",This suggests that the effect of Facebook communication frequency on the likelihood of noticing a missed post could be different at different levels of closeness,"However, this was not the case in the data: the effect size of the interaction term in the model was also very small and was not statistically significant ( =1.008,  =0.66)","I also included an interaction in the model between whether the participant's recall was accurate, and whether the Friend had created a post in the past week","If the Friend had not been active, there wouldn't be any posts for the participant to have missed, regardless of what he or she remembered","The effect size for this interaction was positive but not very large ( =1.76,  =0.05), and not statistically significant, so this is evidence that there is no reliable interaction between these two variables","Two control variables were, however, statistically significant: Internet literacy ( =1.25,  ) and how often the participant visits Facebook ( =0.72,  )","This means that participants who were more Internet-savvy were more likely to notice missed posts, and who reported visiting Facebook more often were less likely to notice missed posts",This could illustrate an overall effect of time spent online,"The pattern of the results is easiest to understand as the likelihood of a missed post being noticed based on different combinations of values of the predictors in the model, illustrated in  
                         ","The bar chart illustrates the impact in the model of closeness, recall accuracy and whether the Friend posted in the past week, for participants who talked on Facebook with their Friend frequently in the past month and for whom their Friend is highly visible their News Feed. (The remaining controls in the regression are held at their median or mean; see   for those values.) There is little difference in the likelihood of noticing a missed post between the levels of closeness (the three bars); however, participants were much more likely to notice a missed post when their Friend had created posts in the past week (Yes on the x-axis)","In summary, the most important factors in terms of both effect size and statistical significance for predicting whether a participant would notice a missed post were how recently the Friend was active on Facebook and how well the participant remembered the Friend's activity","Consider a participant with accurate recall for the posting behavior of a Friend she is not very close to, but who she has talked with on Facebook a few times in the past month","If that Friend has not created a post in the past week, the model indicates there is a 4% chance that upon visiting that Friend's Timeline she would encounter a missed post","However, if the Friend is someone she talks to daily on Facebook and who has created a post in the past week, the probability is 27%—a greater than one in four chance of noticing a missed post","I had expected the data to show evidence that participants are more aware of—in essence, that they “keep up with”—more of the posts created by Friends they feel closer to","This would happen if the News Feed algorithm prioritizes closer Friends’ posts, and would manifest in the data as noticing fewer missed posts from closer Friends",But the results suggest that this is not the case,"After controlling for participants’ recall accuracy, characteristics of the Friend's posting behavior such as recency and visibility in the participant's News Feed, and the participant's own Facebook use, closeness had no impact on the likelihood of noticing a missed post","Noticing a missed post happens equally across Facebook users’ close and distant relationships, indicating that the algorithm may not be prioritizing close Friends’ posts for display. 4.2 Greater closeness is related to more surprise Recent studies of user reactions to algorithmic curation in the Facebook News Feed have found that noticing a missed post can be an unpleasant surprise for Facebook users ( )","I expected that it would be more surprising for a participant to find a post she had missed on a close Friend's Timeline than a distant acquaintance's, because people use Facebook to stay in touch with and feel connected to others they care about ( )","I used a second mixed-effects regression with the same predictors as the previous model, on a subset of the data that included only the Friends for whom each participant had noticed at least one missed post",The dependent variable is how surprised participants were when they noticed a missed post on a Friend's Timeline,I compared this model with a null model containing only the intercept and random effects term  ; statistical significance means the more fully specified model does a better job of explaining the data than the null model,The regression results are presented in   under  ,"The intercept in this model is 4.41 ( =0.21, 4.0 is “Neutral”), and represents the level of surprise experienced by a participant about a missed post from a Friend who had not created a post in the past week, whose activity the participant did not remember accurately, with frequency of communication on Facebook in the past month and the two Facebook activity variables (participant visit and post frequency, number of Facebook Friends) held at their median, and internet literacy (mean), and age (median) also centered","In other words, the model predicts that a participant with 101–300 Friends who is 35–50 years old and visits Facebook a few times per day and posts a few times per week would be slightly surprised to notice a missed post on the Timeline of a Friend that has not created a post in the past week, when the participant is not that close to the person and doesn't remember the Friend's past posting activity very accurately","As expected, closeness was positively associated with surprise",Surprise increased by 0.09 for each level of closeness  ,Participants also expressed more surprise about missed posts from Friends that were more prominent in their News Feeds  ,"Both of these results are in contrast to the   model, where these predictors were very small and not statistically significant","How often the participant and the Friend talked on Facebook in the past month, as reported by the participant, also had a positive relationship with surprise, although it was not statistically significant ( =0.08,  =0.11)","However, I again included an interaction between closeness and frequency of communication on Facebook in the model, and this time it was statistically significant  ","This means that talking with a Friend on Facebook more times in the past month has a different effect on surprise, depending on the levels of closeness","For low closeness relationships, more frequent communication on Facebook increased surprise about missed posts; but for high closeness, it decreased the amount of surprise","Nevertheless, despite the interaction, surprise was highest overall for high-closeness relationships","Recall accuracy   is an important control in this model, like the   model",The effect is negative and large compared with the other predictors: accurate recall meant a half point less surprise,"Unlike the   model, whether or not a Friend had created a post in the past week was not statistically significant ( =−0.10,  =0.60)",This is likely because 85% of the Friends for whom participants said they had noticed a missed post had been active on Facebook in the past week,"I also included the interaction between recall accuracy and recent activity, which was not statistically significant in this model either ( =0.47,  =0.07)","The low end of the range of predicted values is 3.46 out of 7, for a participant who is not close to a particular Friend that he has not talked with on Facebook in the last month or seen any posts from recently, with accurate participant memory, when the Friend has not posted in the past week",This means that a missed post from a low closeness relationship with a Friend that is not very active on Facebook would not be very surprising,"Predicted surprise increases to 4.68, more surprised than not, for a Friend that the participant is very close to but that otherwise has the same characteristics as the previous example","On the high end of the predicted values for surprise is 4.98, when the participant is very close to a Friend that was recently active and is typically very visible in the participant's News Feed, but the participant has not talked with them on Facebook lately","Greater closeness is associated with more surprise, even when past interactions are infrequent and the Friend is not visible in the participant's News Feed","The predicted values generated from the model and displayed in  
                          illustrate these patterns","The graph presents three factors: frequency of communication on Facebook (x-axis), Friend visibility (left and right panels), and closeness (lines)","To generate these values, all other variables in the model were held at their median or mean",The closeness × Facebook communication frequency interaction is clearly visible in the graph,"For low and median closeness, more frequent communication on Facebook was associated with more surprise at noticing a missed post","But, for high closeness Friends, there was actually slightly less surprise about missed posts as frequency of communication increased","A large number of the surprising missed posts in this study came from Friends that participants reported they had not recently communicated with via any channel.  
                         a and b present two heatmaps with three facets for surprise: No (values 1–3), Neutral (4), and Yes (5−7)",Each facet shows a comparison between two channels of communication,"The x- and y-axis of each facet show increasing frequency of communication in the past month via the given channel, ranging from “Never” to “Daily”","For example, there are 177 cases in the dataset where the participant “Never” talks to the Friend Face to Face or via Phone or SMS, and was surprised by a missed post","This means that Facebook could be a primary channel of communication for some participants and their Friends, and for these relationships missed posts might have greater impact than for those who communicate via multiple channels","Relationships that are primarily maintained on Facebook and through no other channels would be most sensitive to being made invisible by the algorithm. 4.3 No effect for algorithm awareness Users who were aware of the News Feed algorithm might be more likely to notice missed posts, and also less surprised by them, than users who were not","Therefore, in addition to the above results, I also ran both the   model and the   model with an additional variable controlling for participants’ awareness of the News Feed Algorithm","At the end of the survey, right before the demographics questions, participants were asked the following:  
                      I compared the models with the algorithm awareness variable against the models reported above","In both cases, there was no statistical difference between them (notice.missed:  ,  =0.17; surprise:  ,  =0.362)","This means that the additional variable does not change the results, and awareness of the algorithm was not meaningfully related to the likelihood of noticing a missed post on a Friend's Timeline or the experience of surprise about missed posts. 4.4 Causal beliefs about missed posts There is some evidence from   and   that Facebook users would not be surprised by posts they believe they missed because of their own actions, such as not visiting Facebook often enough, or because they were skimming and not fully reading every post, or did not scroll down far enough to see the missed post","However, the survey discussed thus far did not include questions about participants’ causal beliefs regarding why they had missed particular posts","To address this, I conducted a second survey, also in April 2014","The two surveys will be referred to from now on as the “initial survey” and the “follow-up survey”. 
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                      4.4.1 Method, participants and measures The follow-up survey instrument was very similar to the initial survey, with a few differences","A set of questions were added that displayed only if participants reported noticing a missed post, which asked,  ","The closed-ended items were developed after summarizing responses to an exploratory open-ended question included in the initial survey, about why participants believed they had missed posts","The new items were presented in random order for each missed post a participant reported, and included: 
                         Items 1–3 above were combined into a composite variable representing participants’ beliefs about how their own behaviors can cause them to miss posts ( ,  =2.61,  =1.01, Cronbach's  ), and items 4–7 into a second composite variable representing beliefs about how the filtering algorithm can cause missed posts to occur ( ,  =2.90,  =0.89, Cronbach's  )",There was no meaningful correlation between the two variables (Pearson's  ),The scale of the   dependent variable question was modified to use the same 5-point agreement scale described above,"And, participants in the follow-up survey were shown 5 close Friends and 5 randomly selected Friends, instead of 4 of each as in the initial survey","These additions made the survey significantly longer, so the questions about communication frequency via different media with each Friend and Friend visibility in the News Feed were removed from the follow-up survey, to keep the total completion time about the same in both surveys","The median completion time for the follow-up survey was 19 min ( =29,  =95).  
                             shows a comparison of the items included in the two surveys","Participants for the follow-up survey were recruited using Amazon Mechanical Turk, and were required to meet the same recruiting criteria as the initial survey, with two differences: only workers from the USA who had a 90% or higher approval rating after completing at least 500 tasks were eligible, and there was no age quota",Participants received $5 for completing the entire survey,"A total of 505 respondents finished the follow-up survey, and after following the same data cleaning procedure as the initial survey, 464 remained in the dataset",There were some demographic differences between the Qualtrics panel and the Amazon Mechanical Turk workers who participated,"The sample from Qualtrics had more women than men, while the MTurk sample was the opposite","Qualtrics participants had a lower mean Internet Literacy score, and were older overall than MTurk participants","Finally, the MTurk sample had twice as many college graduates as the Qualtrics sample.  
                             contains demographic information about both samples. 4.4.2 Greatest surprise with strong filter beliefs The purpose of the follow-up survey was to learn more about why people were surprised by missed posts, and whether the correlation between closeness and surprise observed in the initial survey would still be present after controlling for participants’ causal beliefs","It is possible that surprise about missed posts is a function of how participants think the system works or their own behavior when reading posts, more than the closeness of their relationships","If this is the case, it implies that surprise might be reduced with better transparency about cause-and-effect in filtered systems","To investigate this, I conducted a mixed-effects regression using data from the follow-up survey with   as the dependent variable, restricted to the Friends for whom each participant had noticed at least one missed post","Thirty-one participants reported zero missed posts, so the total number of participants included in this model was 433","The model was specified as follows; (F) indicates a Friend-level variable, and (P) indicates a participant-level variable: 
                         I compared the above model with a null model containing only the intercept and random effects term, and also with a model containing all of the above variables except   and  ","The more fully specified model does a better job of explaining the data than the null model ( 
                            (3,16)=206.99,  ) or the model without the beliefs variables ( 
                            (12,16)=64.80,  )","The intercept is 3.15 ( =0.10, 3.0=“Neither Agree nor Disagree”), and represents the level of surprise experienced by a participant about a missed post from a Friend who had not created a post in the past week, whose activity the participant did not remember accurately, with the remaining variables in the model centered at the median or mean","As in the initial survey, closeness was positively correlated with surprise ( =0.19,  =0.02,  )","The regression results are presented in  
                            .  
                            a and b show the predicted values from the both the initial and follow-up   models side-by-side, using a standardized dependent variable",From these graphs it is clear that the relationship between closeness and surprise is very similar in both samples,"I included separate interactions in the model between closeness and each of the causal belief variables, to find out if the relationship between surprise and closeness varied based on users’ causal beliefs","These interactions are illustrated in the predicted values generated from the model, displayed in  
                            a and b",The difference in the level of surprise between close and distant Friend relationships increases as participant beliefs implicating filter behavior as the cause of the missed post get stronger ( a),"When filter beliefs are weak, predicted levels of surprise are neutral or low at all levels of closeness","This means that when participants did not assign responsibility for the missed post to the system, they were not surprised when they noticed a missed post","But, when filter beliefs are strong, predicted surprise is very high for close friends and low for distant friends","This indicates that participants with filter-related beliefs about how the News Feed works expect the system to take action to make posts from people they feel close to more visible, but not those posts from people they do not feel close to",Participants’ beliefs that their own behavior caused them to miss posts had a different relationship with closeness and surprise,The difference in predicted surprise about missed posts between high and low closeness Friends decreases as user behavior beliefs become stronger ( b),"When user behavior beliefs are strong, surprise about missed posts is low","This means that when participants believed their own behaviors caused them to miss posts, they were not very surprised about it","But when user behavior beliefs are weak, predicted surprise is very high for close friends and low for distant friends","This indicates that in the absence of user behavior to the contrary, participants expected that posts from close friends would be highly ranked by the News Feed. 5 Limitations One limitation of this research is that it relies on self report which can be biased","Some of the variables of interest can only be measured by asking participants about their perceptions, like closeness of the relationship between the participant and their Friends, noticing a missed post, and surprise","However, others, such as the frequency of communication on Facebook, number of Facebook Friends, and how often the participant posts to Facebook ask participants to remember or estimate details about their behavior and their Friends’ behavior on Facebook","These responses could be biased if participants answer inaccurately, either intentionally or unintentionally",There are several sources of bias that often appear in surveys,"For example, participants’ responses might be inaccurate if they did not follow the instructions carefully regarding their selection of Friends, whether they actually visited their Friends’ Timelines, or if they did not stop scrolling when they reached posts more than a week old","Not following instructions would cause variability in the data, because the responses from participants who were more careful about following directions would be more accurate than from participants who did not","However, there is no reason to expect these inaccuracies to vary systematically with the variables of interest within the sample","This means that, while it is harder to statistically detect a true effect than it would be if participants all followed directions perfectly, the results presented here are still valid","Another source of bias in surveys is social desirability, which causes participants to purposefully answer incorrectly in situations where true responses could be uncomfortable, embarrassing, or risky in some way ( )","There is little reason to expect this is a problem in this survey; the questions do not ask about sensitive topics, nor is there a clear socially acceptable response or reason why participants might answer in a particular way to avoid embarrassment",A third source of bias is imperfect memory,"Some of the survey questions ask participants to remember things about their own or their Friends’ past behavior, and recognize whether they had already seen their Friends’ posts","They might not remember these things accurately, and there are a few ways in which this might have systematically biased the results","For example, participants may have overstated or understated how often they communicate with their Friends on Facebook","When participants were asked to estimate the frequency of past occurrences, they may unintentionally discount particular instances that are less salient or memorable, or a particularly memorable or recent instance might cause them to make their estimates too high ( )","It is reasonable to guess that this might vary based on the closeness of the relationship, which is why I included the interaction between   and   in the models","Also, there are a couple of ways that imperfect memory might result in incorrect estimates about the incidence of missed posts","The older the posts are on Friends’ Timelines, the more likely participants are to have forgotten them",This might artificially increase the number of missed posts,"I attempted to minimize the effects of this by directing participants to only scroll back one week in their Friends’ Timelines, but as mentioned above some participants may not have followed this instruction",This means there could be an interaction between the age of the Friends’ posts and whether the participant followed instructions that the analysis did not directly account for,"Some of this within-participant variability should be captured by the random effects term in the models, however","Also, some of the posts that participants said they missed may have been created since the participant's last visit to Facebook before starting the survey, and so they could not reasonably remember them. (Participants reported that 32% of the Friends’ most recent posts had been created “Today”)","In addition, participants may falsely recognize a post they did not see in their News Feeds, or fail to recognize a post that they had actually previously seen","There are likely some instances of both kinds of errors in the data, but since data about the conditions under which participants first encountered the information in their News Feeds or the characteristics of the posts was not available using this method, it is very difficult to to substantively characterize how common these errors might be","However, recognition accuracy is highest for more recent events ( ), and when relevant contextual details are activated in memory as part of the recognition task ( )","Participants viewed posts from people they have an existing relationship with, that were one week old or less, in reverse chronological order","The format and content of posts viewed on one's Facebook Timeline are identical to how they appear in the News Feed, including details such as photos, link summaries, comments, etc",These characteristics of the task should reduce the incidence of both kinds of errors,"Also, the recall accuracy measure should control for some of the factors that might affect both kinds of errors, because recall is correlated with recognition ( )","Other measures, like surprise, are not susceptible to imperfect recall","The survey asked about surprise immediately after participants were asked about missed posts, which yields more accurate results than if they had been asked to remember previous feelings of surprise ( ). 6 Discussion Using an online survey, Facebook users were prompted to visit the Timelines of some of their Friends where in many cases they were exposed to posts that they did not remember seeing in their News Feeds",This experience made aspects of the socio-technical system's behavior visible that participants might not otherwise have been aware of,"After controlling for variability in participants’ memories of their Friends’ past Facebook posts, noticing a missed post on a Friend's Timeline was not related to the closeness of the relationship between the participant and the Friend","However, the amount of surprise about missed posts was: greater closeness was associated with more surprise in data collected from two different samples","In other words, the system behaved more unexpectedly for participants when they missed posts from Friends they felt closer to, and also from those Friends they remembered seeing most often in their News Feeds","When participants believed they missed a post due to their own actions, such as not scrolling down far enough or not visiting Facebook often enough to see the posts, they were less surprised about missed posts","But, when they believed they missed posts because of actions taken by the system, they found it more surprising that they had missed posts created by close Friends","Even after controlling for these causal beliefs, missing a close Friend's post was nearly always more surprising than missing a post created by a distant Friend","These findings show that because users have offloaded the work of curating their News Feeds to the algorithm, they have effectively delegated curating and maintaining their relationships as well, without being aware of it","The algorithm may not prioritize relationships the same way a user would; in fact, the feeling of surprise upon seeing some missed posts is some evidence of this","The findings also suggest that there are systematic patterns regarding which opportunities for interaction are being promoted versus demoted that make algorithmically curated relationships different from how people manage their “real world” interactions. 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   6.1 Visibility and invisibility I have been careful so far to refer to posts as “missed” rather than “missing”, to avoid causal judgments in reporting the results","A missed post might be the result of a user's selective attention, inaccurate memory, or it could have been made effectively invisible by the algorithm","However, the analyses included controls for alternative explanations including visibility of Friend activity, participant recall accuracy, and the number of Friends participants have",This means that “missed” and “missing” posts can reasonably be conceptualized differently from each other,"Posts participants were surprised about can be thought of as “missing” rather than missed from the perspective of the user, because that feeling is a symptom that their expectations have not been met","In this case, it signals that participants expected to see posts from these closer, more visible Friends in their News Feeds","It is important to separate what users “expect” from what they “want” from a content filtering algorithm. “Expect” implies an outcome that is different from what the user thought would happen, while “want” implies satisfaction with the outcome that occurred","Facebook has argued that if users were to see an unranked version of their News Feed, they would actually be   likely to miss posts they want to see than they are with the News Feed Algorithm in place ( )","In fact,   found something similar to this in their study","When they gave participants an opportunity to move posts they had missed higher in a representation of their News Feeds, many of them concluded that the algorithm had actually done a decent job deciding which posts to show them","These are both arguments in favor of filtering as a mechanism to increase individual satisfaction with the News Feed, by offloading the work of categorizing desirable from undesirable content","Despite the importance of user satisfaction, optimizing for it does not preclude other kinds of systemic algorithm effects that are harder to measure, due to content that remains invisible",Focusing on user satisfaction as a metric for evaluating the effects of algorithms limits the attention of research and design to only a small part of the work filtering algorithms do in socio-technical systems,"By focusing on expectations and surprise rather than satisfaction, this study identified an effect of invisibility at the relationship level","In essence: if I don't see your posts, I won't be reminded to communicate with you on Facebook","If I don't communicate with you on Facebook, the News Feed will think we aren't close friends","If the News Feed thinks we are not close friends, it won't show me your posts","The behavioral traces, which are all the evidence the News Feed Algorithm has to use to infer the strength of friendships, disappear",This pattern is suggestive of how relying on an algorithm to direct attention can have unexpected consequences beyond satisfaction that it would be difficult for individual users to identify on their own,"In addition to being potentially costly for individuals, large-scale patterns across users regarding whose posts are seen and whose are not can also be costly for the system overall","For example, the algorithm could act as a constraint on a user's visible network, artificially limiting the reach and impact of the user's posts","Facebook's reciprocal Friend connections may in reality behave more like Twitter's directional ties, if the constraints imposted by the ranking decisions of the filtering algorithm cause post visibility between two Facebook Friends to not be reciprocal","Whether constraints like these can be considered “bias” depends on the frame of reference (user vs. system operator, for example), and what kinds of outcomes each is hoping to achieve",In this case the underlying network structure of reciprocal Friend connections may be effectively altered by the visible and invisible output of the algorithm,"These findings raise a fundamental question about the power of algorithms and automation in general: if the algorithm exists to direct users’ attention by showing them some posts and not others, and the system is biased in a particular direction, how will this ever be detected? Detection of bias is much easier when the system produces a clearly unfair or undesirable outcome, such as unintentionally reminding people about years-ago sad or painful past experiences ( )","But if the undesirable outcome is something that is   displayed and therefore interactions that do not happen, systemic bias is much harder to identify","Because filtering is invisible for many users without a triggering event like a Friend pointing out a missed post ( ), on a post-by-post basis it is impossible for users to notice and react to the fact that posts from some close Friends are less likely to be highly ranked, and certain relationships may become harder to maintain than others over time, whether or not the user is satisfied with the visible contents of the News Feed overall. 6.2 Implications for user control and algorithmic transparency In July 2015, after the data collection for this paper took place, Facebook announced new “Updated Controls for News Feed” that allow users to “actively shape and improve the experience” by selecting particular Friends whose posts will be displayed at the top of the News Feed. 
                          Using these controls is advertised to change the way posts are ranked, and would thereby change the set of posts available for users to consume","Assuming a user is already spending as much time as he or she wants to on Facebook, choosing to see posts from some Friends first means other posts will receive less attention","This will shift the characteristics of the algorithm-imposed visibility constraints—but in what way? Consider two possible ways users might choose to prioritize people: based on a mental list of which friends they want to stay in touch with the most, or based on noticing they've been missing posts from some Friends in particular","At the level of each individual user, these two different approaches both could result in similar improvements in user satisfaction","Actually, any user-controlled prioritization will likely increase satisfaction with the News Feed; previous research in recommender systems has found that just giving users the appearance of control increases satisfaction whether or not their selections actually change any aspect of system ranking or performance ( )","However, at the system level, the possible aggregate outcomes of these individual choices are harder to predict","Which posts will receive more interaction than before, aggregated across the entire system, and which will receive less? Person-level controls could plausibly cause users to see an increase in their News Feeds of posts that are less popular overall, as posts with fewer likes and comments (relative to before the introduction of the mechanism) might be ranked higher for individuals based on who created them","This could spread user attention in the system out over more posts than before resulting in more posts being seen by fewer people, and more diverse opportunities to give and receive social support","Depending on what criteria users base their choices on for which Friends to prioritize, perhaps this intervention will eliminate the correlation between closeness and surprise that observed in this study","It might also end up encouraging the formation of polarization and echo chambers by reducing the likelihood of serendipitous encounters with new perspectives from different people, as users’ feeds fill up with posts from close Friends","But the missed posts will remain, as does the problem of measuring the invisible opportunity cost","A different approach that has been proposed to help users better understand how the system works and promote more accurate expectations is greater  : disclosure of information that would make details of the inner workings of the “black box” more visible, and thereby provide the necessary context for outsiders to understand cause and effect in the system ( )",Transparency is one path to giving users more control by helping them understand the consequences of their actions in the system,"The results from the follow-up survey showed that participants’ causal beliefs about why they missed posts were important; surprise was high for close friends when filter beliefs were strong, but not when user beliefs were strong",This indicates that participants who thought Facebook was making choices about which posts to show them nevertheless had a mistaken impression of how those choices were made,"There are many ways that transparency might be implemented, but no consensus on the most effective way to provide transparency information to users","Facebook already provides some transparency: users can see a chronologically ordered view of the News Feed by selecting the “Most Recent” view, which is ostensibly a less-filtered option","Also, the “facebook for business” blog ( ) includes a description of the kinds of information the algorithm uses when making ranking choices","However, both of these ways of conveying transparency information require users to know about them and be able to imagine what might cause differences in the ranking; it seems unlikely that either would be effective","Other options for increased transparency about relationship-level effects might include telling users which of their Friends have been active recently whose posts have not been displayed to them in their News Feeds, or characterizing which Friends’ future posts are likely to be highly ranked","This would place the output of the algorithm in the context of users’ experiences and relationships, and provide more information about cause and effect","But, this still requires the user to connect input (their own behavior) and output (algorithm ranking) on their own, and individual users typically do not have a broad enough view of the system to be able to do this accurately",Explicit feedback about cause and effect might look more like delivering information about how clicking on particular photos or Liking particular posts would affect the post author's visibility in one's News Feed,This would allow users to imagine possible future scenarios and then make choices based on which actions would allow them to achieve the outcomes they prefer,"It would also potentially reveal proprietary trade secrets and make it easier for users inclined to do so to “game” the system, which would be undesirable from the perspective of system designers ( )","It is not a given that greater transparency is in the interest of system designers; however, it is also not universally true that transparency is better for users","Transparency adds additional information for users to attend to in a situation in which filters were added to reduce information overload. 7 Conclusion There are public benefits to be had from the scientific study of socio-technical systems, like Facebook, that aren't in the public domain","It is valuable to study these interactions between human behavior, which is highly variable but not completely random, and the algorithms that attempt to separate signal from noise and make decisions on the user's behalf","Systems that involve algorithms and automation are becoming more complex, and more common","The constraints on what people consume on Facebook are a function of both their own behavior and the algorithm's rankings, and it is difficult to separate them","Findings and implications like these allow us to better understand how interdependencies between algorithm and user behavior affect the performance of socio-technical systems, and what designers must consider when working to produce desirable, emergent properties from the interactions between users and algorithms.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0957417415003590," 1 Introduction In recent years, mobile communications have grown both in traffic volume and offered services",This has increased the expectations for quality of service,"In this scenario, service providers are pressed to increase their competitiveness, and therefore to increase quality and reduce costs in the maintenance of their networks",This task can only be achieved by increasing the degree of automation of the network,The Next Generation Mobile Networks (NGMN) Alliance ( ) defined   as a set of principles and concepts to add automation to mobile networks,"Recently, these ideas were applied to   networks by the 3rd Generation Partnership Project (3GPP) ( ) in the form of use cases and specific SON functionalities","The SON concept is composed by three fields:  ,   and  ","This paper is focused on self-healing, which includes all the functionality targeted towards automating troubleshooting in the radio access network (RAN)","Currently, the task of RAN troubleshooting is manually done, so the ability of automating it is a great competitive advantage for operators","The benefits of self-healing are numerous, since it will offload troubleshooting experts of repetitive maintenance work and let them focus on improving the network","It also reduces the downtime of the network, therefore increasing the quality perceived by the users","Self-healing is composed of four main tasks: fault detection, root cause analysis (diagnosis), compensation and fault recovery ( )","Currently there are no commercial implementations of these functions, although some research effort has been recently made","The reason of this shortage of implementations, according to the COMMUNE project ( ) is the high degree of uncertainty in diagnosis in the RAN of mobile networks",The COMMUNE project uses a case based reasoning algorithm ( ) where a vector of Performance Indicators (PIs) is compared against a database of known problems,The cause will be the same as the case of the nearest stored problem,In   a Bayesian Network (BN) is used to do the diagnosis,"To implement the system, it is required that an expert sets the parameters of the BN, so a Knowledge Acquisition Tool is also presented in  ",The UniverSelf project ( ) combines BNs with case-based reasoning ( ) for the diagnosis process,"Apart from the previous references in diagnosis, research in self-healing has also been extended to detection ( ) and compensation ( ), which are out of the scope of this paper",This paper proposes a method for learning troubleshooting rules for diagnosis methods based on Fuzzy Logic Controllers (FLCs) ( ),"FLCs use fuzzy logic ( ) to assign fuzzy values to numerical variables, and by applying fuzzy rules, they obtain the value of an output variable (e.g. a parameter value) or a given action","FLCs have been used for diagnosis in other fields, such as industrial processes ( ), machinery operation ( ) or medical diagnosis( )","Although FLCs have been used in mobile networks for self-optimization ( ), there are no previous references proposing its application to self-healing",The implementation of the diagnosis process is done by Knowledge-Based Systems (KBS) ( ) such as BNs and FLCs,KBS are composed of two main parts: the Inference Engine (IE) and the Knowledge Base (KB),This separation permits the algorithm to be used in a variety of situations by changing only the KB,"Still, the construction of the KB, or Knowledge Acquisition (KA) ( ) is a major research topic",This is where the previous KA proposals in literature for diagnosis in wireless networks fail to deliver convincing results,The KA process involves the troubleshooting experts in a time consuming process ( ),It is based on the fact that the knowledge is contained in the experience of the expert,An alternative approach ( ) considers that the knowledge applied by the experts in the troubleshooting process will also be contained in its results,Every pair composed of the PIs and the fault cause and/or action(s) to be taken provided by the expert contains information about what aspects are observed by the expert and how they are related to each other,"Therefore, a problem database ( ) can be created, where each problem is saved along with its diagnosis; and this database will hold the expert’s knowledge implicitly",Data mining (DM) consists of the discovery of patterns in large data sets through the application of specific algorithms ( ),The result of DM is a model of the studied system,"DM is used to process information from sensor networks ( ), in scientific data collection ( ) or computer science ( )",DM is also used in commercial applications to find marketing trends ( ),"Modern data collection and monitoring systems produce large amounts of data containing valuable knowledge, but due to the huge amount of information, this knowledge is hidden and needs to be extracted and easily visualized","In fact, the traditional DM techniques are often insufficient or ineffective for the large amount of available data","This leads to the development of Big Data techniques ( ), which deal with databases that have special requirements due to one or more of the following factors, known as the  : 
                   In this work, a DM algorithm for obtaining fuzzy rules in the “  …” form for the diagnosis of the RAN of LTE networks is proposed, which relates certain behaviors in the PIs of a sector at a given time to the possible problem that is present in it",The rules reflect the knowledge of the experts that is implicitly contained in the results of their work (a database containing the solved problems),"Other learning algorithms have been proposed for fuzzy rules in other fields, but none of them have been adapted to mobile communication networks ( )",This algorithm has been designed to be easily parallelizable in order to fit the velocity requirements described earlier and to minimize the memory footprint so a large volume of data can be processed,The remaining of this paper is organized as follows,"In Section   the traditional processes of troubleshooting is presented, along with the use of KBS for automating it",In Section   the proposed DM algorithm for automating the KA process is presented,"Afterwards, in Section  , the system is tested and its performance is compared with another learning algorithm","Finally, the conclusions are discussed in Section  . 2 Problem formulation 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   2.1 Troubleshooting in LTE networks The process of troubleshooting is made up of four main tasks: 
                      This process is usually manually done","Experts monitor a reduced set of Key Performance Indicators (KPIs), aggregated over the whole network",KPIs are composite variables that contain the information of several low-level PIs and reflect the general behavior of the sector,"When one (or several) of those KPIs is degraded (e.g. it is lower than a certain threshold), a list of the worst offenders is obtained, showing the sectors that more strongly degrade the KPI","Those sectors are then diagnosed by observing further symptoms (i.e. abnormal values of PIs and other low-level metrics, such as counters, measurements and alarms)","The relations among these symptoms, described by heuristic “  …” rules, determine the diagnosis","This task is a   process, where each case (a set of PIs of a sector on a given time interval) is assigned a class describing a specific cause","With the result of the diagnosis, corrective actions are taken and the process is repeated if the sector is not fixed. 2.2 Knowledge based systems The process of diagnosis, as explained above, can be replicated with an FLC that applies fuzzy rules over a subset of PIs of the sector in order to classify the case","Since FLCs are KBS, they have a separation between the IE and the KB","The KB is divided in two parts: 
                      In this work, we will use DM only to obtain the RB",The DB is usually provided by the experts or by the specifications of the individual PIs,"Typically, there are two ways for acquiring the knowledge needed in the RB from the experts: 
                      These methods of KA rely heavily on the experts involvement","Unfortunately, in the industry of mobile communication network management, the time of the experts is a scarce resource","Therefore, the attempt of creating a RB by traditional KA is bound to failure",In Section   an algorithm for extracting the knowledge from a set of solved troubleshooting cases is proposed,"The output of the algorithm is a RB that can be used in an FLC to classify cases and therefore automate the diagnosis process. 2.3 Fuzzy Logic Controllers FLCs imitate the process of human thinking, focusing on two of its main aspects: 
                      A properly configured FLC can be easily interpretable ( ), that is, it is given in a simple and understandable way",Interpretability of a model is important since it can be more easily used and modified when needed,"Also, an interpretable FLC may offer information about the inner workings of the modelled system, helping to expand human understanding about it","FLCs have three parts; a   block, that translates from numerical   values to fuzzy values, an   block, that applies fuzzy rules on the fuzzified input variables to obtain a fuzzy output, and a   block that transforms the fuzzy output of the inference block into a crisp value or a specific action.  
                          depicts the typical contents of an FLC",Knowledge is used in the process of fuzzification in the form of fuzzy set definitions (the DB) and in the form of rules (the RB) in the IE,"The diagnosis process in troubleshooting of LTE networks is basically a process of applying heuristic rules on PIs, alarms, counters, configuration parameters, etc. to obtain a diagnosis","Therefore, the process will start with the fuzzification of the input variables to, typically, two fuzzy sets such as  /  or  / ","The membership functions of these sets will be trapezoidal ( 
                         ), since only two thresholds are needed","Experts use this two threshold classification for the KPI values very often, so the use of trapezoidal sets is the most natural choice for the task of diagnosis","Subsequently, the fuzzy rules will use the fuzzified values to assign a validity to each of the possible causes, depending on the  ","Finally, in the process of defuzzification the most likely cause will be selected",The validity assigned in the previous step indicates the confidence that the chosen diagnosis is correct,The fuzzy rules are composed of two parts: the   (the “  …” part) and the consequent (the “  …” part),The antecedent is made up of individual assertions about the fuzzy variables (such as “ ”),Each assertion has a   equal to the degree of membership of the value of the variable to the assigned fuzzy set,Assertions are joined by   ( ) or   ( ) operators,"In the scope of this study, only   ( ) operators are used, since they provide the basic functionality for pattern recognition on the PI observations, whereas the   operator can be omitted safely",The only case where the   operator would be needed is where two different combinations of PI values in the antecedent identify the same problem,"In that case, two separate antecedents can be created, thus sparing the use of   operators and simplifying the learning process","With the   operator, the degree of truth of the whole antecedent is either the minimum (in this work, this option is chosen) or the product of the degrees of truth of its assertions (in the case of   operators, it would be the maximum)",The consequent assigns the degree of truth of the antecedent to an assertion about the diagnosis belonging to a certain class (such as “ ”),A full example of the application of several rules on an input vector of KPIs is shown in  ,An input vector with four variables is introduced in the FLC,"In this example, each variable takes values between 0 and 1","The variables are then fuzzified, each of them in two sets, either   or  ",The membership functions defined on the variables are the same for simplicity in this example,"Once the vector has been fuzzified, the rules are applied","Note that for each rule, the degree of activation is given by the minimum value of the membership degrees in the antecedent","The second rule has the biggest degree of activation, therefore, its consequent will be the diagnosis","The degree of activation expresses the confidence of the diagnosis; therefore, the higher the degree of activation, the lower the uncertainty of the result","For simplicity, only the diagnosis with the highest score is used, although in cases where more than one root cause is present, several rules should have the same score","Even if only one diagnosis has the maximum score, it is still useful to provide a list of all the activated diagnoses ordered by descending score so the experts have a broader picture of the state of the sector","Another alternative to the scoring approach used in this study is weighted voting ( ), where each diagnosis is compared individually with each of the other diagnoses through a single rule with two possible outcomes","Each rule will then   for one of two possible diagnoses. 3 Data mining in LTE troubleshooting databases 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   3.1 Data mining DM is the process that extracts a model out of a data set by exploring the underlying patterns",Knowledge Discovery and Data Mining (KDD) ( ) is the larger process extending from the collection of the relevant data saved into a data base to the interpretation of the knowledge contained in it,The DM algorithm proposed in this work obtains fuzzy rules from a set of solved cases ( ),These rules will take a set of PIs as input and produce a diagnosis (they will   the cases),"Therefore, the DM algorithm will solve a   problem",The   model (the fuzzy rule set) is a  ,"Ideally, the learned rules will be similar to those used by the experts","Each entry in the training set (case) is a tuple formed by an  -dimensional   (in this case, the values of PIs) and a   that identifies the class to which the case belongs (in this study, this is the diagnosis)","Since the training set contains the class label, the DM algorithm is a   algorithm","To obtain the required vectors, a previous process of   ( ) must be carried out",The data extracted from the network will be a set of time-dependent arrays representing time series for each PI,"This time series covers the time interval where the problem occurred, but also the surrounding time intervals where the fault did not show up in the data","Therefore, two steps must be achieved: 
                      The issue of data reduction is out of the scope of this article, but it is a necessary prior step to the data driven learning process. 3.2 Data driven learning in troubleshooting databases In this study, a novel supervised data driven learning algorithm for wireless networks based on the WM ( ) method is described",It introduces some modifications in order to better adapt to the obtention of the rules used in troubleshooting of LTE networks,The WM method is a well known algorithm that can easily be parallelized since the creation of new rules from the data is independent from the creation of previous rules,"Therefore, the data can be divided among several processes arbitrarily without loss of information","Another advantage of the WM method is that it is deterministic, that is, the results of two equal training sets are always equal, independently from the order that the data is provided or how it is divided among parallel processes","Other learning algorithms have been proposed ( ), but the WM method was found the best for parallelization and for its deterministic behavior, which helps understanding the results of the learning process",The algorithm obtains the RB of an FLC from a training set composed of labeled cases,The learning cases are tuples   composed of a vector   as the attribute vector representing the values of PIs   and a label   as the class label among possible root causes  ,"The algorithm has three consecutive steps: 
                      In  
                          the flowchart of the algorithm is depicted. 3.3 Parameters The algorithm has three parameters: 
                      4 Evaluation 
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   4.1 Case study Although the principle of a fault database ( ) is simple, the troubleshooting process carried out by LTE experts currently does not include a problem collection step where solved cases are saved along with their solution","Therefore, newly developed diagnosis systems need to somehow generate these cases in order to validate their operation","In this work, the training cases are generated by a network emulator based on the knowledge of troubleshooting experts",The emulator provides the required cases (PI vectors and fault cause),"The advantage of this method is that it can provide as many cases as necessary, while being close enough to what a real case extracted from the network would be","To define the emulator, troubleshooting experts were asked to define the most common problem categories in LTE, and their related PI values",The frequency of occurrence of each type of problem was defined and the Probability Density Function (PDF) of each PI conditioned to the presence of each cause was modeled,"With the information of the probability of occurrence of each case and the PDF of each PI, the LTE emulator creates two sets of solved cases: the training cases and the testing cases that will be used to evaluate the accuracy of the algorithm","The fault categories defined by the experts, and their probability of occurrence (given that there is a problem), are the following: 
                      These fault categories are complemented by an additional   state that models the behavior of the network when there is no problem present","The probability of normal behavior for the study will be 70%, versus 30% of probability for the occurrence of a problem","The PIs that the emulator creates are the following: 
                      The thresholds for Accessibility, Retainability and HOSR are obtained from commercial requirements, whereas the RSRP and RSRQ are obtained from experts","The PDFs of each PI conditioned to the occurrence of a root cause are described in  
                         ","The PDFs of bounded PIs (Accessibility, Retainability and HOSR) are modeled as beta distributions, with two shape parameters,  ","Unbounded PIs (RSRP and RSRQ), on the other hand, are modeled by gaussian distributions, and they are parametrized by the average and the standard deviation ( ). 4.2 Experimental design The tests carried out on the algorithm evaluate the influence of its main parameters ( 
                         )",The tests determine the diagnosis accuracy of the FLC that uses the obtained RB,"For this purpose, three measurements are assessed: 
                      The meaning and importance of these errors vary depending on if the detection phase (determining if a specified case is normal or problematic) relies on the same FLC that will do the diagnosis","If the FLC also does the detection, the main objective should be to minimize the Undetected Rate","This is done usually at the cost of an increase in the False Positive Rate, because loosing the conditions for detecting a problem will cause an increase in the number of non-problematic cases that are wrongly detected",The increased number of scenarios that the FLC must detect will also cause more normal cases to match them,"If there is a previous detection step, independent from the diagnosis system, then the Undetected Rate represents the proportion of cases that cannot be diagnosed by the system, but still are detected and can be diagnosed manually","These measurements can be used to obtain other representative errors: 
                      The tests have been carried out with two sets of cases generated by the emulator described in Section  ",A training set of 2000 cases (containing 600 problems and 1400 normal cases) and a testing set of 5000 cases (1500 problems and 3500 normal) have been generated,"The algorithm proposed in this paper is first trained with the training cases, and afterwards, it is evaluated with the testing cases","By default, the normal cases are not used in the training stage, except for a specific test to find the impact of using them","These results are compared with the performance of a supervised learning process for creating a Bayesian Network, using the software package GeNIe ( )",The BN is trained and validated with the same set of emulated cases used for the data driven method,"Two training sets are used for the BN training, one including the normal cases and one excluding them","Finally, the algorithm will be tested with a reduced set of real cases to demonstrate its validity for real life scenarios","Since the number of available cases is small, a cross validation process will be used. 4.3 Results 
                         
                      This experiment tests the influence of the MDA parameter over the error rate",This parameter regulates the minimum degree of truth of an antecedent for a case to consider it covered,"This modifies the base (that is, the term of the score of the rule that depends on the number of covered cases) of the rules, and consequently their scores.  
                          depicts the results",It is observed that the three errors remain stagnant for values of MDA between 0.1 and 0.8,The minimum for the Undetected Rate is 0 for values of MDA of up to 0.5,"Between 0.6 and 1, the value of the Undetected Rate increases slightly, up to 0.2%","The False Positive Rate has a constant value of 34.5% except for MDA = 1, where it decreases to 32.7%",The Diagnosis Error Rate reaches a local maximum of 27.5% at MDA = 0 and a minimum of 4.9% at MDA = 0.4,"For MDA values above 0.8, the diagnostic error rate grows to a maximum of 18.5%","Since the Diagnosis Error Rate shows the classification errors, its increase shows that the rule that identifies a problem   is actually also covering certain instances of another problem  ","Since a high value of MDA is restrictive for cases that have not very clearly classified PIs, it means that some rules that should have appeared have not been created, and therefore, the cases covered by them are confused with a different cause","On the other hand, for low values of MDA, the restrictions of loosely covered cases are lower, so some rules that cause confusion (i.e. they cover cases that should not be covered) are created and included in the final rule set, therefore increasing the classification error",The results show that the algorithm is insensitive to values of MDA in a wide range of values,"For extreme values of MDA, the Diagnosis Error Rate increases","Therefore, the recommended values for this parameter are 0.1 ⩽ MDA ⩽ 0.8","Specifically, for MDA ⩽ 0.5, the Undetected Rate reaches the minimum value of 0, so the optimal range of MDA is 0.1 ⩽ MDA ⩽ 0.5. 
                          
                         
                      This experiment finds the influence of  ",This variable regulates the sensitivity of the algorithm to rare cases,A small value of   gives a low score to rules that cover uncommon cases,A higher value lets the score of a rule grow rapidly as its base increases,"The Diagnosis Error Rate, undiagnosed rate and False Positive Rate are depicted in  
                         ",The Diagnosis Error Rate and Undetected Rate decrease as   increases,"Both errors are maximum for  
                         = 0.01, whereas the False Positive Rate is minimum for this value",The False Positive Rate grows with  ,The increase in   means that the score increases rapidly as the base of a rule grows,This increases the diversity of rules that achieve the minimum score to be integrated in the final RB; resulting in a RB that covers more cases (lower Undetected Rate),"This comes at a cost, because the larger number of covered cases (especially rare cases) drives to an increase in false positives up to 34.5%","Therefore, there is a trade-off between the Undetected Rate (reliability) and the False Positive Rate, which has a great influence on the performance of the resulting FLC","To illustrate the relation between the False Positive Rate and the performance, by using Eq.   the complementary of the Positive Predictive Value can be obtained, reflecting the probability that a certain diagnosis is a false positive","With a proportion of normal and problematic cases of 70% and 30% respectively, and the results for the default values ( 
                         = 0.345,  
                         = 0) the probability results in  
                         = 0.446 (44.6%)",This remarks the importance of using a detection stage prior to the diagnosis,This stage should separate the abnormal cases from the normal cases,"Supposing an ideal detection stage, the False Positive Rate would have no meaning anymore",The Undetected Rate would reflect the proportion of cases that the diagnosis system cannot classify; but at least those cases are detected and can be diagnosed manually,"Summarizing, the optimal value of   would be  
                         ⩾ 0.06, since for these values, the Undetected Rate is 0, and the gain of using values of   between 0.02 and 0.05 in terms of false positives is small (in both cases, the False Positive Rate is too high and the system needs a previous detection stage). 
                         
                      The MIS parameter determines the minimum score that a rule must have to be fused with other rules to produce more general rules",This process was described in Section  ,"The results are shown in  
                         ",The Diagnosis Error Rate shows a stagnant behavior as MIS increases,"For MIS = 1, the Diagnosis Error Rate is 0, but as observed in the Undetected Rate, there are no diagnosis in that case","MIS determines the minimum score that a rule must have to be included in the final RB, so a high value of MIS reduces the diversity of the RB","Since no rule reaches a score of 1, there is no output RB for MIS = 1","As expected, since an increase in MIS means an increase in restrictions for including rules that cover rare cases (also for rules that have a low success rate), the Undetected Rate grows when MIS increases, and the False Positive Rate decreases","The best interval for this parameter is 0.1 ⩽ MIS ⩽ 0.4, because the Undetected Rate is minimum. 
                         
                      The tests with the default parameters have been repeated using the normal cases in the training set","The result with the three measurements is shown in  
                         ","When using the normal cases in the training, the Diagnosis Error Rate and the Undetected Rate grow slightly, whereas the False Positive Rate decreases significantly","To better visualize the meaning of this change, the calculation described in Eq.   is used, and results in  
                         = 0.347 (34.7%, against 44.6%)","Although there is a slight improvement in  , the gain is insignificant, since the system still needs a detection stage",This small gain comes at the cost of an increase in execution time due to an increase in the number of operations when including normal cases,The execution time increases by a factor of 4.95,"This increase may not have a great impact in the diagnosis system, since the training phase is done offline","Nevertheless, since a prior detection stage is needed anyway, the primary objective is to minimize the Diagnosis Error Rate and the Undetected Rate, and therefore it is recommended not to use normal cases in training. 4.4 Comparison with other technique Bayesian Networks have also been previously proposed for troubleshooting in mobile networks ( )","In this Section, the cases used in the data driven algorithm will be used for training a BN.  
                          depicts the BN equivalent to the fuzzy system trained by the proposed data driven algorithm","The BN is designed, trained and validated in the GeNIe software package ( )",Each PI is discretized with a single threshold,This threshold will be the middle point between the high and low values previously used in the fuzzy set membership functions described in Section  ,The discretized cases are then used for supervised learning,"The results are shown in  
                         ","Two different tests are done, one excluding normal cases from the training set and one including them","When normal cases are not used, the BN is not trained to recognize them","Therefore, since the BN always provides a diagnosis, it will obtain a high probability for one of the problems, producing a False Positive Rate of 1 and an Undetected Rate of 0","This effect vanishes when using normal cases in the training, so the False Positive Rate is reduced to 14.5%","On the other hand, the Undetected Rate increases from 0 to 17.4%",The Diagnosis Error Rate also increases slightly when using normal cases from 21.9% to 25.5%,"The first conclusion from these results is that, although using normal cases reduces the False Positive Rate of the BN at the cost of reducing the reliability and the accuracy, it is still too high","Therefore, a detection stage is still required","Secondly, it can be concluded that, under the same conditions, the data driven method produces a more accurate diagnosis system, with a Diagnosis Error Rate of 5.1% versus 21.9%, with a lower False Positive Rate (34.5% versus 100%)",The BN can achieve a lower False Positive Rate than the data driven algorithm with the cost of increasing its Diagnosis Error Rate and Undetected Rate,"In addition, the use of fuzzy logic has other side advantages over BN, such as producing understandable rules and simplifying the process of integration of learned and manually elicited rules. 4.5 Results in live LTE network In order to demonstrate the validity of the algorithm for real world applications, we will use it to learn rules on a reduced set of cases from a real LTE network","There are 72 available cases, belonging to four possible categories ( ,  ,   and  ), each one having 18 cases","Note that in a real network, the proportion of normal cases will be much higher","In this test, the proportions have been modified in order to better understand the behavior of the DM algorithm when using problematic cases","Each case has values for 5 PIs ( ,  ,  ,   and a   indicator)","One of these cases is depicted in  
                         ","The time series of the first four PIs is shown, with the interval where the CPU overload indicator is active marked in gray","In this case a traffic surge caused a high CPU usage, leading to dropped calls (low retainability) and rejected connections (low accessibility)",Handovers to other sectors were not affected,The RSSI also grew due to the large number of users,"Due to the reduced available number of cases, a cross validation technique has been applied","The set of cases is divided in two partitions, each containing 9 random instances of each problem",One partition is used for training and the other for validating the results,"Since the normal cases are not used in the training process, they are all included in the validation set","This process is repeated 100 times with different training and validation sets, and the errors are averaged","With the algorithm parameters adjusted to the default values indicated in   (MDA = 0.4,  
                         = 0.14, MIS = 0.1), the obtained Diagnosis Error Rate is 0, the Undetected Rate 14.3% and the False Positive Rate 0","The Diagnosis Error Rate shows that the system is very accurate when a diagnosis is produced, given that the case is not a normal case",The reason of the relatively high (compared to the emulated cases tests) Undetected Rate is the low number of training cases,"Since the algorithm does not have enough information, the aggregated rules are too restrictive, that is, they impose a value on a PI that may be irrelevant","For example,   cases are identified by the experts when the   and   are low, regardless of other values in the set of PIs chosen here","In one of the executions of the algorithm, the rule obtained for diagnosing these problems is “ ”","Therefore, a case that behaves as the experts expect, but has a low   (for instance due to a high traffic at the same time as the   problem), will not be classified as a   problem, and will contribute to the Undetected Rate",The only way that the algorithm can overcome this problem is if that case (or several similar cases) is present in the training set,"In this scenario, the only action that can be taken to improve the performance of the resulting FLC is to loosen the parameters so even one occurrence in the training set produces a rule with a score high enough to be part of the final RB","With this objective, the MIS setting can be changed to 0, so the score does not influence the validity of a rule for its inclusion in the RB","Repeating the same experiment with the new MIS settings, the Diagnosis Error Rate is still 0, the Undetected Rate is slightly reduced to 10% and the False Positive Rate remains 0. 5 Conclusions A novel DM algorithm for obtaining the RB of FLCs from a set of solved cases for mobile network troubleshooting has been presented","The proposed method is a supervised, data driven learning algorithm, that uses vectors of PI values, alarms and configuration parameters labeled with the problem present at that time, as diagnosed by experts",There are currently no learning algorithms devoted to extracting troubleshooting rules from real troubleshooting cases,"In the case of LTE mobile networks, the task of extracting troubleshooting rules is a Big Data problem, due to a high volume of data, a high speed of data generation and the high variability in the format and values of the variables","The presented algorithm is designed to be easily parallelizable, so it can perform the learning process over a large dataset in a limited time","The described learning algorithm must be embedded in a data processing pipe that includes a prior step of data reduction and formatting, since the inputs to the algorithm do not follow any standard data format that is used in PI collecting systems",Experiments that cover the three main configuration parameters of the algorithm and the presence or absence of normal cases in the training set have been performed,"The results show that the parameters should allow diversity in the RB; that is, they must not filter rare cases or cases that loosely follow a rule","If the parameters are too restrictive, the output RB is unreliable, in the sense that it will often be unable to diagnose (or even detect, if the FLC is assigned this task) a proportion of the problems","On the other hand, the increase in reliability comes at a cost; the fuzzy rule set will detect some normal cases as problems","This problem is solved if there is a detection stage, which is normally the case, that filters out normal cases prior to its analysis by the diagnosis stage","In addition, it has been demonstrated that in a wide range results are quite insensitive to the selection of the parameters, so the algorithm is fairly robust and does not require a very complex fine tuning","The algorithm has also been compared with another supervised learning algorithm based on BN, showing that the proposed method outperforms that based on BN","Finally, the algorithm has been tested in a live LTE network, showing promising results.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0921889018307474, 1 Introduction It is desirable that autonomous robots should work closely with humans in areas such as housework and nursing care,The robots that operate in environments related to daily human life should be flexible in terms of both hardware and software,"With respect to hardware, soft actuators are expected to be more suitable than conventional electromagnetic motors because they ensure soft contact with both humans and environments because of which robots with soft actuators have gained research interest  ","With respect to software, it is preferable that the designers of such robots should ensure that their controllers are easy to use in different and changing situations",This study presents a flexible learning control scheme that can be applied not only to electromagnetic motor systems but also to soft actuator systems such as artificial muscles,"To this end, we focus on the ability of a robot controller to adapt to the variations in robot structure that are beyond the conventional notions of link structure and serial link manipulators",The physical properties of both the internal components and the external environment of a robot may be altered,"Therefore, the physical properties of the components of a robot, such as sensors, actuators, links, attached end-effectors, and tools used by the robot, as instances of internal and external factors, can be unknown or unpredictable","In this study, we present a novel framework of motor-control learning that focuses on identifying the relations among sensor variables and the application of this novel framework to controller construction","The basic concept of controller construction was presented in  , however, the automatic controller generator is extended for adapting it to more general cases in which the target variables can be directly realized without feedback and in which more various controllers can be created",The remainder of the paper is organized as follows,The relations between the presented concept of controller construction and related work are discussed in Section  ,"After defining the problem in Section  , the proposed controller construction method is presented in Section  ","Further, the controller generation is evaluated using the simulation in Section  , which is followed by the discussion and conclusion presented in Section  . 2 Related work 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   2.1 Learning of kinematics and dynamics A simple and effective approach was promised to adapt to unknown factors, which utilized Jacobian estimation for controlling the robot manipulators  ","Because the information related to vision and manipulator joint angles is directly used for performing Jacobian estimation, it is not necessary to measure the parameters such as the link lengths of the manipulator and the camera focal length","Generally, in a robotic system there are multiple relations between the sensors that utilize different modalities","Thus, a learning scheme should include the automatic selection of pairs of sensors as well as the estimations of the relations between various sensors","In this study, we present an extension of Jacobian estimation that allows the proposed framework to perform pair selection and evaluation",A system can adapt to the changes in physical properties by learning the total dynamics of the robotic system,"Based on the information related to the input and the output of the system, a function approximation, such as a feed-forward neural network, can be used to learn the dynamics of the system  ",A drawback of this approach is that it is difficult to effectively reuse the obtained knowledge,"Hence, if the robot re-learn its dynamics from scratch each time even when a small part of the system changes, it is considered to be ineffective","If there are sufficient sensors for observing the intermediate states of the system, re-learning can be more effective. 2.2 Structure learning Various learning approaches have been proposed for the manipulator control problems that incorporate the “structure” of the system from different perspectives",Sturm et al. proposed an application of the Bayesian network   for learning the kinematic relations between various links  ,"In their approach, the kinematic structure of a robotic system, which may be initially unknown, is automatically obtained using random motions","However, this approach only focuses on kinematic relations; hence, it is not directly applicable to dynamic relations, including acceleration and force",Learning tool use   is a type of structure learning by which the robot extends its body and performs different functions depending on the available tools,"In a neural network with parametric bias (PB)  , a variety of tool effects can be represented using the PB values, enabling partial modification of the acquired manipulation model","Further, the model in the aforementioned study was improved to use deep learning in  , where a body model and a body-tool model were smoothly related using the body modulation parameters","If there is no discrimination between the tool and the robot at the algorithm level, a learning model can be flexibly applied to different situations without any intervention by the designer","Further, a motion planning method utilizing a variable state space   was proposed using an algorithm that does not explicitly discriminate between robot, objects, and tools","This method can be applied to various types of control problems, including collision avoidance, reaching to a target, and manipulation","However, this method assumes quasi-static motion of the robot and objects; hence, it is only applicable to kinematics-level motion learning","In this study, an extension of Jacobian-based learning is proposed for an extensive variety of control problems. whereby the structure of the system may be unknown or variable","In this study, “structure” refers to the relations among different sensor modalities","For example, we normally know that applied force and acceleration behave according to a certain relation based on Newton’s law  ","The proposed framework does not use this knowledge; instead, this framework finds the knowledge by itself",The discovery and verification of these relations by itself allow the robot to find and attempt suitable control methods online even when the system dynamics vary from the initial assumption,"This method is characterized by the fact that the physical meaning and functional discrimination do not appear at the algorithm level, making it applicable to the dynamic control problem. 
                         
                      2.3 Soft actuators as artificial muscles for musculoskeletal robots The series elastic actuator (SEA)   was developed with an objective to potentially provide flexible mechanisms to robots driven using conventional electromagnetic motors","In this approach, a spring is connected to a motor in series","Although SEAs are compliant with the robot, it is difficult to reduce the weight of the robot when the electromagnetic motors are used",The musculoskeletal mechanisms driven by the soft artificial muscles are another method for achieving mechanical flexibility (as well as for reducing weight),"McKibben artificial muscles  , which are well-known pneumatically driven soft actuators, are used as artificial muscles in musculoskeletal robots","Polymer actuators, which have considerably developed on recent years, are attractive novel options for artificial muscles  ","Although the actuation stress of polymer actuators is sufficient for practical applications, their actual force is usually small because of the difficulty that is associated with the fabrication of large actuators","Consequently, it is difficult to utilize polymer actuators in humanoid-sized robots. 
                         Recently, a new class of polymer actuators manufactured from fishing line or sewing thread was developed by Haines et al.  ","The actuator, known as a fishing-line/sewing-thread artificial muscle or twisted and coiled polymer fiber (TCPF) actuator, is observed to contract because of thermal stimulation; further, this actuator generates a force that is comparable with that generated by the skeletal muscle",TCPF is electrically driven by both Joule heating and thermal heating,"In addition, TCPF is advantageous because it can be easily fabricated by twisting the commercially available fishing line.   depicts a TCPF actuator that is used in a preliminary experiment in our laboratory","Studies have been conducted to use TCPF actuators in robots  , and the expectations for musculoskeletal robots driven by artificial muscle are increasing","Therefore, in this study, a TCPF actuator is used in a musculoskeletal robot simulation. 3 Problem definition 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                   3.1 Robotic system and task 
                          illustrates a musculoskeletal robotic arm with two joints and four artificial muscles, where the base link (link 0) is fixed to the floor","Because each muscle generates only attractive force, two muscles are attached to each joint","Each joint has a limitation described by  ,  ","Further, it is assumed that the following sensors permit their corresponding physical entities to be observed. 
                         
                      Two tasks are presented as depicted in  (a) and (b) where the end-effector should reach the target position in (a) and exert a target contact force against a fixed object in (b).  
                          Note that only one variable (either   or  ) is controlled at a task","We develop the algorithm for controller generation based on the following assumptions: 
                         
                      Based on the first assumption, the attribution of each sensor variable is discarded, which is denoted by   as indicated in  ","The rightmost row in this table provides the physical attribution of each variable, which is unknown to the algorithm","The second assumption is based on the usual technique that is used for obtaining the derivative of a variable   as a time-difference  , indicating that it is plausible to assume that the algorithm can utilize derivative relations","Therefore, we have a total of   variables including the control input  ","Additionally, it is assumed that state vector  , which contains information about state discrimination, is available for function approximation","The robot has to achieve a specified value (trajectory) of the target variable, denoted by  ",The target variable is set as   in a position control task and   in a force control task,"The reference value (trajectory) is denoted by  , where   denotes the time step","Related to the requirement for the robot and the reference trajectory specification, it is assumed that the robot collects samples and performs learning control within a certain common region in the state space","That is, discovery of dependencies and utilization of the dependency network are both based on the same distribution of samples","It is also assumed that the robot moves along with a periodical trajectory to ensure that the latest samples collected during trials cover a constant region in the state space. 
                         
                      3.2 Structure of the musculoskeletal robot arm TCPF is used for modeling the muscle dynamics","Yip et al. analyzed the force generation and modeled the dynamics of the TCPF   as  where   [N/ ],   [N/m], and   [N/m s] represent the coefficients of heat, elasticity, and damping, respectively","To simulate the fact that each muscle can only generate an attractive force, the muscle force can be updated as  
                          can be rewritten using vectors as follows:  where the coefficient matrices are defined as  ,   and  ","The temperature of the muscle   is restricted as the control input to avoid overheating, which causes muscle to be fractured","The admissible interval of the temperature is set as  , and it is assumed that   cannot exceed the interval","Further, the restriction of the temperature difference in a control cycle is given by  , which provides the maximum and minimum time difference of   as  . 3.3 Dynamics of the robot arm The motion equation of the robot arm is given by  where  ,   and   denote the inertia, Coriolis including viscosity, and gravity, respectively.   is determined using the following relation:  where   denotes the Jacobian matrix of the muscle lengths, which is defined by the first-order approximation of the muscle length using the joint angle vector  Thus, the control input   generates muscle force   by  , which allows the calculation of Eq.   via  . 4 Structure estimation and controller generation 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                   4.1 Overview An overview of the proposed controller generator is depicted in  ","First, the robot collects all the observable sensor variables while applying random control inputs as indicated in  ","After the collection, the relations among the sensor variables are estimated using mutual information","Further, the estimated relations are used to generate chains, which denote the sets of variables that are related to one another",These chains are used to generate controllers,"The generated controllers are verified using actual motions, and the results of the verification are reflected to the structure of chains, which lead to re-evaluation and regeneration of the candidate controllers","In all the trials, the latest   samples are stored for performing nonlinear approximation of the relations between pairs of sensor variables. 
                          
                         
                      4.2 Structure estimation based on dependency network The relations among sensor variables are estimated using mutual information","There are several definitions of mutual information  ; in this study, least squares mutual information (LSMI)  , which is based on the squared-loss mutual information, is used","Let   and   denote sensor variables   and  , respectively","The LSMI between   and   can be defined as follows:  
                          has a larger value when the two variables are dependent on each other",The LSMI is estimated by the linear function approximation of   using parameter vector   as follows:  where   denotes the vector of the basis functions; typically the Gaussian basis functions as used in this study,"Note that LSMI is a symmetric measure, however, a causal relation should be ultimately identified between the variables","Taking this partially into consideration, the estimated LSMI values are translated into costs between sensor variables as follows:  The first and second cases reflect the knowledge of the relations of the time derivative because it is obvious that   can be controlled if we can control   whereas we cannot control   by controlling  ","Further, the inverse of LSMI is applied to cost definition to indicate that high dependency indicates high expectation of controllability, which is represented by a small cost to generate the chain of the controller. 4.3 Controller generation Our expectation for the proposed automatic controller generation is that we can control a sensor variable by controlling another variable if they have high dependency, which is represented by a small cost   between them",The controller is generated by a chain of sensor variables beginning with the target sensor variable   and ending with the control input  ,A chain is represented by a vector of sensor variable index that can be given as follows:  where   denotes the total number of sensor variables in the chain and   denotes the indices of the sensor variable,Let   denote the set of possible chains,"The most promising chain is explored by the following minimum value search:  Dijkstra’s algorithm   is used for performing the exploration, where the dependency network is represented by a graph with cost   defined by  ",The controller generation algorithm is based on PD control to provide (1) feedback based on the difference between the desired value and the current value and (2) a damping term using the derivative term,"The pseudo-code of the controller generation algorithm is presented in Algorithm 1.  
                      The relation between   and   , where   and  , is approximated by the following locally linear regression matrix:  
                         where   is the homogeneous coordinate vector produced by  ","Note that the regression is locally linear with respect to  , which indicates that the coefficient irrelevant to   is also approximated","This term can be used to perform a steady error compensation, which allows control without integration term such as PID control. 
                         
                         
                      Example","As an example, suppose we have a chain   with knowledge of information about the derivatives between   and   and with others being unknown","Initially, the control input is set as  ","Further, by approximating the unknown relation between   and  , the control input can be updated as  Based on the knowledge of the time derivative between   and  ,   can be updated as  Next, the unknown relation between   and   yields  which is finally used as a controller corresponding to the chain  . 4.4 Estimation of the local regression matrix The Jacobian matrix   is approximated by the locally weighted regression (LWR)  , which is known as the least-squares approximation with weights based on the Euclidean norm of the difference between the query input and each sample","The locally linear approximation of   by   can be represented by  where   can be obtained by rearranging vector  , which can be calculated as follows: Let   denote the number of samples (pairs of   and  ) and  ,   and   denote the data matrices as follows:   where   denotes the  th sample of the sensor variable  ","The regression vector   is calculated from   where the weight matrix   is given as  with the weighting coefficient of the sample   obtained as follows:  where   and   denote the  th sample of the state vector and the scaling parameter for the weighting function, respectively",The scaling parameter   is determined by cross-validation with the minimum squared-error regression error,"For ensuring simplicity of calculation, the maximum number of samples is set as  ","After the number of samples exceeds   during both sample collection and online control, the oldest sample is replaced by the latest sample during each collection cycle","Note that the local linear approximation of   includes a constant term. 4.5 Evaluation and re-exploration of chain Once a controller is generated by a chain, it is evaluated on the basis of the results of actual control in which   is made to follow a desired trajectory  ","The error function as a performance index can be defined as  If the error   exceeds the threshold  , the chain   that generates the current controller is discarded","To re-explore the chain, the cost of the current chain   can be updated as  where   denotes the update ratio of the cost","Because of the update, each cost in the current chain that fails to generate an appropriate controller is increased","After all the updates are completed using  , Dijkstra’s algorithm is used again to find a new chain","If a chain that is previously tested and that is observed to produce an unsuccessful control result is observed, it is discarded while re-updating all the costs using  ","The pseudo-code of the summary of the controller exploration is presented in Algorithm 2.  
                      5 Simulation 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   5.1 Condition Four simulations were conducted to verify the proposed controller generator using physical simulation","In simulations 1 and 2, the fundamental ability of the controller generation is verified, whereas the adaptation to the variation of the system is verified in simulations 3 and 4",All the simulations have followed the procedure that is indicated in  ,The common parameter settings for the simulations are presented in  ,"Further, it is assumed that the arm moves in a horizontal plane and that the gravity does not affect its motion","The parameters for controller generation are set as  ,  ,   and  ","Larger value of   and   will increase the length of the computation, and the precision of the approximation of   and estimation of LSMI will be improved","A small value of   will cause strict evaluation and rejection of controllers, which will result in the generation of less controllers","This may be avoided by taking a sufficiently large  , which allows control with high precision","If   is large, there is less probability that any pair of sensor variables in the tested chain is selected as the controller chain in the subsequent step","However, small   will cause the of pairs of variables selected again","Further, the value of   affects the order of chain exploration, however, it will not completely diminish the possibility of the discovery of a useful chain (controller). 
                         
                         
                      Comparison with simulation and real system. 
                            Because this simulation intend to verify the basic properties of the proposed learning control scheme, the aforementioned parameters are selected to be simpler (larger or smaller) than those of the real system","For example,   that is used in the simulation is considerably larger than that of a real TCPF manufactured from the thin conductive yarn (Denier: 240/34f) that has been evaluated in  ","However, the actuator coefficient  , or equivalently, actuation force, could be increased using parallel configuration or using thick fiber","For example, twelve parallel conductive yarns generate 3 [kgf], or 800 [ m] nylon mono-filament over 7 [kgf]  ","The stiffness constant is considerably smaller than a real TCPF actuator, however, by adjusting the scales of time and length, we may regard the system having parameters presented in   as a dimensionless system, which preserves a substantially similar behavior to that of the real system. 5.2 Simulation 1: Position controller generation In the position control task, the position of the end-effector of the robot arm was the target, that is, variable   was set as the target variable  ","The control input was set as  , where   was omitted",The reference trajectory for the end-effector   was a circle with a radius of 0.2 m and the center at  ,"The dependency network was initially generated on the basis of LSMI calculation, as depicted in  ",The costs   between the sensor variables are presented in  ,"From the definition of LSMI,   is observed to be non-negative for all the variables","However, in actual calculation, there are cases for which approximation results in the negative value of  ","In such cases, the LSMI values are replaced by  ; hence,   is infinity","It can be observed that the pairs of physically related variables,   ( ), ( ), ( ), ( ), exhibit small costs. 
                          
                         Using the procedure presented in Algorithm 2, the chain   achieved appropriate control satisfying the condition  ","However, for performing evaluation, we discarded the obtained controller to further explore other possible controllers until 20 chains (and their corresponding controllers) were verified","The obtained chains   are depicted below, with those that achieved successful control marked with “ ”. 
                         
                      In total, five chains, –  ,  ,  ,  , and   – satisfied the tolerance condition  ",The trajectory generated by the controller with chain   is depicted in  ,It was confirmed that other controllers that satisfy this condition realized similar trajectories,The tracking performance was improved after the initial several seconds,"This is because the samples were more appropriately collected around the desired trajectory during the trial motion. 
                          
                         The controller obtained from chain   is expressed by  By interpreting the physical properties of the matrices and by ignoring the constant term of the homogeneous coordinate, this controller is equivalent to  where   denotes the pseudo-inverse matrix of   and   denotes the Jacobian matrix between   and  , denoted by  .   is identical to the position control of the musculoskeletal arm presented in  ","By omitting   for simplicity, the controller obtained from chain   can be expressed as  which can be interpreted as being equivalent to  where   denotes the kinematic relation between   and  , given by  ","The controller corresponding to   is expressed by  which can be interpreted as being equivalent to  where   denotes the matrix corresponding to inertia, which relates the torque to the acceleration of the muscle length displacement. 
                          
                         The trajectory obtained by the controller corresponding to chain   is depicted in  ",One possible reason for the imprecise tracking is the presence of an approximation error of LWR with respect to the artificial muscle,"Its temperature cannot be lower than the room temperature, and it can generate only contract force, which causes a strong nonlinearity in Jacobians   and  ","For example, the precision of the approximation of   can be improved if a sufficient number of samples are collected around the configuration corresponding to  ","Although the arm follows the desired trajectory, the samples are collected and updated at a specific angular velocity","This may have produced a biased sample distribution due to the online updating of the samples. 5.3 Simulation 2: Force controller generation In the force control task, the force sensor information was added to the system as  ","The end-effector was constrained to move along the red line, as indicated in  , whereas the constraint force was exerted upon it from the environment","There was no friction along the constraint line, which indicates that  ","Further, the target variable was set as  , whereas the control input was the same as  ","The target trajectory of the contact force was set as  . 
                          
                          
                         The dependency network based on the LSMI calculation and the table of costs are depicted in   and  , respectively","The following chains have been discovered using the proposed algorithm: 
                         
                      
                          
                         From the table, it can be observed that the sensor variables that are not directly relevant to   and  , such as   and  , have low dependencies on one another","Among the generated controllers,  ,  ,  ,  , and   are observed to successfully follow the target trajectory","The trajectories of the contact force (along the  -axis) of controllers   and   are depicted in  , respectively","By omitting the constant terms of the homogeneous coordinates as in the previous simulation, the controller generated from  , can be expressed by  which can be physically interpreted as  This equation is not in the form of an error-feedback controller; however, it transforms the reference contact force into the muscle temperature",The generated controller succeeded in following the trajectory because the model of the artificial muscle in this study does not include the delay in the heating process of the muscle,"However, in the actual artificial muscle, it takes some time to heat the muscle, which would require an error-feedback mechanism","The controller generated from   is expressed as  where the approximated relation   corresponds to the relation between   and  , expressed as  In the above equation,   denotes an inertia matrix relating the torque and the acceleration of the muscle length displacement.   corresponds to the relation between   and   and can be expressed as follows by differentiating  :  
                          relates the angular acceleration   and the temperature   and is physically interpreted through  , Jacobian   and the inertia matrix   as  When compared with the controller generated by  ,   contains more relations between sensor variables",The tracking performance in   is worse than that in   partially because more errors of approximation may have accumulated,"Additionally, the approximation may be insufficient because of the constraint motion, which is especially relevant to configuration variables such as   and  . 5.4 Simulation 3: Adaptation to the changed parameters In this simulation, the adaptability of the controller was evaluated by changing the physical parameters online",The controller that is obtained from simulation 1 with chain   is used,"By assuming a situation in which the robot grasps a tool and attempts to control the end of the tool, the length of the second link (denoted by  ), the mass of the second link, and the center of mass of the second link are altered to   and  , respectively (see  )","In the proposed method, the samples that are used for the approximation of   are always replaced by the latest samples, which are supposed to be able to adapt to the parameter changes","For performing comparison, an adaptation strategy equivalent to “learning from scratch” was implemented, which discarded all the samples once while preserving the structure of the controller (chain information). 
                         The trajectories generated using the two aforementioned controllers are depicted in  , where the parameters were changed at 20 s",It can be observed that the proposed controller adapted to the change in physical parameters more quickly than the controller that learns   from scratch,"Both the controllers are expressed by  , and comprise of multiple approximated relations  ","Among these relations, the physical parameter change affects   and  , denoting the link length and link mass, respectively",The change does not affect   and  ,"Hence, the adaptation was faster than re-learning all the relations because the proposed controller was continuously able to use them",This comparison indicates that partial modification is faster and more effective than an “end-to-end” learning scheme to properly adapt to the changes of the system,"The difference will be significant when the entire system is large and complicated. 
                          
                         
                      5.5 Simulation 4: Adaptation to sensor malfunction In this simulation, the malfunction of a sensor is simulated to demonstrate the utilization of multiple controller candidates","Two controllers generated in simulation 1, corresponding to chains   and  , are depicted in  , respectively",These controllers are evaluated in a scenario in which the torque sensor malfunctions,"The main difference between these controllers is that the controller obtained from   does not include information on  , however, the controller corresponding to   includes the information","The torque sensor information drops to zero at 20 s. 
                          
                          
                          
                         The   and   coordinates of the trajectories with different controllers are depicted in  , respectively","In case of control with chain  , the controller was unable to control the end-effector position after the sensor malfunctioned","However, the controller corresponding to chain   was not affected by the malfunction because it does not use   and relies on the relation of  , which is equivalent to that of  ",This result indicates that the proposed framework can adapt to sensor malfunctions by determining the appropriate information flow from multiple controller candidates. when the online reconstruction of the chain is implemented using Algorithms 1 and 2,"Further, the online reconstruction of the chains will require implementation of a mechanism that is able to detect changes of the LSMI online. 5.6 Discussion One of the advantages of the proposed framework is that some of the intermediate representations in the constructed controllers can be interpreted unlike end-to-end learning controllers, as has been discussed in the previous sections","Even in a case in which some relations are not clearly described by physical models, the performance of the controllers can be verified by dividing the entire process into multiple sub-modules",This structure allows us to (1) understand and improve the performance of the controller and (2) make the controller robust and adaptive to the partial changes in system dynamics,"Further, the changes in the robotic systems are observed to often occur partially; however, it is difficult to predict the portion of the system that will change",The presented framework denotes that the redundant sensors help considerably in making the entire control system adaptive to partial changes as presented in Section  ,"Further, one drawback of the proposed control method is that it is difficult to consider the stability and optimality issues","Dynamics approximation and controller generation using linear function approximator were presented in  , where the asymptotic convergence of tracking error was discussed","In the adaptive controller, the parameters of the linear approximator are updated using the tracking error","However, in the proposed method, the dynamics of the system are separately approximated; hence, it is difficult to analyze the stability in a similar way to  ","Further, it is known that humans can learn smooth and energy-efficient motion through trials, which is explained in the optimal control framework ( ,  )","It is also preferred that the proposed control framework can consider the optimality of some criterion, such as time cost or energy consumption",One possible extension to overcome these problems is to utilize the proposed controller generation as a transient learning-controller to effectively collect samples against a sudden change in the robot system,"Further, it will be possible to build an optimal controller by considering the time or energy efficiency ( , by reinforcement learning) by collecting samples with the proposed controller","Based on the observation of motor-learning of humans, it can be understood that we do not adapt to new situations by learning from scratch","We can initially realize a target motion in an awkward way, and it will get gradually efficient and smooth through iteration",The proposed controller generator can be used as an initial-response learning controller,"In this study, the gains of the controllers were not adjusted","To achieve better convergence, the gain of each relation should be adjusted,  , by following PD automatic gain-tuning method  ","First, a proportional gain   is multiplied with the whole term of the generated controller; further, a derivative time   is considered as   instead of the fourth line in Algorithm 1","By fixing   and gradually increasing  , find   which causes continuous oscillation against a fixed point control",The proportional gain can be tuned as  ,"By observing the period of the oscillation as  , the derivative time can be fixed as  ",The calculation time of the LWR was 0.12 s for one control cycle of   s on an average in a Matlab environment using a Core i5 2.33 GHz CPU,"For real-time use, reducing the computational cost of LWR (  LWR extension discussed in   using a  -d tree) is necessary. 6 Conclusion In this study, a structure-estimation-based learning controller was proposed to improve the ability of a robotic system to adapt to the changes in its body and environment","Further, the relations between the sensor variables are estimated by a nonlinear function approximation based on the estimation of the physically related pairs of sensor variables",The proposed controller generator does not require any specific knowledge of the control problem except for the derivative relations between certain variables,The presented controller generator was verified by the simulations of a musculoskeletal robotic arm system driven by an artificial muscle and was tasked with position control and force control,It was confirmed that the proposed automatic generation scheme generated appropriate controllers that were equivalent to PD-based controllers based on the physical knowledge of the system,It was also proved that the obtained controllers can effectively adapt to the partial changes in the system,An expectation from the extension of the proposed method is to apply the same framework to problems that contain more unknown factors instead of concealing the (actually known) physical properties,"Redundant sensing capabilities were assumed in the robot system, which allowed the learning controller to be robust with respect to the partial malfunction of the system","Further, the robustness should be investigated in various situations while extending the method to complex cases,  , distributed contact sensing",Physical interpretations of the obtained controllers were discussed by comparing them with the existing PD controllers,The analysis and evaluation of stability will be discussed in a future work.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0957417417307698," 1 Introduction Nowadays, users are the main source of alternative sensor information in a city, although this huge source of information is often overlooked","Being ubiquitously connected to the Internet with their mobile phones, they intensively use services which promote user generated content such as Online Social Networks (OSNs), one of the most massively alternatives employed","Content in OSNs is a combination of text/images (e.g. a user post, a reply to other users posts, etc.) and meta-data information (number of likes, stars of user posts, number of posts made by the user, GPS-location, etc.)","When using a GPS-enabled device, users also add a very valuable information: from where the post is shared","Thus, by analyzing the geo-located posts it is possible to know what is happening and where it is happening ( )",This is especially relevant in OSNs adapted for fast consumption (e.g. microblogging or image messaging) in which the time lapse between an event and its appearance in the platform is very low,Our previous work ( ) introduces an approach to take advantage of the information given by the geo-located posts shared in social media,"Abnormal location patterns were detected in the urban area under study, such as unusual city states or dynamics",The input data of the model was restricted to the posts’ geolocation,This information was employed in order to find out when the shared posts in a specific area at that time of the day and that day of the week can be considered usual or an outlier (too much or too many),"For this to be possible, a density-based clustering technique was applied","After a training stage which obtained the usual pulse of the city, the technique allowed the detection of abnormal behaviors on-the-fly",The work introduced in this paper supplements our previous findings,Here we set up a two-folded approach,"Once the abnormal location pattern is detected, it identifies what is going on, where and when","Taking the set of posts which lead to a geo-anomaly as an activity seed, our new proposal enlarges the focus to all those posts which are considered linked to the seed","Opposite to pure NLP (Natural Language Processing) for geo-dependent topic modeling ( ), we apply Content Aggregation Models as the one in   to identify meaningful threads of content that reflect what is happening in the area under study in a timely fashion",We do this in order to react to potential threats as soon as possible,The paper is organized as follows.   summarizes other research proposal that are relevant for our work.   overviews the proposed methodology of the events detection system,"In   we describe the dataset and the reasons behind our selection of Instagram as data source.   details the main aspects that have focused the evaluation of our proposal, whereas in   we enumerate the obtained results after the different experiments performed","The results are discussed in   and, finally, in   we outline the conclusions and future work. 2 Related work Analyses of data gathered from social media (text and location linked to geo-tagged posts) have been recently applied for different and interesting purposes related to mobility patterns","In   for instance, a worldwide analysis of travelers is performed by using geo-located tweets","The approach was validated by comparing the results to global tourism information, showing a strong correlation","This travelers flow enables the detection of different communities in different countries, reflecting a regional division of the world","Another interesting approach is introduced in  , where sentiment analysis techniques are applied to about 180,000 geo-located tweets to infer the relationship between happiness and movements within a city","In this paper, we focus our attention to another interesting field: the detection of crowds and events in urban areas","With this aim, information gathered from shared posts supports the application of different analysis techniques applied to both the text and the location of the geo-tagged posts. 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   2.1 Crowds and events detection With an approach which constructs clusters of tweets according to their number in a given area (density), the detection of local events is the main aim in  ","Afterwards, these clusters are scored according to different criteria: textual content, number of users, number of tweets, etc","Quite similarly, the authors in   developed a customized density algorithm to obtain socially interesting locations in a city using geo-located tweets","In the first stage, they obtain the prevalence of locations for each user","After that, interesting locations, from the point of view of group behavior, are discovered combining the per-user results",In   posts from both Twitter and Instagram are clustered according to their hashtags,"After that, the density-based clustering algorithm DBSCAN is applied to these clusters in order to associate a single place to each cluster","A different clustering approach is presented in  , where k-means is used to group the geolocated tweets and define Regions of Interest (RoI)","Over these regions, the number of tweets is analyzed in order to detect outliers",The objective is to develop a geo-social event detection to monitor crowd behaviors and local events,The approach introduced in   tries to infer spatio-temporal information about the events mentioned in the shared tweets,"Authors applied text mining techniques (a density-based online clustering method), with the aim of detecting events in urban areas","In this approach, the location of an event is extracted directly from the text content when the geo-tagged information linked to the tweets is not available","In  , most visited locations are detected applying the EM-Algorithm to the location of tweets in intervals of two hours",These popular places are associated to a ZIP code,Those ZIP codes are processed using Latent Dirichlet Allocation (LDA) to find patterns in the movements of the crowds and track events with a strong relation with the city,"LDA is also applied in  , in this case to the text content of the tweets, in order to find popular topics","Then an abnormality estimation is calculated using Seasonal Trend Decomposition based on Loess smoothing (STL), in an iterative process which requires expert human supervision",Other LDA-based approach is detailed in   to relate topics and regions,"Once the topics are obtained, a clustering technique is used to aggregate regions with similar topic distributions","Topic distribution is also the base of the approach in  , where local events are detected from analyzing microblogging data","Geohash application 
                          is used for clustering location data and authors",They apply keyword frequency as the discriminatory factor for aggregating content,Keywords are associated with regions when both appear jointly more than three times in the dataset to identify local events in a region,"Although the simplicity of this approach is suitable for online analysis, the event extraction schema is too naive to provide the filtering capabilities needed for anomaly incident detection. 
                         
                         
                         
                         
                      2.1.1 Clustering and outlier techniques There are multiple clustering methods, which are generally classified in four groups: partitioning approaches (where the number of clusters is pre-assigned), grid-based (where the object space is divided into a pre-assigned number of cells), hierarchical (where the data is organized in multiple levels) or density-based (where density notion is considered)","For our purpose, density-based algorithms are the most suitable since they are able to (i) discover clusters of arbitrary shapes, (ii) handle sparse regions (which are considered as noisy regions) and (iii) work without knowing the number of clusters in advance","Among the different proposals in the literature ( ), we selected DBSCAN ( ) (Density-Based Spatial Clustering of Applications with Noise)",Two parameters are necessary in DBSCAN to define the density measure to obtain the clusters: the radius of a circle around the data point (ϵ) and the minimum number of points that should be in this circle in order to be considered a cluster ( ),"The algorithm is very sensitive to both parameters, so it is essential to select their values properly","Our estimation algorithm, detailed in  , is adaptive (since it is based on the nature of the dataset) and has less time complexity than other approaches in the literature","After being able to detect groups of geographically close citizens with activity in social media (crowds) by using DBSCAN, the second step is defining the conditions under which these crowds are considered outliers",According to Hawkins ” ” ( ),"As described in  , we treat a cluster as an outlier whenever the number of points differs from the number of points of other clusters found in a similar location, day and hour","We also differentiate between mild and extreme outliers ( ): the former lies outside the interval   whereas the latter lies outside the interval   being   the Interquartile Range. 2.2 Content aggregation models Content aggregation usually involves one-to-one similarity comparisons of records (with  ( 
                         ) complexity)","In applications that operate in the order of millions, a greedy comparison strategy is unmanageable","Hence, clustering techniques, such as the aforementioned LDA ( ), reduce the number of required comparisons","However, LDA presents two characteristics that determine its scope","Not only the number of clusters has to be previously specified but also topics should be extracted over historic data, a time consuming task",Other alternatives rely on Bayesian non-parametric models such as DPMM ( ) to obtain topics without supervision ( ),One important advantage of DPMM over LDA is that clusters must not be predefined,This is of paramount importance in social media due to the dynamism of the content in these services,Locality Sensitive Hashing ( ) ( ) is an online clustering technique with application in  ,"Although it is a general purpose clustering technique, it has interesting applications in text analysis, which can take advantage of its speed and dimensionality reduction",LSH uses random projected vectors (hypervectors) to split the space in buckets ( ),These buckets are organized in such a way that vectors closer in the original   have more chances of being in the same bucket that those which are further apart,"Therefore, those buckets work as a filtering technique to drastically reduce the number of comparisons performed among records ( )","The complexity of the model is thus characterized by the number of hypervectors, the original dimensionality of the space, and the records that lie in each bucket",There is a tradeoff between precision and complexity,The number of buckets increases with the number of hypervectors which as a consequence will lower the number of records stored in a bucket,"Nevertheless, the system will be less precise since the chance of two points being separated by a hypervector increases as well",LSH has been applied in   to find the points closest in space and to build threads of stories of an OSN (Twitter),"They use the TF-IDF ( ) algorithm to obtain the set of characteristics of the input vector, but they strip the system of hashtags","As they apply word-level characteristics, the system must fix beforehand the words that might be used as vector components in the input","For this reason, they remove hashtags and user mentions despite being extremely relevant information for content aggregation in social media. 3 Methodology: an overview Our approach combines the analysis of two kinds of data obtained from the same source (LBSNs): text and GPS location of the shared posts ( 
                      
                      
                      )","Whereas location information is forwarded to the Crowd Dynamics Analyzer module, text is the input of the Threads Discovery module","Both modules work independently to detect unexpected citizens’ activity in social media, assuming that this behavior reflects that something unusual is happening in the area","The Crowd Dynamics Analyzer ( ) focuses on the GPS-data of the posted messages to check if the locations are the usual ones for the combination of day of the week, time and area",The Threads Discovery Module ( ) deals with the content (text) of the posted messages to infer sequences of posts belonging to the same theme,"Finally, both modules feed the Threads Ranking ( ), whose main aim is to check if the detected threads effectively correspond to an unusual crowd behavior. 
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   3.1 Crowds dynamics analyzer As aforementioned, this module is in charge of detecting unexpected behaviors in the urban area under study by analyzing the citizens’ activity in social media","Our approach firstly infers the activity pattern of the city, i.e. the usual behavior (number of shared posts)","This behavior varies according to three different parameters: time of the day, day of the week and geographic area within the city","Thus, for instance, the usual activity on Saturday night at the Meatpacking District is totally different to the usual activity at the Financial District","Exactly the same happens if we select another day and time (Monday morning, e.g.)","This is, in fact, the main purpose of the first stage of our methodology ( ), the Training Phase, and one of the aspects that differentiate our approach from others in the literature ( ), where no patterns are provided","In order to obtain these patterns, we combine data from several days that can be considered similar (same day of the week in our approach) and then we analyze the social media activity in slots of 30 min","Thus, we have 7 reference days (from Monday to Sunday) and 48 time slots per reference day",The procedure can be described as follows: (i) we firstly apply the DBSCAN algorithm to the data (geo-located info linked to each post) obtained for each temporal slot and for each day,"Thus we are able to identify clusters or crowds (i.e. closer locations) and discard noise; (ii) then, we apply the DBSCAN algorithm to the data of all days together with the aim of identifying reference clusters or crowds in the city and (iii) we finally specify two thresholds to detect moderate outliers and extreme outliers","Therefore, after this first Training Phase, we finally provide different patterns (one per combination of day of the week and time slot) outlining where the crowds are located (detected clusters) and specific measures to detect outliers",More details can be found in  ,"Once the patterns of the city are available, it is time to run the Detection Phase",This is performed on-the-fly: i) gathering the data from LBSN; (ii) clustering the data using the same parameters obtained in the Training stage; and (iii) analyzing if there are outliers or not,"In this phase, it is essential to specify a measure of distance to match individual clusters obtained in the Detection Phase,  , with the reference clusters defined in the pattern of the city,  ","We have defined this distance,  , as follows:
 were   is the number of points in the cluster   and   is the distance between the point   (point   in cluster  ) and the nearest point   in the Reference Cluster  :
 We apply the Haversine distance to take into account the fact that the points are on the surface of the Earth","Finally, we consider the cluster   to fit Reference Cluster   if it holds that:
 
                      Having this mathematical framework, we decide if the activity can be considered unusual or not by comparing the individual clustering made on-the-fly with the reference clustering that corresponds with the same day of the week and temporal slot (pattern)","Briefly, (i) if no cluster fits a reference cluster that is considered a lower outlier; (ii) if two or more clusters fit the same reference cluster they will be considered as an only cluster; and (iii) all clusters that do not fit any reference cluster are considered as upper outliers","Therefore, our approach provides (i) a methodology to calculate the DBSCAN parameters under two different circumstances (individual clustering and reference clustering), (ii) a sequence of two clustering procedures in order to obtain the patterns of activity; (iii) a measure to match or compare individual clusters and reference clusters, (iv) a procedure to specify thresholds to detect moderate and extreme outliers, and (v) the appropriate criteria to detect such outliers","With this approach, it is also possible to detect unusual low activity levels in the city, being another differentiating factor with previous approaches ( ). 3.2 Threads discovery module As previously mentioned in  , LDA has important shortcomings that make it unsuitable for our scenario","On the one hand, it is not possible to pre-determine the number of events in the urban area under study, so we cannot specify the number of LDA clusters in advance","On the other hand, LDA-based topics extraction works over historic data, which has two problems for online events discovery: (i) it is a time consuming task and, even more important, and (ii) new topics only could be discovered after the algorithm is applied over new historic records","Therefore, LSH is the most convenient technique for our problem, being the work introduced in   quite close to our approach","However, in our proposal, we demonstrate that character-level features have a great performance for thread aggregation with LSH","Additionally, we avoid the limitations related to the appearance of new words and word misspellings, which would remove a word from the equation regardless of its usefulness","Thus, the Threads Discovery module is a LSH-based stage where the number of buckets is limited by the number of hypervectors   as follows:
 where  -hypervectors are random vectors in the range   of the same dimension of the input vectors  ","For each hypervector   and input vector   the following operation is performed:
 
                      Thus, the binary representation of the bucket for an input vector is obtained as the concatenation of the operations of all hypervectors",As the hypervectors are selected randomly there is a probability of collision in the same bucket of samples which are not closer in euclidean space (  where  ( ) is the angle between   and  ),"To improve the robustness of the system, multiple sets of hypervectors are employed in parallel   spaces","We take the number of collisions in the buckets of the different spaces as the criteria for sorting the candidates from the full set ( 
                         )","These algorithms are applied in this Thread Discovery Module, whose structure is depicted in  
                         ","According to this flow, the involved tasks are the following ones","Firstly, a   stage where a frequency vector is obtained from the given text (post)",The components of this vector are the frequency of character N-grams,"Please, note that some characters were not considered for the N-grams (e.g. #) because better clusters were generated this way","Later, the LSH stages take place, where for every space, we obtain the bucket in which the register lies",The register is also indexed in a temporal buffer which stores the most recent registers that enter the platform,"After that, the list of better candidates is obtained, i.e. the list of registers in the platform that share bucket with the new register more frequently in the different spaces","Then, using the cosine distance we obtain the most similar register among the best candidates","Finally, if the similarity of the new register with the best candidate is below a given threshold, a new thread is generated",Otherwise the register joins the thread of the best candidate,"With a lower threshold, less threads will be generated as it will have more chances to join an existing thread","It is important to remark that this is an online process, where registers are analyzed as they enter the system","Besides, the size of the buckets and the temporal buffer were conditioned to memory constraints","That is, a register is indexed in the system until it overflows the temporal buffer of recent registers or it is removed from all the buckets in which it was initially placed. 
                         
                          shows the parameters employed in the text aggregation system in all the tests","The number of comparisons was restricted to 100 saving a lot of computation power per register.  
                          shows the best candidate sorted by the number of collisions for a big dataset",Noticeably the best candidate is usually the one that collides more times in the LSH subspaces (with a mean of 27),"Therefore, a limit of 100 candidates is indeed conservative in this scenario. 3.3 Threads ranking This module takes advantage of the outputs of both the Crowds Dynamics Analyzer and the Thread Discovery modules, being its main task the measurement of the cluster diversity of each discovered thread according to the following relationship:
 where   is the current time step,   is the window in which the thread appears for the first time,   are the thread samples in the time window   and 1  is 1 if the thread is spread in more than one cluster in the   window or 0 otherwise","Active events are ranked by relevance, taking into account that with the given ranking strategy, events which are relevant but do not imply crowd significance are extremely penalized (e.g. events not directly related to city dynamics), whereas middle-sized threads about local events will have more chances of appearing at the top. 4 Dataset In order to conduct our experiment, we need a large data set populated with publicly accessible geo-located posts obtained from an OSN","Thus, features provided by the APIs of each OSN and the number of posts that can be obtained in the area under study are the criteria we used to select the OSN","We analyzed three different options: Twitter, Foursquare and Instagram","Twitter Streaming API (free-access), designed to gather tweets that are currently being posted, had two main restrictions","On the one hand, it was only possible to access to the 1% of the published tweets and only 3.17% of those were geo-located tweets","On the other hand, although Twitter Search API is also free-access and designed to collect old tweets, they limit the number of calls both per user (180 calls/15 min) and per application (450 calls/15 min)",Foursquare had also a main problem: posts are always linked to the venues in which users share their opinions and comments,"Therefore, posts location is biased by the venues location","Finally, Instagram offered several advantages: (i) it did not impose any restriction to the location linked to posts, being directly the users’ GPS-location; (ii) it was possible to directly recover all the posts shared within a specific geographic area; (iii) due to its growth in the recent years ( ), Instagram already had more monthly active users than Twitter (in January 2016), and a higher number of the posts contain information about location; and (iv) finally, Instagram API went also further than Twitter: (a) allowed gathering posts published at any moment in the past and (b) imposed less calls restrictions (500 calls/h for Sandbox mode, 5000 calls/h for Live mode)","According to the aforementioned reasons, Instagram was selected to populate our NYC data set by gathering data from a circular area (5 Km) centered in Times Square (40.756667 N, 73.986389 W) from 23rd August 2015 to 28th February 2016 (190 days): 4,335,880 posts","For this experiment we focused our attention on Saturdays, 783,907 posts, which were grouped in chunks of 30 min","This long period covered: (i) special days when the city is traditionally more crowded like Christmas time; (ii) unusual days, like the weekend when Storm Jonas hit the United States, when the city was less crowded; (iii) days which are considered normal, when no special events or phenomena are expected to happen and (iv) days during which some events happen with high impact in small areas for a short period of time, like the New York Comic-Con. 5 Evaluation In order to validate the Events Detection System, we performed different experiments","Firstly, we proceed only in delimited areas where well known events were celebrated to manually check how the behavior of the system was, i.e. the content threads extracted",The intention was mainly to check the consistency of the methodology,"Then, we checked the whole processing pipeline, i.e. the Crowds Dynamics Analyzer, the Thread Discovery module and the Event Ranking module",The objective here was to confirm the filtering capabilities of the whole system to be used for automatic incident detection,"In these experiments, content threads were obtained using the whole dataset","Finally, after running the system without previous restrictions, not only all relevant events were detected, but also new ones that were overlooked during the manual review","Among all the experiments that were performed, we highlight four because of their specific characteristics. 
                      
                   
                         
                         
                         
                         
                         
                      
                            
                            
                         
                            
                            
                         
                            
                            
                         
                            
                            
                         
                            
                            
                         New Year’s Eve New Year’s Eve is a highly interesting data since an unusual amount of people concentrates all around the city for a short period of time (a few hours)","In our case, we focus our attention on Times Square and its surroundings ( 
                               (a)) to analyze the data gathered from this area","Finally, we compared these results with the analysis performed on the data obtained out of this area","As a brief summary, a total number of 47,617 posts were analyzed","Although the area next to Times Square is small, a significant percentage of the posts came from there, around 5%",Comic-Con NY Comic-Con was interesting because an unusual amount of people concentrates in a relatively small area,"After checking the venue, we restricted the area under analysis as  (b) shows and the time to the weekend where the event took place","As a brief summary, the sub-area of influence of the Comic-Con contained all the posts about this topic, having analyzed all the posts shared in 2015/10/10 (23,545)","Storm Jonas Lastly, Storm Jonas hit NYC and literally paralyzed the urban area from 21st to 24th of January 2016",Our interest here lies in the analysis of a totally opposite behavior: abnormally low activity level all around the city and potentially located in different areas than it is usual,"We checked if the Crowds Activity Analyzer answered as expected and if the Threads Discovery Module was able to infer relevant threads of content related to this event, despite of the large influence area","In this case, we analyzed a dataset of 25,355 posts","Unexpected Events in order to check the ability of the system in detecting geolocated events when no prior knowledge exists, we use as input all the Saturday posts from 2015/08/29 00:00 to 2016/01/23 23:30 in the area of analysis (around 800 mil posts)","Technical restrictions Finally, it is important to remark that all the operations were carried in commodity hardware (Intel(R) Core(TM) i5-4430S CPU @ 2.70GHz) with 16GBytes of RAM",The Content Thread generation module was able to work at a pace of 250 posts/s,"Memory is also constrained because of the characteristics of the algorithm employed (it depends on the number of spaces, hypervectors, internal temporal memory and bucket size). 6 Results As aforementioned, the experiments were oriented to check two main aspects: (i) the Threads Discovery Module and (ii) the whole Events Detection System","The performance of the Crowds Dynamics Analyzer module was already assessed in detail in  , having obtained very good results. 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   6.1 Threads discovery module In order to assess this module we focused our attention on two events with local influence: the New Year’s Eve and the NY Comic-Con","After analyzing the datasets specified in  , we compared the obtained threads with the topic of each event","Regarding New Year’s Eve the most relevant threads identified by our system are summarized in  
                          and  
                         , comparing respectively the results within and out of the restricted area",Threads detected within the Times Square Area mention the location where the celebration takes place,"In contrast, outside the Times Square area, the relevant threads mention other important landmarks of the city (such as Central Park) or mentioned only the event (New Year) without making reference to any specific location",Threads are calculated for different similarity thresholds,"Despite outputs differ slightly in terms of content aggregation, the relevant information is displayed with all the considered configurations",Similar results were obtained for the NY Comic-Con experiment,"The Threads Discovery Module is able to extract meaningful content threads with information about the conference within the restricted area ( 
                         ), whereas the detected threads out of the restricted area do not make reference to the location at all ( 
                         )","Finally, and with the aim of analyzing the influence of the similarity threshold ( ), we also performed the very same experiments but changing its value (0.6, 0.65, 0.7 and 0.75)","The results, as   and   show, were consistent, regardless of the selected threshold","Despite of these good results, we decided to check the effect of the similarity threshold over the whole dataset","The results are depicted in  
                          by showing the number of identified threads","Clearly, the less restrictive the threshold is, the higher aggregation is exhibited by the model","Therefore, and unless specified, we decided to use a similarity threshold of 0.65. 6.2 Events detection system To evaluate the effectiveness of both unsupervised approaches (content and geolocation analysis) we picked up a specific time window (from 17:30 to 18:00) in the NY Comic-Con day to manually assess the top detected stories in this time slot for each geo-located cluster or crowd","On the one hand, the Crowds Dynamics Analyzer detected eleven crowds all around the city that clearly do not fit with the usual clustering or reference clustering of a Saturday at that time ( 
                         )","On another hand, the Threads Discovery Module detected a cluster (number 1) with 74 posts, whose top threads are detailed in  
                         ","Additionally to this analysis, we have also checked what happens with all the posts that are already shared in Instagram, but that are not geo-located in any cluster (308)",Again the top threads are detailed in the same table,"As it can be inferred from the results, the collection of posts which are not included in any cluster is a mishmash of stories within different topics (tourism, a new fossil discovery, etc.), whereas in cluster 1 stories are strictly delimited to NY Comic-Con related information","Finally, the Threads Ranking module obtained the results depicted in  
                         , where relevance is calculated each 30 min","Automatically, the system detects the anomalous event without any a priori information","The majority of them are related to the Comic-con topics ( , etc.)","However, other threads are completely unrelated to this event such as mentions to exhibitions and important landmarks of the city that people massively visit","Using the dataset of Storm Jonas, the discovered threads ranked by their relevance at the end of the day are depicted in  
                         ","As expected, only a few threads are interesting from the geolocation perspective","Pretty curiously, the most relevant thread was a Mattise exhibition in the Museum of Modern Arts of New York (MOMA)",This can be explained by the absence of big geolocated clusters during this period,"In contrast, the biggest content thread in size of this dataset made reference to the Storm Jonas (nearly 1000 posts)","However, this thread spreads over multiple geo-clusters as it affects the city as a whole","As conclusion, when there is a so large area of influence it is better to check directly the output of the Threads Discovery Module instead of checking the final output. 6.3 Unexpected events As a final test we threw all the data available and we checked the ability of the system in extracting the geolocated clusters with relevant information.   shows the cluster relevance over time and day",A cluster relevance is obtained as summation of the content relevance of a story at each time step in the cluster,Only the most relevant cluster is depicted at each time interval,"Here, a four-scale color intensity portrays the relevance of the thread in comparison with the other threads of that day for easy visualization (from the light red, being the less relevant threads, to the intense red, the most prominent ones)","As it turns out, highest relevant clusters with uncommon events can be extracted without prior knowledge. 
                         
                         
                          shows some of the important events manually labeled according to the top-3 threads of each cluster",Many of these events are small in absolute terms,"For example, both the   and the   events are less than the 4% and the 2% of the total amount of posts analyzed that day respectively","Readers must take into account that the posts were gathered during Christmas time which makes difficult to find rare events not related with this festivity simply by manual observation (608,492 different threads are extracted)","With our approach, geolocated events easily stand out, drastically lowering the time required by an analyst. 7 Discussion The experiments in this paper show that combining clustering techniques over GPS-location and text from post shared in LBSNs can effectively discover relevant events which are linked to specific locations in urban areas",The combination of both modules is stronger than each one working on its own,"Whilst using only text information the system cannot infer with high precision the locations, although several attempts have been carried out ( ), the crowd detection system did not extract the root cause of an event and cannot distinguish posts lying within a certain area but which are not semantically related","In spite of the fact that user tags are widely used in social networks such as Instagram, only slightly more than a half of the registers included tags in the caption text ( ≈ 56%)","Thus, naive approaches relying only on tags or certain keywords will filter important data that otherwise will be available for aggregating content",Our approach succeeds in finding relevant content without using specific elements of a social network (e.g. hashtags),"Thus, this model could be applied to other scenarios with similar characteristics (e.g. other microblogging services) without extra effort","It is also remarkable that a costly infrastructure that must be deployed all over the city, and that normally entails high maintenance costs, is not needed","In fact, the sensing devices (mainly smart phones) are bought, maintained and connected to the Internet by the users","Besides, the whole pipeline of the system is nearly plug and play and it is fed by the users from the posts they publicly share in social media",That entails it could be easily applied to other urban areas only by specifying the configuration parameters: similarity threshold (for the Threads Discovery Module) and ϵ and   for the Crowds Dynamics Analyzer,"In the first case, the experiments show that, although with slight differences, the aggregated events detected are equivalent for the interval [0.65, 0.75]","In the second case, both parameters are easily obtained from the clustering analysis ( )","Furthermore, as the algorithms are fast and low resource consuming, threads resulting from multiple thresholds could be calculated in real time when different granularities are provided","Finally, and since both approaches are unsupervised and can be executed with commodity hardware, this solution opens the door for its application in real scenarios. 8 Conclusions Citizens carrying smart devices are turning into sensor on the move","The text and images they actively share in social media, together with the metadata linked to these posts, are a definitively important source of information",In this paper we introduce a Events Detection System that uses geo-located posts to analyze both location and text,"On the one hand, detecting unexpected behaviors in the city, i.e. unusual number of people in the same area for the time and day of the week","This task is performed by the Crowds Dynamics Analyzer, which works directly with the GPS-locations","On the other hand, our interest focuses on inferring the reasons behind these abnormal behaviors",This task is performed by the Threads Discovery Module,"Therefore, we tackled both problems with a combined approach that shares the strengths of a purely mathematical model to detect crowded points in a city, with a Content Aggregation system which is capable of extracting relevant threads of content over vast amounts of data","Our approach is low resource consuming, since all the calculations could be done with commodity hardware which closes the gap for exploitation in real systems","Our system does not impose any restriction about the data source if both location and text are provided, although the results are better for short texts","Instagram captions share several characteristics with other social networks (such as Twitter): lack of formality or syntactic structure, misspelled words, new handcrafted tags which glue words or invent new ones","In this scenario, traditional NLP tools for processing input data (e.g. syntactic/dependency parsers, PoS analysis) are more prone to failure","Thus, the shallow clustering technique proposed in this paper is a good alternative to those methods, so that it is more resilient to typos and text informality",We have assessed this approach over a large dataset of Instagram posts obtained in the New York City area for seven months,The results are promising since it is possible to detect abnormal behaviors in the area as well as being able to infer the reason of that social media activity,"Because of these good results and because of the fact no costly infrastructure is needed, this approach might be an additional source of information, helping along with video-surveillance systems in the cities","This is specially the case for Instagram, and the add-ons to include photos in Twitter posts, so that these photos can be taken anywhere (no city infrastructure) and then uploaded to social media","Nevertheless, considering this information as part of a cybersecurity strategy, it should be accompanied by the assessment of credibility, veracity and provenance risks","Finally, the combination of both modules may enrich the activity patterns obtained by the Crowds Dynamics Analyzer","Nowadays, these patterns only have information about the usual location of citizens all around the urban area at different days of the week and at different times","However, the Threads Discovery Module can add useful information, such as the detection of citizen groups with common interests or the identification of current trends in specific locations of a city","In the current state, the ranking module allows ordering the discovered threads according to their geographical diversity, so prioritizing threads which remain in a neighborhood","As a future work we consider it is also interesting to analyze the geographical and temporal dynamics of the threads, moving from one cluster to another and, and more specifically, the requirements under which a thread in a cluster turns into a multi-hop geolocated thread","According to the classification of rumors as   and   ( ), we are working now in analyzing topic, user and time requirements which characterize geo-dependent threads","Our aim is to expand all over the city at the end, by adding geo-location and thread linkage to well-known algorithms for rumor detection ( ).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s1071581917301404," 1 Introduction New forms of social media (SM), provide the means to generate and share multiple digital identities, but the resulting identity landscape is complex and the data underpinning digital personhood are fragmented, offering little in the sense of a coherent individual life story or presence","New SM can embrace both individual expressions of self as well as other reflections of self that can be difficult to control, often leading to a form of ‘context collapse’ where private aspects of the digital self may leak out to an inappropriate audience (e.g.  )","Increasingly, researchers are recognising that the management of digital identity can present a burden and new solutions are being explored that support people in understanding and shaping their digital selves (e.g.  )",Many of us have a digital identity but fail to fully understand how it is represented and interpreted in the digital realm ( ),"This paper reports on three converging studies that   data drawn down from SM platforms, asking whether such transformations can be used to enhance understanding of the digital self","Following   and  , we ask what different digital storytelling media offer individuals, SM researchers and designers in terms of the construction, evolution and consumption of the digital self and the processes involved in digital self-management","We discuss the context and process of digital self-formation before considering the spaces where the digital self resides and describing what happens when SM is transformed into new, often tangible and primarily visual forms","We then present the findings from three studies where we work with individuals to transform or remediate their SM into different types of visual storytelling media (a physical book, three photographs presented as a triptych, and a film)",We find that this remediation of personal digital data allows users to reflect on their digital identity in new ways,We note that current assumptions about the bounded nature of SM sites are overly simplistic and that there is a need to develop more sophisticated SM use and management practices,"We offer a curation framework which can be used guide the design of systems that promote self-reflection and self-presentation and that would support improved digital literacy. 2 Related work 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   2.1 The digital self For the individual user, a prime purpose of SM is “ ” ( :19)","Digital selfhood is performed by posting episodic narratives and curating data (i.e. liking, sharing and re-posting) within an infrastructure that reproduces a world ‘out there’ ( )",Digital selfhood emerges as a response to such online exchanges and is sustained within partitioned digital environments ( ),"Many SM researchers apply   dramaturgical approach (e.g.  ), arguing that digital self-presentation is analogous to a theatrical performance, directed at a particular audience. 
                          argued that people have a ‘front stage’ whereby they can present an idealised self to others but he supposed that such interactions took place between known individuals and were bounded within a specific context",A challenge in the digital age is that the digital self is dispersed across a range of contexts (platforms) and aspects of our digital selves (in the form of posts or other shared digital content) will reach both intended and unintended audiences,Depictions of the digital self as ‘ ’ ( : 325) may not appropriately recognise this digital dispersal,"Further, the desire to compartmentalise oneself in SM, by presenting different selves on different platforms, may often fail when ‘context collapse’ occurs across platforms ( ) or over time, when users new to a platform are allowed inadvertent access to posts that leak from another's’ past ( )",Whilst theory is rich in this space there is limited empirical insight into how people can better understand and ‘manage’ their multiple selves,Researchers have examined self-presentation through personal websites ( ) with a more recent focus on SM ( ) but few capture the different ways that self-reflection is supported by the specific forms of SM,Yet it is clear that people use a range of digital narrative devices to tell their own ‘story’ ( ) and it is also clear that different SM systems favour particular storytelling approaches,"Here, we wish to understand more about how these digital forms can shape users’ personal stories ( ) and what new self-learning is gained from new presentation formats. 2.2 Spaces for the digital self 
                          demonstrate that people actively use signs, symbols, objects, and places in their personal Web spaces, to differentiate between selves (e.g. family oriented, intellectual, career driven, attractive, out going, etc.) whilst attempting to convey an idealised self","Personal photographs take an important role here, becoming a vehicle for the performance of self both in the home ( ) and when shared in SM ( )","Digital storytelling as a methodology allows researchers to capture these multiple selves within a single story, allowing new insight into issues such as longitudinal adjustment to social norms; self censorship; the construction of the self as content; and the unexpectedness and uncontrollability of SM interaction",This unexpectedness distinguishes the digital self from   more controlled notions of self management,"SM platforms allow multifunctional communication at scale, through text, photographs, instant messenger, direct messages and videos ( )","Facebook, as the largest SM platform, offers three different functions for its users: (1) a performance region for managing and sharing recent data; (2) an exhibition region for longer term presentation of self image; and (3) a personal region to archive meaningful facets of life ( )","Those authors argue that individuals are likely to be more adept at the first function, managing their content as a performance focused on presenting themselves ‘as they are now’ on SM, implying that they would ‘edit out’ content that might conflict with the current, curated version of the digital self. 
                          also perceives SM spaces as a mix of situational and curatorial self-presentation perspectives","Hogan distinguishes between   spaces, where actors engage each other online, and   spaces, where individuals submit and share artefacts","In describing SM as a form of personal museum, Hogan steers away from the notion of SM as predominantly an interaction space towards recognition of the increasing role of SM as life repositories or curated information stores that can take on a personal narrative of their own ( ).  , explored SM's   elements, asking participants to create digital keepsakes by ‘clipping’ valued SM content to an online note taking tool","Despite being asked to save content from multiple SM accounts to construct their digital story,   participants drew heavily on Facebook, partly as it offered many more valued photographs than rival SM sites. 2.3 Social media and self-reflection Self-reflection is a core human activity that occurs naturally and is often associated with the processes of life review and reminiscence ( )",It is also expressly encouraged in some forms of professional practice ( ),"Increasingly, researchers are recognising that self-reflection is important for personal growth and that the processes of self-reflection can be supported by digital data ( )","Within HCI, we see studies on self-reflection associated with two different domains of enquiry","Firstly, people now have the ability to quantify aspects of self (heart rate, sleep patterns, mobility data etc) via a raft of mobile applications that allow people to reflect upon their own lifestyle choices and consequences",A number of studies in ‘personal informatics’ or ‘quantified self’ have explored the ways in which such data can be used ( ),"Secondly, the digital data associated with everyday SM can also be used for personal reflection","Sometimes this occurs naturally and is associated with the curation of personal SM data ( ) but more recently, we have seen the development of new services and applications that re-present personal data in order to encourage reflection","For example, Facebook's A Look Back 
                          service was launched in 2014 to compile highlights of a user's profile in filmic form","Individual's Facebook content; photographs, events, liked posts, shared photographs and statuses are collated and transformed into a two minute video for the account holder to watch","Similarly, services like TimeHop 
                          and On This Day 
                          encourage reflection on past SM posts","These services which involve remediation and allow for reflection, reveal the value in time-delayed electronic communication","Online application FutureMe 
                          encourages content to be posted after a specified time, which has demonstrated instances of ‘profound reminiscence’ for users, as well as unsettling encounters with past content and personal challenges ( )","One FutureMe user sent his parents a future message telling them that he was gay, creating a self-imposed deadline for the same conversation face-to-face","Another time-delayed media sharing system, ‘Postulater’, allows e-mail exchanges between friends and family for six weeks, requiring a specified time for the release of photos and videos ( )","Postulater users felt vulnerable, as reactions to their future messages were unknown","Finally, systems like the ‘Ripening Room’ ( ) that encourage a delay between writing and sharing posts can prove fruitful in countering experiences of regret ( ) and bullying ( )",Also relevant in this space is the work on designing for remembering,"In recognising the fallibility of memory, we can begin to see how systems designed to encourage a revisitation of personal data can be flawed","Information may not always be presented in the way it was captured, and is deemed less manageable than organising a physical collection of keepsakes ( )","An overreliance on digital material has been highlighted elsewhere ( ), resulting in shifts in how we conceptualise our memories and experience recollection. 
                         : 5) point out “older electronic and print media seek to reaffirm their status within our culture as digital media challenge that status.” As new forms of content gain popularity, there is still value in older methods",Work on ‘material speculation’ has explored how the digital sits alongside more traditional forms – e.g. creating an experience of viewing digital media using a traditional wooden chest to store photographs ( ) thereby   digital data in a way that differs from its original source,"In remediation theory, this notion of   ( ) offers an important construct in our work, and we are particularly mindful of work on personal photography remediation and the changing role for personal photography in digital storytelling","As  : 283) observe: “ .” We note, however, that theories of remediation are more typically applied to the ways in which physical entities (photographs, letters) take on new meaning when digitised","Here we look at the process in reverse, exploring the new meanings that come from taking a fluid, extended digital presence and reducing this to a bounded film or a tangible book or triptych of photographs. 2.4 Research gap There is an ongoing need to theorise about the nature of digital selfhood, and to inform design processes that might support future SM users in understanding their digital selfhood and managing their digital interactions","Following  , the performance of self is central, but we recognise that the   for this performance is also critical, recognising that  ” ( : 183)","Thus, we delve further into the   of particular forms of SM asking how transformations away from the original form might be used to reveal new insights about the digital self","Specifically, we ask: How can remediating users’ digital data assist them in understanding their digital identities? 3 Study design and methodology This paper reports on three converging studies which explore what happens when personal digital data undergoes a physical transformation into one of three different forms: (1) a printed book (‘MySocialBook’), sourced entirely from Facebook data and curated by the Facebook account holder; (2) a triptych consisting of three printed and framed photographs, sourced from Facebook or Instagram and curated by the account holder and (3) a film, sourced from a range of SM accounts including Facebook, Instagram, Twitter, YouTube and curated by a professional filmmaker",These three formats were chosen to explore how the remediation of social media might change reflective practice around digital personhood,All three forms were aligned as they provided a simpler and more tangible digital self-construction that should better support reflection ( ),These forms have been assessed individually in other publications.   develop the concept of multiple temporal selves (where different types of data represent different selves from different life stages),"This film study revealed the problem of social media leakage; where older selves may find their way to unintended audiences as use of a platform changes over time.   highlight the reminiscence function of SM data transformed into a MySocialBook, and show how this data transformation gives new groups access to old social media data. 
                      
                       outlines the purpose and nature of the data and the remediation method in each of these studies, while  
                       summarises key details of recruitment, participant demographics and the data collected","The three forms also differed in interesting ways, as described below","Overall we recruited 26 participants, and the research was conducted by the two research teams between May 2014 and June 2015","In study 1, the aim was for participants to review a self-curated, semi-automated artefact; a printed book compiling their Facebook data",The study was conducted over two sessions in the lab at Northumbria University in the UK,"In Session 1 participants were told that they would design and print their own Facebook scrapbook using MySocialBook 
                       and were asked to sign into the site using their Facebook credentials","This is a publically accessible website, with no affiliation to the academic institutions involved in this research",The purpose of this study was to allow participants to reflect on Social Media content that they post,"As Facebook is a popular Social Media platform, it was appropriate to focus on remediating this data They were guided through the design process and encouraged to maximise the personal connection to the book using website features to facilitate curation, the only restriction being a price limit of £30 per book",Participants were then shown a preview of how their book would look and refined their selections until happy,Session 1 lasted approximately 45 min,"Once their MySocialBook was printed, participants were invited to participate in Session 2, where they were presented with their book and asked to browse for approximately ten minutes before being interviewed about it","They were encouraged to annotate their MySocialBook, noting anything that they liked, disliked, or thought could be improved","An example of a completed MySocialBook can be seen in  
                      ","Examples of the book annotations can be seen in  
                      .)","Session 2 lasted approximately 60 min. 
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                   3.2 Procedure 2: triptychs We aimed to encourage self-curation of SM data in the form of photographs","Again, there were two sessions",We first invited participants to review photographs they had taken and uploaded to SM selecting three that ’told a story’,They sent in these three photographs and any associated photo captions/ hashtags from their SM accounts (Facebook or Instagram were recommended) via e-mail,"The researcher printed the three images, and mounted them in a triptych frame ( 
                         )",Participants were invited to a second session where they collected the triptych and participated in a face-to-face interview. 3.3 Procedure 3: films Studies one and two focused on data presented on either one SM platform (MySocialBook) or one form of SM data (photographs) respectively,"This third study involved a wider range of data from a variety of SM platforms, allowing greater focus on the dispersed digital selves of our users","We engaged two groups of participants, professional filmmakers and their subjects ( ) who were SM users",There were three broad stages to this study,"Firstly, a call was circulated to invite professional filmmakers to propose a subject for a film that would be comprised entirely of SM data",They were given the explicit condition that filmmakers would need to have full access to all of their subject's SM data,"Six were selected from 26 entries based on the filmmakers’ experience, their interpretation of the brief, including the types of narratives proposed in their entry, as well as the overall interest of the subject, alongside their SM presence and activity (i.e. active on at least three types of SM)",Each criterion was graded between one (weakest) and five (strongest) providing a total average mark for each competition entry,The highest six averages were chosen,"Secondly, the filmmakers extracted the subjects’ longitudinal SM data to construct a film",They worked with existing SM data to develop a narrative of self,"This involved a substantial research and planning process prior to production, whereby the filmmakers sifted through all of their subjects’ SM data to extract key themes to construct a narrative","In four of the six films, participants collaborated with filmmakers on material to be included, in two cases, the participants wished to leave decisions on what would be included to the filmmaker","Thirdly, both the filmmakers and their subjects were interviewed about the process and the presentation of digital self at various points in the project up to the completion and viewing of the final films","Although the identities of the film subjects and filmmakers are in the public domain, we do not attribute quotations cited in this paper to specific participants in order to prevent the reader from attributing specific information to our participants","The film, MySocialBook and Triptych studies were carried out in accordance with the recommendations of University of Birmingham and Northumbria University Ethics Committees, with informed consent from all participants, including permission to publish social media content. 3.4 Data analysis All interviews were audio recorded, transcribed, and a thematic analysis was conducted","As the data gathered were predominantly in the form of “free-flowing text” ( : 771), they were analysed by finding meaning in large segments of the texts rather than in isolated words, hence coding the data ( )","Theory was derived inductively, where the themes emerged from the data as a result of open coding",Two members of the four-person research team coded each data corpus,"Following each stage of coding; (i) familiarization with data (reading and re-reading transcripts); (ii) generating initial codes (constant comparison between data); (iii) searching for themes (identified when patterns and repetition emerged in the data); (iv) reviewing themes (checking themes against extracts and overall data set); and finally, (v) explicit naming of themes ( ), discussion took place between the research team members regarding the emergent themes","Themes were loosely identified as concepts, but the naming of these emerged after data analysis",This was an iterative process where we moved from descriptive codes to thematic codes,"Once final themes were agreed, each transcript was re-examined for final write up","Reliability coding was conducted between two members of the research project team in each study, with initial coding undertaken by one researcher and shadow coding to safeguard against definitional drift ( ) by a second researcher, with coding finally cross-referenced for compatibility across studies by the authors",The focus of this analysis was on how the participants responded to their digital selves as presented in the different formats and what they learned from this process regarding the nature of their SM usage and performance of self,"This was the analytical approach from the outset, and our findings represent the analysis of our complete data set","This led to the development of our key findings in establishing a dynamic curation framework, which represented the nature of our participants’ response to their digital selves and is outlined below. 4 Findings and analysis We found that storytelling in these different formats (film, book, photographs) supports different forms of self-reflection and sense making in a large, personal SM digital dataset","We found that remediation was underpinned by four value-judgements: (i) participants eliminated   that would not be suitable for sharing, effectively reducing the noise in the data, (ii) they considered the   of the remaining data, understanding the importance of curating content for later use and, (iii) they assessed the   of the data in terms of its value in the storytelling process and (iv) they then assessed the appeal and usefulness of the material in its  ","At the end of the sessions, we asked participants to engage in a process of  ; a counter-factual process of imagining how future remediation systems might best support the different facets of self-representation. 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   4.1 Inappropriate content Participants had to sift through an overwhelming amount of personal data, encompassing vast collections of photographs, videos, status updates, shared links and personal messages generated as part of the ‘performance’ of SM","For most participants, their first task involved reducing ‘noise’ and eliminating disliked content, to produce something that was more valued and reminiscent of significant events, i.e. material that would be more suitable for consideration as part of the ‘exhibition space’ in   terms:
 
                         
                      We saw a desire to edit material that might support notions of the ‘idealised self’, although we should recognise that participants were also motivated to protect others in editing any material that might prove uncomfortable viewing or material that might be considered insensitive:
 
                         
                      Such concerns echo the sentiments of  ) who discuss the privacy implications for self and others associated with the digital remediation of physical photographs and who note the way this leads to complex “ ” (p. 24) in support of creative self-representation","The persistence of unwanted data on certain platforms is a known issue for SM and has led to the rise of new ‘ephemeral data’ exchanges via platforms like Snapchat, Gryphn and Wickr that celebrate the fact that they make data disappear ( )","However, it is interesting to note that the nature of inappropriate data was more closely contested in the film study, where both subjects and filmmakers would give priority to the integrity or creative facets of the final film over personal feelings:
 
                         
                         
                      Again, we see that the repurposing of SM acts as a trigger for such reflections","We have seen something similar in the ‘slow technology’ movement, where remediation can trigger a more thoughtful approach to the selection of material ( ) and lead to greater levels of introspection around both ownership and nature of content ( ). 4.2 Archival value Participants recognised that material could, in   terms, not be suitable for either performance or public exhibition",This is a theme recently elaborated by   who found that young adults could recognise the personal value of embarrassing content in their Facebook profile,"Our participants were also aware of the role of such material for future reflection:
 
                         
                         
                      Goffman's work is less comfortable as a framework for understanding such private forms of contemplation","Instead, we find work in relation to prompted reminiscence to be more helpful ( ), noting that people of all ages are able to gain personal reward from a process of life review, but also recognising the value of such a process in making sense of one's identity.
 
                         
                      Participants from all three studies were interested in major life events; graduations, birthdays, weddings, and anniversaries",Previous work has indicated these types of events can be easily identified within SM and often used as a framework to guide storytelling and life review ( ),"However, the process of reviewing archived material can be shocking, since users often forget that SM data is public","In the film study, Alice was surprised that her personal world would feature so heavily in the film:
 
                      This captures the tension that many of our participants experienced at this stage – between an immediate benefit coupled with the realisation that SM gives permanence to such creations and that they are not as private as they might seem","Again, we see the tensions emerging between the performative aspects of SM and the exhibition value of that same material ( ). 4.3 Narrative value Participants took their time to make very deliberate selections of the material that could tell a story or reflect a unique aspect of self","This involved considerations of form, content and structure but, for the first time, we start to see a differentiation between the demands made of different formats as participants were cognisant that, at the end of the project, they would receive something both tangible and public",This drove a desire for more careful crafting of material that would represent specific aspects of self,"Here, the MySocialBook participants sometimes struggled with tone, some wanting a funny or more playful tone that would capture the performative aspects of identity, with others wanting to capture something of their longer term social context, whilst for the triptychs, narrative was key and participants strongly considered which photographs to use to tell their story:
 
                         
                      With film, we saw a tension between creating an accurate representation versus a compelling narrative","The subjects were generally more concerned with the former; seeing the film as an opportunity to produce a more holistic perspective and/or correct any wrong impressions and misrepresentations:
 
                         
                         
                      While the filmmakers were more concerned with the latter; narrative was key, but they struggled to find the most engaging story arc from SM data:
 
                      Again, we see the importance of a digital storytelling framework and the way a particular transformation (into film) demands a strong narrative thread, perhaps at the expense of recognising the simultaneous performance of different selves. 4.4 The remediated form Our participants exhibited mixed reactions in response to their redesigned and remediated SM content","On the one hand, there was a sense of dismay that the mundane had been elevated by the transformation:
 
                      However, a more common response was a genuine delight at the new artefact:
 
                         
                      As the range and nature of material drawn upon for remediation was much greater in the case of the film subjects, they were quick to question the extent to which the identities captured on film were congruent with their own sense of self","Putting the digital storytelling into another's hands meant that the film gained new power as a tool for reflecting on the role of SM in identity construction:
 
                         
                         
                      This idea is quite fundamental and relates to the way in which Goffman talks about the search for self-sameness and a continuity of the self ( )","It can also help to explain the importance of the ‘life story’ as a means of anchoring self within one central narrative ( ), When the narrative breaks down, or appears inconsistent, as it can in SM, then this can be challenging to the individual","Here, we are reminded of the three components involved in   ( ), namely: (i) Self-explanation where individuals analyse their own behaviour and provide explanations of successes and failures, (ii) data verification where people are confronted with a different perception or interpretation of the same data, and (iii) feedback, where people ask themselves what worked, what didn't, and what might happen in the future","The remediated forms of SM data would particularly support the verification process (ii), where people see things in a new light","However, we found evidence for self-explanation and feedback reasoning underlying the kinds of value-judgements we described earlier. 5 Discussion and conclusions We have used different forms of digital storytelling and remediation to gain new insight into digital selfhood","We find that SM data can easily be repurposed, but that the transformation says much about the original and changing value of the underlying data",Our original goal was to try to understand more about the ways in which SM could better support the process of self-reflection,"As noted earlier, we have seen a marked change in the way that personal informatics tools have been used to support an enhanced sense of self, but these are highly deliberative systems where the user makes an explicit decision to collect and use data for personal reflection","There has been a less systematic enquiry of the ways in which SM can support reflection, perhaps because much SM data is simply a by-product of everyday digital communication","We do know that people   Facebook in terms of different functional platforms: for the everyday performance of self, the longer-term curation and exhibition of self and the personal archiving of sensitive data (e.g.  )",The problem is that most applications do not currently allow for the retrospective extraction of personal data in these terms,"Our work reveals a need for smarter sorting of SM data according to narrative, archival or display function that would, in turn, support more meaningful reuse of personal data","Note that here, we distinguish our work from those curation systems that have been designed to explicitly support reminiscence by the deliberative creation of digital mementos (e.g.  )","In those systems, the digital artefacts are chosen with the intent to use them in future reminiscence and reflection, whereas our findings are more explicitly concerned with the ways in which existing SM data can be repackaged for such purposes","With this in mind, we have also tried to understand what is gained by the remediation of SM data into markedly different forms","We note  : 170) observation that understanding identity work in SM necessitates a theory of polymedia, where:
 
                   We have captured, in this paper, some of the ‘affordances’ of a book, triptych and film and concur with   that there are indeed significant emotional and social consequences associated with choice","Our three transformations also amplify content that may otherwise have been forgotten, encouraging a re-visiting of the past","We find less explicit recognition in the SM literature concerning the value of an autobiographical life review, although we have noted the work by   that captures the known psychological values of reminiscence and we recognise the importance of the life story for individual meaning-making ( )","At the end of our studies, participants engaged in an ‘ideation’ process that allowed them to consider the benefit of   future transformations of their digital data, in line with   suggestion of ‘envisioning alternatives’ to allow for reflection","For [ ], a film provided her with food for thought about how she might maintain a better integrated sense of self in the future, and a desire to be able to bring disparate SM together in one platform:
 
                   Here, we gain a sense of an artist negotiating the presentation of self in a multi-platform digital world and again we get a strong sense of the importance of the continuity of self in a consistent life story",There is a tension here,"It is clear that the SM offer individuals a chance to tell their story in “ ” ( , p. 25)",These self-contradictions lead to problems when audience boundaries break down – i.e. when the different SM platforms ‘collapse’ ( ),Yet the concern about digital fragmentation expressed by   would lead us to think that more support for a coherent and consistent digital self would be welcome,"For an artist such as   in search of a particular form of  , this tension may be felt most acutely as a battle between the desire to belong to a part of a community and the desire for ‘ ’ ( , p. 30)","Transformations of SM data often meant that a story or image was stripped of its historical context, resulting in a rather empty narrative, and one that becomes more fixed in time","This ‘fixing’ of a life story is an interesting dimension to the remediation process, particularly given new ‘backstalking’ practices ( ) in which friends can playfully (or maliciously) mess with archival Facebook data, sometimes with the intention to highlight embarrassing moments from the past","Our participants were generally satisfied with the ways in which their selves became ‘fixed’ by our three remediations, but did express concerns about   was portrayed, noting that design decisions made in support of the performance of self might clash with the interests of others:
 
                      
                   Again, we see that the celebration of the   ( ) was sometimes compromised in the various digital expressions of self that formed the source material for our artefacts","This led to some expressions of concern that a particular artefact might not accurately portray ‘who they were’ .
 
                   The literature on both the narrative and the archival value of SM data is growing ( )",Our participants appreciated the personal value of an SM archive but often expressed this in terms of some potential benefit to a ‘future self’,"In part, this theme of the changing value of SM over time was triggered by the way SM could regurgitate unwanted and irrelevant content that once seemed relevant or smart","In other words, the processes of remediation itself offered the opportunity for reflection and consideration of the value of specific content","A re-visiting of the past was bound to occur, but the precise form of each transformation led to subtle differences in reflective practice","The triptychs were most likely to provoke a sense of sentimentality and consideration of life milestones (births, graduations, relationships)","Often the photographs chosen would come from an older time period, thus evoking reminiscence about events that may not have been as fresh in the memory","They also seemed to be most valued as a material object, compared with the MySocialBook",Participants emphasised that they would keep the framed triptych and pick a place in their home where it could be displayed publically,"MySocialBook, in contrast, offered participants a chance to reflect on the nature of their idealised self, and on the social value of their book for deliberate acts of shared reminiscence with loved ones (something they would not do if in digital form)","However, participants were not in full control of the content of their books, which limited their potential as tools for long-term reminiscence","The films allowed for significant acts of self-reflection, as the material covered a wide range from text, to video, photographs by the participants and those in their network","In addition, as material spanning a number of years, in some cases, over a decade, was made available, the films had the potential to show a rich, detailed account of their digital identity as it had evolved","Finally, the involvement of professional filmmakers, who identified strong narratives from among their dispersed digital identities, brought an external viewpoint to the stories of their digital selves","Therefore, the films, more than any other form, offered the film subjects the chance to adopt the perspective of the ‘other’ ","We find that   arguments hold when considering the different, contextually nuanced presentations of self, which take place in real time",In our work we see how a performance can be viewed both within and outside of its original context,"We take and post a photo, a tweet, a comment etc. for a particular purpose and selected audience, and while we recognise that other unintended audiences may engage with these postings, we seldom consider the potential benefit to our future selves","We have described the decisions and judgments that underpin a digital curation and remediation process and believe that this could be useful for SM providers, data aggregators and researchers wishing to develop better tools for SM analysis and display","Inevitably, our findings are somewhat dependent upon the particular design transformations we offered our participants, but we made these choices deliberately, to reflect some of the very different ways in which SM could be repurposed","Beyond our research context, the tools we assessed hold real world applicability",There has been an influx of applications such as TimeHop and Facebook's life event films in recent years,Facebook users are regularly presented with friendship videos and anniversary slideshows of photographs,"The MySocialBook platform is available to anyone, and visitors to the MySocialBook website are encouraged to design books to give as gifts to friends and family (provided you have access to their content)",We can clearly see that the number of automated biographical tools designed to repurpose SM content is on the rise ( ),"We have discovered that allowing people to review these transformations reveals to them the realities of their SM profiles- who sees their content, how much data is stored, and often how mundane their posts are- resulting in considerations around the ‘cleaning up’ of content in order to remain in line with idealised identities","By exploring these three different ways that SM can be repurposed, we have begun to understand how SM content is perceived by its creators, and continue to demonstrate the value in undertaking these repackaging exercises","There is clearly a market for digital remediation services, but there simply is not a current research literature that can help us understand how such digital remediations might be used in future",Our future selves will have access to a vast repository of digital memories that they may find overwhelming,"Here we have gained some understanding of how people might seek to reduce the ‘noise’ in their SM data in order to make it more usable, useful and appealing","Current systems tend to do this in a rather ad-hoc way, such as presenting people with a single time-stamped ‘memory’ from a year or two earlier, but we can imagine the development of more sophisticated systems in the future","Such systems will capture some of the different archiving, narrative and display functions that we describe here and could better marry the form and function of digitally remediated artefacts.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0921889014002164," 1 Introduction: historical overview While the first modern-day industrial robot, Unimate, began work on the General Motors assembly line in 1961, and was conceived in 1954 by George Devol   , the concept of a robot has a very long history, starting in mythology and folklore, and the first mechanical predecessors (automata) having been constructed in Ancient Times","For example, in Greek mythology, the God Hephaestus is reputed to have made mechanical servants from gold (  in p. 114, and    verse 18.419)","Furthermore, a rich tradition of designing and building mechanical, pneumatic or hydraulic automata also exists: from the automata of Ancient Egyptian temples, to the mechanical pigeon of the Pythagorean Archytas of Tarantum circa 400 BC   , to the accounts of earlier automata found in the Lie Zi text in China in 300 BC   , to the devices of Heron of Alexandria    in the 1st century","The Islamic world also plays an important role in the development of automata; Al-Jazari, an Arab inventor, designed and constructed numerous automatic machines, and is even reputed to have devised the first programmable humanoid robot in 1206 AD   ","The word “robot”, a Slavic word meaning servitude, was first used in this context by the Czech author Karel Capek in 1921   ","However, regarding robots with natural-language conversational abilities, it was not until the 1990s that the first pioneering systems started to appear","Despite the long history of mythology and automata, and the fact that even the mythological handmaidens of Hephaestus were reputed to have been given a voice   , and despite the fact that the first general-purpose electronic speech synthesizer was developed by Noriko Omeda in Japan in 1968   , it was not until the early 1990s that conversational robots such as MAIA   , RHINO   , and AESOP    appeared","These robots cover a range of intended application domains; for example, MAIA was intended to carry objects and deliver them, while RHINO is a museum guide robot, and AESOP a surgical robot","In more detail, the early systems include Polly, a robotic guide that could give tours in offices   ","Polly had very simple interaction capacities; it could perceive human feet waving a “tour wanted” signal, and then it would just use pre-determined phrases during the tour itself",A slightly more advanced system was TJ   ,"TJ could verbally respond to simple commands, such as “go left”, albeit through a keyboard","RHINO, on the other hand   , could respond to tour-start commands, but then, again, just offered a pre-programmed tour with fixed programmer-defined verbal descriptions","Regarding mobile assistant robots with conversational capabilities in the 1990s, a classic system is MAIA   , obeying simple commands, and carrying objects around places, as well as the mobile office assistant which could not only deliver parcels but also guide visitors described in   , and the similar in functionality Japanese-language robot Jijo-2   ","Finally, an important book from the period is   , which is characteristic of the traditional natural-language semantics-inspired theoretical approaches to the problem of human–robot communication, and also of the great gap between the theoretical proposals and the actual implemented systems of this early decade",What is common to all the above early systems is that they share a number of limitations,"First, all of them only accept a fixed and small number of simple  , and they respond with a set of  ","Second, the only   (in the sense of Searle   ) that they can handle are requests","Third, the dialogue they support is clearly not flexibly  ; in most cases it is just human-initiative","Four, they do not really support  , i.e. language about their physical situations and events that are happening around them; except for a fixed number of canned location names in a few cases","Five, they are not able to handle  ; i.e. emotion-carrying prosody is neither recognized nor generated","Six, their       capabilities are almost non-existent; for example, gestures, gait, facial expressions, and head nods are neither recognized nor produced","And seventh, their dialogue systems are usually effectively stimulus–response or stimulus-state-response systems; i.e. no real   or purposeful dialogue generation is taking place, and certainly not in conjunction with the motor planning subsystems of the robot","Last but quite importantly, no real  , off-line or on-the-fly is taking place in these systems; verbal behaviours have to be prescribed","All of these shortcomings of the early systems of the 1990s, effectively have become desiderata for the next two decades of research: the 2000s and 2010s, which we are in at the moment","Thus, in this paper, we will start by providing a discussion giving motivation to the need for existence of interactive robots with natural human–robot communication capabilities, and then we will enlist a number of desiderata for such systems, which have also effectively become areas of active research in the last decade","Then, we will examine these desiderata one by one, and discuss the research that has taken place towards their fulfilment","Special consideration will be given to the so-called “symbol grounding problem”   , which is central to most endeavours towards natural language communication with physically embodied agents, such as robots","Finally, after a discussion of the most important open problems for the future, we will provide a concise conclusion. 2 Motivation: interactive robots with natural language capabilities but why? There are at least two avenues towards answering this fundamental question, and both will be attempted here",The first avenue will attempt to start from first principles and derive a rationale towards equipping robots with natural language,"The second, more traditional and safe avenue, will start from a concrete, yet partially transient, base: application domains existing or potential","In more detail: Traditionally, there used to be a clear separation between design and deployment phases for robots","Application-specific robots (for example, manufacturing robots, such as   ) were: (a) designed by expert designers, (b) possibly tailor-programmed and occasionally reprogrammed by specialist engineers at their installation site, and (c) interacted with their environment as well as with specialized operators during actual operation","However, not only the phenomenal simplicity but also the accompanying inflexibility and cost of this traditional setting is often changing nowadays","For example, one might want to have broader-domain and less application-specific robots, necessitating more generic designs, as well as less effort by the programmer-engineers on site, in order to cover the various contexts of operation","Even better, one might want to rely less on specialized operators, and to have robots interact and collaborate with non-expert humans with a little if any prior training","Ideally, even the actual traditional programming and re-programming might also be transferred over to non-expert humans; and instead of programming in a technical language, to be replaced by intuitive tuition by demonstration, imitation and explanation   ","Learning by demonstration and imitation for robots already has quite some active research; but most examples only cover motor and aspects of learning, and language and communication is not involved deeply",And this is exactly where natural language and other forms of fluid and natural human–robot communication enter the picture: Unspecialized non-expert humans are used to (and quite good at) teaching and interacting with other humans through a mixture of natural language as well as nonverbal signs,"Thus, it makes sense to capitalize on this existing ability of non-expert humans by building robots that do not require humans to adapt to them in a special way, and which can fluidly collaborate with other humans, interacting with them and being taught by them in a natural manner, almost as if they were other humans themselves","Thus, based on the above observations, the following is one classic line of motivation towards justifying efforts for equipping robots with natural language capabilities: why not build robots that can comprehend and generate human-like interactive behaviours, so that they can cooperate with and be taught by non-expert humans, so that they can be applied in a wide range of contexts with ease? And of course, as natural language plays a very important role within these behaviours, why not build robots that can fluidly converse with humans in natural language, also supporting crucial non-verbal communication aspects, in order to maximize communication effectiveness, and enable their quick and effective application? Thus, having presented the classical line of reasoning arriving towards the utility of equipping robots with natural language capabilities, and having discussed a space of possibilities regarding role assignment between human and robot, let us now move to the second, more concrete, albeit less general avenue towards justifying conversational robots: namely, specific applications, existing or potential","Such applications, where natural human–robot interaction capabilities with verbal and non-verbal aspects would be desirable, include: flexible manufacturing robots; lab or household robotic assistants   ; assistive robotics and companions for special groups of people   ; persuasive robotics (for example,   ); robotic receptionists   , robotic educational assistants, shopping mall robots   , museum robots   , tour guides   , environmental monitoring robots   , robotic wheelchairs   , companion robots   , social drink-serving robots   , all the way to more exotic domains, such as robotic theatre actors   , musicians   , and dancers   ","In almost all of the above applications, although there is quite some variation regarding requirements, one aspect at least is shared: the desirability of natural fluid interaction with humans supporting natural language and non-verbal communication, possibly augmented with other means","Of course, although this might be desired, it is not always justified as the optimum choice, given techno-economic constraints of every specific application setting","A thorough analysis of such constraints together with a set of guidelines for deciding when natural-language interaction is justified, can be found in   ","Now, having examined justifications towards the need for natural language and other human-like communication capabilities in robots across two avenues, let us proceed and become more specific: natural language, indeed but what capabilities do we actually need? 3 Desiderata—what might one need from a conversational robot? An initial list of desiderata is presented below, which is neither totally exhaustive nor absolutely orthogonal; however, it serves as a good starting point for discussing the state of the art, as well as the potentials of each of the items:  
                   The particular order of the sequence of desiderata, was chosen for the purpose of illustration, as it provides partially for a building-up of key points, also allowing for some tangential deviations","Not all desiderata are necessarily of equal difficulty, and arguably D1, D3–4, and D7–8 have so far proven to be particularly hard","One of the main reasons underlying this situation has to do with the divide between the two worlds that interactive robots usually live in: the symbolic/discrete world of logical representations and language on the one hand, and the continuous and noisy world of sensorymotor data on the other","And it is not only the uncertainty that arises from the unreliability of the sensorymotor end that contributes to the difficulties, but also the fact that sensor data tends to be structured in ways that are not easily alignable to the requirements of symbolic representations, as we shall see","Let us now proceed and examine the desiderata in detail one by one: 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   3.1 Breaking the “simple commands only” barrier The traditional conception of conversational robots, as well as most early systems, is based on a clear human-master robot-servant role assignment, and restricts the robots conversational competencies to simple “motor command requests” only in most cases","For example, in systems such as  , a typical dialogue might be:  
                      What are the main points noticed in this example? Well, first of all, (p1) this is primarily a single-initiative dialogue: the human drives the conversation, the robot effectively just produces motor and verbal responses to the human verbal stimulus","Second, (p2) apart from some disambiguating questions accompanied by deixis, there is not much that the robot says the robot primarily responds with motor actions to the human requests, and does not speak","And, (p3) regarding the human statements, we only have one type of speech acts   : RequestForMotorAction","Furthermore, (p4) usually such systems are quite inflexible regarding multiple surface realizations of the acceptable commands; i.e. the human is allowed to say “Give me the red one”, but if he instead used the elliptical “the red object, please” he might have been misinterpreted and (p5) in most cases, the mapping of words-to-responses is arbitrarily chosen by the designer; i.e. motor verbs translate to what the designer thinks they should mean for the robot (normative meaning), instead of what an empirical investigation would show regarding what other humans would expect they mean (empirical meaning)","Historically, advanced theorization for such systems exists as early as   ","Actually, if one extends from physical robots to systems comprising a virtual robot in a virtual world, Winograds SHRDLU program    from the early seventies could already support multiple speech acts and basic mixed initiative dialogue","There is still quite a stream of active research which, although based on beautiful and systematic formalizations and eloquent grammars, basically produces systems which would still fall within the three points mentioned above","Such an example is   , in which a mobile robot in a multi-room environment, can handle commands such as: “Go to the breakroom and report the location of the blue box”","Notice that here we are not claiming that there is no importance in this research that falls within this strand; we are just mentioning that, as we shall see, there are many other aspects of natural language and robots, which are left unaccounted by such systems","Furthermore, it remains to be seen, how many of these aspects can later be effectively integrated with systems belonging to this strand of research. 3.2 Multiple speech acts The limitations (p1)–(p5) cited above for the classic “simple commands only” systems provide useful departure points for extensions",Speech act theory was introduced by J.L,"Austin   , and a speech act is usually defined as an utterance that has performative function in language and communication","Thus, we are focusing on the function and purpose of the utterance, instead of the content and form","Several taxonomies of utterances can be derived according to such a viewpoint: for example, Searle   , proposed a classification of illocutionary speech acts into assertives, directives, commisives, expressives, and declarations",Computational models of speech acts have been proposed for use in human–computer interaction   ,"In the light of speech acts, lets us start by extending upon point (p3) made in the previous section","In the short human–robot dialogue presented in the previous section, the human utterances “Give me the red one” and “Give me the green one” could be classified as Request speech acts, and more specifically requests for motor action (one could also have requests for information, such as “What colour is the object?”)","But what else might one desire in terms of speech act handling capabilities, apart from RequestForMotorAction (which we shall call SA1, a Directive according to   )? Some possibilities follow below: 
                      And many more exist","Systems such as    are able to handle SA2 and SA3 apart from SA1-type acts; and one should also notice, that there are many classificatory systems for speech acts, across different axes of classification, and with multiple granularities","Also, it is worth starting at this stage to contemplate upon what might it mean to respond appropriately to different kinds of speech acts","For example, an appropriate response to a RequestForMotorAction (a Directive) is the motor action itself, if unambiguous and feasible; however, an appropriate response to an Assertive or a Declarative consists of a change to some form of a “mental model”    or “situation model”    that the robot might be keeping; i.e. creating an appropriate mental token for an object in the case of “There is a red object at the left”, or changing the name label for a mental object token in the case of “Let us call this small doll Daisy”; i.e. both statements elicit primarily internal (mental) actions, instead of external (motor or verbal) actions",Another relevant aspect of speech act theory is the handling of indirect speech acts,"For example, consider the following utterance:  
                      This substitution of an Assertive for an implied Directive (to be inferred by the listener) is a classic example of an indirect speech act","Usually, the analysis of such acts is based on the Gricean maxims of conversation   ; and numerous computational implementations for handling such indirect speech acts have been proposed, such as   ","Finally, yet another problem related to speech acts, is the issue of their classification from the robot, after hearing them","Classic techniques such as those described in    rely on linguistic information only; however, paralinguistic information (such as prosodic features) can also prove useful towards speech act classification; the interested reader is referred for example to   . 3.3 Mixed initiative dialogue Now, starting again from the shortcoming of the traditional “simple commands-only” systems, let us extend across another axis, namely (p1): human-initiative dialogue is not the only option; one could also have robot-initiative, or ideally, full mixed-initiative","Consider FaceBots   , a conversational robot utilizing facebook-derived information","A typical dialogue might include turns such as: 
                      This is a classic example of robot-initiative dialogue, and actually with very simple responses expected from the human mainly Yes or No","On the other hand, consider a dialogue such as (from the robot BIRON at Bielefeld, dialogue in   ), which is an example of a dialogue with limited mixed initiative: 
                      Here, it is neither the robot nor the human driving the dialogue all of the time; for example, the opening pair, R–H is robot-initiative (R: “Hello Human!” H: “Hello!”)","However, directly afterwards, the second half of H together with R creates a second pair of exchanges, which is human-initiative (H: “Who are you?”, R: “My names is BIRON…”)",And thus the initiative can be reversed in multiple points throughout the dialogue,"For an investigation of the state of the art towards mixed initiative, the interested reader is referred to examples such as the Karlsruhe Humanoid    the Biron and Barthoc systems at Bielefeld   , and also workshops such as   . 3.4 Situated language and symbol grounding Yet another observation regarding shortcomings of the traditional command-only systems that are worth extending from, was point (p5) that was mentioned above: the meanings of the utterances were normatively decided by the designer, and not based on empirical observations","For example, a designer/coder could normatively pre-define the semantics of the colour descriptor “red” as belonging to the range between two specific given values","Alternatively, one could empirically get a model of the applicability of the descriptor “red” based on actual human usage; by observing the human usage of the word in conjunction with the actual apparent colour wavelength and the context of the situation","Furthermore, the actual vocabularies (red, “pink”, etc.) or the classes of multiple surface realizations (p4) (quasi-synonyms or semantically equivalent parts of utterances, for example: “give me the red object”, “hand me the red ball”), are usually hand-crafted in such systems, and again not based on systematic human observation or experiment","There are a number of notable exceptions to this rule, and there is a growing tendency to indeed overcome these two limitations recently","For example, consider   , during which a wizard-of-oz experiment provided the collection of vocabulary from users desiring to verbally interact with a robotic arm, and examples such as   , for which the actual context-depending action models corresponding to simple verbal commands like “go left” or “go right” (which might have quite different expected actions, depending on the surrounding environment) were learnt empirically through human experiments","Embarking upon this avenue of thought, it slowly becomes apparent that the connection between local environment (and more generally, situational context) and procedural semantics of an utterance is quite crucial","Thus, when dealing with robots and language, it is impossible to isolate the linguistic subsystems from perception and action, and just plug-and-play with a simple speech-in speech-out black box chatterbot of some sort (such as the celebrated ELIZA    or even the more recent victors of the Loebner Prize   )","Simply put, in such systems, there is no connection of what is being heard or said to what the robot senses and what the robot does","This is quite a crucial point; there is a fundamental need for closer integration of language with sensing, action, and purpose in conversational robots   , as we shall also see in the next sections. 
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      3.4.1 Situated language Upon discussing the connection of language to the physical context, another important concept becomes relevant: situated language, and especially the language that children primarily use during their early years; i.e. language that is not abstract or about past or imagined events; but rather concrete, and about the physical here-and-now","But what is the relevance of this observation to conversational robots? One possibility is the following; given that there seems to be a progression of increasing complexity regarding human linguistic development, often in parallel to a progression of cognitive abilities, it seems reasonable to: first partially mimic the human developmental pathway, and thus start by building robots that can handle such situated language, before moving on to a wider spectrum of linguistic abilities",This is for example the approach taken at   ,Choosing situated language as a starting point also creates a suitable entry point for discussing language grounding in the next section,"Now, another question that naturally follows is: could one postulate a number of levels of extensions from language about the concrete here-and-now to wider domains? This is attempted in   , and the levels of increasing detachment from the “here-and-now” postulated there are: First level: limited only to the “here-and-now, existing concrete things”",Words connect to things directly accessible to the senses at the present moment,"If there is a chair behind me, although I might have seen it before, I cannot talk about it—“out of sight” means “non-existing” in this case","For example, such a robotic system is   ","Second level: (“now, existing concrete things”); we can talk about the “now”, but we are not necessarily limited to the “here”—where here means currently accessible to the senses","We can talk about things that have come to our senses previously, that we conjecture still exist through some form of psychological “object permanence”   —i.e., we are keeping some primitive “mental map” of the environment","For example, this was the state of the robot Ripley during   ","Third level: (“past or present, existing concrete things”), we are also dropping the requirement of the “now”—in this case, we also possess some form of episodic memory    enabling us to talk about past states",An example robot implementation can be found in   ,"Fourth level: (“imagined or predicted concrete things”); we are dropping the requirement of actual past or present existence, and we can talk about things with the possibility of actual existence—either predicted (connectible to the present) or imagined   ","Fifth level: (“abstract things”) we are not talking about potentially existing concrete things any more, but about entities that are abstract","But what is the criterion of “concreteness?” A rough possibility is the following: a concrete thing is a first-order entity (one that is directly connected to the senses); an “abstract” thing is built upon first order entities, and does not connect directly to the senses, as it deals with relationships between them","Take, for example, the concept of the “number three”: it can be found in an auditory example (“threeness” in the sound of three consecutive ticks); it can also be found in a visual example (“threeness” in the snapshot of three birds sitting on a wire)","Thus, threeness seems to be an abstract thing (not directly connected to the senses)","Currently, there exist robots and methodologies    that can create systems handling basic language corresponding to the first four stages of detachment from situatedness; however, the fifth seems to still be out of reach","If what we are aiming towards is a robot with a deeper understanding of the meaning of words referring to abstract concepts, although related work on computational analogy making (such as   ), could prove to provide some starting points for extensions towards such domains, we are still beyond the current state-of-the-art","Nevertheless, there are two interesting points that have arisen in the previous sections: first, that when discussing natural language and robots, there is a need to connect language not only to sensory data, but also to internalized “mental models” of the world in order for example to deal with detachment from the immediate “here-and-now”","And second, that one needs to consider not only phonological and syntactical levels of language but also questions of semantics and meaning; and pose the question: “what does it mean for a robot to understand a word that it hears or utters?” And also, more practically: what are viable computational models of the meaning of words, suitable to embodied conversational robots? We will try to tackle these questions right now, in the next subsection. 3.4.2 Symbol grounding One of the main philosophical problems that arises when trying to create embodied conversational robots is the so-called “symbol grounding problem”   ","In simple terms, the problem is the following: imagine a robot, having an apple in front of it, and hearing the word “apple” a verbal label which is a conventional sign (in semiotic terms   ), and which is represented by a symbol within the robots cognitive system",Now this sign is not irrelevant to the actual physical situation; the human that uttered the word “apple” was using it to refer to the physical apple that is in front of the robot,"Now the problem that arises is the following: how can we connect the symbol standing for “apple” in the robots cognitive system, with the physical apple that it refers to? Or, in other words, how can we ground out the meaning of the symbol to the world? In simple terms, this is an example of the symbol grounding problem","Of course, it extends not only to objects signified by nouns, but to properties, relations, events, etc., and there are many other extensions and variations of it","So, what are solutions relevant to the problem? In the case of embodied robots, the connection between the internal cognitive system of the robot (where the sign is) and the external world (where the referent is) is mediated through the sensory system, for this simple case described above","Thus, in order to ground out the meaning, one needs to connect the symbol to the sensory data say, to vision","Which is at least, to find a mechanism through which, achieves the following bidirectional connection: first, when an apple appears in the visual stream, instantiates an apple symbol in the cognitive system (which can later for example trigger the production of the word “apple” by the robot), and second, when an apple symbol is instantiated in the cognitive system (for example, because the robot heard that “there is an apple”), creates an expectation regarding the contents of the sensory stream given that an apple is reported to be present","This bidirectional connection can be succinctly summarized as: 
                         This bidirectional connection we will refer to as “full grounding”, while its first unidirectional part as “half grounding”","Some notable papers presenting computational solutions of the symbol grounding problem for the case of robots are: half-grounding of colour and shapes for the Toco robot   , and full-grounding of multiple properties for the Ripley robot   ","Highly relevant work includes:    and also Steels   , and also    from a child lexical perspective","The case of grounding of spatial relations (such as “to the left of”, and “inside”) reserves special attention, as it is a significant field on its own","A classic paper is   , presenting an empirical study modelling the effect of central and proximal distance on 2D spatial relations; regarding the generation and interpretation of referring expressions on the basis of landmarks for a simple rectangle world, there is   , while the book by    extends well into illustrating the inadequacy of geometrical models and the need for functional models when grounding terms such as “inside”, and covers a range of relevant interesting subjects","Furthermore, regarding the grounding of attachment and support relations in videos, there is the classic work by   ","For an overview of recent spatial semantics research, the interested reader is referred to   , and a sampler of important current work in robotics includes   , and the most recent work of Tellex on grounding with probabilistic graphical models   , and for learning word meanings from unaligned parallel data   ","Finally, an interesting question arises when trying to ground out personal pronouns, such as “me, my, you, your”","Regarding their use as modifiers of spatial terms (“my left”), relevant work on a real robot is   , and regarding more general models of their meaning, the reader is referred to   , where a system learns the semantics of the pronouns through examples","A number of papers have recently also appeared claiming to have provided a solution to the “symbol grounding problem”, such as   ","There is a variety of different opinions regarding what an adequate solution should accomplish, though","A stream of work around an approach dealing with the evolution of language and semiotics, is outlined in   ","From a more applied and practical point of view though, one would like to be able to have grounded ontologies    or even robot-useable lexica augmented with computational models providing such grounding: and this is the ultimate goal of the EU projects POETICON   , and the follow-up project POETICON II",Another important aspect regarding grounding is the set of qualitatively different possible target meaning spaces for a concept,"For example,    proposes three different types of meaning spaces: sensory, sensorymotor, and teleological","A number of other proposals exist for meaning spaces in cognitive science, but not directly related to grounding; for example, the geometrical spaces proposal of Gardenfors   ","Furthermore, any long-ranging agenda towards extending symbol grounding to an ever-increasing range of concepts, needs to address yet another important point: semantic composition, i.e. for a very simple example, consider how a robot could combine a model of “red” with a model of “dark” in order to derive a model of “dark red”","Although this is a fundamental issue, as discussed in   , it has yet to be addressed properly","Last but not least, regarding the real-world acquisition of large-scale models of grounding in practice, special data-driven models are required, and the quantities of empirical data required would make collection of such data from non-experts (ideally online) highly desirable","Towards that direction, there exists the pioneering work of Gorniak    where a specially modified computer game allowed the collection of referential and functional models of meaning of the utterances used by the human players","This was followed up by   , in which specially designed online games allowed the acquisition of scripts for situationally appropriate dialogue production","These experiments can be seen as a special form of crowdsourcing, building upon the ideas started by pioneering systems such as Luis Von Ahns peekaboom game   , but especially targeting the situated dialogic capabilities of embodied agents","Much more remains to be done in this promising direction in the future. 3.4.3 Meaning negotiation Having introduced the concept of non-logic-like grounded models of meaning, another interesting complication arises","Given that different conversational partners might have different models of meaning, say for the lexical semantics of a colour term such as “pink”, how is communication possible? A short, yet minimally informative answer, would be: given enough overlap of the particular models, there should be enough shared meaning for communication","But if one examines a number of typical cases of misalignment across models, he will soon reach to the realization that models of meaning, or even second-level models (beliefs about the models that others hold), are very often being negotiated and adjusted online, during a conversation","For example: (Turquoise object on robot table, in front of human and robot)  
                         But why is this surreal human–robot dialogue taking place, and why it would not have taken place for the case of two humans in a similar setting? Let us analyse the situation","The object on the table is turquoise, a colour which some people might classify as “blue”, and others as “green”",The robots colour classifier has learnt to treat turquoise as green; the human classifies the object as “blue”,"Thus, we have a categorical misalignment error, as defined in   ","For the case of two humans interacting instead of a human and a robot, given the non-existence of another unique referent satisfying the “blue object” description, the second human would have readily assumed that most probably the first human is classifying turquoise as “blue”; and, thus, he would have temporarily adjusted his model of meaning for “blue” in order to be able to include turquoise as “blue”, and thus to align his communication with his conversational partner","Thus, ideally we would like to have conversational robots that can gracefully recover from such situations, and fluidly negotiate their models of meaning online, in order to be able to account for such situations","Once again, this is a yet unexplored, yet crucial and highly promising avenue for future research. 3.5 Affective interaction An important dimension of cognition is the affective/emotional","In the german psychological tradition of the 18th century, the affective was part of the tripartite classification of mental activities into cognition, affection, and conation; and apart from the widespread use of the term, the influence of the tri-partite division extended well into the 20th century   ","The affective dimension is very important in human interaction   , because it is strongly intertwined with learning   , persuasion   , and empathy, among many other functions","Thus, it carries over its high significance for the case of human–robot interaction","For the case of speech, affect is marked both in the semantic/pragmatic content as well as in the prosody of speech: and thus both of these ideally need to be covered for effective human–robot interaction, and also from both the generation as well as recognition perspectives","Furthermore, other affective markers include facial expressions, body posture and gait, as well as markers more directly linked to physiology, such as heart rate, breathing rate, and galvanic skin response","Pioneering work towards affective human–robot interaction includes    where, extending upon analogous research from virtual avatars such as Rea   , Steve   , and Greta   , Cynthia Breazeal presents an interactive emotion and drive system for the Kismet robot   , which is capable of multiple facial expressions",An interesting cross-linguistic emotional speech corpus arising from children’s interactions with the Sony AIBO robot is presented in   ,"Another example of preliminary work based on a Wizard-of-Oz approach, this time regarding children’s interactions with the ATR Robovie robot in Japan, is presented in   ","In this paper, automatic recognition of embarrassment or pleasure of the children is demonstrated","Regarding interactive affective storytelling with robots with generation and recognition of facial expressions,    present a promising starting point","Recognition of human facial expressions is accomplished through SHORE   , as well as the Seeing Machines product FaceAPI","Other available facial expression recognition systems include   , which has also been used as an aid for autistic children, as well as   , where the output of the system is at the level of facial action coding (FACS)","Regarding generation of facial expressions for robots, some examples of current research include   ","Apart from static poses, the dynamics of facial expressions is also very important towards conveying believability; for empirical research on dynamics see for example   ","Still, compared to the wealth of available research on the same subject with virtual avatars, there is still a lag both in empirical evaluations of human–robot affective interaction, as well as in importing existing tools from avatar animation towards their use for robots","Regarding some basic supporting technologies of affect-enabled text-to-speech and speech recognition, the interested reader can refer to the general reviews by Schroeder    on TTS, and by Ververidis and Kotropoulos    on recognition",A wealth of other papers on the subject exist; with some notable developments for affective speech-enabled real-world robotic systems including   ,"Furthermore, if one moves beyond prosodic affect, to semantic content, the wide literature on sentiment analysis and shallow identification of affect applies directly; for example   ","Regarding physiological measurables, products such as Affectivas   sensor   , or techniques for measuring heart rate, breathing rate, galvanic skin response and more, could well become applicable to the human–robot affective interaction domain, of course under the caveats of   ","Yet another important question for which still many aspects remain unanswered, is concerned with conveying emotions for the case of robotic embodiments which are not anthropomorphic and do not support speech, an initial investigation of which is presented in   , and an example empirical study for the case of the emotions conveyed by a UAV can be found in   ","Also, another interesting option is concerned with utilizing non-linguistic utterances (NLU) for conveying emotion, i.e. non-verbal sounds, as is done in   , which are interpreted categorically   ","Finally, it is worth noting that significant cross-culture variation exists regarding affect; both at the generation, as well as at the understanding and situational appropriateness levels   ","In general, affective human–robot interaction is a growing field with promising results, which is expected to grow even more in the near future. 3.6 Motor correlates of speech and non-verbal communication Verbal communication in humans does not come isolated from non-verbal signs; in order to achieve even the most basic degree of naturalness, any humanoid robot needs for example at least some lip-movement-like feature to accompany speech production","Apart from lip-syncing, many other human motor actions are intertwined with speech and natural language; for example, head nods, deictic gestures, gaze movements, etc","Also, note that the term correlates is somewhat misleading; for example, the gesture channel can be more accurately described as being a complementary channel rather than a channel correlated with or just accompanying speech   ","Furthermore, we are not interested only in the generation of such actions; but also on their combination, as well as on dialogic/interactional aspects",Let us start by examining the generation of lip syncing,"The first question that arises is: should lip sync actions be generated from phoneme-level information, or is the speech soundtrack adequate? Simpler techniques, rely on the speech soundtrack only; the simplest solution being to utilize only the loudness of the soundtrack, and map directly from loudness to mouth opening","There are many shortcomings in this approach; for example, a nasal “m” usually has large apparent loudness, although in humans it is being produced with a closed mouth","Generally, the resulting lip movements of this method are perceivable unnatural","As an improvement to the above method, one can try to use spectrum matching of the soundtrack to a set of reference sounds, such as at   , or even better, a linear prediction speech model, such as   ","Furthermore, apart from the generation of lip movements, their recognition can be quite useful regarding the improvement of speech recognition performance under low signal-to-noise ratio conditions   ",There is also ample evidence that humans utilize lip information during recognition; a celebrated example is the McGurk effect   ,"The McGurk effect is an instance of so-called multi-sensory perception phenomena   , which also include other interesting cases such as the rubber hand illusion   ","Yet another important aspect of communication that requires non-verbal elements is backchannel signalling, primarily accomplished through head nods that the listener provides as feedback to the speaker while listening, in order to for example signal acknowledgement of understanding and continued attention, so that the speaker can continue to provide more verbal input to the listener",An example of a study on backchannel head nods for the case of human–robot communication is given in   ,"Now, let us move on to gestures","The simplest form of gestures which are also directly relevant to natural language are deictic gestures, pointing towards an object and usually accompanied with indexicals such as “this one!”","Such gestures have long been utilized in human–robot interaction; starting from virtual avatar systems such as Kris Thorissons Gandalf   , and continuing all the way to robots such as ACE (Autonomous City Explorer)   , a robot that was able to navigate through Munich by asking pedestrians for directions","There exist quite a number of other types of gestures, depending on the taxonomy one adopts; such as iconic gestures, symbolic gestures, etc","Furthermore, gestures are highly important towards teaching and learning in humans   ","Apart from McNeills seminal psychological work   , a definitive reference to gestures, communication, and their relation to language, albeit regarding virtual avatar Embodied Conversational Assistants (ECA), can be found in the work of Justine Cassell, including   ","Many open questions exist in this area; for example, regarding the synchronization between speech and the different non-verbal cues   , and socio-pragmatic influences on the non-verbal repertoire",Another important topic for human–robot interaction is eye gaze coordination and shared attention,"Eye gaze cues are important for coordinating collaborative tasks   , and also, eye gazes are an important subset of non-verbal communication cues that can increase efficiency and robustness in human–robot teamwork   ","Furthermore, in the tour guide setting of    a robot that engages visitors in mutual gaze is seen as more humanlike, and slight gaze preference biases towards one of the visitors can positively influence attitudes towards the robot","An attention control system for social robots that can adaptively attract and control a target persons attention is described in   , extending from the work reported in   ",The design of robot eyes in order to maximize suitability for gaze reading in investigated in   ,"Also, it is worth noting that gaze plays a significant role in tasks such as robot-to-human handovers   ","Furthermore, eye gaze is very important in disambiguating referring expressions, without the need for hand deixis   , and in shaping participant roles in conversation   ","Shared attention mechanisms develop in humans during infancy   , and Scasellati authored the pioneering work on shared attention in robots in 1996   , followed up by   ","A developmental viewpoint is also taken in   , as well as in   ","A well-cited probabilistic model of gaze imitation and shared attention is given in   , In virtual avatars, considerable work has also taken place; such as   ",Eye-gaze observations are also very important towards mind reading and theory of mind    for robots; i.e. being able to create models of the mental content and mental functions of other agents (human or robots) minds through observation,Children develop a progressively more complicated theory of mind during their childhood   ,"Elemental forms of theory of mind are very important also towards purposeful speech generation; for example, in creating referring expressions, one should ideally take into account the second-order beliefs of his conversational partner-listener; i.e. he should use his beliefs regarding what he thinks the other person believes, in order to create a referring expression that can be resolved uniquely by his listener","Furthermore, when a robot is purposefully issuing an inform statement (“there is a tomato behind you”) it should know that the human does not already know that; i.e. again an estimated model of second-order beliefs is required (i.e. what the robot believes the human believes)",A pioneering work in theory of mind for robots is Scasellatis   ,An early implementation of perspective-shifting synthetic-camera-driven second-order belief estimation for the Ripley robot is given in   ,Another example of perspective shifting with geometric reasoning for the HRP-2 humanoid is given in   ,"A major technoeconomic obstacle in the past decade regarding the widespread use of systems which can monitor and react to human gaze, and estimate human attention, has been the cost of precise wearable eye-tracking systems, and the need of placing artificial landmarks in the field of view in most of the traditional realizations of such systems","However, with recent developments, including the google glasses, the situation is rapidly changing","An innovative method for the estimation of human fixations in 3D environments that does not require artificial landmarks and enables attention mapping in 3D models with high precision, is presented in   , which might well be promising to bring forth new opportunities for studies in joint attention as well as applications for human–robot interaction","Finally, a quick note on a related field, which is recently growing",Children with Autistic Spectrum Disorders (ASD) face special communication challenges,A prominent theory regarding autism is hypothesizing theory-of-mind deficiencies for autistic individuals   ,"However, recent research    has indicated that specially-designed robots that interact with autistic children could potentially help them towards improving their communication skills, and potentially transferring over these skills to communicating not only with robots, but also with other humans",Yet another important observation to be made is concerned with the relation between the human–human and human–robot interactions,"Models arising from observing human–human interactions, can later be used as a basis in order to develop and further refine human–robot interaction","An example of a study of human non-verbal behaviours during teaching, which was made with this purpose in mind, is given in   ","Last but not least, regarding a wider overview of existing work on non-verbal communication between humans, which could readily provide ideas for future human–robot experiments, the interested reader is referred to   . 3.7 Purposeful speech and planning Traditionally, simple command-only canned-response conversational robots had dialogue systems that could be construed as stimulus–response tables: a set of verbs or command utterances were the stimuli, the responses being motor actions, with a fixed mapping between stimuli and responses","Even much more advanced systems, that can support situated language, multiple speech acts, and perspective-shifting theory-of-mind, such as Ripley   , can be construed as effectively being (stimulus, state) to response maps, where the state of the system includes the contents of the situation model of the robots",What is missing in all of these systems is an explicit modelling of purposeful behaviour towards goals,"Since the early days of AI, automated planning algorithms such as the classic STRIPS    and purposeful action selection techniques have been a core research topic","In traditional non-embodied dialogue systems practice, approaches such as Belief-Desire-Intention (BDI) have existed for a while   , and theoretical models for purposeful generation of speech acts    and computation models towards speech planning exist since more than two decades","Also, in robotics, specialized modified planning algorithms have mainly been applied towards motor action planning and path planning   , such as RRT    and Fast-Marching Squares   ","However, it is worth mentioning that the traditional approach towards planning and reasoning faces a very important problem when applied in real-world robots: a considerable amount of uncertainty exists, arising from the imperfections of the current state-of-the-art of speech and language processing as applied on robots, as well as from the multiple sources and often considerable variance of robot sensory-motor errors","Thus, special techniques are required, supporting graceful operation under considerable uncertainty: and one of the dominant mathematical approaches towards this problem involves Partially Observable Markov Decision Processes (POMDPs)","Kaelbling, Littman, and Cassandra    for the case of robot planning, and Young    have proposed the usage of such POMPDP models","Also, methods for representation and reasoning with probabilistic knowledge, such as those described in   , can play an important role towards dealing with uncertainty","Thus, such tools provide interesting avenues for wider application in real-world interactive robots, and show a highly promising direction for bridging the gap between symbolic representations and the noisy sensorymotor data of real-world robots","However, the important point to notice here is that, although considerable research exists for motor planning or dialogue planning alone, there are almost no systems and generic frameworks either for effectively combining the two, or for having mixed speech- and motor-act planning, or even better agent- and object-interaction-directed planners","Notice that motor planning and speech planning cannot be isolated from one another in real-world systems; both types of actions are often interchangeable with one another towards achieving goals, and thus should not be planned by separate subsystems which are independent of one another","For example, if a robot wants to lower its temperature, it could either say: can you kindly open the window? to a human partner (speech action), or could move its body, approach the window, and close it (motor action)","An exemption to this research void of mixed speech-motor planning is   , where a basic purposeful action selection system for question generation or active sensing act generation is described, and implemented on a real conversation robot","However, this is an early and quite task-specific system, and thus much more remains to be done towards real-world general mixed speech act and motor act action selection and planning for robots. 3.8 Multi-level learning Yet another challenge towards fluid verbal and non-verbal human–robot communication is concerned with learning   ","But when could learning take place, and what could be and should be learnt? Let us start by examining the when","Data-driven learning can happen at various stages of the lifetime of a system: it could either take place (a) initially and offline, at design time; or, it could take place (b) during special learning sessions, where specific aspects and parameters of the system are renewed; or, (c) it could take place during normal operation of the system, in either a human-directed manner, or ideally (d) through robot-initiated active learning during normal operation","Most current systems that exhibit learning, are actually involving offline learning, i.e. case (a) from above","No systems in the literature have exhibited non-trivial online, real-world continuous learning of communications abilities","The second aspect beyond the when, is the what of learning","What could be ideally, what could be practically, and what should be learnt, instead of pre-coded, when it comes to human–robot communication? For example, when it comes to natural-language communication, multiple layers exist: the phonological, the morphological, the syntactic, the semantic, the pragmatic, the dialogic","And if one adds the complexity of having to address the symbol grounding problem, a robot needs to have models of grounded meaning, too, in a certain target space, for example in a sensorymotor or a teleological target space",This was already discussed in the previous sections of normative vs. empirical meaning and on symbol grounding,"Furthermore, such models might need to be adjustable on the fly; as discussed in the section on online negotiation of meaning","Also, many different aspects of non-verbal communication, from facial expressions to gestures to turn-taking, could ideally be learnable in real operation, even more so for the future case of robots needing to adapt to cultural and individual variations in non-verbal communications","Regarding motor aspects of such non-verbal cues, existing methods in imitation and demonstration learning    have been and could further be readily adapted; see for example the imitation learning of human facial expressions for the Leonardo robot   ","Finally, another important caveat needs to be spelt out at this point","Real-world learning and real-world data collection towards communicative behaviour learning for robots, depending on the data set size required, might require many hours of uninterrupted operation daily by numerous robots: a requirement which is quite unrealistic for today’s systems","Therefore, other avenues need to be sought towards acquiring such data sets; and crowdsourcing through specially designed online games offers a realistic potential solution, as mentioned in the previous paragraph on real-world acquisition of large-scale models of grounding","And of course, the learning content of such systems can move beyond grounded meaning models, to a wider range of the what that could be potentially learnable","A relevant example from a non-embodied setting comes from   , where a chatterbot acquired interaction capabilities through massive observation and interaction with humans in chat rooms","Of course, there do exist inherent limitations in such online systems, even for the case of the robot-tailored online games such as   ; for example, the non-physicality of the interaction presents specific obstacles and biases","Being able to extend this promising avenue towards wider massive data-driven models, and to demonstrate massive transfer of learning from the online systems to real-world physical robots, is thus an important research avenue for the future. 3.9 Utilization of online resources and services Yet another interesting avenue towards enhanced human–robot communication that has opened up recently is the following: as more and more robots nowadays can be constantly connected to the internet, not all data and programs that the robot uses need to be onboard its hardware","Therefore, a robot could potentially utilize online information as well as online services, in order to enhance its communication abilities","Thus, the intelligence of the robot is partially offloaded to the internet; and potentially, thousands of programs and/or humans could be providing part of its intelligence, even in real-time","For example, going much beyond traditional cloud robotics   , in the human–robot cloud proposal   , one could construct on-demand and on-the-fly distributed robots with human and machine sensing, actuation, and processing components","Beyond these highly promising glimpses of a possible future, there exist a number of implemented systems that utilize information and/or services from the internet","A prime example is Facebots, which are physical robots that utilize and publish information on Facebook towards enhancing long-term human–robot interaction, are described in   ","Facebots are creating shared memories and shared friends with both their physical as well as their online interaction partners, and are utilizing this information towards creating dialogues that enable the creation of a longer-lasting relationship between the robot and its human partners, thus reversing the quick withdrawal of the novelty effects of long-term HRI reported in   ","Also, as reported in   , the multilingual conversational robot Ibn Sina   , has made use of online google translate services, as well as wikipedia information for its dialogues","Furthermore, one could readily utilize online high-quality speech recognition and text-to-speech services for human–robot communication, such as [Sonic Cloud online services], in order not to sacrifice onboard computational resources","Also, quite importantly, there exists the European project Roboearth   , which is described as a World Wide Web for robots: a giant network and database repository where robots can share information and learn from each other about their behaviour and their environment","Bringing a new meaning to the phrase experience is the best teacher, the goal of RoboEarth is to allow robotic systems to benefit from the experience of other robots, paving the way for rapid advances in machine cognition and behaviour, and ultimately, for more subtle and sophisticated human–machine interaction","Rapyuta   , which is the cloud engine of Roboearth, claims to make immense computational power available to robots connected to it","Of course, beyond what has been utilized so far, there are many other possible sources of information and/or services on the internet to be exploited; and thus much more remains to be done in the near future in this direction. 3.10 Miscellaneous abilities Beyond the nine desiderata examined so far, there exist a number of other abilities that are required towards fluid and general human–robot communication","These have to do with dealing with multiple conversational partners in a discussion, with support for multilingual capabilities, and with generating and recognizing natural language across multiple modalities: for example not only acoustic, but also in written form","In more detail: 
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      3.10.1 Multiple conversational partners Regarding conversational turn-taking, in the words of Sacks   , the organization of taking turns to talk is fundamental to conversation, as well as to other speech-exchange systems, and this readily carries over to human–robot conversations, and becomes especially important in the case of dialogues with multiple conversation partners",Recognition of overlapping speech is also quite important towards turn-taking   ,"Regarding turn-taking in robots, a computational strategy for robots participating in group conversation is presented in   , and the very important role of gaze cues in turn taking and participant role assignment in human–robot conversations is examined in   ","In   , an experimental study using the robot Simon is reported, which is aiming towards showing that the implementation of certain turn-taking cues can make interaction with a robot easier and more efficient for humans",Head movements are also very important in turn-taking; the role of which in keeping engagement in an interaction is explored in   ,Yet another requirement for fluid multi-partner conversations is sound-source localization and speaker identification,"Sound source localization is usually accomplished using microphone arrays, such as the robotic system in   ",An approach utilizing scattering theory for sound source localization in robots is described in    and approaches using beamforming for multiple moving sources are presented in   ,"Finally, HARK, an open-source robot audition system supporting three simultaneous speakers, is presented in   ","Speaker identification is an old problem; classic approaches utilize Gaussian mixture models, such as   ","Robotic systems able to identify their speakers identity include   , as well as the well-cited   ","Also, an important idea towards effective signal separation between multiple speaker sources in order to aid in recognition, is to utilize both visual as well as auditory information towards that goal","Classic examples of such approaches include   , as well as   ","Of course, special considerations are needed not only in the case of multiple verbal conversational partners, but also multiple interactive communication partners at large, also covering non-verbal components",An example of an assisted care robot serving tea to the elderly which is able to gracefully deal with requests from multiple individuals simultaneously is presented in   . 3.10.2 Multilingual capabilities and mutimodal natural language Yet another desirable ability for human–robot communication is multilinguality,"Multilingual robots could not only communicate with a wider range of people, especially in multi-cultural societies and settings such as museums, but could very importantly also act as translators and mediators","Although there has been considerable progress towards non-embodied multilingual dialogue systems   , and multi-lingual virtual avatars do exist   , the only implemented real-world multilingual physical android robot so far reported in the literature is   ","Finally, let us move on to examining multiple modalities for the generation and recognition of natural language","Apart from a wealth of existing research on automated production and recognition of sign language for the deaf (ASL)   , systems directly adaptable to robots also exist   ",One could also investigate the intersection between human writing and robotics,"Again, a wealth of approaches exist for the problem of optical character recognition and handwriting recognition   , even for languages such as Arabic   , the only robotic system that has demonstrated limited OCR capabilities is   ","Last but not least, another modality available for natural language communication for robots is internet chat",The only reported system so far that could perform dialogues both physically as well as through facebook chat is   ,"As a big part of human knowledge, information, as well as real-world communication is taking place either through writing or through such electronic channels, inevitably more and more systems in the future will have corresponding abilities","Thus, robots will be able to more fluidly integrate within human societies and environments, and ideally will be enabled to utilize the services offered within such networks for humans","Most importantly, robots might also one day become able to help maintain and improve the physical human–robot social networks they reside within towards the benefit of the common good of all, as is advocated in   . 4 Discussion From our detailed examination of the ten desiderata, what follows first is that although we have moved beyond the canned-commands-only, canned responses state-of-affairs of the nineties, we seem to be still far from our goal of fluid and natural verbal and non-verbal communication between humans and robots",But what is missing? Many promising future directions were mentioned in the preceding sections,"Apart from clearly open avenues for projects in a number of areas, such as composition of grounded semantics, online negotiation of meaning, affective interaction and closed-loop affective dialogue, mixed speech-motor planning, massive acquisition of data-driven models for human–robot communication through crowd-sourced online games, real-time exploitation of online information and services for enhanced human–robot communication, many more open areas exist","What we speculate might really make a difference, though, is the availability of massive real-world data, in order to drive further data-driven models","And in order to reach that state, a number of robots need to start getting deployed, even if in partially autonomous partially remote-human-operated mode, in real-world interactive application settings with round-the-clock operation: be it shopping mall assistants, receptionists, museum robots, or companions, the application domains that will bring out human–robot communication to the world in more massive proportions, remain yet to be discovered","However, given recent developments, it does not seem to be so far away anymore; and thus, in the coming decades, the days might well come when interactive robots will start being part of our everyday lives, in seemless harmonious symbiosis, hopefully helping to create a better and exciting future. 5 Conclusions An overview of research in human–robot interactive communication was presented, covering verbal as well as non-verbal aspects","Following a historical introduction reaching from roots in antiquity to well into the nineties, and motivation towards fluid human–robot communication, ten desiderata were proposed, which provided an organizational axis both of recent as well as of future research on human–robot communication","Then, the ten desiderata were explained, relevant research was examined in detail, culminating to a unifying discussion","In conclusion, although almost twenty-five years in human–robot interactive communication exist, and significant progress has been achieved in many fronts, many sub-problems towards fluid verbal and non-verbal human–robot communication remain yet unsolved, and present highly promising and exciting avenues towards research in the near future.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s1071581916300453, 1 Introduction Information and Communication Technology (ICT) offers many potential possibilities to teachers for creating educational activities that students can perform in the classroom. “The Survey of Schools: ICT in Education” ( ) sheds light on the use of ICT in the classroom,"First, most students think that using computers for learning is interesting, they are motivated to learn with them, and they are used to performing tasks with electronic devices at home","Second, school heads and teachers agree about the relevance of ICT use in different learning activities and acknowledge that technologies motivate students","However, despite the significant differences between EU countries regarding ICT and electronic equipment use in the classrooms, “on average across the EU countries […], only between 20 and 25% of students are taught by digitally confident and supportive teachers having high access to ICT and facing low obstacles to their use at school.” This information points out the need for teachers who are confident and supportive in effectively using ICT infrastructure and exploiting their potential","In agreement with  , we believe that this poor use of technology in the classroom is mainly due to teacher limited knowledge of existing technological resources","This problem is worsened by the severe scarcity of educational tools (although this is slowly being solved by the inclusion of tablets in classrooms), as well as the rigidity of available tools, which do not comply with teacher needs in most cases ( )",One approach to this issue is to steer development of applications specifically designed to support teaching in a particular area of knowledge,"However, developing a final application is a daunting task involving a costly and time-consuming process of analysis, planning, coding, and testing (and later support) to target a particular need",Targeting needs poses a new problem of applications not being adaptable enough to teacher needs ( ),"In this situation, technical expertize is needed to add new requirements to a certain application, which will require teachers to have a strong technological background or technical support","Usually, teachers do not have technical skills, and since having technical support is commonly expensive, teachers will end up abandoning ICT ( )",Putting the creative power in the teacher hands requires authoring tools with which teachers can continuously and smoothly improve the creation process,"In this study, we aim to highlight the factors affecting the learnability of an authoring tool, which is understood as “the capability of the software product to enable the user to learn its application” (ISO/IEC 9126-1, 2000)","As   suggested, we strongly believe that learnability is one of the critical factors for making teachers adopt a specific technology","In designing an authoring tool, we should improve the familiarity (“the extent to which a user's knowledge and experience in other domains can be applied when interacting with a new system”), the predictability (“support for the user to determine the effect of future action based on past interaction history”), and the generalizability (“support for the user to extend knowledge of specific interaction within and across applications to other similar situations”) ( )","In this sense, learnability can be optimized by measuring the teacher's performance in designing educational activities over time, which can allow us to identify flaws or characteristics of the developed tool and improve it to ease the learning curve",Teachers show different learning curves when using different authoring tools that are affected by both the user interface design ( ) and the teacher's background skills and prior knowledge ( ),"Thus, design decisions will affect how easy teachers find the tool upon first use, how fast they acquire skills, and how competent they become after using the tool for a fair amount of time",These effects can in turn be observed in the learning curve of each user,"We were thus motivated to design and implement DEDOS-Editor, a novel authoring tool meant to provide a consistent design metaphor to smooth the learning curve for teachers","Our approach capitalizes on the creativity and expertize of teachers in the creation process, as well as the group dimension of the educational system by allowing teachers to access, use, and build over content developed by other teachers","In the design of DEDOS-Editor, we focused on improving learnability through a direct-manipulation interaction style ( ) and a consistent interaction metaphor ( ) for the creation of educational activities","We started with the following questions: 
                   To test our hypotheses, we performed a controlled experimental study with bachelor's students aged 21–30 years old in primary education and early childhood education (future teachers) at Universidad Rey Juan Carlos, Spain",The experiment compares DEDOS-Editor with an authoring tool that can be seen as a fair example of those available,"After reviewing the state of the art, we chose JClic-Author ( ), which is the most popular educational creation tool for personal computers and digital blackboards in Spain","Both tools were compared to collect information about their learnability, which helps us to comprehend teacher struggles when first facing a new tool in their classrooms and the main reasons they stop using them",This paper is structured as follows,"In  , we analyze the current educational authoring tools and whether they support collaborative learning using different devices.   discusses how design principles affect the learning curve and which designing principles we have followed.   detail the experimental study, the characteristics of the participants, the methodology, and the results","Finally,   summarize the discussion and conclude the work. 2 Related work The most frequent ICT-based activities in the European Union are related to the preparation of educational activities, creating digital resources, or using a virtual learning environment ( )","However, the resources most used are multimedia tools such as PowerPoint, exercise software, online quizzes/tests, and broadcasts (podcasts, Youtube), among others","Therefore, available tools and devices are not being fully exploited",We strongly believe that this is due to developers overlooking the importance of the learning curve of certain applications,Learning applications should consider short-term efforts and long-term limitations ( ) for users to gain knowledge of the application quickly while avoiding boredom for experts,We go deeper into this topic in  ,"In exercise software, teachers use special authoring tools to create interactive learning activities for students to use through PCs or digital blackboards","In addition, teachers can easily share the created contents with these applications, receive feedback from other teachers, and improve the learning content provided to the academic community ( )","We looked at the design paradigms of the most popular examples that had any kind of evaluation behind them ( ), and we have found that the menu-based interaction style is widespread (see  
                      )","A prototypical example may be JClic ( ), which allows for different types of educational activities, such as puzzles, associations, text activities, and crosswords",Activities are grouped in projects,"Its interface is based on a menu, and it has four main tabs: “Project” for defining general features; “Media Library,” where the teacher must provide all the multimedia contents such as images, audio, or video; “Activities,” where the teacher selects the next activity to be created or updates previously defined ones; and “Sequences,” where the order to accomplish the activities is specified","A different set of options is displayed, depending on the type of activity selected",Hot Potatoes is another project with a similar approach ( ),"It requires previous knowledge related to web design or the use of image manipulation programs if teachers want to include images of different sizes (e.g., GIMP, Photoshop, or Paint)",Smart Notebook ( ) uses an alternative approach where activities are designed by dragging and dropping visual elements or by directly painting on the screen,"However, the activity creation is not consistent across different types of activities, since each type must be selected in a menu and different options appear in each element to configure the corresponding activity","In the same line of dragging and dropping, the Mouse Mischief add-on ( ) harnesses the potential of the PowerPoint interaction paradigm to turn it into a simple but powerful authoring tool","However, it has a small variety of activities","Authoring exercise functionality can also be found embedded in tools allowing teachers to create course-based learning environments, where activities are designed to be executed as part of a course","A fair representative of this class of tools is Microsoft LCDS ( ), which helps teachers in creating learning environments that will be executed as computer-based training applications by specifying their own pedagogical principles","Other projects that focus on authoring courses are Xerte ( ), REDEEM ( ), Bookbuilder ( ), Cognitor ( ), and Quandary ( )",Such applications are based on configuring a course across multiple menus and forms,"In these tools, activities are organized according to a course structure rather than a project structure","Therefore, the output combines educational material with activities in which content creation occupies a significant portion of the design process",Interactive activities are designed in a similar way to JClic or Hot Potatoes,"However, being more ambitious in scope, they tend to focus on professional content creators rather than on teachers with time constraints and a basic computer background in most cases","This impedes the learnability of the tool, making help mechanisms and virtual assistants critical for the tool's adoption","For instance, REDEEM assists in the creation of a learning environment by guiding the user through a question and answer process","This allows teachers to design complex tasks with fairly simple actions, although it may be a time-consuming process","Likewise, LAMS also facilitates the definition of course structures by using direct manipulation, but being oriented to multimedia content creation, activity types are limited to questions and answers ( )",One relevant aspect usually forgotten when analyzing learnability is variation in teacher computer skills,"An interesting approximation that accounts for these differences is JeLSIM toolkit ( ), a software suite for easily creating and customizing physics and math simulations",The toolkit shows different interfaces according to the user's computer proficiency,"To create identical simulations, a Java programmer has access to a programming interface, whereas a teacher without proper programming skills can use a graph-based graphical interface to simplify the creation and simulation process","The variety of user interfaces is very interesting from the point of view of learnability, although it makes it more complicated to jump from one level of expertize to the next, and the implementation might have benefited from including an intermediate proficiency level","Apart from the interaction mechanism, another issue is that few tools offer the possibility of creating activities that multiple students can solve at the same time and on the same device","For example, SMART Notebook activities are intended to be performed by a single student, despite the fact that more than one can interact with the digital blackboard to do the activity","If teachers want students to perform the same activity in different workspaces, they should replicate the activity for each student","The groupware approach of Mouse Mischief allows interaction with multiple pointers in a single display, although its approach is constrained to the use of mouse devices","LAMS provides the possibility of performing collaborative activities, but it is online collaboration where each user performs activities on a separate device","In summary, most tools seem to overlook the importance of their learnability","All of them partially ignore the necessity of providing a coherent and consistent mechanism for creating different types of activities, and in our experience, many of them provide clumsy user interfaces that do not reflect that somebody has to design an activity before a student performs it","Furthermore, the very few tools have been evaluated, like REDEEM or ARIES ( ), have focused on the student's performance in accomplishing the activities rather than on the teacher's performance in designing them","In the following section, we introduce the importance of learnability and the design principles we have followed to improve learnability in DEDOS-Editor. 3 Learning curve and design principles The design of computer-supported pedagogically sound group-learning activities still remains a complex task that requires both technological and pedagogical skills and knowledge","To manage these multidisciplinary needs, it is common for pedagogical teams to join efforts with technological ones, which leads to a completely new set of problems from a designer's point of view","These problems can be summarized in two main design requirements: flexibility and ease of use. 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   3.1 Learning curve It is necessary to consider the learning curve when a computer tool is designed","As   pointed out, based on the optimal flow of motivation in learning ( ), anxiety occurs when challenges exceed user abilities, while boredom arises if skills are above the challenges that can be tackled with the tool","That is, limited knowledge must suffice to do simple things, while advanced knowledge should gradually lead to increasing construction power",This means paying particular attention to short-term efforts and long-term limitations ( ) to reduce newcomer anxiety while avoiding expert boredom,"Therefore, it is important to minimize both the effort required to begin using a tool as well as its long-term limitations, which would allow users to tackle increasingly complex challenges as they gain knowledge and experience. 
                          identified certain factors that kept teachers from fully embracing computers as teaching tools: a varied set of comfort levels with computers, strong doubts about computers as good pedagogical tools, lack of training, and the time required to understand and use computer tools ( )","This explains why some teachers shy away from computers and authoring applications with unintuitive interfaces, despite the possible quality of the outcome",A key remark that can be extracted from these works is the importance of the time it takes to accomplish a task with a technology (or rather the perceived time) in regard to the general willingness to adopt it as a pedagogical tool by teachers,Obtaining an optimal flow during the learning process of a technological tool requires consideration of the variety of users and their different needs and motivations,"Many factors influence how people think and reason, their knowledge, and their skills","Consequently, when designing a user-centered system, it is crucial to account for these variations ( )","In the end, this is a problem of diversity","Users can be classified into four categories according to  : i) novice users with little or no knowledge of a tool, ii) regular users with basic knowledge of a tool, iii) advanced users who can perform some technical tasks, and iv) expert users or professionals with deep knowledge of the tool","Ideally, users would be able to increase their knowledge over time and move to higher categories","However, several fundamental barriers impede this progression","First, as mentioned, excessively complicated basic use of the tool can lead to early abandonment by novice and regular users, in the same way that a limited functionality may drive away more experienced users","Second, since not every user has the same needs, users may decide to remain in a particular category that is complex enough to satisfy their needs and feel no need to learn more advanced functionalities",Interface design can tackle these issues by hiding functionality to increase simplicity but making it easy for users with more expertize to find these functions to maintain constructive power,There are two immediate issues arising from the user evolution process,"Firstly, users abandon the tool, mainly due to difficulty or limited functionality",Both issues should be considered during the design process,The approach of discoverable interfaces showing functionality as required is thus one of the most important aspects of our design process,"Secondly, not every user ends up being proficient and many stall in a particular category","This is a result of user idiosyncrasy, motivation, and needs","Thus, if the authoring tool satisfies the user's expectations, there is no need for improvement in this respect","In other words, there is a unique critical threshold at which the cost of further learning is higher than the benefits it brings",Reusing expert knowledge and feedback is an additional tool to minimize the learning cost of less-skilled users who have reached their personal learning threshold,"For instance, in a word processor example, advanced or regular users reuse macros created by proficient users","In summary, these issues arise from the diversity of users, and therefore, it is crucial to keep in mind that users approach tools in very different ways. 3.2 Searching for regularities in different types of educational activities One challenge for maintaining an optimal flow of learning is finding a consistent metaphor that eases the reuse of knowledge when using the tool",We wanted to identify which types of educational activities are used most often to devise a metaphor that encompasses their common characteristics in them,"First, we reviewed the state-of-the-art authoring tools (see  ) and classified the type of activities they support",We found that all of them support multiple choice activities except ARIES (a system for building augmented-reality interactive learning environments),"Pair-matching activities are the next most popular type of activity (provided by five tools), followed by “fill in the gap” activities","Although less common, other types of activities can be found, such as word ordering, puzzles, crosswords, and memory games","In addition, we conducted a study to determine the most popular among the educational projects designed by professional teachers using JClic","The JClic website has educational projects for different levels ranging from early childhood to university, and we randomly selected a sample of 189","Among these, 84 early childhood projects were included from a total of 284, while 105 were primary school projects from a total of 928",We analyzed 2586 activities from early childhood projects and 3471 activities from primary school projects and identified 14 different types,"The size of each sample ( ) was calculated as follows: where   is the population size,   is the standard deviation,   is the z-score associated with the desired confidence level, and ME is the margin of error",N was set as the total number of projects (284 and 928 for each case),"We assumed a   of 0.5 since it is unknown in both cases.   was calculated as 1.96 from a confidence level of 95%, and we chose a margin of error of 9%. 
                         
                          summarizes the activity distribution of the analyzed educational projects created by professional teachers","The most popular activity was pair matching (33% in early childhood projects and 26% in primary school projects), closely followed by multiple choice activities (26% and 21%, respectively)","In summary, it seems that teachers prefer information, exploration, multiple choice, and pair-matching activities, which cover 78% of early childhood projects activities and 66% of primary school project activities","We developed DEDOS-Editor with this information in mind, focusing first on the design of multiple choice and pair-matching activities and providing a consistent interaction metaphor when creating them","This will give teachers a tool that will allow them to design the most popular activities without too much effort. 4 Design principles of Dedos-Editor Based on the considerations described in  , we decided to use a card-based metaphor to implement the design principles of direct manipulation and consistency","In DEDOS-Editor, each activity is inspired by a card-based game in which the teacher decides the card content and the goals to accomplish (see  
                      )",Cards are virtual objects that resemble their physical counterparts and are used similarly by dragging and dropping,One immediate advantage is that novice teachers can use their real world-knowledge for guessing how to define an educational activity,"For example, an activity's layout is configured by dragging cards from a toolbar and dropping them on the board",Continuous feedback from the actions is visually reflected on the screen to facilitate understanding of the creation process and its progress,These features increase the predictability of the next action and reduce the effort in progressing with the tool,Both multiple choice and pair-matching activities are consistently defined in DEDOS-Editor using cards (tokens),"For multiple choice, this is done by choosing which tokens are correct answers to the activity by dragging and dropping a target on a card, while in pair matching tokens are connected with arrows (which are also placed by dragging and dropping)","These tokens can be placed in a zone, which acts as a container for all the tokens and targets that are dropped in it","We experimentally evaluated whether this metaphor is flexible enough to incorporate completely different activities in other tools, like word searches, crosswords, fill in the gap, memory activities, and so on","We believe that reusing concepts, elements, and operations provides an overall consistency that eases the creation of new types of activities by enabling reuse of the most needed concepts, as well as growing confidence toward mastery by eliminating conceptual jumps and strengthening a philosophy of “the more I know, the easier it will be to learn what I do not.” We gauged whether DEDOS-Editor provides internal consistency in the creation metaphor","By reusing concepts, layouts, and actions in different types of activities, we intend to allow teachers to extend their goals easily to more complex tasks, which improves the learnability of the tool","In addition, we measured whether reuse improves the learning curve of DEDOS-Editor","Taking into account user diversity ( ), the tool was designed to hide advanced configuration options from new users instead of showing the full potential of the tool right away","Advanced options are hidden on the back of the cards, which can be flipped to reveal the information","As teachers acquire experience with the use of the tool, they end up discovering these options to fine tune designs",This strategy avoids overloading new users and moderates the process of discovering new functionality and gaining competency,Logical constraints preventing novice users from making mistakes are also embedded in the user interface due to lack of knowledge,The next section explains the experimental study for evaluating the tool and learning paradigm in comparison with other tools based on menus,"DEDOS-Editor is also described in more detail. 5 Validation We experimentally measured the intuitiveness, progression, and retention of DEDOS-Editor (i.e., the ease of maintaining proficiency in the tool after a period of not using it)",The experiment was also designed to compare with JClic-Author,The experiment consisted of three sessions in which participants had to design different types of educational activities,"Throughout all sessions, observers recorded both user issues and the time it took to finish activities","The next sections present the features of the participants and the experimental methodology. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   5.1 Participants The experimental study was conducted with 42 students aged 21 to 30 years old and majoring in either early childhood education or primary education at the Universidad Rey Juan Carlos","Participation was voluntary with no monetary compensation, but participants were awarded equivalent course points for the time spent","All students had previous knowledge of JClic-Author as part of their basic training for making educational projects, but they all were using DEDOS-Editor for the first time","Since they had previous knowledge of JClic-Author, we could expect good performance with the tool, so we could measure whether DEDOS-Editor is learnable enough to reach the same competence","Participants were divided into two groups, one beginning the experimental study with JClic-Author and ending with, while the other started with DEDOS-Editor","The participant distribution is described in  
                          along with the number of participants in each group and the order of the tools used in each session",Groups were balanced to have a similar number of students,Degree type was not considered since both involve the same technological training. 5.2 Methodology Each subject was asked to design different educational activities in three sessions,"The first two sessions were done in one day, and students came back two days later to perform the third session",We separated the third session from the first and second since learnability is also measured by the knowledge retention,Information on how to use the tools was provided incrementally at the beginning of each session without giving any additional help for the rest of the sessions,"After the three sessions, participants gave comments so we could gather their impressions on both tools",Details on the provided information can be found later subsections for each session,Educational activities consisted of simple multiple choice or pair-matching activities,Participants had to design each activity in less than five minutes,"If time ran out, participants could not finish the activity afterwards and they were told to start designing the next activity","We decided to use this time limit since it was about twice the time needed for the activities by the developers of this tool, who are experts in creating educational activities with DEDOS-Editor and JClic-Author",Activities became complex throughout the sessions,All of the images that are required to build the activities were provided before starting the experiment,We did not want the participants to waste time looking for them since it would not affect the focus of our research,"Participants were first asked to design two multiple choice activities with one tool and then with the other tool, followed by two pair-matching activities","One group used DEDOS-Editor first and JClic-Author next, which was reversed in the other group","Participants had five minutes to finish each activity before moving to the next one, and before designing each activity, we showed them how the final result should look.  
                          shows an example of the activity distribution throughout the sessions and the information given to the participants","The increasing difficulty made participants think about the spatial arrangement of the elements, since the clarity of the content could affect student learning",The activity content was the same for both tools,"Although more types of activities are possible, we focused on only the two most popular for JClic-Author","The previous experience with JClic-Author helped strengthen the results should performance be better with DEDOS-Editor. 
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                      5.2.1 Session 1 No details were given about the tools in the first session other than where to find the main menus and elements",The first two activities of this session were multiple choice activities consisting of a text area stating the problem and three choices with only one correct answer,"In the first activity, under the question, “Which of these animals lays eggs?” three graphical choices are to be shown using images of a gorilla, an elephant, and a turtle","In the second one, under the question, “Which of these countries does not belong to the European Union?” three text choices have to be provided for Canada, Germany, and Spain",Using DEDOS-Editor in the first activity requires dragging and dropping one zone in the editing panel,The goal of the zones is to group cards,"In this example, students must combine one game zone with one player zone",Individual zones will be replicated if there are two or more students doing the activity,"However, collaborative zones are shared and located in the middle of the physical space","When the zones are added, students place a text token where they type the statement in the game zone and place the three image tokens for the answers in the player zone","Finally, users have to drag and drop a choice icon on the turtle token (the correct answer)","One example of using collaborative and individual zones can be seen in  
                            ","In the second multiple choice activity, the creation steps are the same as in the previous activity with the exception that text tokens have to be used instead of image tokens","In JClic-Author, multiple choice activities are called “identification activities.” The first step is to find and select this type of activity in an initial menu","Then, in the “Panel” tab, users can set the number of desired answers and the way they will appear (one row and three columns in this case) for creating an activity with similar look and feel to that in the other tool",This process creates a matrix that can be filled cell by cell by adding the appropriate images and setting the correct answer in the “Relations” tab,"Finally, the question statement has to be set in the “Messages” tab","An example of the result can be seen in  
                            ","As with DEDOS-Editor, JClic-Author allows the user to add text to the cells instead of images, which can be used for creating the second multiple choice activity",The two pair-matching activities in the second part were an image-to-image activity to associate three types of products with a corresponding recycling bin and an image-to-text activity to associate three characters with their corresponding profession,"In DEDOS-Editor, participants again have to drag a game zone and a player zone into the editing panel","In the game zone, they have to add the statement and three tokens that will be used as the destination of the tokens placed in the player zone",Participants should then place a pair-matching icon on each of the tokens and move the arrow that appears over the element intended for pairing,"As with multiple choice activities, participants can use either text or image tokens interchangeably.  
                             presents a recreation of one of the pair-matching activities designed with DEDOS-Editor where the relationships are visually represented by arrows between two cards","In JClic-Author, these types of activities are called “complex association,” and participants need to identify and find this type of activity in the initial menu to start the activity construction","Then, participants need to set up the number of elements of the relations in the “Panel” tab and choose the position of each cell as well as the images or text they should contain","JClic-Author automatically creates a relation between each cell and the one above it, and which can be changed in the “Relations” tab","However, this process turns out to be non-trivial for many participants because the relationship between the different elements of the activity is not clear in the interface (see  
                            ). 5.2.2 Session 2 Activities in this session where slightly more difficult and included more tokens in play and more multiple choice targets","On the other hand, participants were given more details about user interfaces","In the case of DEDOS-Editor, “zone,” “token,” and “target” concepts were explained, while for JClic-Author, participants were shown how to create activities, use messages and panels, and set up targets in the “Relations” tab","After the explanations, we repeated the same methodology used in   for designing two multiple choice and two pair-matching activities with both tools","In the first multiple choice activity, we increased the numbers of tokens, images, and targets so that the user had to discover that several images can be added to the same element and that the same activity can have more than one correct answer",The second multiple choice activity required participants to use a different layout with a single column of options instead of the 2×2 matrix used in the previous one,"As in  , these activities had to be done both with DEDOS-Editor and JClic-Author","In the pair-matching activities, the number of tokens on screen was incrementally increased to force participants to manage the available space and adjust the zones and card size appropriately","There were also more elements than the first session in this case. 5.2.3 Session 3 Session 3 started with a full explanation of both tools, including how to take advantage of Windows Explorer thumbnails for easy location along with several detailed examples of solving the activities of previous sessions",This provided participants with full information to create activities in a timely manner and to make the most of both applications,Each participant had to design two multiple choice activities and pair-matching activities with both DEDOS-Editor and JClic-Author,"To analyze the skills acquired with both tools, these activities included text and image tokens, multiple choice with one and with multiple goals, and a varied number of tokens in play. 5.3 Measures One of the key points of the experimental study is the lack of information that forces users to experiment with the tool to discover its uses and possibilities",Gradually increasing the complexity also allows us to analyze how previously acquired knowledge helps the user in progressing to more complicated tasks,"These factors allow us to evaluate the learning curve regarding the entry point, learning evolution, and competence acquired","We consider the entry point as the difficulty the user finds when using a tool for the first time with little or no knowledge, which is represented by the completion rate of the first activity of  ","The learning evolution is the rate at which users acquire skills throughout use, which is represented by the slope of the completion rate curve as a user gradually masters tool activities","Finally, the acquired competence is the level of mastery a user can reach after fair use of the tool, which is represented by the last point of the curve of the completion rate from the last activity","Since all these factors strongly depend on the experience and competence of the students in using computers and educational software, we analyzed the results by dividing participants in two groups according to their performance in the last session",We assume that the computer skills did not vary throughout the experimental study and thus consider the last session as a proper proxy of this competence,"It is worth noting that data related to the user profile (e.g., course or age) did not provide any meaningful segmentation of the population","We used the k-means method to make this division, where the objective is to divide N observations into X clusters in which each observation belongs to the cluster closest to its mean","With this method, users were divided into novices (15 participants,  =165.5 s) and experts (27 participants,  =111.5 s). 
                          A chi-square test of independence was used to determine whether the order in which participants performed the experimental study affects the clustering (see Groups 1 and 2 in  )","The relation between these variables was not significant (Χ2 (1,  =42)=0.08,  =0.78; see  
                         )","Thus, the participant allocation did not interfere with the students' performance","Using this segmentation, we analyzed the evolution of novice and expert users with both tools and compared their entry points, learning curves, and competencies acquired",The next section presents the results obtained during the experimental study,"We present the ratio of multiple choice and pair-matching activities during the three sessions grouped by type of users according to their technological skills. 6 Results In analyzing the results of the sessions, we paid special attention to the completion rate of activities as a correlate of intuitiveness and ease of use by comparing the number of participants that finished each task for each tool in each session",We consider a task to be complete if the activity is designed correctly or the final result is equivalent to the requested one in less than five minutes,"Sometimes, the same activity can be designed in different ways with both tools",We have used McNamara's test to compare the proportions of completion rates using each tool,This test checks the equality of proportions using the within-subject design of the experimental study,"In subsequent figures and tables, we use the abbreviation “Sel” to refer to multiple choice activities and the abbreviation “Emp” to refer to pair-matching activities. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   6.1 Novices – multiple choice activities As expected, we observed a steadily increase throughout the sessions completion rates of novice users in multiple choice activities, which resulted from gaining familiarity through use","Although not statistically significant, the completion rate was higher with DEDOS-Editor than with JClic-Author","However, we found a discrepancy in the first multiple choice activity in the third session, where the completion rate was much lower",This activity was a multiple choice activity similar to those designed in the first and second sessions,We believe that it was caused by the poor retention of the JClic-Author interface because of the many menus that the user has to memorize to use the tool well,This factor made the participants underperform in this activity and produced a considerable drop in the completion rate,"The results can be seen in  
                          and  
                         ",This table shows the number of participants who finished the corresponding activities within the given time,"In addition, we can distinguish which participants finished the activity with DEDOS-Editor, JClic-Author, both applications, or neither of the tools","For instance, the table shows that a total of 10 participants from the novice group could not finish the first multiple choice activity with either of the tools, one participant succeeded with only JClic-Author, and four participants succeeded using only DEDOS-Editor. 6.2 Experts – multiple choice activities Expert users show a higher entry point than novices (37% and 25.9% in DEDOS-Editor compared to 26.4% and 6.7% in JClic-Author) followed by a stronger increase in the completion rate that remains over 75% for the rest of the experimental study with both tools","DEDOS-Editor seems to have higher completion rates in all activities except the second, but there is no statistical significance","Nevertheless, it is important to remember that DEDOS-Editor had not been used previously by any participant, whereas all participants had used JClic-Author during their degree","The activity with the smallest p-value was Sel3 ( =0.025), 
                          in which users finished about 20% more activities with DEDOS-Editor than with JClic-Author",This reaffirms our thoughts when analyzing the results of novice participants regarding retention for the JClic-Author interface,"Considering that users fall into the expert category, both learning curves are quite satisfactory with a reasonably high entry point, fast achievement of competence, and full competence acquired","The results can be seen in  
                          and  
                         . 6.3 Novices – pair-matching activities Pair-matching activities proved to be more difficult than the multiple choice activities","In addition, throughout the experimental study, we found that the amount of time spent by users for finding and adding images to the tokens or cells was significantly large compared to the total amount of time spent in finishing the tasks","Users had to insert from six to ten different images into the activity, and given the five-minute limitation, those who did not add the images fast enough tended to run out of time before completing the task","The learning curves of DEDOS-Editor and JClic-Author are significantly different in this case (see  
                         )","DEDOS-Editor had a higher entry point than JClic-Author, and this entry point is higher than that for novice users doing multiple choice activities using DEDOS-Editor",This can be explained by the fixed creation paradigm of DEDOS-Editor in that the experience users acquired in doing the multiple choice activities of   served as an entry point for the pair-matching activities,"In the second activity, the completion rate increased to 73.3%","In contrast, with JClic-Author, less than 10% of users were able to finish the pair-matching activities of   despite more than 50% having finished the multiple choice activities","This happened because designing multiple choice activities and pair-matching activities is fundamentally different in JClic-Author, so the experience acquired in designing the former cannot really be exploited in the latter","In addition, finding the right template to begin with is not really intuitive, and participants had serious problems with choosing the correct type of activity from the initial menu","Astonishingly, this low rate did not improve until the end of  , despite users being explicitly told which type of activity they had to choose at the beginning of the session","Overall, DEDOS-Editor shows a higher entry point followed by an immediate increase in completion rates, while JClic-Author shows a flat curve that remained close to 0 until half the experimental study was completed","In the second session with DEDOS-Editor, there was a slight decrease in the number of finished tasks","Since the first and the second sessions were performed on the same day, we believe this was caused by the large amount of images the participants had to add in   rather than by users forgetting how to use the tool","The results are summarized in  
                         ","It is worth noting that Emp8 and Emp9 are statistically significant, even after considering Bonferroni correction for multiple comparisons. 6.4 Experts – pair-matching activities The analysis of expert users with pair-matching activities shows a remarkable difference in entry points (see  
                         )","The completion rates differ by an order of magnitude in the first activity of   (74.1% with DEDOS-Editor, 7.4% with JClic-Author), which has special significance when considering that most participants had previous experience with JClic-Author while none of them had any with DEDOS-Editor","As shown in  
                         , these significant differences between tools are visible during the first two sessions","Although applying the Bonferroni adjustment to Emp7, Emp,8 and Emp10 makes them not statistically significant, along with Emp9, they constitute more than half of the remaining tests showing a real effect","In this case, it seems students need the full explanation for JClic-Author to create pair-matching activities",Two factors can explain these results,"First, the DEDOS-Editor user interface is based on a direct manipulation paradigm that avoids menus, right clicks, and other commonly used computer paradigms in favor of a more direct and visual representation of options and actions","With this paradigm, intuitiveness plays a very important role in facing unknown situations so that solutions can be constructed by instinct without the proper knowledge or specific training","Second, the DEDOS-Editor user interface is designed as a holistic experience in which all possible designs emanate from common principles","There is no single choice that can constrain the possibilities of a future design, and every new type of design needs only a small amount of new concepts",This allows users to take advantage of a variety of common concepts used in every possible design,"Therefore, completion rates were similar between the first instances of both designing pair-matching activities and multiple choice activities with JClic-Author","In contrast, DEDOS-Editor users showed similar completion rates to the last time they designed multiple choice activities since they could use all the knowledge and experience gained in designing one type of activity when facing a new type of challenge","Besides the difference in entry points, users with both tools display a satisfactory growing learning curve","In the case of DEDOS-Editor, we observed a slight drop in completion rate at the beginning of session 2 due to the increased number of the images they had to add, but the rate returned to 100% once users mastered image insertion","In JClic-Author, completion rates do not exceed 75% until the last session due to the extremely low entry point, despite the monotonic growth of the learning curves. 7 Discussion DEDOS-Editor and JClic-Author were designed with the intention of providing teachers with powerful tools to improve the way they transfer knowledge to the students",Both tools let teachers to design their own educational projects composed of learning activities that will be solved by students either individually or collaboratively,Teachers can design content for almost any topic and address all kinds of students,"However, it seems that there are certain factors that prevent the full adoption of ICT by teachers","In this section, we discuss our experimental results to shed light on those factors that could affect adoption.  
                       shows the completion rate of both multiple choice and pair-matching activities for all users in the order in which they were performed",Vertical lines delimit each session,"In general, the completion rates with DEDOS-Editor are always above those achieved with JClic-Author","In addition, both learning curves eventually show close to 100% completion rates through use and training, although entry points are below 40%","It is important that we did not provide any kind of mentoring with DEDOS-Editor at the beginning of the experimental study, even though it is highly advisable for improving teacher acceptance and knowledge when facing an authoring tool for the first time ( )","However, students should have full knowledge of JClic-Author and a positive attitude towards this tool since learning it is part of their degree","According to  , participants should be more skilled with JClic-Author since they have had previous access to it and most of had used it in real classrooms during their internship placements","Nevertheless, our experimental data shows that having previous knowledge in a certain tool does not guarantee proper use of it in the future, which could prevent full adoption ( )","Although the entry points are quite similar with both tools, the difference in their learning curves is nonetheless impressive","DEDOS-Editor shows a smoother learning curve with small fluctuations when changing the type of activities, while JClic-Author displays shows drops when changing activity","As we have seen in previous sections, these drops do not occur when analyzing the learning curves of both types of activities separately","Nevertheless, due to the characteristics of JClic-Author, the knowledge gained when designing multiple choice activities cannot be used when designing pair-matching activities (“I do not even know where to start” 
                      )","Thus, when plotting both curves together (see  ), there are drastic drops corresponding to the learning curve intersections","As a holistic tool, knowledge and experience in one type of activity help boost performance in any other type in DEDOS-Editor (“Designing pair-matching activities with DEDOS seems way easier than with JClic”)","This smoothens the learning curve as a whole, as shown in the upward trend throughout all sessions with DEDOS-Editor","We also believe that as the activities become more complicated to design (e.g., multiple choice activities are easier to design than pair-matching activities), the difference in design paradigms becomes more noticeable","We estimated the complexity of both tools according to Murray's variables ( ) and discovered that the main problem of current authoring tools could be the great number of elements and functionality offered, which increases the difficulty when trying to manage all of them","In DEDOS-Editor, there are few elements that can be dropped into the editing area to create the activities, while JClic-Author has a more complex structure where elements have more parameters that must be learned to properly use the tool (“JClic has a lot of confusing options”)","In contrast, our experiment showed that a little extra help allows users to completely master DEDOS-Editor in the last session, reaching 100% completion rate in all activities (“I have learned how to use DEDOS without asking anyone”)","When analyzing expert and novice users separately, we observe that not even expert users are capable of avoiding the drastic drops with JClic-Author (see  
                      )","Advanced users with DEDOS-Editor, on the other hand, are able to master the tool at the end of   and maintain a completion rate near 100% throughout the rest of the experimental study","In addition, users are able to reach completion rates similar to that of expert users with DEDOS-Editor at the beginning of  , while they fail to do so with JClic-Author","Therefore, background knowledge was not a determinant factor in the learning curve as other authors have stated ( )","If this were the case, the JClic-Author learning curve would be smoother than that of DEDOS-Editor since participants had previous knowledge of the tool","We believe that the complexity of the JClic-Author interface does not help the participants to retain the knowledge acquired from using the tool previously ( ), making it difficult when using the tool again after some time","However, further studies should be done with more participants since each of the groups (novices and experts) had different numbers of participants, which made the expert group more sensitive to differences",DEDOS-Editor's design to favor intuition and continuous learning is done through a direct manipulation paradigm and maintaining a strong core of common concepts regardless of activity type,We believe that these features help enhance the tool's generalizability,"Despite not having the same amount of activities to design (only multiple choice, pair-matching, math, and point-connection activities), this paradigm choice has proven results in three significant outcomes, which have a close relationship with those presented by   regarding direct manipulation interfaces","First, and most importantly, the competence acquired in designing one type of activity improves the general results of doing any other type","Second, with some experience and information, novice users rapidly gain expert competence in the tool","Finally, expert users (meaning those more fluent in the use of technology rather than having any prior experience with the tool) just need to experiment with the tool for a short to master it completely, even with a complete lack of information",JClic-Author interface is based on menus and templates and offers a guided creation process in which the interface changes (more or less subtly) with user choices to offer only options relevant to the current development,"Intuitively, this type of interface may seem a really good choice in which an apparently simple initial choice restricts options to those meaningful to that choice","However, the results suggest that the initial choice is not always as simple as intended by the interface developer, leading many users astray when dealing with an option for the first time",Second and most important is the clustered user interface were a tool has mini-tools for specific tasks drastically affects learning by clustering the learning process and majorly impeding reuse of knowledge in future tasks,"To be fair, JClic-Author lets the user design a great variety of activities, so it is difficult to use the same design metaphor for all of them","However, this does not impede the developer in using the same metaphor in activities that are similar to each other (such multiple choice and pair-matching)","Besides resulting in a slower learning curve, this issue gives rise to a dangerously frustrating situation in which users with relative mastery of the tool for one particular task see themselves pushed back to square one when dealing with another slightly different one","Our experimental study was done with young students, so we cannot conclude whether the results would be the same if done with actual teachers","Nevertheless, participants did not have any previous knowledge about DEDOS-Editor and they gained expertize with this tool quickly throughout all the sessions","Thus, we could expect similar results with actual teachers, whose knowledge about DEDOS-Editor would be at least as high as the participants in this study",Our research shows how different designs aimed to solve the same tasks (multiple choice activities and pair-matching activities) can produce a different user experience,"The direct manipulation interface and the holistic, coherent creation paradigm seem to allow expert users to master the tool through just limited experience while allowing novice users to gain expert competence quickly with little information","Most importantly, the learning process allows a higher entry point due to the intuitiveness of direct manipulation and is smoother","In turn, the user take full advantage of any prior experience with the tool in dealing with new situations and thus avoid any frustrating drops in competence when facing new challenges","Although we need to perform more studies with different authoring tools, it seems that a direct manipulation interface and a coherent paradigm design help teachers to ease their workload, since they do not need to spend much time learning how to use the tool or creating the learning materials ( )","In addition, authoring tools with direct manipulation interfaces usually follow a “what you see is what you get” metaphor (e.g., Smart Notebook), which makes it easier to provide situational awareness to users and lets them clearly see the consequences of their actions",This makes the application easier to explore ( ) and therefore enhances the tool predictability,Authoring tools should focus on not only functional aspects but also on non-functional characteristics,Providing support to non-functional requirements is fundamental to allowing users of to complete their tasks ( ),Most of the authoring tools reviewed seem to focus on only functional aspects and fail to provide efficient support to users,"With our experimental study, we have made a first step towards clarifying which aspects could be relevant to teacher adoption of a certain authoring tool, showing that direct manipulation interfaces could help provide the non-functional factors that appear to be disregarded when designing new learning tools. 8 Conclusions In the last decades, technology has been steadily entering the classroom in attempts to realize the greatest shift in education since the Industrial Revolution","Not only an information revolution but also a newer interaction revolution are claiming their space in education, pushing to redefine what and how students are taught","Nevertheless, while technology is undeniably entering the classroom, the revolution is still waiting outside","As final deliverers of education, teachers have been plunged into a chaotic world of technologies and systems that are still far from their necessities and reach","With this problem in mind, many researchers in technology-enhanced learning have made efforts to design and develop authoring tools to provide teachers with the necessary means to design their own content according to (ideally) their own teaching styles, necessities, and preferences","However, there is still a gap between the availability and the use of technology",More efforts are required to engage the teaching community in filling the void of content,"While technology has received profound attention, and authoring tools exist to create for almost any application, we believe the failure of these tools is partially due to a low entry point when first used, as well as a slow and irregular progression during the learning process",Considering the most preferred activities by teachers in authoring tool design is a key factor in promoting the adoption of ICT in education,"However, further research should be done to increase the types of activities that can be created and improve the tools",This will let teachers create richer content that will help students in their learning processes,We have aimed to shed light onto factors affecting the learning curve of an authoring tool and the final determinant of its adoption,"While several tools are available, very few have been rigorously evaluated",We have presented an experimental study performed by future teachers (students of early childhood education and primary education degrees) and compared their interaction paradigms,"A menu-based and inconsistent user interface design is the basis of JClic-Author, the most used Spanish authoring tool, whereas a direct-manipulation interaction style and a consistent interaction metaphor were implemented in DEDOS-Editor, a novel authoring tool for traditional computers and multitouch surfaces","DEDOS-Editor turns teachers into developers, allowing them to create learning individual and collaborative activities or to change existing ones in an easy way",The experimental study showed that the user interface design choices have a tremendous impact on the learning curves of the tools,"The direct-manipulation interface strongly affects the entry point, allowing completely new users to complete more tasks and gain knowledge through experience than with a menu-based user interface","In addition, the consistent interaction metaphor allows experience and expertize to persist through domains, resulting in smoother and faster learning curves","In contrast, choice-guided interfaces drastically decreased performance in slightly different tasks with the same tool, which could cause frustration and abandonment and should be avoided at all costs",The implications of user interface design in authoring tools for education require further study to fill the gap separating technology and education,We hope these results will offer valuable guidance for designers and serve as grounds for further studies in the scientific community,"Supplementary data 
                      
                      
                      
                      
                      
                   Appendix A Supplementary material Supplementary data associated with this article can be found in the online version at  ","Appendix A Supplementary material 
                      . 
                      .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0952197619301253," 1 Introduction The study of the dynamic aspects of traffic is essential for a proper modelling of traffic and its related phenomena, like traffic congestion among others","Intelligent Transportation Systems (ITS) address this level of uncertainty through advanced monitoring systems of the traffic network in real time, which makes possible to determine the system state and to respond to unexpected situations","The implementation of these systems (traffic responsive) requires significant economic investment and complex maintenance processes, which means that at the present moment, the number of urban areas equipped with these systems remains small",Many studies and methods for traffic planning and control in urban networks are based on the assumption of time-of-day (TOD) intervals which determine dynamic congestion patterns,"A TOD is a period of time where traffic dynamics can be considered stationary, so it is possible to assume that traffic in this interval follows a specific behaviour, which is defined through a pattern","Once TODs have been determined, timing plans are developed and optimized for each TOD using heuristic and metaheuristic algorithms","Typically, between three and five plans are run in a given day","Traditionally, TOD determination and optimization plans are tasks that has been performed visually by traffic engineers making it challenging and subjective","Currently, there are two possibilities to address the problem of defining control strategies for traffic networks based on TOD intervals. 
                      
                   Determining TOD intervals has been widely addressed in the literature using mainly cluster analysis techniques, among others","Nevertheless, the applied methods do not consider the so-called   (see  )","The omission of these constraints leads to a noisy and infeasible detection of TOD intervals as well as transitions between different TODs with a high cost, in terms of quality of modelling",These noisy TODs are clusters which do not follow an intuitive TOD scheme as the majority of clusters,"For that reason, these clusters have to be re-assigned to other TODs, since if it is not done, the transition between these unfeasible clusters and the other ones will be highly expensive and the obtaining of a larger number of clusters will produce worst results",The re-assigning process of unclean clusters was made again by traffic engineers,The time-domain-constrained data clustering problem tackles a clustering problem in which data are labelled with the time where they were gathered,"Furthermore, the time-domain constraints impose that the obtained clusters need to be contiguous in time","That is, if two data are grouped in the same cluster then all data with a time label between both data must be assigned to the same cluster","Thus, time series segmentation is solved through the partition of a time series into homogeneous clusters which are close in terms of time, solving the problem of noisy clusters and minimizing the costs of transitions between clusters",A second challenge for TOD determination and traffic light optimization is the development of a bilevel optimization model which addresses simultaneously the definition of control strategies,"However, it implies a high computational cost, derived from the two hard optimization tasks of determining TODs and optimizing timing plans","In order to solve successfully the model, memetic algorithms which combine the advantages of metaheuristics and local search strategies must be developed, in order to build more accuracy and faster algorithms which achieve an optimum trade-off between exploration and exploitation","Hybridizations of metaheuristics and local search algorithms have been proven that outperform the results of them individually (see  ,  ,  , and  )","Moreover, there is not in the literature an automated methodology for determining TOD breakpoints and optimizing traffic signal times in each segment","For this reason, in this paper the problem is formalized following a simultaneous methodology through an approach given in  ",The model is formed to simultaneously determine the TODs and the traffic control strategy,In the upper level the TODs breakpoints are determined by optimization and the lower level problem is represented by a traffic control problem,The integration of both levels avoids local optima in the TOD breakpoints pursued,This paper focuses on a control strategy based on timing plans for intersections but can be easily extended to methods using the whole traffic network,"The main contributions of the paper are: 
                      
                   The article is organized as follows",Section   reviews traffic signal control and optimization based on TODs,In Section   the bilevel model is formulated,In Section   the memetic computing for its resolution is described,The automatic determination of the number of TODs is carried out in Section  ,"In Section   the numerical experiments over real and synthetic data are carried out and finally the conclusions obtained are analysed. 2 Related work During the last few decades traffic signalization has experienced a great evolution, from the first pre-fixed signals with fixed times to the real-time traffic signalization","A roughly taxonomy of traffic signal systems can be stated as follows (see  ,  ). 
                      
                   Despite the fact that traffic adaptive systems with distributed processing are the most promising systems, the infrastructure in the most populated and important cities in the world are not ready for implementing this kind of system now, since an expensive technological infrastructure is required","Relevant studies of this type methodology are  ,   and   that propose traffic responsive systems based on multi-agent systems for traffic signal optimization",The above discussion motivates the study of determining TOD intervals and it is the why it is still a very current research topic,"Moreover,   showed that non-adaptive systems, for example pre-fixed time systems, with adequate design and with regular updates obtain acceptable results in comparison with traffic responsive systems",For this reason it is crucial to have specialized tools capable of automating the planning process and making planning changes when new mobility patterns are detected,"In addition, robust optimization techniques allow robust pre-timed systems which are less sensitive to traffic flow fluctuations (see  ,  ). 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   2.1 Reviewing the time-of-day identification problem Traditionally, traffic engineers determined TOD breakpoints, developed and optimized traffic timing plans based on their expert knowledge and with the only historical data of traffic volumes in critical intersections","With the appearance of data mining and machine learning techniques, different approaches have been widely used in this process",The TOD determining problem can be addressed through cluster analysis or by means of segmentation of multivariate time series,The first approach deals with solving the so called   and the second with the  ,"The standard solution of change-point detection problem involves: (i) the number of change-points, (ii) their locations, and (iii) functions for determining curve fitting between successive change-points",In TOD determination problem the issue (i) is fixed and the fitted function related with issue (iii) is considered constant.   reviews the change-point detection problem but it does not report applications in traffic domain,Both approaches are considered to be equivalent,"However, as far as we know, only cluster analysis techniques have been applied in current literature to solve it. 
                          provided a solution based on hierarchical clustering taking into account the volumes and occupancies of different intersections, determining different TOD intervals and optimizing plans for each of them. 
                          proposed a new method based on hierarchical clustering, with the novelty of making a high-resolution definition of the state of the system, taking into account not only the flow or occupancy as traffic parameter but also the density","With these data, different plans were developed for each TOD","In order to address the unfeasible clusters problem,   proposed an approach based on clustering using a Genetic Algorithm (GA)","To do that, the fitness function in the codification of genetic algorithm introduces a penalty with the purpose of avoiding unclean clusters","Furthermore,   proposed a method based on  means algorithm and silhouette index","This method uses the traffic parameters flow, speed and occupancy and it determines the TODs following an iterative scheme between a TOD determination stage and a traffic control stage",In each iteration the TODs are refined using the new information of the traffic control problem and the empty clusters are deleted,This method can be viewed as heuristic to solve the bilevel model which appear in the simultaneous methodology,"However, these works did not consider the time-domain-constraint into the clustering process, which produced the so-called   clusters","Although the approach of removing unclean clusters provide the minimization of transition costs, they did not consider the transition costs in the optimization tasks","To achieve it,   proposed an approach based on a GA to explicitly consider the transition costs during the optimization tasks in a coordinated-actuated traffic signal system","Moreover, local search strategies have also been applied to determine TOD intervals","One example is  , where the problem is solved using the optimal cycle length per time interval through a greedy search algorithm",The authors proposed this methodology due to the quickness of local search strategies,"This work takes into account the transition cost but it does not consider explicitly the time-domain constraints. 
                          proposed a methodology based on the  means algorithm and the VPLUSKO index – which is defined on the volume and occupancy – to determine TOD breakpoints",The VPLUSKO index helped to calibrate the model and adjust the breakpoints,"However, this work do not address the problem of automatically determining the optimum number of TODs under any traffic conditions like the rest of reviewed studies",Soft-computing techniques such as fuzzy logic and metaheuristics have also been applied to determine TOD breakpoints,"In  , authors presented a plan selection system based on two setups: off-line and on-line","In the off-line step, different mobility patterns were determined using historical data which were synthesized","In the on-line phase, the current traffic conditions are matched with one of the determined patterns in the off-line phase, where a pattern corresponds to a fuzzy prototype. 
                          proposed a process to determine optimal TODs based on  means method",This work is a first approximation to address the inclusion of time-domain constraints in the TOD determination problem,"Instead of the traffic volumes, this work proposes the use of   score and the time as features in the clustering process",The approach falls on a sequential methodology,"In last years, new technologies and sensors, like GPS, APC etc., have allowed to collect data in real-time in order to address different transport problems","The problem of determining TOD breakpoints has been applied to other transportation domains.   applied GPS data for bus scheduling, determining TODs for bus lines","In this work, the authors used the dwell times at stops and inter-stop travel times like clustering indexes to partition data, using a hierarchical approach",Various criteria for automatic model selection have been widely used to determine the number of clusters (in our problem the number of TODs) in the data,"The Bayesian information criterion (BIC), Akaike’s Information Criterion (AIC) and Minimum Description Length (MDL) are some notable examples of these criteria","However, despite of the profuse literature about this topic ( ) there is not an index which outperforms the rest in all application domains","The TOD determination problem viewed as a segmentation problem also contributes with its own methods to determine the number of TODs such as the PETE method ( ) or the BIC’s adaptation method ( BIC) for the segmentation of time series ( ). 
                         Briefly,   shows a summary of the works analysed in this section grouped by the approach used to solve determining TODs breakpoints, sequential or simultaneous. 2.2 Memetic-related works to traffic control systems The traffic signal control methods are deployed on the basis of optimization procedures","Genetic algorithms (GA) have been widely and successfully applied (see  ,  ,  ,   among others)",The growing complexity of the traffic signal systems has motivated the use of new metaheuristics like in   where   (PSO) is applied for determining cycle programmes of traffic lights in two large networks,"Nowadays, there are several enhancements of metaheuristics that improve its performance in different ways",One of these strategies are memetic algorithms (MA),The concept of MA was first coined by  ,"In that work, the authors developed an algorithm for the travelling salesman problem using local search heuristics with the purpose of improving the exploitation capability of population-based algorithms","Since then, this concept has been formalized, theoretically studied and applied to many complex optimization problems (see the reviews   and  )","Currently, MAs have shown their ability to achieve high performance and superior robustness across a wide range of problem domains","To mention a few examples, MA has been applied to power systems like in  , where a MA is used for feature selection in order to forecast mid-term interval loads, or aerodynamic, as in  , where it has been used to optimize aerodynamic shape or aircrafts and determine the optimal settings of shape parameters","Recently,   applied MAs to an adaptive signal timing settings system","In this work, MA determine good quality signal timing settings within an acceptable amount of time",This task is crucial in real time systems and this motivation has been followed along this work. 3 Models to identify time-of-day breakpoints The main objective of this section is to state the optimization models for the determination of TODs based on the time-domain-constrained data-clustering problem,The goal is to define time intervals in which traffic demand is approximately stationary and therefore the dynamic component within each interval can be considered negligible,This is a resolution strategy for addressing the management and the control of traffic for non-stationary demand (dynamic demand),"We will start addressing the sequential methodology in which TODs are determined in a first step and after that, the traffic control problem is solved in each TOD","Then, this approach will be extended to a simultaneous methodology in which both problems are solved on a single optimization model, in this case, the bilevel optimization model which is formalized in Section  . 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   3.1 Sequential methodology Urban traffic networks are mathematically modelled by a   
                          in which the set of nodes   represents the   and the so called  ",The centroids are dummy nodes which model city areas with generation/attraction of trips,"The set of links   represent urban roads and the so-called  , which are dummy links joining the centroid nodes with the intersection nodes. 
                          shows a representation of a traffic network","This network consists of   centroids,   connectors and   links representing the streets of the urban area modelled",In addition to the network (supply) in these systems the   is considered representing the demand between different centroids,In this example we consider four origin–destination pairs  ,The index   denotes one of these pairs and   the set of all origin–destination pairs,"To model the variation of demand over time the   
                          is introduced","These functions are not directly observable in the network. 
                         New technologies allow the monitoring of traffic networks in real-time","These traffic control systems are located in a subset of links of the network, denoted by   and we will call this the set of  ","The traffic parameters in the link   are the   
                          (veh./hour), the   
                          (km./hour) and the  )   (veh./km.), which is linked to the two above by the equation:  
                      For simplicity, we will focus on the flow observed in each arc of the traffic network","This methodology can be generalized in order to consider the three types of traffic parameters on a simple way.   proposes the use of geodesic distance over euclidean distance, since the first one allow to incorporate the three types of traffic parameters and take into account the correlations between the link counts","Let us a data set  , where  ",This data set is composed by tuples which in turn consist of   which is a vector of link flows,"Indeed, flow data   are labelled with an index which represent the time instant   for which the data was gathered","Thus, it is possible to assume the data are time ordered","Henceforth, it is mathematically expressed as follows,   where  . 
                         
                         
                      TOD determination problem has been dealt in the literature as a clustering problem",Cluster analysis tries to find TODs with a certain level of internal homogeneity and heterogeneity between different clusters,The scheme most widely used addresses the problem of minimizing the variability into each group or cluster,"The most popular criterion followed to do that consists of minimizing the  , whose mathematical formulation is the following: Supposing that it is wanted to partition   traffic observations in clusters  ",Each cluster   is defined by the so-called  :  where   is the number of objects in cluster   and represents the mean flow vector in each TOD  ,The TOD determination problem used in the literature can be formulated according to:  where the objective function is the sum squared error within each cluster and the binary variable   takes the value   if   observation is assigned to cluster   and   otherwise,The first constraint imposes that all objects are assigned to some cluster and the second one requires that there is not empty clusters,"The k-means algorithm can be considered as a greedy algorithm to solve the optimization problem  . 
                          illustrates the determining TOD problem","In the figure on the left, a possible solution for the TOD problem solving   is shown","In this figure, it is shown that the transition between clusters   and   and between clusters   y   is chaotic and produce higher transition costs between traffic control strategies","This solution does not fulfil the temporal constraints, formulated by:  
                      We introduce decision variable   (instead of  ) in order to incorporate the temporal constraints   into the cluster analysis","Assume that   is the whole time period in which data has been extracted and henceforth, this is the period that must be partitioned",Suppose the limit points   (see  ) as decision variables,"Thus, it is possible to describe the set  
                      
                         The centroid has been considered as the mean flow in the  th TOD and it is shown in Eq.  :  where   is the cardinal of a set","According to the previous formalization, the optimization model is stated as:  
                      
                          
                          Once the set of TODs has been determined, the mean flow   is computed and it is assumed this is the traffic pattern which operates in each TOD period","The traffic control problem find an optimal strategy, which will be defined by a parameter vector  , which will optimize a certain criterion, like waiting time, etc","In a general way, these kind of problems are formulated by:  
                      3.2 Simultaneous methodology These methodologies try to find the optimum traffic control strategy in terms of accuracy and efficiency","In order to do that, they determine the TOD intervals where pre-time (which corresponds with the first step in the sequential methodology) and traffic signal timing plans (step two in the sequential methodology) will be applied","The simultaneous methodology proposes the following bilevel optimization model which integrates both steps:  
                      The decision variables in this case are the control strategies   and the period   where the strategy will be applied","Besides, the objective function in this problem change the goal of finding time intervals with minimum variations in flow intensity to finding intervals where optimal traffic control strategies are obtained",Another important feature is that traffic control problem is not posed exclusively on   but also on the set of observations belonged to each TOD,"Note that   may have a short and insufficient number of observations or even it can be empty, leading to make the traffic control problem   bad posed","To deal with this problem, we assume that  ","As may be seen from the previous formalization, the problem   depends on the application that is being tackled",Note that the formulation is general enough in order to tackle the use of simulation models to analyse the traffic network,"If we note  , it is possible to consider that flow depends on the traffic control strategies implemented in the system   and simulation models allow to compute   as a quality index   which it is being considered",In this work we consider a combined approach of TOD determination and optimal traffic signal timing,"Without loss of generality, we consider an isolated fixed-time signalled intersection","The parameter   represents the optimal timing plan for the TOD  , and the criterion   for period   is the total delay time in the intersection in that period  ",In that case the lower level problem cannot be explicitly solved and the problem have a bilevel nature,In this problem the control variables are   where   is the cycle length (seconds) and   denotes the vector of effective green time for each line group   at TOD  ,"We use the delay equation in the Highway Capacity Manual (HCM) ( ) to estimate the delay per vehicle for period  
                          where 
                         
                      The total delay time at period   can be computed as  
                      Therefore, the optimization model of signal timing can be written as:  
                      3.3 Reformulation of simultaneous TOD determination problem using unconstrained optimization Briefly, by summarizing all the formalization proposed in this article, the simultaneous TOD determination problem given in Eqs.  –  can be written as follows:  
                      The bilevel optimization problem   proposed in this work includes two optimization tasks or levels","In the first or upper level, clustering with time-domain constraint is made","The vector   defines the decision variables which will be used in the upper level problem, since when this vector is arranged, the evaluation of   leads to   independent traffic control problems",We assume that optimization tools are available to obtain the optimum solution to traffic control problem   for the   variable included in the upper level,"According to this definition, the previous model can be reformulated as an unconstrained optimization model",To do that we define  where   function put in order the elements of   from the bottom to the top and   is   a hypercube in  K,"In this way,   can be reoriented as an unconstrained optimization problem as it is shown in Eq.  :  The main advantage of this reformulation is that it allows the use of plenty developed algorithms to unconstrained optimization","It must be underlined that this reformulation can be also applied to step one in the sequential methodology, since it is enough to identify  . 4 Memetic algorithms for the bilevel TOD determination problem Then, the problem shown in Eq.   will be named as the bilevel TOD determination problem","In order to harness the advantages of local search,   developed a memetic algorithm to introduce local search into meta-heuristic algorithms based on population with the purpose of improving the solution in promising regions",These memetic algorithms are based on a   of individuals or candidates,Each one of them constitutes a solution in the solution space,The population-based algorithms explore successfully the search space due to the use of a population allows to avoid local optima,"However, these algorithms do not encourage a exploitation phase","For this, the proposed MA is an instance of this class of algorithms and it provides a trade-off between accuracy and computational cost",The proposed algorithm is shown in  ,The algorithm has two important parameters which must be fitted   and  ,"The parameter  , is used in exploration phase and it takes into account the number of successive improvements achieved by the global optimization algorithm","In this case, the local exploitation method will start from the best solution achieved. 
                       illustrates the basis of this algorithm","When an algorithm fall into a local minimum environment  , the value of the objective function is difficult to enhance and new improvements are not obtained until the algorithm scapes successfully from this region in the search space",The role of   parameter measures the number of successes (successive improvements) and it is an indirect measurement to check if the algorithm has get out from the   neighbourhood to other neighbourhood   which contains better solutions,"Once this fact has been detected (the algorithm has changed its application environment to other more promising) is appropriate to apply a method with good capabilities of local search, it means, a good convergence to the local minimum. 
                       
                      The method used in the exploitation phase is the Nelder–Mead (NM) simplex algorithm",This computational scheme was introduced in   to unconstrained optimization problems,Some advantages of this method are that gives notably improvements in first iterations and it is very useful in nonlinear optimization problems for which derivates may not be known,"This paper uses a hybridization of a standard PSO ( ), GA ( ) and SA (  and  ) with a NM algorithm","The first algorithm so-called SPSO-NM has been successfully employed in a timetabling train problem ( ). 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   4.1 A Simulated Annealing (SA ) for the bilevel TOD determination problem Simulated Annealing (SA) is a popular local search meta-heuristic based on the metaphor of a technique for heating and controlled cooling of a material to increase the size of its crystals, reducing their defects","A key point of this algorithm is that it allows hill-climbing moves in order to find a global optimum, thus providing a way of not being trapped in local optima",The general idea of the algorithm is quite easy,The algorithm starts with an initial solution  ,This solution will be compared in each iteration of the algorithm with a new generated solution  ,It can be generated randomly or using some specific rule in a neighbourhood   of the current solution  ,"A realization of SA is effected by choosing the neighbourhood function, since its definition has a great impact on the performance of the algorithm","In this paper, the next specific neighbourhood function is proposed.  
                      Another issue which must be addressed is the choice of the generation probability function which determines the probability of generating a new solution","In this case, a uniform distribution has been used and the probabilities are proportional to the size of the neighbourhood  ","Thus, the generation of a new solution   will be carried out in three steps:  
                      
                          shows the neighbourhoods   considered by SA and its variant SA  when  ","The essential advantage of SA  is that allows wide movements, being able to avoid local minima","For this reason, it is an excellent candidate as an algorithm to exploration stage in hybridization. 
                         The probability of accepting the new generated solution   is defined by the next rule.  
                      Following this scheme, best solutions are always accepted, although there is a fraction of non-improving solutions which are accepted",It is made with the purpose of avoiding local optima and can be compared with the mutation process in a genetic algorithm,The fraction of worst solutions which are accepted depends on a temperature parameter   which is particular of SA algorithm,"The use of the neighbourhood function   and the probability function   in the basic SA originates the so-called SA  and its pseudocode is shown in  . 
                         
                      5 Determination of the optimal number of TODs The bilevel TOD determination problem assumes that the number of TOD intervals   is known",This section looks at four methods for determining the optimal number of TOD intervals,"The first two are based on the widely used Bayesian Information Criterion and employ all the registered time series, while the third algorithm is based on   and the fourth is proposed in this work and it is problem-oriented approach","Those last two methods operate with the average observed values. 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                   5.1 Bayesian Information Criterion (BIC) The BIC is a likelihood criterion for model comparison that penalizes models with additional parameters",The BIC is defined mathematically as:  where   is the complete data to be modelled,The first term represents the maximum log likelihood of the data under the model with   parameters,The second term   is responsible for penalizing the candidate models according to their number of parameters   and   is the penalty weight (  according to the BIC theory),"The optimum model corresponds to the one for which the value of BIC, given by Eq.   is minimum","If we assume that the observations in a TOD are drawn from a full-covariance Gaussians  , the BIC for the  TODs solution is defined  in which   denotes the number of articles in cluster   and   is the dimension of the flow vector space",A two-stage process presented by   for determining the optimum number of clusters,"The procedure is described below, following the work of  ",The two-stage process first examines the BIC for all potential clustering solutions,"The goal is to find the smallest number of clusters that have the lowest BIC, because the BIC decreases first and then increases as the number of clusters increases, the BIC for each   (clustering solution) is calculated","Beginning from  , the first   value that satisfies   is chosen as a coarse estimate of the number of clusters","In the second stage, the ratio of changes in dispersion measurement is used to determine the optimum number of clusters based on the coarse estimate obtained in the first stage","The ratio of changes in dispersion measurement is defined as   for  , in which   denotes the change in dispersion measurement if   clusters are merged into   clusters","The parameter   can be computed as  , in which  ",This second stage is based on the understanding that a significant increase in   will be observed when two clusters that should not be merged are merged,"The   value for each   is calculated, and the two largest   values are identified as   (the largest) and   (the second largest).   uses an empirical threshold value of  ; that is, if  ,   is set to  ; otherwise,   is set to max . 5.2 
                         BIC In segmenting an audio stream the BIC has widely used ( )","It can be shown ( ) that if the expression   is positive, the time   is a good candidate for a segment boundary",Note that   represents the variance–covariance matrix of the observations  ,It is possible to apply this criterion to the  solution to determine if its border points   are significant,"The criterion used is to choose the solution with significant points of maximum cardinality  . 5.3 A modified PETE algorithm 
                          present the so-called PETE algorithm to determine the number of time segments",This method generates a   value for each increase in the number of segments,In this paper a modification of this method is adapted in order to reduce its computational cost,Let   be the segmentation error of the solution  ,"By using a Monte Carlo simulation the random error of adding a new segment in the partition is calculated as follows: A random segment   is selected, then the observations   are randomly ordered and the segment is randomly partitioned",The new error denoted by   is calculated,"Drawing from the random sample   the  quantile   of the random error is calculated, if it satisfies  then the  -solution is selected and the procedure is repeated. 5.4 An approach oriented to the TOD determination problem The desired objective is a suitable model of the dynamic mechanisms which operate in the traffic network",In this context the selection of a high number of TODs has as a consequence a greater analytic cost but gives more satisfactory results,Henceforth a trade-off between computational cost and accuracy must be achieved,"To that end a natural method is to require a number of clusters   in which the mean relative error   in describing the time series as a set of stationary flows does not exceed a value   given by the planner, that is  
                      This value   has a physical interpretation and thus allows a priori determination. 6 Computational experiments The objectives of these computational tests are: 
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   Experiment  : A comparison of the performance of different MAs The first data set was generated through a simulation experiment, using the dynamic traffic load model developed in  ; the Nguyen–Dupuis network shown in   has been used. 
                         The results are shown in  ","These results consist of   daily traffic patterns, considering traffic flow or density over a set of three sensors located in the network","Eight synthetic data sets are considered and   TODs is set. 
                          
                          
                         The combined model given in Section   has a bilevel structure, in which the evaluation of the objective function requires solving   optimization problems",This has motivated the need for efficient resolution algorithms,"In Experiment  , the standard PSO, SA, SA , GA algorithms and their hybridizations with the NM have been tested on the problem  ",Note that the works ( ) use GA algorithms for solving this problem and the GA can be considered as the baseline test,Metaheuristic algorithms depend on a large amount of parameters and the adjustment of these parameters is one of the main challenges of their application,"The parameters used in GA, SA and SA  takes their values from Matlab functions   and  ","In PSO algorithm, inertia weight has taken the value  , and the learning parameters   and  ","Furthermore, two neighbourhood topologies have been considered, the   in PSO and the   in its hybridization, taking an specific number of particles (neighbour count)  ","Finally, gbest swarm topology has been considered to the crude PSO because it converges more quickly than lbest topology",Population size in GA and PSO has been fixed to   individuals/particles,"Due to the number of parameters and the endless possibilities to compare and adjust them, we are focus on the effect of the parameter   in the hybridization proposed in the paper",The selected parameters for the hybridization are   and  ,"The objective of this test is not to find the best algorithm, since it depends on the kind of traffic network, the number of link counts, the TODs number and the settings chosen in the algorithms","In this paper, the main objective is to test if the proposed MAs improve the performance of baseline algorithms. 
                         As the algorithms have a probabilistic nature each instance was run   times over the eight test problems","In order to visualize the results obtained, the average change in the   runs and for all the test problems is considered, and in addition the value of the objective function has been standardized as  where   and   are respectively the maximum and minimum found for all the algorithms in a given problem",This standardization means that the   test problems are equally important,"The evolution of the algorithms is shown in  , while   shows the results of each algorithm when 500 and 1000 evaluations are computed",It is observed that the hybridization speeds up the original algorithm,"In addition it can be seen that the   and   algorithms with   outperforms the other algorithms. 
                          
                          
                         In the previous experiment, it has been shown that the hybridization of metaheuristic algorithms with NM method allow to accelerate the convergence of the base method","Besides, it is observed that all methods converge to the same solution in each of the ten executions completed (if the mean value in   tests is  , henceforth, each one individually also converges to zero)",The acceleration in the base procedure is due to the NM algorithm,"In this article, it is intended to answer the question about if it would be better to apply exclusively NM algorithm","In the next test, the previous   problems have been solved   times, departing from different random points in each execution",A success in the experiment is considered if the reached value satisfies  .   shows the mean value obtained   and the success percentage,"The results indicate that in   runs, NM has converged into a local minima in   times","It demonstrates that MA algorithms are more robust and allow to alleviate the shortcomings of NM method. 
                         
                      Experiment 2: A real case study This example is taken from   and consists of a real-world intersection between  th Street SW and Alderwood Mall Parkway in the City of Lynwood, Washington","The flows were recorded in March–April  ,   observed flow patterns were retrieved for the PM peaks (4:30–6:30 p.m.) between Tuesday and Thursday","Based on these data two test problems were designed, in the first the   patterns are considered as consecutive over a day (named Test  )","The second (named Test  ) is obtained by modifying the original data to introduce a dependence on the current time of day, in particular the flows of the  th-period have been multiplied by the weighting factor   with  . 
                          
                         The intersection is shown in   and the saturation flow rates   for groups   are  ","A specific lead–lag phasing sequence is used in the example and the resulting constraints for the traffic-signal timing problem are:   where   is the total lost time per cycle,   s used in the example;   is the minimum green time,   s used, and   and   are the minimum and maximum cycle length, specified as   s and   s, respectively","When using Eq.   to calculate the total delay, the duration of each time period   is set as  . 
                         Over the two test problems a   and a   methodology is applied","The sequential methodology is the one followed in practice, first the TODs are determined and later the optimal traffic signal timing is calculated for each TOD",The simultaneous methodology is the one described in this paper and it directly minimizes total time in the intersection,Both methods require an algorithm to solve the problems  .   use ant colony and a genetic algorithm to solve this problem,"These algorithms present two disadvantages, the first is its high computational cost due to the large number of solutions and the second is, because the SA algorithm is used for the bilevel model, conflicts may appear between the accuracy of resolution of the lower level problem and the objective function value as shown in  ",This numerical experiment employs an interior-point algorithm (implemented in the MATLAB function  ),"In our numerical tests, there is evidence that this option is faster and more efficient than the GA algorithm implemented in the MATLAB function  ",The results obtained are shown in  ,"The first column shows the number of TODs considered, the second column the methodology used and the third the time required to obtain the best solution",The fourth column shows   where   is calculated by using Eq.   and the fifth shows the value   where   is calculated by Eq.  ,The results obtained agree with expectation,Each methodology achieves better results for the objective function it is trying to minimize,The simultaneous methodology minimizes the total waiting time while the sequential gives TODs with lower variability,"It is seen, however, that the sequential approach is very efficient and is capable of obtaining similar solutions to the simultaneous method",Both methodologies allow the systems to be recalibrated automatically,"One conclusion is this, considering that traffic regulation in different time intervals is advantageous",The reduction in the waiting time by considering   instead   is about  ,"To go into this question more deeply,   shows the solutions obtained by employing the simultaneous methodology in Test  ",It depicts the TODs obtained for several values of   and how the optimal traffic signal timing is strongly dependent on the number of TODs,"It shows that the cycle amplitude   is dependent on the level of congestion of the TOD. 
                         
                      Experiment 3: Determination of the optimal number of TODs In this section the algorithms described in Section   to identify the optimal number of TODs are tested","In order to assess the methods, real traffic data collected by the California Freeway Performance Measurement System (PeMS) is used","The PeMS collects the traffic data in real time from over 25,000 individual detectors across all major metropolitan areas of the State of California","In order to test the methodology proposed in this article, observations over   days in   have been collected from two dual loop detector stations in the Bay Area of California",The data have been previously classified into different traffic patterns on different days using the methodology given in  ,The traffic profiles based on speed and occupancy are shown in  ,The data used are available at  ,"For each of the   traffic patterns the TODs have been identified for  , and by using the BIC,  BIC, modified PETE and the oriented-problem (OP) methods, the optimal number of TODs have been selected",The   value employed in the modified PETE method was   and the mean relative error   for the OP,The optimal number of TOD intervals resulting from each algorithm is shown in  ,"The observed results indicate that the BIC is the most parsimonious method since it determines a reduced number of TODs, the  BIC method is highly sensitive and establishes a higher number of TODs",On the other hand it seems that the modified PETE algorithm finds an acceptable number of TODs,"The OP method employs the same threshold for both types of data (occupancy or speed) but the variability level is different for each, which means that for the occupancy a high number of TODs is established but this is not the case for the speed",To gain additional insight into the results the different procedures are displayed,The clustering results using the BIC and the  BIC criteria are shown in  ,The first column is associated with the occupancy and the second is based on the speed; finally the vertical lines separate the different TODs according to their algorithm,As can be observed in the figures the  BIC criterion produces a non-significant number of TODs which may not be helpful for traffic control,"These methods tackle the existing variability between days. 
                         Unlike the two previous methods, the modified PETE and the OP algorithms work with a average traffic profile.   illustrates the way the modified PETE algorithm works",The first graph corresponds to pattern   (occupancy) and the optimal number of time segments for this particular case is  ,"It is possible to observe in the graph that the stopping criterion of the algorithm is when the   value of the mean error of adding a new random segment is less than the error obtained by increasing one TOD in the current solution. 
                         
                          shows the results obtained from the OP method; the first graph shows the occupancy-based tests and the second the speed-based tests",In both graphs the red line represents the required threshold for the mean relative error value that must not be exceeded when selecting the number of clusters,"This method has resulted in practice in more consistent and comprehensive partitions, for example the optimal number of partitions for the occupancy problem varies between   and   while for the speed problem it varies between   and  ",The criterion of fixing a maximum mean relative error is easily interpreted by planners,This makes the method more easy to apply than the previous one. 7 Conclusions In this paper a methodology for automatically updating pre-timed traffic control systems is set out,"For this purpose a bilevel model has been formulated, which simultaneously includes the problem of determining the time-of-day breakpoints and the traffic control problem for each time interval",The proposed model has been solved by the use of a class of MAs,"The effectiveness of the algorithms SA +NM and GA+NM has been demonstrated on a collection of synthetic and real problems and they outperform GA, SA+NM and SPSO+NM methods","Furthermore, this feature allows us to apply SA +NM or GA+NM algorithms to bilevel TOD determination problems",The sequential and the simultaneous methodologies have been illustrated over a real problem,As might be expected the simultaneous method achieves better results than the sequential but the computational cost is higher,"It can be seen that the sequential methodology, which is used in practice, achieves satisfactory results","Finally, the automatic determination of the optimal number of TODs has been studied by employing the most promising methods, based on BIC,  BIC, a modified PETE algorithm and a OP approach",It is observed numerically that the criteria based on the BIC is conservative and produce a reduced number of TODs in comparison with the other methods,The index  BIC produces a large number of TODs,The oriented-problem method has an interpretation which allows its easy application and achieves fair values,The modified PETE method also achieves solutions in accordance with what is expected.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s1071581918300971, 1 Introduction Speech is a natural form of communication which is rich in information,"Since the early twentieth century, radio broadcasting has been used to transmit and consume speech-based content","Today, radio listenership remains high and podcasting continues to grow in popularity","Although many radio programmes are still broadcast live, a large proportion are pre-recorded and put together using audio editing software",Efficient navigation and editing of speech is crucial to the radio production process,"However, unlike text, speech must be consumed sequentially, and does not naturally support visual search techniques ( )","Audio editing interfaces display a visual representation of the amplitude of the sound, called a ‘waveform’",This gives users some ability to visually search and scan audio content,"Although the waveform is useful for many editing tasks, it displays very limited information, especially when zoomed out ( )","Semantic audio analysis technology can be used to extract higher-level information from the sound, such as whether it is speech or music ( ), where different people are speaking ( ) or an approximate transcript of what they are saying",Presenting this information to the user could allow them to navigate and edit audio content much more efficiently ( ),"We are interested in investigating whether semantic speech editing can be used for radio production, and understanding what effect semantic editing and automatic speech recognition (ASR) transcripts have on the production process","In this paper, we describe the design and development of   – a semantic speech editing interface for radio production",We explain how we used our system to run a contextual user study that evaluated semantic speech editing for the production of real radio programmes at the BBC,We find that semantic speech editing can be used to improve the radio production workflow,"We learn about the effect of semantic speech interfaces on the production workflow, and identify opportunities to better address the needs of producers. 2 Background Semantic speech editing techniques have been used to enhance a number of interfaces in a variety of applications by using either manual or ASR transcription",SCAN ( ) is an interface designed to support retrieval from speech archives,"It used ASR transcripts to allow users to search for keywords, and to visually search the recording by reading the transcript","This system was developed into SCANMail ( ), an interface designed for interacting with voicemail","It added a number of features including paragraph segmentation and the ability to skip to a point in the transcript by clicking on a word.   evaluated SCANMail using eight expert users and found that the transcript display enabled users to visually scan the content of recordings to quickly extract information and judge which parts were relevant, without having to play the audio","The interface was later enhanced with error correction functionality and confidence shading, which greys-out words that may be incorrect ( )",Avid released a feature for their Media Composer video editing software in 2007 called ‘ScriptSync’ ( ),This feature aligns a user-supplied transcript to a video recording using a patented method ( ),"The feature places a marker in the video for each line of the transcript, allowing users to jump to a particular line, or see which line in the transcript corresponds to the current point in the video",We did not find any reported user studies of ScriptSync,Transcripts can also be used as a mechanism for editing media content,SILVER ( ) is a video editor that included various ‘smart editing’ features including an editable transcript window,It used ASR to align the words from subtitles to video,The video could then be edited by selecting and deleting text in the transcript,"Gaps, errors and edits were displayed in the transcript using special characters and the user could correct missing or wrong words",All of the video editor’s features were evaluated together in an informal study with seven students,The report of the study did not provide any results on the transcript-based editing feature,"When editing a video interview, you want to avoid making a cut when the person speaking is in shot, because it causes the image to suddenly jump.   used image processing algorithms to create a video editor that can help the user hide these edit points",The editor has an editable transcript window that displays suitable edit points and allows the user to edit the video by selecting and deleting text,The transcripts were generated using a paid-for crowd-sourcing service with speech alignment software,The system also allowed users to easily remove ‘umm’s or repeated words,"No user study was conducted in the reported study, however positive feedback was received from nine professionals who were given a demonstration. 
                       created an interface for editing voicemail messages using ASR transcripts",Users could cut-and-paste parts that they wanted and delete other parts,"The system was evaluated in a formal study of 16 voicemail users, which found that semantic editing was faster and as accurate as editing with a waveform-based interface","Crucially, this was true even though the transcripts had an average word error rate of 28%",This shows that semantic editing is beneficial even when using imperfect transcripts,"Hyperaudio Pad is an open-source audio and video editor, first proposed by  , and now available online as a free service ( )",This is a web-based interface that allows users to navigate and edit online media using transcripts,The transcripts are generated from subtitles by using speech alignment software,Editing is done by selecting a part of the transcript and dragging it into a window to create a ‘clip’,"Other clips can be added and re-ordered, and the edited version can be played and shared with others","No user studies of this system could be found. 
                       presented a novel interface for creating ‘audio stories’ that combine speech and music","The interface uses an editable transcript with two columns, one for each of a pair of speakers","It allowed the user to cut, copy, paste and delete the audio using the text","It also highlighted repeated words, similar sentences, ‘umm’s, breaths and pauses in the transcript",The transcripts were generated using a crowd-sourcing service with speech alignment software that also detected breaths,"The system from Rubin et al. included additional functionality for finding and adding music tracks, and for varying the length of music using automatic looping",The system was evaluated through a short informal study of four participants where the editing capabilities received positive feedback,"No follow-up studies have been reported since. 
                       created a semantic editing system for asynchronous voice-based discussions, where users could quickly edit their speech recording before sending it to the recipient",Their system used near-live ASR and detected pauses in the speech,"Their interface allowed users to delete selected words/pauses, insert additional pauses and fix incorrect words","In a formal qualitative study of their system with nine users, they found that text-based editing was considered good enough to replace waveform editing, and to be more accessible","They observed that most users only used the system to make fine-grained edits, instead of editing large chunks","Users said that the transcript also allowed them to quickly review all the points that were made, and that the errors in the transcript weren’t a heavy distraction","A quantitative study of 28 students and teachers found that including editing functionality resulted in the students reporting lower levels of mental task load, effort and temporal demand. 
                       created a collaborative tablet-based document annotation system called RichReview, which offered users three modalities in which to annotate documents - freeform inking, voice recording and deictic gestures","The voice recordings were displayed using a waveform, overlaid with an ASR transcript of the speech",Users could trim or tidy the voice recordings by drawing a line through words or pauses to remove them,The system was evaluated using a qualitative study of 12 students which found that the editing features were considered easy to use and efficient for removing ‘umm’s and long pauses,However many participants reported that the transcripts were not accurate enough to use without having to listen to the audio,The systems so far have only considered handling speech that has already been recorded,"Often, speech is recorded from a pre-written script or from notes.   created a system called Voice Script that supports an integrated workflow for writing scripts, and recording/editing audio",An informal study with four amateur participants found that it could support various workflows including multiple iterations,"It included a ‘master script’ layout which was used to bring together different recordings, and that was found to work well","A second study of four amateur participants directly compared the system to that of  , which found that participants were able to complete an audio production task 25% faster using the Voice Script system","This study demonstrates that for workflows that involve pre-written scripts, there is potential to improve the audio editing by using an integrated writing and editing system","ASR transcripts were used by  ,  ,   and  , but   and   chose to use perfect transcripts from a crowd-sourcing service.   used aligned subtitles and   used a combination of subtitles and ASR","In summary,   found that semantic editing of speech in voicemail is faster and as accurate as using waveforms.   found that for editing discussions, semantic editing is a more accessible alternative to waveform editing","Systems have been developed for audio and video production ( ), but these were mostly designed without prior user requirements, and the studies were informal and used amateur participants","In this paper, we describe the design of our system, which is based on the results of a published pilot study ( ), and present our formal study, which uses real content and professional users in an uncontrolled working environment. 3 System requirements Our interest in applying semantic editing techniques to radio production first emerged from a pilot study we conducted at the BBC ( )","In this section, we discuss the findings of the study and the resulting system requirements for our semantic editing system. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   3.1 Pilot study The objectives of the pilot study were to discover how radio programmes are created, and to identify any opportunities to improve the process","Three representative programme types were studied: a news bulletin, a drama and a documentary","The producers of each programme were observed and interviewed to fully document their workflow, which took between half a day (for news) and four days (for the documentary)","The main finding of the study was that the participants preferred to work with text-based representations of audio, rather than working with the audio directly","For example, the producers of the documentary ‘logged’ each interview they recorded by transcribing it themselves, or by paying a third-party service to write a full transcription","They then used the transcripts to select which bits they wanted to use, and copied the text to create a rough script of the programme","Once the script was mostly complete, they had to find and cut each piece of audio for the programme to create what is known as a ‘rough edit’",Both the logging and rough edit processes are very time-consuming for the producer,"From this study, we identified an opportunity to apply semantic speech editing techniques to these parts of the production workflow. 3.2 Transcripts Radio programmes are assigned a slot in the broadcast schedule, so producers have a strict deadline for finishing their programme","Programmes are often scheduled about three weeks in advance, but sometimes as little as a week in advance",This means that producers have very little time to spare,"If a programme’s budget allows, interview recordings can be sent to a transcription service where they are transcribed by a person overnight","However, most programmes do not have the budget for this, so the producer transcribes the recordings themselves","Transcripts are used to help the producer make editorial decisions, but are usually not published","For this reason, the transcripts only have to be sufficiently accurate to use for editing",Both   and   found that the errors in the transcripts did not prevent users from being able to edit using them,"However, both also found that users wanted to be able to fix incorrect words in the transcript. 
                          Our semantic editing system needs to be able to produce a transcript quickly and cheaply","It should be accurate enough to be useful for editing, and allow for correction where necessary. 3.3 Editing There are already well-established systems and software in place for producing radio programmes","Producers use a digital audio workstation (‘DAW’) to select the parts of each interview that they want to use in their programme, and to arrange them into a narrative",The BBC provide two different DAWs – dira! StarTrack (made by SCISYS) and SADiE (made by Prism Sound),"Some producers prefer to use other DAWs, but as installation of software is restricted on corporate computers, they must use their personal computers",Waveforms are used to visually represent audio in the DAW to help the user navigate the recordings,The edits performed in a DAW are ‘non-destructive’ because the original recordings remain untouched,This allows the producer the flexibility to adjust or undo their decisions at any point during the editing process,"Specialist sound engineers (known as ‘studio managers’) are sometimes brought in on the last day of production to ensure that the sound is well balanced, and to do any advanced editing that is required",This includes removal of unwanted ‘umm’s or breaths in a process called ‘de-umming’,"Being able to de-umm speech in a way that is inaudible to the listener is considered to be a skilled task that requires precision, judgement and experience","Music is often included in a programme, either as a theme tune, a short interlude or in the background","Producers select the music, often from their personal collection","However, a number of services are also used for finding suitable commercial or rights-free music, such as Audio Network 
                         ",The music is added and edited using the DAW,"At the end of the editing process, the producer’s supervisor (known as the ‘editor’) listens to the programme with the producer to give their feedback and sign-off","As part of this process, they both listen out for repeated words or phrases","However, this is only usually a problem in drama production where multiple takes of the same lines are recorded. 
                          Our semantic editing system needs to be able to select and arrange parts of audio recordings","Given that there are well-established radio production systems for advanced editing tasks such as de-umming and addition of music, it also needs to be able to integrate with these so that it can be used in professional radio production. 4 System design This section describes the design of our system, as guided by the requirements set out in  ","We explain the rationale behind our design decisions, outline our implementation, and describe the functionality and operation of Dialogger",The goal of our system is to improve the radio production process by using semantic speech editing to make it easier and more efficient to navigate and edit recordings of speech,"To fulfil our system requirements, we designed our system to produce transcripts quickly and cheaply, efficiently select and arrange parts of audio recordings, and integrate with existing radio production systems. 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   4.1 Automatic speech recognition Three factors were considered when choosing a transcription method – turnaround time, cost and accuracy","Previous semantic speech editing system have used manual transcription ( ), ASR ( ) and alignment of subtitles ( )","Manual transcriptions are nearly 100% accurate, however they are expensive (about $1 per minute) and slow (typically 24 hours)","ASR transcripts are imperfect, but cheap (about $1 per hour) and fast (quicker than real-time listening)","Our system requires quick and cheap transcripts that are sufficiently accurate, so we chose to use ASR transcripts generated by a state-of-the-art commercial web service. 
                          
                          and   found that users want to be able to correct the transcript, so we designed our system so that users can fix incorrect words","As part of the transcription process, the ASR system performed speaker diarization ( ), which gave each speaker an identification number and estimated their gender","We segmented the transcript into paragraphs to indicate changes in speaker, and used a text label at the beginning of each paragraph to display the gender and identification number (e.g. [M2], [F5]).   also identified speakers by placing their respective parts of the transcript in different columns","However, this approach limits the number of speakers by the number of columns that can be displayed","By labelling paragraphs, we are able to support multiple speakers",The ASR system also gave each word a score to represent the confidence of the word being correct,"We used this confidence rating to shade any words that fell below a threshold, known as ‘confidence shading’",This can help the user identify or ignore words that may be incorrect ( ). 4.2 Editing interface Our semantic editing system needs to be able to efficiently select and arrange parts of audio recordings,"Previous approaches have used select/delete ( ), cut/paste/delete ( ), and drag-and-drop ( ) as methods of editing the transcript","Select/delete interfaces allow parts of an individual transcript to be chosen or removed, but this does not support re-ordering","Cut/paste/delete interfaces do allow re-ordering, but it can be difficult to extract a small clip from a long recording, and to track the clip boundaries and their origin","We chose to use a drag-and-drop interface as it allows selection of smaller clips from long recordings, re-ordering of clips, mixing of clips from different recordings, and has clear boundaries between clips",Our editing system also needs to integrate with the existing radio production system,"To facilitate this, we designed our system to export an EDL that describes the edit points","This ‘non-destructive’ approach allows integration with other audio editors, and for them to re-adjust previous edits, add new ones and insert additional components such as music. 4.3 System description This section gives a brief overview of Dialogger, including details of our implementation, the functionality and its operation","A screenshot of the interface and numbered list of the main features are shown in  
                         ","A live demo of the system is also available 
                         . 
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      4.3.1 Implementation We designed our interface to be browser-based, as the BBC corporate policy meant that it was not possible to install new software on the producers’ computers","This came with the added benefit of allowing users to work from anywhere in the world on any operating system, but the downside that they have to be connected to the Internet","We included two export options – a ‘destructive’ WAVE audio file output, where the edits cannot be re-adjusted, and a ‘non-destructive’ EDL output, where they can",The time taken by the transcription service to process each uploaded recording was approximately half as long as the length of the recording,"The time depends primarily on the length of the recording but also on noise, accents and the complexity of the speech",The ASR system we chose was evaluated using a large multi-genre television dataset ( ),"It had an overall word error rate of 47%, however for news content, which is clearly spoken by a native speaker, this dropped to 16%","As the speech on radio programmes is similar in nature to speech on television news, we found the error rate to be comparable",However recordings with non-native speakers or significant background noise had a higher error rate,"For comparison, the reported error rate of the system used by   was 28%, and for   it was 10%",We did not include features for adding or editing music,"During the pilot study, we found that specialist tools are already used for finding and choosing music, and that editing of music is already efficiently handled by the DAW.   included features for finding music tracks and creating loops within them. 
                             also included detection of repeated words and phrases","We chose not to include this, as our pilot study found that repeats are only an issue in drama production","As the production of drama involves a very different workflow of recording multiple takes of lines from a script ( ), we chose to focus on production workflows for pre-recorded content in our system design or study. 4.3.2 Operation The functionality and operation of the system is described below as a typical user journey",Each feature is numbered and shown in  ,Users access Dialogger by navigating to a web page in their browser,They start by logging into the system using their account (1) and create a project where they can upload their speech recordings (2) that appear in a list on the left (3),Each recording is automatically transcribed,"When it is opened, the waveform appears at the top and the transcript appears in the middle section",The recording can be played (5) and navigated by using the waveform (4) or by clicking on a word in the transcript (6),The transcript shows where different people are speaking using paragraphs labelled with the speaker’s gender and an identification number (e.g. [F2]),"Words which are likely to be incorrect are shaded grey (7), known as ‘confidence shading’",Incorrect words can be fixed by double-clicking them and typing the correct word,The transcript text can be copied or printed using buttons at the top,"The audio can be edited by selecting a range of words (8), then using drag-and-drop to place it in the area to the right which creates a clip (9)",Clips can be re-ordered and deleted,The total duration of the edited clips is displayed (10),"The edited audio can be played through to preview the result, and the edit can be saved","Finally, the edited clips can be exported as a .wav audio file or as an EDL (11) for further editing in a DAW. 5 Evaluation methodology We were interested in determining whether professional radio producers could successfully use a semantic speech editing workflow with ASR transcripts as part of the production of a real radio programme","We wanted to investigate what effect semantic editing had on the production workflow, and whether there were any specific features that could be added to improve the functionality","Additionally, we were interested in measuring whether this approach was faster than their existing workflow, and if it continued to be used after the trial",We also wanted to take this opportunity to continue our research on the existing radio production workflow to learn more about the challenges producers face and the tools they use to produce their programmes,"Our pilot study did not explore requirements in-depth, and there is not much previous literature that analyses actual radio production practice, so we wanted to be able to inform researchers and designers about real requirements and behaviour in this field","To achieve these goals, we conducted a qualitative contextual study of radio producers working under two conditions – the existing editing workflow and the semantic editing workflow. 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   5.1 Approach Radio production is not well-studied within academia","Although a number of unrelated studies on television production systems have previously been conducted ( ), but we were unable to find any that studied radio producers. 
                          However, in this case we were able to recruit professional radio producers from the BBC due to the access available to us as an internal research department","We therefore chose to take a qualitative approach to data collection so that we could learn about the radio production process, and develop an academic understanding of the production workflow","We used direct observation, where we witnessed radio production first-hand without taking part",Direct observation allowed us unobtrusively to collect the data without adding to the producer’s existing high workload,"Additionally we could observe the real-world process, as opposed to a theoretical or reported one, and take into account the context of the working environment",We also hope that our qualitative inquiries could help to bridge the gap between academics and practitioners,"To take advantage of the available access to the work environment, we chose to use contextual inquiry techniques to learn about the workflows, tools, and the social, technical and physical environments","This took the form of an initial interview, followed by a period of observation, then a final interview",Radio producers find it difficult to step away from their day-to-day work for too long,"To account for this, each system was used for the production of the programme that the participant was working on at the time, and the audio content they needed to edit that day",Participants were interviewed and observed in their normal working environment. 5.2 Design and procedure We designed a five-stage experimental procedure that followed a typical contextual inquiry format of interview/observation/interview,"In addition, we recorded some simple metrics such as task completion time and feature usage. 
                         
                         
                         
                         
                         
                      
                            
                            
                         
                            
                            
                         
                            
                            
                            
                         
                            
                            
                         
                            
                            
                         Stage 1: Background interview Participants were first given an overview of the project and the study, and asked to complete a consent form","This was immediately followed by a semi-structured interview to learn about the participant’s background, their existing production workflow and the tools they used as part of that","The investigator asked participants to describe the radio production process in detail, and used follow-up questions to clarify their understanding",This information was recorded using written notes,Stage 2: Dialogger training A short training session on the Dialogger interface,"Each participant was trained on the interface’s functionality using a pre-written ‘tool-tip tour’, in which the participant was presented with a sequence of instructional pop-up dialog boxes overlaid on the interface",This ensured consistency of training between participants,Each participant was then issued with a series of tasks that utilised all of the system functionality,The investigator observed this stage and wrote notes of any unexpected behaviour or stumbling blocks,Stage 3: Task observation Observing the participant as they produced a radio programme,"Each programme is composed of a number of interviews on a single topic, or set of topics",We observed the participant while they logged and rough-edited two different interviews for the same programme,"They did this by editing an interview under each condition – one using their existing production workflow, and the other using Dialogger",The order of the conditions was counterbalanced,"The investigator sat beside the participant during the task and wrote notes on the workflow, tools, generated metadata, usability issues, task completion time, and unexpected reactions or usage",The actions of the participant on Dialogger were logged electronically,"After they completed the task, they were asked to rate each condition using the NASA Task Load Index metrics ( )",Stage 4: Interview A semi-structured interview about the participant’s experience of each system and how they compared,"The investigator questioned participants about the advantages and disadvantages of each workflow, then asked about any specific topics, issues or questions that cropped up during observation",The audio from this interview was recorded and transcribed to allow for further detailed analysis,"Stage 5: Longitudinal deployment Each participant was then given access to Dialogger for a further month, and was invited to continue to use it if they wished","Each week, they were asked via email whether they had been using the system, and if so, which features they valued most/least or were missing","During this time, their usage of Dialogger was also logged electronically. 5.3 Recruitment We invited professional radio producers with at least five years of experience to take part by sending an email to departments in BBC Radio that create pre-produced factual programmes",Drama programmes were excluded from the study as their production workflow involves making multiple recordings of lines in a script and selecting the best ones ( ),This is a sufficiently different process to other programme genres that it warrants a different interface,"Five participants (P1–P5) were recruited (4 male, 1 female) who each had between 6 and 20 years experience in working as a radio producer","Although we had a small number of participants, the experience of the producers and the depth of the study means that their feedback should carry significant weight",Five participants is also considered sufficient for identifying most usability problems ( ),"During the experiment, the participants worked on programmes of different lengths from a range of genres: P1 produced a single 27-min documentary, P2 produced a 27-min documentary as part of a ten-part series, P3 produced a single 45-min documentary, P4 produced a 14-min archive programme (based around material from the archive) as part of a ten-part series, and P5 produced a single 27-min magazine show (covering multiple stories on a single theme). 5.4 Analysis Our study produced observation notes, interview transcripts and metrics","We used thematic analysis with open, flat coding to interpret the textual data, and we used statistical analysis to process the numeric data, as described below. 
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                      5.4.1 Coding We manually transcribed the audio recorded from the interviews in Stage 4 to produce a verbatim transcript, and collated them with notes made by the investigator from Stages 1, 2 and 3","To organise and process this textual information, we employed the use of coding techniques",We performed a two-stage coding process,"Firstly, we openly coded each part of the transcripts into a flat structure","As there are not many previous studies on radio production, we decided to use open coding so that the categories would emerge from the data we collected, rather than attempting to test an existing model",We used the software package RQDA ( ) to execute this stage,"Once all of the text had been processed, we grouped the codes that had common themes",We used mind-mapping software to help us re-arrange the codes into various hierarchical structures until a logical solution was found,"The coding and grouping was performed by the investigator that collected the data. 5.4.2 Metrics To enrich the qualitative data we collected, we also measured some basic metrics including task completion time, cognitive load and post-trial system usage","The number of participants in our study was small, our study was conducted in an uncontrolled setting and we did not set clear hypotheses before starting","As such, these metrics will not be statistically valid, nor have confirmation power","Nevertheless, we chose to collect this data to gain an initial insight into how semantic editing might affect the editing speed and workload during the study, and work practices after the study",We used task completion time from stage 3 as a metric for editing speed,"As participants used different interviews of varying lengths for each condition, we measured task completion time relative to the length of the audio recording being edited",We used a paired  -test to test for any significant difference between the existing and semantic editing workflows,"To measure the cognitive load of each task, we used the raw TLX metrics gathered from the questionnaire in stage 3",We used a paired  -test on each of the six metrics to test them individually for any significant differences,"Finally, to measure the level of usage during the longitudinal deployment in stage 5, we collected the time spent using the interface, the number of new uploads and the number of exported edits","As this data is only relevant to the semantic editing workflow, the raw numbers will be reported. 6 Results The coding process resulted in 40 codes, which were grouped into ten categories and four themes (see  
                      )","The codes contain comments about both the existing and semantic editing workflows, however for clarity we will present these results individually","We start by going through the existing radio production workflow in detail, with an emphasis on the challenges that were identified, and the tools that are used as part of the process",We then consider the semantic editing workflow and expand on the four themes identified during coding,"Finally, we look at the results of the metrics that we captured during the observation and longitudinal deployment. 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   6.1 Existing workflow This section describes the existing radio production workflow",The process is documented as described by participants in the interviews (Stages 1 and 4) and as witnessed by the investigator during the observation (Stage 3),"We start by providing an overview of the workflow, then discuss the topics identified through coding. 
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      6.1.1 Overview This section describes the high-level production workflow for pre-produced content as described by the participants","This is also represented by the numbered diagram in  
                            , and the description below uses numbers to make reference to each part of the diagram","When a programme is commissioned, the producer starts by researching the subject (1) in detail to identify a compelling story and to find the best contributors","This is done by reading, listening and watching existing material on the subject, and finding and talking to relevant people",During this time they also recruit a presenter for the programme,"After researching the topic, the producers then arrange interviews with contributors and record them (2) with the presenter either in a studio, an external venue or via a telecommunication link","Once material has been recorded, the next step is for the producer to select which parts of the audio they want to use in the programme",This selection process is often aided by creating ‘logs’ of the interviews,"Logging (3) helps the producer by allowing them to see, on screen or on paper, what was said in each interview, when and by whom, without having to listen to it",This allows them to quickly find and share the pieces that they want to use in the final programme,"It also helps them structure their thoughts, identify themes running through discussions, and make links between different interviews",Logs are usually written by the producer themselves,"As they have done the research and are normally present at the recordings, they can use their memory to navigate the material and use their experience to quickly determine which parts are relevant","Some programmes that are under particular time pressure will use a third-party to create a verbatim transcript of a few interviews ( ), but most do not because it is too expensive","In the observation, P1 and P3 wrote their logs in Microsoft Word whilst using a digital audio workstation (DAW) to play the recording (see  
                            )","P5 reported that they usually followed a similar process, but did not do any logging during observation","The exact format of the logs varied between individuals, but they usually contained a rough transcript of the interview with occasional timestamps and notes",They reported that this helped them to find important bits in the recording later on,"Each producer has their own syntax, but there are commonalities","Timestamps were written on the logs, approximately   (P1) with minutes and seconds in parenthesis:  , for example",This allows the producer to navigate to a particular piece of audio much faster than they would otherwise by narrowing down their search range,"The next stage is to rough edit (4) each recording, which involves segmenting the audio into chunks, removing the chunks they don’t intend on using, and to label and arrange the remaining chunks","The editing reduces the amount of material they need to work with, and the labels make it easier to identify which parts are which","If a recording is short (around 15 mins or less), then this process is usually done without logging the content first","This process of recording, logging and rough-editing is repeated until the producer has enough material for their programme","Throughout this process, a script is used to organise and structure (5) the content of the programme","The script usually takes the form of an MS Word document, but can also be written on paper","Some producers only write a rough outline, whilst others will copy in full transcriptions of the content they are using","Some producers have an idea of what the programme will look like before they make any recordings, so will create the script first","Others will be guided by the content of the interviews, so will wait until they have some recordings before creating a script. 
                             (P3) The script is also used to collaborate with and get feedback (6) from the presenter","Using a written document rather than audio files makes it easier to quickly review the content of the programme, make notes and suggestions, and to do this over large geographic distances","The presenter uses the document to write and insert ‘links’, which are the narrative elements spoken by the presenter to link the interview clips into a story",They also insert comments and notes for the producer,"Once the script is nearing its final stages, the presenter records (7) their links and sends them to the producer","If the programme is part of a series, it will often have theme music","Any other music is chosen (8) by the producer, often using production music services such as Audio Network 
                            , or the BBC’s Desktop Jukebox","Once the interview, links and music are ready, the producer will assemble these into an edit (9) that matches the script, using the DAW","This edit will be about twice as long as the final programme, sometimes significantly longer (e.g",P5 reported that they once created 22-hour-long rough edit for a 37-min programme),"The producer then continues to edit the audio, primarily to ‘get it down to time’, but also to give it a final polish by cleaning up the audio","Cleaning involves finely adjusting sound levels to be consistent, removing ‘umm’s and breaths, and adding non-speech sounds to create a rich auditory scene","Producers who are skilled at using a DAW will usually clean up the audio themselves, but others will bring in a sound engineer (known in the BBC as a ‘studio manager’), to help with this (10)","Before a programme is broadcast, it must be signed-off by the department head, known as the ‘editor’","Most producers will get feedback from the editor (11) before this, so that they have time to make any requested changes",Often this happens a day or two before transmission,"Some participants reported that they sit in the room with the editor while the current version is played, while others sent an audio file to the editor to listen by themselves","In both cases, the editor gives oral feedback and suggests changes","Once the editing is complete, a final version is rendered to an audio file and added to the playout system for the editor to sign-off (12). 6.1.2 Challenges of comprehension The skill of the producer is to   (P1, P3, P4 – all verbatim) and to find the clips which will make an interesting programme. 
                             (P1) However, the sheer quantity of recordings means this process adds significant overhead. 
                             (P3) Interviews recorded for speech radio often cover complex topics in fine detail","Keeping track of all the points raised and forming a compelling narrative from them is a challenge. 
                             (P4) Writing the logs takes a lot of concentration as the producer must listen to what is being said, work out how it ties in with other contributions and the story, and make swift judgements on whether it should be used. 
                             (P3) P1 and P5 reported that they find the office environment distracting, so often work at home or outside the office. 
                             (P1) 
                             (P5) Although P4 did not do any logging during observation, they explained that for longer recordings, they would normally write logs by hand in a notebook whilst listening on a portable music player somewhere away from the desk, such as in a café","The high level of concentration required, combined with the repetition of typing and listening to the interview again means that producers need to take regular breaks.   (P3) Dialogger addresses the burden of manual logging by using ASR to automatically generate transcripts that could be used to supplement or replace logging. 6.1.3 Programme script The producers organised the programme by writing a ‘script’","This is primarily used to help them structure their thoughts, but also to help communicate with the presenter over email","In the study, P1, P2 and P5 started their scripts during the research stage by writing an ordered list of bullet points of topics to cover and a list of draft questions to ask contributors","P3 and P4 waited until after they had done some interviews to start the script, as they wanted to structure the programme around the discussions that they recorded",P3 and P5 updated the script after every edit to ensure they were always in sync,This added significant overhead but gave them a visual structure to follow when making the final changes,"Having an accurate script also makes it easier to re-use the programme afterwards, when creating another version of a different length, or for pulling out clips for the website. 
                             (P5) Although Dialogger does not include a written script, it may reduce the need for keeping a separate script as the producer’s edited audio is presented as text. 6.1.4 Mark-up P1, P3 and P5 would make comments for themselves in the log to help them when editing","For example, “ ” or “ ′ ”","P1 also used a star rating system to rate the quality of each point, for example “ ”. 
                             (P3) Bold highlighting was also used by P1 and P3 to mark bits of the transcript which are important and worth keeping. 
                             (P1) P2 used a different approach to logging their material","Instead of logging the material by writing a transcript, they played the recording in a DAW and used a keyboard shortcut to create timed markers at any points of interest","By seeing where the markers clustered, they identified where to make clips, then gave each of the clips labels","This approach allowed them to focus more on the audio, but didn’t allow them to make any detailed notes. 6.1.5 Sound quality Radio is an audio-only medium, so the quality of the content is highly dependent on the quality of the sound","The criteria producers use for deciding whether a piece of audio is good enough to use in their programme is not just about what was said, but how it was said and how well it was said. 
                             (P5) On the one hand, producers need to listen out for any poor quality sound that might negatively affect the programme, such as people mumbling, stumbling, coughing, or any excessive background noise. 
                             (P3) However, the producers were are also listening out for anything that worked particularly well, such as a moment of comedy or passion, or a sound that perfectly captures the right feeling",Identifying these using the text of a transcript is very difficult or impossible,Every participant that performed logging played the audio faster than real time at least once,This allowed them to efficiently listen out for anything they might want to use while reviewing parts of the interview that may not be of interest (e.g. off-topic or ‘off-mic’ discussions),"P2 also used faster playback to prevent themselves from over-thinking their edit decisions and picking out too much material. 
                             (P2) Dialogger includes integrated playback, where clicking on a word replays the audio from that position","This should make it simple for the producer to hear how things were said and identify poor quality audio. 6.1.6 Edit technique If the recording was short and had been recorded recently, as was the case for P4 and P5, it can be edited without first creating a log. 
                             (P3, also P4 and P5) In this situation, we observed that the producers listened through the recording using a DAW and pressed a keyboard shortcut to split the recording, usually at the beginning/end of questions/answers",They then went back to remove unwanted segments and add labels to the remaining ones,"In the cases where the recording was logged (P1, P2, P3), the producers used the log to decide which parts to select or remove",They used the timestamps written in the log to narrow down their search area for each clip they extracted,"However, even with a reduced search area, the producers found it time-consuming to find the exact start and end point of each clip using the DAW interface","In the study, three of the participants (P3, P4 and P5) used SADiE as their DAW, which is provided to the producers by the BBC","However, the other two participants chose to use other software packages that aren’t formally supported","P1 used Adobe Audition because they were familiar with the interface and it was installed on their laptop, unlike SADiE which was only available to them on a desktop computer","P2 comes from a television production background and used Apple’s Final Cut Pro, which is primarily a video editor but also includes audio editing functionality",P2 used Final Cut Pro because they were familiar with the interface and had it on their laptop,"In addition, they enjoyed being able to import audio directly from video content without having to use another program to extract the audio first, and being able to use the video ‘titles’ feature to make written notes that can later be viewed in time with the audio","Dialogger includes EDL export, which will allow it to integrate with existing digital audio workstations, as used by the producers. 6.2 Semantic editing workflow This section discusses the results and themes that emerged from the evaluation of the semantic editing interface",Participants were first introduced to Dialogger through a training stage (Stage 2),All of the participants completed the training without any major issues,"However, this stage highlighted a requirement for keyboard shortcuts which was not previously identified",P2 and P3 kept trying to use the space bar to start and pause audio playback,This is a common shortcut in most DAWs which these participants naturally reached for,"Reports on previous semantic editing systems have not mentioned keyboard shortcuts, however they could be used to assist the editing process","In the rest of this section, we will discuss each of the themes that came out of the coding (see  ). 
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                      6.2.1 Navigation Participants reported that having the transcript available in the semantic editing interface allowed them to read and search the recordings much faster than they normally would with a waveform, which is in line with previous findings from   and  . 
                             (P1) The transcripts also allowed the participants to quickly cross reference what was said in various interviews without having to listen through multiple times. 
                             (P2) Being able to click on a word to navigate to that point in the audio also enabled the participants to use visual search to quickly find and listen to bits they were looking for. 
                             (P4) Participants reported that editing with a transcript was primarily useful when working at the sentence level","When the granularity of editing involves removing individual words, ‘umm’s or breaths, they said that the DAW software is much better suited to these tasks","This supports our design decision to integrate with DAWs. 
                             (P3) 6.2.2 Transcript accuracy When using the semantic editing interface, editing decisions are based on an ASR transcript which is only partially accurate","Previous research has shown that for editing voicemail recordings ( ), discussions ( ) and spoken comments ( ), ASR transcripts were considered sufficiently accurate","However, the ASR accuracy required for navigation and editing in radio production is currently unknown","The participants in our study suggested that the transcripts were, generally speaking, sufficiently accurate for their purposes. 
                             (P2) If the recording being edited was made recently, the producer can use their memory of what was said to make sense of the inaccuracies in the transcript. 
                             (P1) In the existing radio production process, transcripts are used to aid the producer and presenter, but are not shared outside of the production team","In our study, the producers we observed only used the transcript to navigate and edit the audio","However, P3 and P4 noted that they were interested in correcting the transcript later so it could be shared or published. 
                             (P4) Being able to provide corrected transcripts has the potential to make an impact beyond improving the editing workflow","For example, transcripts of the finished programme could make the audio content searchable and re-usable for print media. 6.2.3 Mark-up During the study, P1 and P3 copied the transcript text from the interface into MS Word",They reported that they did this because there was no annotation functionality available within Dialogger,"They inserted paragraph breaks, added notes after paragraphs, and highlighted desired parts of the transcript in bold","Once the transcript was annotated in MS Word, they went back to Dialogger, found the parts of the transcript they wanted by scrolling though the text, then dragged and exported each clip individually as a .wav file. 
                             (P3) Producers are very familiar with the MS Word interface so a later version of our system could seek to provide a similar interface","This would allow producers to make annotations in the same way they do already. 
                             (P4) The most basic feature that could be added is highlighting, which is often used to note parts of interest 
                             (P3) 6.2.4 Portability P5 reported that working on paper allowed them to be productive outside of the office, such as during their commute. 
                             (P5) Additionally, working on paper allows them to work anywhere as it does not require electricity. 
                             (P2) In the observed task, after uploading their recording, P2 immediately printed the transcript and read through it on paper so that they could work away from the screen. 
                             (P2) P2 then used a highlighter pen to select the desired parts of the recordings (see  
                            )","After highlighting all the pieces they wanted, they then used the Ctrl+F text search to find the highlighted words in Dialogger. 
                             (P2) However P2 noted that having timestamps on the printout may be a faster way of achieving the same thing","Once they had found and clipped all of the highlighted parts in Dialogger, they exported the clips into SADiE","P4 explained that for an upcoming programme, they were planning to print out transcripts from Dialogger to help them collaborate with their presenter. 
                             (P4) 6.2.5 Sound quality Part of the appeal of having a transcript is that it frees the user from listening to the audio in real-time","It also allows users to work on paper, away from any electronic devices","However, disconnecting the audio from the text fundamentally changes the production process. 
                             (P4, also P2, P3, P5) There was also concern that parts which sounded great but didn’t come across as well in the transcript may have been overlooked. 
                             (P2) As discussed in  , the participants existing workflow includes playing the audio faster than real time, but that feature was not included in Dialogger","Several of the participants noted that they would like to have this feature added. 
                             (P2) Although faster than real time playback normally reduces intelligibility, this may be less of a problem if the transcript was available. 
                             (P4) As listening is an important part of the production process, semantic audio interfaces would benefit from providing easy access to the underlying audio to allow multi-modal interaction","Once the link between the audio and the text is broken, re-linking the two together can be costly. 6.2.6 Drag-and-drop In Dialogger, we used a drag-and-drop technique for users to create clips from various interviews and re-order them in a clipboard area","All of the participants were able to use this successfully, however we quickly encountered issues when dealing with longer clips. 
                             (P5) We performed our initial testing by pulling short clips, but for real-life usage, participants were mainly interested in creating large clips","This quickly filled up the clipboard area and users struggled to find the space to add more clips.   found that participants were mainly interested in making small edits, which contradicts what we found","However,   tested their system on recordings of voice-based discussions, rather than radio interviews",This suggests that the context and content may have an effect on the granularity of the user’s edits,P2 suggested modifying the interface so that clips were created by selecting the text and using a button to add the clip to the end of the clipboard,The problem could also be addressed by collapsing and expanding the clips to minimise the area they occupy. 6.2.7 Usability Users could transfer their edits from Dialogger to a DAW by saving and opening a file,"However, some participants wanted much tighter integration with the DAW, including bi-directional transfer of edits, so that edits made in the DAW were reflected in the semantic editor and vice-versa. 
                             (P3) None of the participants found the waveform display in Dialogger to be useful, and found it to be an unnecessary addition to the transcript text. 
                             (P5) Some participants also noted that they would prefer a cut-and-paste approach to copy-and-paste, as this prevents any duplication of content","This could also be achieved by marking which parts of recordings have already been used. 
                             (P4) 6.3 Metrics 
                         
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                      6.3.1 Time The time taken to complete the observed tasks was recorded (see  
                            )","As various recordings of different lengths were used for the existing and new workflows, the times are reported relative to the length of the original unedited audio","In all cases, the producers were able to run the ASR processing as a background task so this was not included in the calculation","P1, P2 and P3 used the semantic editor after their existing process, while P4 and P5 did the opposite","However as different recordings were edited on each system, the presentation order is not expected to affect the results","The mean average time for semantic editing was 0.79 minutes per minute of audio, versus 1.13 minutes for the existing method, which is a 44% improvement","However, a paired  -test revealed that there was no statistically significant difference (  = 0.24)","This is due to the small sample size and the large variations in timings resulting from P4 and P5 not doing any logging, and P2 printing out and annotating their transcript before editing","Semantic speech editing may have the potential to reduce the time needed for logging and rough-editing material, but further investigation with a larger sample and consistent workflow is required to measure time performance. 6.3.2 Cognitive load After completing both tasks in the observation, the participants were asked to rate both the old and new workflows using the raw NASA-TLX metrics ( )",No significant differences were detected for any of the metrics using the paired  -test,"With only five participants and marginal differences, it was not possible to draw any conclusions about cognitive load from these results","They indicate that Dialogger requires slightly less effort and mental demand, and is less frustrating","However it is considered more physically demanding, temporally demanding and scores lower in performance. 6.3.3 Longitudinal deployment After the interviews and observations were complete, the participants were given access to Dialogger for a further month (Stage 5)","During this time their actions were logged electronically and they were emailed each week to ask which features they found useful, or were missing","P3 was unavailable immediately after the study, so could not take part in this stage",Most of the comments received in the longitudinal deployment were already picked up by the first part of the study,"In the remaining comments, all of the participants said they enjoyed being able to use Dialogger outside of the office and at home","Some reported that they had issues uploading content with their slow network connections, and P2 suggested that allowing multiple simultaneous uploads would allow them to leave it running overnight",Participants were given access to the system for one month after the study,The logs from the interface were analysed to see how the participants used Dialogger during this stage of the study,All of the participants continued to use the semantic editor of their own accord as part of their work,"The total time spent by the four remaining participants (P1, P2, P4, P5) using Dialogger during the one-month deployment was 23 hours and 58 minutes","Over 14 hours of those were from P2, with P4 using it for 5 hours, P1 for 3 hours and P5 for 20 minutes","During this period, 86 recordings were uploaded and 58 audio edits were exported",Users could navigate the content by either clicking on the waveform or by clicking on a word in the transcript,"The interaction log showed that over 98% of navigation actions were executed by clicking on a word, which shows a clear preference for navigating by text compared to waveforms. 7 Discussion We found that producers face a number of challenges with audio editing in radio production",There is often a large quantity of audio to process so it can take a long time,"The content of the speech is usually complex and contains interconnections to things said in other recordings, which can be difficult to keep track of","Making editorial decisions also requires a high degree of concentration over an extended period, which is demanding, especially in the noisy and distracting office environment","We observed that in their existing workflow, participants tackled these challenges by employing a number of techniques to filter and arrange their audio content","They started by listening back to all of their recordings, which allowed them to simultaneously assess the editorial content and sound quality of the audio","For long recordings, many participants ‘logged’ the audio as they listened, by typing rough transcriptions and notes into a word processor, which they later used to help them edit the audio using a digital audio workstation (DAW)","For short recordings, instead of logging, participants segmented their recordings in the audio editor during playback, and went back to remove unwanted segments and label the rest",All of the participants used a word processor to create a programme script in which they developed the structure and content of their story,"They used annotations to highlight or rate the transcripts, and wrote notes to help them with selecting and assembling the final content","We introduced a semantic editing system into professional radio production, which the study participants were able to successfully use as part of their workflow","On average, the semantic editing workflow was much faster than the existing workflow, in line with previous findings from  , but our results were not statistically significant so this requires further investigation","All participants continued to use the system after the trial, which shows that semantic editing performs a useful function in their production workflow","However, we identified a number of important features that were missing or could be used to improve future semantic speech editing systems","These related to listening, annotation, collaboration and portability, each of which are discussed below. 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   7.1 Listening Logging is an important process that primarily involves labelling and organising content, however it is time consuming","Some participants found the logging process to be valuable because it gave them the opportunity to listen back through their recordings, and make connections between various bits of content",This cross-referencing could also be assisted by providing links between words within and between recordings,"For example, selecting a word could display and replay other mentions of that word in other recordings",Another important reason for listening is to ensure a high ‘sound quality’,"Participants wanted to avoid low quality audio such as ‘umm’s, mumbling, coughing and excessive background noise, but they also wanted to ensure they didn’t miss any high quality audio moments that might not have been identified using the transcript","Faster playback is already used in radio production to reduce the time spent listening to material, however more sophisticated time compression algorithms such as those described by   could be used","Time compression has not been included in previous semantic editing systems, but should be considered in the future, especially as   found that the maximum time compression factor is significantly higher when an ASR transcript is present","Removal of ‘umm’s and breaths, known in radio production as ‘de-umming’, is either done by the producer themselves or with the help of a sound engineer, depending on the producer’s experience and time pressure","To maintain sound quality, the removal of umms/breaths must be audibly transparent and participants reported that this can difficult to achieve","Previous semantic editing systems have included functionality to remove umms ( ) and breaths ( ), however these were made possible because the manually generated transcripts explicitly transcribed those items","ASR systems are normally trained to ignore umms/breaths rather than transcribe them, which prevented us from including this functionality","A transcription system that includes these would allow us to add this functionality, however further research is needed into the extent to which de-umming can be automated in this way. 7.2 Annotation All of the participants used a script document to structure and assemble their programme, and as a medium to inform and gather feedback from the presenter about the content and layout of the programme","Although the clipboard of our semantic editing system acted much like a programme script, the participants did not use it in that way because it was missing some key functionality for annotation and collaboration","Annotation features were an important requirement that we did not pick up on during the design specification, and which have not been included in previous semantic speech editing systems","Two participants in our study deviated from the expected workflow in order to annotate the transcript, and the other participants noted the absence of such functionality","Participants wanted to be able to annotate the transcripts as they would with a word processor, in order to highlight or rate particularly good parts of their recordings, add personal comments, and to segment and label the content","These activities serve primarily as a mnemonic device, but can also be used for collaborating with others","A simple solution for achieving this would be to allow the transcripts to be formatted, and for textual comments to be inserted and edited","To make the semantic speech editing interface more like a traditional word processor, drag-and-drop editing could be replaced with cut/copy/paste similar to   and  ","Alternatively, rather than adding word processing functionality to a semantic speech editor, semantic speech editing could be added to a word processor. 7.3 Collaboration Scripts are used as a tool for collaborating with colleagues such as the presenter because the programme’s content and structure can be quickly reviewed and commented on by others without them having to spend time downloading and listening to the audio","Our semantic editing system was designed for individual access to transcripts and edits, however this meant that they could not be shared with the presenter",A better approach would be to allow multiple users to navigate and edit the same material,This could be achieved using operational transformation ( ) which can support concurrent users editing the same content,Participants were also interested in tighter integration with the DAW,"The same technology could be used to create bi-directional integration with DAWs, so that any edits made in the DAW are automatically updated in the semantic editor and vice-versa. 7.4 Portability Participants reported that the open-plan office environment in which they worked was often noisy and distracting, and that they had difficultly working on screens for extended periods","As a result, many reported that they work from home to get away from the office or print transcripts so they can get away from the screen",A more portable semantic speech editing system would allow producers the flexibility to work where they wanted,Digital pen interfaces such as the Anoto system could be used to create a paper-based semantic editor that can be used anywhere and does not involve screens,"Additionally, it naturally supports freehand annotation and may be a better medium for face-to-face collaboration.   has previously explored how speech can be navigated using paper transcripts and   describes how an Anoto system can be used to edit digital documents, however these approaches have yet to be combined. 7.5 ASR Transcripts Participants reported that the ASR transcripts were sufficiently accurate for editing, supporting similar previous findings from   and  ","This is helped by the fact that radio producers record the audio themselves, and can use their memory to cope with inaccuracies","Most participants were only interested in correcting errors that were distractingly wrong, which were often names or locations related to the story","However, as these are known ahead of time, they could be provided to the ASR system as a way to tweak or expand the language model","Currently transcripts of each programme are not published due to the high cost and overhead of producing them, however several participants were interested in fully correcting their transcripts so they could do this","The availability of ASR could have the potential to extend the scope of radio production to include publication of transcripts, such as on a programme’s website","This could help to improve discoverability of the programme’s content through web search, for example. 8 Conclusion We applied semantic speech editing techniques to professional radio production by designing and developing a semantic speech editor based on user requirements, and performing a contextual study of semantic speech editing with producers at the BBC",We found that the participants were successful in using semantic speech editing to produce real programmes and continued to do so after the study,Our results highlighted a number of opportunities to better address the needs of radio producers,"Annotation features such as highlighting, ratings and comments are needed to aid producers in organising and structuring their content","Radio production is a collaborative process, so semantic editing tools should support multiple users",Use of operational transformation would allow concurrent editing and integration between multiple interfaces,"Some participants struggled with office and screen-based working so portable interfaces, such as those offered by digital pen technology, would give producers the flexibility to work where they are most productive","Unwanted noises such as ‘umms’ and breaths must be removed transparently, which is done by the producer or sound engineer","By training ASR systems to transcribe these noises, this could be done in the semantic editor","However, further research is required into the sound quality achieved by this approach","Finally, ‘radio is made with your ears” so there are limits to how much editing can be done using a text-based interface","Editing tools should provide easy access to playback and use time compression features, which allow users to listen much faster, particularly in combination with the transcript. 9 Future work In this study, we compared the semantic editing workflow, which included a transcript, to the existing workflow, which did not","Future research could consider how much benefit is derived from the transcript itself, compared to the semantic editing interface",Natural language processing could applied to the ASR transcript to help users navigate and structure long recordings of speech,"For example, a text segmentation algorithm ( ) could divide an interview into different topics, and a keyword extraction algorithm ( ) could provide a summary of what was discussed","Based on the results of this work, we developed the prototype further to take into account the feedback from the producers in our study",We handed the prototype over to a development team at the BBC who have now turned it into an officially supported production tool,This has allowed producers from around the BBC to use the tool as part of their normal workflow,"As of October 2016, the system has 45 active users and has processed 265 audio recordings",We will continue to collect usage and interaction metrics for later analysis,"After this study, we added the ability to print augmented paper transcripts, whose annotations result in automatic edits to the source audio","This was done in a collaboration between the BBC and Anoto, and is currently being evaluated against screen-based editing in a further study.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0306437919304909," 1 Introduction Information security, while being one of the top priorities for companies, is currently shifting from prevention-only approaches to detection and response — according to Gartner  ",One analyst states:    ; predicting a   billion worldwide spending on information security by 2020,"The vehicle to integrate processes, users, and technology and hence to drive digitalization efforts in companies are executable processes implemented through Process-Aware Information Systems (PAIS)","Security problems in PAIS can be found based on anomaly detection approaches, i.e., by revealing anomalous process execution behavior which can indicate fraud, misuse, or unknown attacks, cf.  ","Typically, whenever anomalous behavior is identified an   is sent to a security expert","Subsequently, the expert determines the alarm’s root cause to choose an appropriate anomaly  , such as, terminating an anomalous process, ignoring a false alarm, or manually correcting process execution behavior, cf.  ","The survey on security in PAIS  , however, shows that the majority of existing approaches focuses on prevention and only few approaches tackle detection and response, particularly at runtime and change time of processes. 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   1.1 Problem statement and research questions Analyzing anomaly detection alarms and choosing countermeasures is  , cf.  ",This applies also to the process domain as many processes operate in    ,"Hence, thousands of alarms must be carefully analyzed as they could be false positives that report benign behavior as anomalous   (e.g., because of incorrectly interpreted noise or ad hoc changes)","Existing work on process anomaly detection, cf.  , reports only if an execution is  ","However, we assume that additional information, e.g., which behavior motivated the (non-) anomalous decisions or the anomaly severity, are a necessity","Without such information, anomalies, likely, cannot be fully understood and it becomes hard to differentiate between harmful anomalies and false positives, but also to choose appropriate countermeasures as anomalies vary in effect and form. 
                         Further on, existing process focused work frequently applies   anomaly detection signatures, cf.  , to identify anomalies","Such signatures, compress all expected execution behavior into a complex interconnected structure","As a consequence, the signatures must be recreated from scratch whenever a process changes, are hard to understand, and can be computationally intense to create",Monolithic signatures were also found to be overly detailed and specific so that benign noise or ad hoc changes (such as slightly varying resource assignments or activity execution orders) are typically reported as anomalies  ,This could hinder an organization as benign process executions could be unnecessarily terminated,"Altogether, this work aims at a flexible approach to detect anomalies, distinguish their cause into benign and malign behavior, and explain their root cause to the users","This is reflected by the following research questions: 
                         
                      1.2 Contribution and research method To address RQ1–RQ3 this work proposes a novel unsupervised anomaly detection heuristic","Instead of monolithic signatures it applies small sets of independent association rules (  RQ1, RQ2) as follows: Let   be a process that should be monitored for anomalies and let   hold all execution traces   of  ","The given behavior in   is represented as a set of association rules   (a signature, resp.)",To analyze if a novel execution trace   is anomalous it is determined which rules in   are supported by  ,Supported means that a trace complies to the conditions specified by the rule,If it is found that   has a lower rule support than the trace   that is most similar to   then   is identified as anomalous and an alarm is triggered,"Doing so, individual rules can easily be replaced if a process changes (  RQ2)","Another advantage of employing a rather simple formalisms such as association rules, when compared to more expressive formalisms such as LTL, is understandability (  RQ3)","Typically, the more expressive rules become the more likely it is that they are misunderstood and the more computational intense it is to mine them, cf.  ","Nonetheless, in order to cover the most prominent process perspectives, the work at hand supports association rules with respect to control flow, resources, and temporal process execution behavior","Furthermore, the proposed approach prevents false positives as it supports noise and ad hoc changes (  RQ1)","This is because process executions, differently to monolithic signatures, no longer must be completely represented by the signatures, but only by a fraction of the rules which a signature is composed of, cf.  ","This also enables to provide more details about the individual anomalies, as it can be reported which rules a process execution (trace resp.) supports and which not, but also the anomaly severity",The latter is composed of the aggregated and automatically calculated rule significance of each non-supported rule,"Overall, this work follows the design science research methodology, cf.  ",The specific method is depicted in  ,"At first, existing relevant literature was thoroughly analyzed based on a systematic literature review on process runtime anomaly detection, cf.  ","Hereby, a range of limitations could be identified, such as, the lack of root cause analysis capabilities in the business process anomaly detection area, forming a foundation to address and identify related research problems and design requirements in the following","Subsequently, design requirements are derived from existing work on anomaly detection in the security domain, in general, and the process domain specifically","As a result an existing rule formalism, i.e., association rules, lays the foundations for a novel anomaly detection approach, cf",This artifact is evaluated in two ways,"In Section  , its feasibility and the quality of the results are shown based on a proof-of-concept implementation and by performing a cross validation with real life process executions, injected anomalies and multiple state-of-the-art comparison approaches","Secondly, the expressive power of the approach is evaluated based on a controlled experiment with users (cf","Section  ). 
                         In this experiment, different visualizations and representations of anomalies and their root cause are compared by the users in finding answers to questions on the anomalies",The correctness of the answers is analyzed in order to gain insights on the effectiveness of the proposed approach,"In addition, the time for giving such answers is also measured to evaluate its efficiency",All findings are then compared with related work (cf,Section  ) and the achieved root cause analysis capabilities are discussed in Section  ,"Stakeholders of the proposed approach are process managers and security experts.  
                         
                      Overall, this work contributes towards a flexible anomaly detection approach that is robust against process change and noise","Its visualization seems promising to foster a more effective and efficient understanding to anomaly root causes than existing approaches. 2 Requirements, fundamentals, and overall approach This section starts off with a summary on design requirements for an anomaly detection approach in flexible process runtime behavior",Fundamental notions are introduced in the sequel,"Finally, we sketch the overall approach which is presented formally in Section  . 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   2.1 Design requirements The design requirements for the approach are selected based on the systematic literature review on approaches for anomaly detection in process runtime behavior conducted in 2017  ","In the following, these requirements are augmented – where applicable – with more general research challenges, as identified in  . 
                         
                      Design requirements DR1–DR3 address a subset of the challenges stated in  ","Another challenge is, for example, to deal with instance spanning anomalies instead of limiting the approaches to find single instance anomalies","For the work at hand, however, DR1–DR3 lead to a balanced approach focusing on a flexible, runtime anomaly detection with integrated root cause explanation support. 2.2 Fundamental notions This paper proposes an anomaly detection heuristic to classify process execution traces as anomalous or not","For this association rules are mined from a bag of recorded execution traces   (i.e., a log)","This is beneficiary as  : 
                         
                      The proposed approach is   as the traces in   are not labeled, formally: 
                         
                      This notion represents information provided by process execution log formats such as, the eXtensible Event Stream,  
                          and enables the representation of the control, resource, and temporal perspective on a trace level","As this work is not yet aiming on the analysis of additional perspectives and event/case attributes, we opted not to represent such information, for the sake of simplicity, in  ","Accordingly, the first event in the running example (cf.  ) would be denoted as  . 
                         The following   functions are used: 
                         
                      
                          provides an overview of the proposed anomaly detection heuristic; algorithms are presented in Section  ","Firstly, a signature   is created for a process   based on the associated execution log   (rule mining), i.e.,   is assumed as given input ①","Here, a signature   is a set of rules that represent behavior mined from  ","A rule represents, e.g., that activity   should be a successor of  ",The applied rule mining approach is inspired from the Apriori algorithm   which mines valuable relationships in large unordered data sets as association rules,"However, process execution traces are temporally ordered, e.g., based on the start timestamp of each event","To take this aspect into account (a) the Apriori algorithm is adapted accordingly, cf","Section  ; and (b) association rules, as defined in  , are extended into Anomaly Detection Association Rules ( ) — (rule for short), cf.  ","The applied adaptions to the Apriori algorithm are inspired from related sequence mining approaches, i.e., GSP   and, especially,  ; enabling to gradually reduce the computational effort (which is relative to the complexity and amount of traces analyzed throughout rule mining) during each rule mining step the longer each rule becomes, cf.  ","Overall this enables to achieve a sufficient rule mining performance, cf","Section  , while the implementation is still simplistic and can directly be applied on the given expandable   of traces, execution events, and logs used throughout this work","Enabling ADARs to represent common process execution behavior, to detect control, resource, and temporal execution behavior deviations (i.e., potential anomalies) from expected behavior as unsupported rules, as those can be indicators for potential fraud, misuse, or novel attacks, cf.  ","For the analysis of temporal execution behavior, ( )   can be applied","However, related mining approaches were found to be designed with different objectives in mind than this work; such as, the identification of seasonality  , the analysis of inter-event-intervals  , or the analysis of unordered item lifespans  ","In comparison, this work is interested into analyzing individual activity execution durations in combination with control flow sequences which requires to combine concepts mentioned beforehand","Accordingly, ADARs are formally defined as: 
                         
                      The following projections were defined for rules ( ) and conditions ( ):   represents that a rule can specify control flow (control for short), temporal, or resource behavior","The latter focuses on the assignment of resources to activities which is analyzed in the form of Separation of Duty (SoD) or Binding of Duty (BoD), cf.  ","Depending on the rule condition type (e.g., control or temporal) different elements (i.e.,   and  ) of each rule condition   become relevant",In the following examples non-relevant elements will be denoted as  ,"For example, a control flow focused rule condition for activity   will, in the following, be denoted as   because the duration representation   is solely relevant for temporal rule conditions, cf.  ","Further, as rules (conditions) are mined based on given traces (events); we define projections for rules (conditions) on their trace (event), such that: 
                         
                      It was chosen to focus on the described three process execution behavior perspectives (i.e., control, temporal, and resource behavior) as we found that these are most commonly covered by existing work, cf.  ","Support for additional perspectives can be added by specifying additional conditions, see Section  . 
                         
                      2.3 Overall approach The applied rule mining consists of   stages (cf.  )","Initially, the basis for the mining is laid by converting each event  , given by  ’s traces, into an individual rule (see ②)","Hence, at this stage each rule only holds a single condition, so that  ","In the following, these initial set of rules (the individual rules in  , resp.) is repeatedly extended and verified to create the final anomaly detection rule set. 
                         
                      Subsequently, rule   and   approaches are applied in an iterative way","The rule extension, see ③, extends each rule in   by one additional rule condition in each possible way to identify new potential rules. 
                         
                      Formally, the rule extension is defined as: 
                         
                      Section   defines how the extension described in   is performed for each of the four rule types, i.e.,  ","Finally, rule verification is applied ④","Each rule is verified by analyzing its support, i.e.,  where function   is defined in  ","The support of a rule represents the percentage of traces in   that a rule could be successfully mapped to (match the rule conditions, resp.)","If the support (i.e., the percentage of traces   that a rule supports) of a rule is below a user configurable threshold  , then the rule is removed from  ","Subsequently, the rule extension and verification steps are applied repeatedly until the rules in   are extended to a user configurable maximum length of   rule conditions",The verification variables   and   enable to fine tune rules for specific use cases and process behavior,"For example, we found that the mining of longer rules resulted in stricter signatures than the mining of short rules",In comparison choosing a low   value could result in overfitting the signatures and a high amount of rules,Further discussions on the variables are given in Section  ,"We also have investigated alternative measures, common to association rules, for the calculation of rule significance, e.g.,   or  ","While those were found to result in mining a smaller set of rules (signature, resp.) it also increased the number of false positives reported by the proposed approach","We were left with the impression that this was caused because the smaller, e.g., more confident rule set resulted in a stricter representation of the behavior in   — providing less freedom for fluctuations and ad hoc chances (i.e., overfitting occurred)","Unfortunately, this effect persisted even after relaxing the chosen thresholds in an effort to mitigate it. 
                         
                      
                          illustrates the ADAR mapping. 
                         
                      Finally, the mined rules   (the signature) are applied to classify a given process execution trace   as anomalous or not","For this a trace   is identified that is most similar to  , see ⑤","The similarity between traces is measured based on the occurrence of activities in the compared traces, cf.  ",Then   and   are mapped onto  ’s rules to determine the aggregated support of both traces,"Finally, if the aggregated support of   is below the aggregated support of   then the given trace   is classified as being anomalous, see ⑥. 3 ADAR based Anomaly Detection This section presents the algorithms for the approach set out in  . 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   3.1 Association rule mining for Anomaly Detection The applied ADAR mining approach, cf","Algorithm 1, combines the main mining steps described in  ","This is the rule set  , along with the iteratively applied rule   (cf.  ) and   steps (cf.  )","Depending on the   rule type   different algorithms are applied to mine either control, temporal, or resource behavior given in   into rules","While each rule type is mined individually, rules of all types can be combined in a single signature (i.e., the union of all individual rule sets).  
                      The auxiliary function   (cf.  ) transforms an  
                          into a rule  
                         ",Depending on the chosen rule type   one out of the three rule condition mining approaches presented in Section   to Section   is applied,"For example, if   then the control flow rule mining approach presented in Section   is used. 
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                      3.1.1 Mining control flow ADARs Control flow rules represent expected activity orders, e.g., that activity   should be succeeded by activity   during a process execution","Hereby, control flow rules enable to identify process misuse, cf.  , such as, the execution of a financially critical “bank transfer” activity without the previous execution of a usually mandatory “transfer conformation” activity. 
                             Accordingly, during rule extension, an event   is transformed into a rule condition  ","Given the running example (cf.  )   would be transformed into rule condition  . 
                             Trace   supports a control flow rule   if   holds all activity executions specified by the rule conditions in  , cf.  , ①","Further the activity executions must occur in accordance to the order of rule conditions in  , ②","This represents that activity executions are mutual dependent on each other. ①and ②are verified by Algorithm 2 to determine if a trace   supports the control flow rule  . 
                            
                         
                            
                         This work applies a relaxed rule matching","Hence, a rule is assumed as supported by a trace as long as this trace contains at least a single combination of events that match to the rule conditions",This enables to deal with loops and concurrency,"Moreover, the approach is flexible enough to avoid struggle with noise and ad hoc changes","However, as found during the evaluation it is still specific enough to differentiate benign and anomalous process executions, cf","Section  . 3.1.2 Mining temporal ADARs Temporal rules focus on activity durations (i.e., the timespan between the start and completion of an activity execution)","Those were found to be a significant indicator for fraud and misuse, cf.  ","However, while control flow rules focus on representing distinct values (e.g., explicitly activity   is expected) this is not possible for temporal rules","This is because distinct durations, e.g., one second or one hour, are so specific that even a minor temporal variation, which we assume as being likely, would render a rule to be no longer supported by a trace","This can, potentially, result in false positives",To tackle this challenge we apply  ,These rules are not representing durations with explicit values but with duration classes,"These classes represent, for example, that the expected duration of activity   is roughly comparable or below/above the average execution duration of activity   — given by the traces in  ","In this work three duration classes are in use, i.e.,  ","Increasing the number of classes would be possible but it was found that this can result in overfitting the generated temporal rules (signature). 
                             A temporal rule condition consists of an expected activity execution along with its expected duration classes",For this the activity execution duration is represented as a subset of the possible duration classes  ,"So, based on an event   a temporal condition   is constructed by defining the expected activity, i.e.,   and selecting one or more duration class which are expected to be observed, e.g.  . 
                            
                         Algorithm 3 determines the representative duration classes for an event   based on  ","Hereby, variable   “widens” the covered timespan of each duration class so that the rule support calculation becomes less strict to prevent overfitting",It depicts the three duration classes of PDC and how widening affects them,"For example, while the activity duration ①can clearly be represented by class   this is not the case for the duration ②","As this duration is between the   and the   class the “widening” ( ) comes into effect, so that ②is represented by both classes","Accordingly, the two exemplary constraints   and   would both match to ②. 
                            
                         
                            
                         
                            
                             A trace   supports the temporal rule   (i.e.,  ) if   holds all activity executions specified by the rule conditions in   with the expected durations","In addition, these activity and duration pairs must occur in the expected order given by  ’s conditions","To verify this Algorithm 2, is extended by calculating and comparing duration classes, cf","Algorithm 4. 
                            
                         3.1.3 Mining SoD and BoD ADARs Separation and Binding of Duty rules represent expected relative pairs of activities and resource assignments, cf.  , i.e., all activities covered by a SoD rule must be executed by different resources while all activities covered by a BoD rule must be executed by the same resource","Failing to support resource rules can be an indicator for fraudulent behavior, cf.  . 
                             Converting an event   into a SoD or BoD rule condition   is performed by extracting the activity related to  , i.e.,  ","Accordingly, for  , in  ,   holds. 
                             To verify if a trace   supports a resource (res. in short) rule  , a set is generated that holds all resources that have executed activities which are specified in  ’s conditions (cf.,  ), i.e.,  
                         For a BoD rule it is expected that all executions utilize the same resource, i.e.,  ","For a   rule the amount of resources taking part in the activity executions should be equal to the amount of conditions, i.e.,  . 
                            
                         
                            
                         3.2 ADAR based anomaly detection The mined ADARs (i.e., a signature) are applied to classify a given trace   as anomalous or not",For this the artificial   of   is calculated and compared with the likelihood of the trace   that is most similar to  ,"If   is identified as   likely it is assumed as being anomalous, cf.  ","Hereby, the presented approach follows and exploits the common assumption that anomalous behavior is less likely than benign behavior, cf.  , while also enabling dynamic anomaly threshold identification","In comparison to existing work, cf.  , the latter increases the flexibility of the proposed approach","This is because this work, other than, e.g.  , does not rely on a fixed predefined detection threshold which is fixated once and from there on needs to be suitable for “all” potentially upcoming process executions",To further extend on this concept one could create an average likelihood threshold based on a range of most similar traces to reduce the risk of overfitting — which will be explored in future work,A comparable idea was implemented by us in  ,"The artificial likelihood of a trace is determined by aggregating the overall support (based on  ) of the rules which the trace is supporting, cf.  ","This implies: the less rules a trace supports the less likely it and its occurrence is assumed to be and the more likely it is anomalous. 
                         
                      The proposed anomaly detection approach requires to identify, for a given trace  , the most similar trace  ",For this function   is applied,In detail: both traces are first converted into bags of activities (each one holds activities executed by the respective trace),"Subsequently, the Jaccard similarity  , cf.  , between both activity bags is calculated","This means: the more equal activities  
                          the traces contain (have executed) the more similar they are","More calculation intensive approaches, such as, edit distances (e.g., the Damerau–Levenshtein distance) could be applied to get a more thorough representation of activity orders, as demonstrated by us in  ",Such can especially be beneficial if activity orders in the analyzed traces are fluctuating while roughly the same activities are executed overall,Such a situation did not emerge throughout the evaluation,"Hence, while the underlying similarity measure can be user chosen the described approach was found to be fast and sufficient during the evaluation, cf",Section  . 3.3 Fostering root Cause analysis and understandability The detection of anomalies constitutes only the first step in a large anomaly management process  ,"In addition it must also be analyzed which context, behavior, policies or errors motivated the classification of an execution as being anomalous, cf.  ","The latter is typically subsumed as   which paves the ground for the selection and application of suitable anomaly countermeasures (e.g., the termination or adaption of anomalous instances)","In general, “ ”  ","So far,   capabilities have been mostly neglected in the process anomaly detection domain, cf.  ",Related work focuses on the technical perspective of detecting anomalies in the first place,"However, this can significantly lower the positive impact of such detection approaches as it is our assumption that an anomaly can only be truly resolved by understanding and addressing its cause","Accordingly, this work proposes the novel visualization technique   to foster anomaly focused root cause analysis to address following limitations: 
                         
                      
                          builds and expands on visualization approaches utilized in related areas, such as, business process compliance research","There, business process models and executions are, for example, colored to indicate that a given execution part, such as an activity, is compliant (  
 ) or not (  
 ), cf.  ","Further, Ishikawa (fishbone) diagrams are applied to organize the relations between problem causes, cf.  ","The latter, are typically created manually while   can be created automatically based on the analyzed traces and mined rules. 
                          gives an example of the proposed visualization approach","It consists of two main parts: The process execution trace ①which was deemed as being anomalous, and ②/③, i.e., the rules which were found to be violated (not supported, resp.) by that trace. 
                         Here, color is applied to visualize differences between process execution events which have not violated any rules (  
 ) and events which have violated rules (  
 ) (i.e., which do not comply to expected behavior)","Note, that also a third color is utilized (  
 ) indicating that a less significant rule was violated (i.e., a rule which is partly also violated by process executions which were, overall, classified as being non-anomalous)","The latter can occur in the case of loops and parallel executions, were, e.g., activity executions overlap each other in different ways and orders or were, e.g., the same activity can be observed four or fife times in a row — depending on the number of loop iterations",The chosen colors/shades were combined in a way that they are still distinguishable by applicants which suffer from red–green deficiency,The latter was verified by checking out multiple potential color combinations with affected test subjects,The proposed visualization takes over ideas from Ishikawa diagrams while positioning each graphical element,"Hence, the rules which are violated by an execution event are positioned just below the event which triggered the rule violation decision","Compare with ②, it depicts a violated resource rule which aims at checking whether or not   and   are executed by different resources (SoD), cf.  ","As this is not the case the SoD rule is violated,   is identified as the culprit and the violated rule is displayed just below the potentially malicious execution event for  ",This rule positioning approach was motivated by our assumption that preceding execution behavior (the IF part of each rule) sets the expected behavior for succeeding behavior (the THEN part of each rule),"Hence, we assume the event related to the THEN part of the rule as the wrongdoer and position the violated rule below it — similar to the concepts used in Ishikawa diagrams",The proposed positioning approach is not applicable at all times,"So, if an activity should be executed but is missing completed from an visualized trace then the executing event matching the THEN part of the rule is missing too","To stay consistent with the general idea to positing the violated rules below the THEN part a workaround is applied by summarizing such events below a “No matching Events” caption, see ③in  ","Overall, A_Viz supports   rule types outlined throughout Section  ","Hereby, small icons, positioned in the top right corner of each rule box, in the form of tridents ( ), clocks ( ), and stick figures ( , i.e., SoD or BoD) visualize the associated rule type","The icons are selected according to suggestions from literature on process compliance, e.g.  ",The explanatory power of the proposed root cause visualization and analysis is evaluated in Section  . 4 Evaluating the Detection quality of the proposed Anomaly Detection approach The evaluation utilizes   process execution logs from multiple domains and artificially injected anomalies in order to assess the anomaly detection performance and feasibility of the proposed approach,It was necessary to inject artificial anomalies as information about real anomalies are not provided by today’s process execution log sources  ,"The utilized logs were taken from the BPI Challenge 2015  
                       (BPIC5) and Higher Eduction Processes (HEP), cf.  ","These logs were chosen because they are: (a) widely applied throughout existing process anomaly detection work, such as,  , fostering comparability; (b) realistic and cover real world business process execution behavior; (c) enabling to evaluate the described approach based on logs with two different complexity levels (high   BPIC5, medium   HEP) and domains; and (d) containing all the relevant information in sufficient granularity, as, for example, we found that alternative logs frequently do not contain details on the related resources","The BPIC5 logs hold 262,628 execution events, 5649 instances, and 398 activities",The logs cover the processing of building permit applications at five (BIPC5_1 to BIPC5_5) Dutch building authorities between 2010 and 2015,"The HEP logs contain 28,129 events, 354 instances, and 147 activities — recorded from 2008 to 2011 (i.e., three academic years   HEP_1 to HEP_3)","Each trace holds the interactions of a student with an e-learning platform (e.g., exercise uploads)","All logs contain sufficient details to apply the proposed approach (e.g., execution events, activities, timestamps, resource assignments, etc.)",The logs were randomly separated into training (for signature generation) and test data (for the anomaly detection performance evaluation),"Subsequently, randomly chosen test data entries were randomly mutated to inject artificial anomalies","By randomly choosing which, how many, and how frequently mutators are applied on a single chosen test data entry (trace, resp.) this work mimics that real life anomalies are diverse and occur in different strengths and forms","Further the application of mutators enables to generate labeled non-anomalous (i.e., non-mutated) and anomalous (i.e., mutated) test data entries","Hereby, it becomes possible to determine if both behavior “types” are correctly differentiated by the proposed approach (cross validation)","The applied mutators inject random control flow, temporal, and resource anomalies: (a)   — mutators which randomly mutate the order and occurrence of activity execution events; and (b)   — randomly chosen activity executions get assigned new artificial execution durations; and (c)   — activity/resource assignments are mutated to mimic, for example, BoD anomalies",The applied mutators were adapted from our work in  ,Combining multiple mutators enables to represent the diversity of real life anomalies,"In addition, the applied random training and test data separation also evaluates if the proposed approach is capable of dealing with   noise and ad hoc changes by not identifying them as anomalous","This is, because the test data contains benign behavior which is not given by the training data (e.g., bening ad hoc changes)","The given evaluation results are an average of multiple evaluation runs, cf.  ","This enables to even out random aspects, such as, the random data separation and trace mutation. 
                       Here, the feasibility of the presented approach is analyzed","For this, a cross validation is performed to determine if known anomalous (mutated) execution traces are correctly differentiated from known non-anomalous (non-mutated) ones","Through this four performance indicators are collected: True Positive (TP) and True Negative (TN), i.e., that anomalous (TP) and non-anomalous (TN) traces are correctly identified","False Positive (FP) and False Negative (FN), i.e., that traces were incorrectly identified as anomalous (FP) or non-anomalous (FN)","Finally, these indicators are aggregated into: (a)   
                       — if identified anomalous traces were in fact anomalous; and (b)   
                       — if all anomalous traces were identified (e.g., overly generic signatures could result in overlooking anomalies); and (c)   
                       — a general anomaly detection performance impression;  ;  ",An optimal result would require that TP and TN are high while FP and FN are low so that the accuracy becomes close to one,"Further, the  -measure, Eq.  , provides a configurable harmonic mean between   and  , cf.  ","Hence,   results in a precision oriented result while   generates a balanced result.  
                   
                       The results were generated based on the BPIC 2015 and HEP process execution logs and following proof of concept implementation:   The implementation was found to be capable of creating a signature within minutes and requiring only seconds to classify a trace as anomalous or not","Once generated the signatures can be reused and easily adapted by adding new rules or removing old ones, e.g., to address concept drift","Primary tests were applied to identify appropriate configuration values, e.g., the maximum rule length   (control and temporal) and   (resource)","Typically utilizing longer rules results in stricter signatures which are prone to overfitting, cf.  ","Here, this seems not to be the case as the given dynamic threshold calculation utilized in this work mitigates this effect as (a) both, the trace analyzed for anomalies and its most similar counterpart in   commonly support similar rules; and (b) shorter rules (i.e., rules with a length below  ) are also generated, based on the outlined iterative extension and validation based rule mining approach, cf.  , and become part of the rule set  ","Hence, a single long unsupported rule does not have a dramatic effect on the detection results (anomaly assessment, resp.)","However, given that lower values for   do not only result in shorter easier to grasp rules but also reduced calculation times low values were used for   in the following",A comparison of the effect of different   rule length configuration values on the anomaly detection performance (represented as average F1-measures) is given in  ,"The best three results are printed in bold letters. 
                       
                       
                      In comparison, the minimum support a rule has to achieve   during the mining phase, to be accepted as a part of the signature, was set to 0.9 for control & resource rules and 0.8 for temporal rules",For this variable it was found that higher values could potentially result in a very small rule set or in finding no rules at all,"In comparison, using a lower value could result in finding a very high amount of rules","This does not necessarily result in better anomaly detection results as it increases, as we found, the risk of generating overfitting signatures",A comparison of the effect of different minimum support configuration values on the anomaly detection performance (represented as average  -measures) is given in  ,The best three results are printed in bold letters,"Finally, the fuzzy temporal rule generation can be configured based on the chosen temporal class widening variable   which was set to 0.2","Lowering this value will result in stricter signatures (temporal rules, resp.) that could potentially struggle when dealing with noise and ad hoc changes while a higher value would result in potentially overlooking anomalies as the signatures become less strict","However, when experimenting with different values for   (between, 0.01 and 0.6) the   results, cf","Eq.  , only fluctuated by 1%–3%","Given the low amount of configuration variables we assume that existing optimization algorithms should, likely, be able automatically find optimal settings for the proposed approach based on given training data",The average evaluation results are shown in  ,"Overall, an average accuracy of 81% was achieved along with an average precision of 77% and an average recall of 89%",Given these results we conclude that the proposed approach is feasible to identify the injected anomalies in the analyzed process execution data,"Moreover, it becomes visible that the detection of diverting anomalous behavior becomes harder the more diverse and complex the benign behavior is (e.g., because of noise or ad hoc changes)",Accordingly the anomaly detection performance of the BPIC 2015 logs are lower than the results for the HEP logs,"Nevertheless, an average anomaly detection accuracy of 75% was achieved for the more challenging BPIC 2015 process execution log data. 
                      
                   We have roughly compared the proposed approach against five alternative anomaly detection approaches, cf.  ","From those,   were specifically tailored for detecting anomalies in business process executions","In comparison,   apply generic anomaly detection techniques, such as, clustering",The tests were executed based on the BPIC 2015 logs as such are more challenging than the HEP logs and thus enable a more conclusive evaluation,"Here, the Area Under the Curve (AUC) metric is applied to compare the listed anomaly detection techniques as this metric was found to be commonly used by comparison approaches, such as,  ","In comparison to alternative approaches, such as,   it was found that the proposed approach achieves a lower anomaly detection performance, cf.  ","However, we were left with the impression that, with regards to quality attributes, such as, understandability, computational performance requirements or visualization capabilities, a significant improvement was achieved","For example,   utilizes large monolithic signatures which need to be completely recreated from scratch if, e.g., the process model which instantiates the to be analyzed traces is changed","In comparison, the proposed approach was found to generate the signatures faster and can also reuse some of the generated rules if the underlying process model changes","Overall, this gives the experts the flexibility to choose the correct tool for their needs as, for now, there seems to be a trade off between detection performance and simplicity/understandable of the results and signatures. 
                      
                   5 Evaluating the proposed root Cause analysis visualization approach (A_Viz) This part of the evaluation has the objective to investigate the effect of   on the analysis of anomalous process executions detected by the presented anomaly detection approach",For this an expected process execution anomaly analysis workflow is replicated,"Within the conducted experiment, the participants are, hence, provided with (a) multiple anomalous execution traces, (b) a related process documentation, while (c) given the task to identify and report as many distinct anomalies as possible",The focus of this evaluation is on correctness (how many anomalies were correctly identified and reported) and response time (how long did it take the participants to complete the given tasks),"Both variables are frequently utilized to construct understandability, i.e., enabling to deduce if the proposed visualization supports the identification of anomalies, cf.  . 
                       The experiments and data collection were conducted throughout calendar week three in 2019 with 25 participants","From these 25, six participants were classified as experts as they can show of more than five years of practical (or research) experience in the field of process management and analysis — either as a researcher or process analyst",The remaining participants were students who enrolled in the course “Workflow Technology” (WT) or “Business Intelligence” (BUS) (optional part of the Master in the Computer Science curricula) at the University of Vienna throughout the winter term 2018/2019; both courses put a distinct focus on business process design and analysis,"It will, in the following, be differentiated between both participant groups of “non-experts”, i.e., students, and “experts” in order to determine if the use of   affects both groups differently","Note, according to  , conducting experiments with students “  Further studies even argue that students can serve as sufficient representatives for experts, cf.  . 
                       Each task of the experiment consists of two steps ( ) reading and understanding a given process documentation, and ( ) analyzing three related process execution traces to spot and report anomalies by comparing ( ) and ( )","For ( ) two types of documentation are utilized: 
                      
                   The group of participants was split into subgroups of three","The members of the same group were always uniformly provided with the same documentation type (i.e., optimal or typical)",It was   determined which group utilizes which kind of documentation throughout the experiment,The latter enabled to achieve an almost perfectly balanced ratio between the optimal (13 participants) and typical (12 participants) documentation,"In addition to the documentation the experiment requires that given process execution traces must be analyzed for anomalies, i.e., ( )","Traces were visualized in three different ways: 
                      
                   
                       
                      
                       For each of the two documentation types three matching execution traces were created (i.e., six in total) and mutated to contain randomly chosen anomalies, on randomly chosen events, with random strengths (at least/most 2/4 events were mutated for each trace) — comparably to the mutation approach applied in Section  ","Each of the mutated traces was visualized in all three style (i.e., 18 different visualizations were generated in total)",For each participant the order of traces was randomized along with the visualization styles,While doing so it was ensured that the traces match the documentation handed out to the group the respective participant belongs to,"Note, each of the three traces was visualized in a randomly chosen style while ensuring that each participant deals with each visualization style exactly once (i.e., each participant had to analyze three traces, each trace was visualized in a unique visualization style)",By following this computer aided and randomized design we aimed at   and  ,"Further, the tasks, traces, processes, and visualizations given throughout the experiment are not taken from the material of the two related courses",All materials handed out to the students are publicly available at   to support a replication of the study,This includes a   which provides basic information on BPMN and the way the execution traces and its events are visualized throughout the experiment,"In addition to the two documentation types, all anomalous traces in the three different visualizations, and the utilized master data sheet are included",The latter is utilized to collect data about previous experience in related areas (such as process research) from the participants (at the start of the experiment),After the experiment that sheet was utilized again to document and obtain a general impression concerning the three different visualization styles,"For this questions were asked, such as, which of the three visualization styles resulted in the highest confidence that no anomaly was overlooked or that the reported anomalies were, in fact, true positives. 
                       
                       
                       
                      
                       Four weeks before the experiments, pre-tests were conducted with four experts to verify if the created documentation was clear and understandable, the technical infrastructure was capable, the tasks were doable in time, and all necessary information was provided before and during the experiment","With regards to the latter aspects it was planned that a single group does not require more than   min to complete the given tasks (i.e., to read the provided documentation and spot anomalies in three different execution traces)","Those   min comprise of a   min preparation session which reiterates the objective of the experiment, answers questions with regards to the different visualization types and the provided process documentation",All participants were able to complete the experiment within the   min period (during the pre test and the real experiment),"Based on the feedback gathered throughout the pre tests some minor changes were performed, ranging from fixing typos to slight wording adaptions to prevent potential misunderstandings","Overall, the feedback was positive and no critical issues were observed. 
                       Two weeks before the experiment five minute talks about its goals and motivations were given throughout one session of both courses (i.e., WT and BUS)",Those talks motivated the relation of the experiment to the respective course and outlined the respective benefits each student could gain when participating (one/two bonus points could be earned by participating in WT/BUS),"Before these talks preparatory material was made available together with a registration form at CEWebS  
                       to enable potential participants to sign up for their preferred group/timeslot",CEWebS is a custom e-learning platform which is utilized by a number of computer science courses at the University of Vienna (including WT and BUS),The preparatory material and all other materials contain informal natural language descriptions of approaches and tasks along with practical examples,"This enabled us to present the material equally to each participant in an approachable manner, as suggested in  ","Note, while performing the experiment all participants were always able to access all the material at once",Before the beginning of each group timeslot each attending participant was seated randomly and the experiment materials were handed out as printed documents,"The latter used a combination of A4 and A3 pages whenever appropriate to increase content readability and clarity (e.g., the process model documentation was always printed on A3 pages so that no page flips are necessary by the participants)","Next, the participants were brought up to speed about the tasks, visualization styles used, and the general procedures to follow","This included the website which was utilized by each participant to report identified anomalies. 
                       Throughout the experiment each participant described/reported identified anomalies by using a custom made website","On this website it was possible to select (a) the event which was the source of an anomaly (or that an expected event is missing), and (b) the anomaly type (i.e., missing activity, swapped activity, novel activity, SoD or BoD violation or temporal violation)",Each anomaly could be reported individually until the participant decided that all anomalies for the current trace/task were identified and switched over to the next task/trace,"Utilizing an electronic data collection (i.e., anomaly reporting) system enabled us to automatically track each participants’ actions (e.g., when he or she started with reporting anomalies and when this process was completed for a given task/trace)","Further, it prevents the occurrence of equivocal markings, timestamp documentations or that participants forget to annotate all necessary details — as it was observed, e.g., in  ","During their work each participant utilized a laptop computer provided by the researchers which had all the necessary tools (such as, the website utilized to report anomalies) already preinstalled",From these machines no network connection was possible and the participants were arranged in a way that limit opportunities for cheating — which was further hampered by the random trace/task/visualization style order each participant was assigned with,"Note, the publicly available material collection also contains a copy of the beforehand mentioned electronic data collection (i.e., anomaly reporting) solution (website, resp.)","Further, the raw data collected throughout the experiment is available at  . 
                       
                       participants took part in the experiment","Each participant was assigned with a unique combination of documentation type, trace order, and trace visualization style.   give the results when applying a strict analysis of the anomaly reports",Here   means that an anomaly is only assumed as correctly reported if the anomaly type   the culprit event were correctly identified,"In comparison,   give the results when applying a relaxed analysis, i.e., an anomaly is also counted as detected if the correct anomaly type was chosen, but the culprit event was wrong",Note that the results collected for the typical documentation do not contain any details on anomalies caused by swapped activity as this mutation operator was never applied on the related traces due to the random nature of the mutation selection and application,"In all scenarios,   (average correctness 0.98) outperforms visualization approaches   (average correctness 0.82) and   (average correctness 0.79)",While the longest experiment run took   min from start to completion the shortest took   min,"Overall, it was found that experts were, on average, able to complete the given tasks faster than non-experts, mainly, because they extracted the necessary details from the given documentations faster",An overview on the anomaly reporting times is given in  ,"At first, it might seem that the participants were significantly faster when reporting anomalies when not being supported by the visualization","However, this was likely not the case as it was observed that the “No support” visualization was frequently handled in a different way than, e.g.,  , i.e., the participants first extracted all the anomalies in advance and subsequently entered them into the report website in bulk which distorts the collected durations/timestamps","Hence, the related data on temporal behavior must be taken with caution",Another difference between experts and non-experts could be spotted in the result quality: experts made almost no errors throughout the experiment,The few errors made by experts occurred at tasks which were not supported by  ,"Unfortunately, only six experts participated in the experiment so that a clear differentiation between the compared scenarios is hardly possible","When comparing relaxed and strict results it seems that almost all anomalies were identified by the participants when applying a relaxed analysis, cf.  ","However, it was found that the participants frequently started to spot anomalies “everywhere” when being unsure if some behavior is anomalous or not",This effect was especially visible in the data collected for the   visualization and seems to be less significant the more support the visualization provides — which could be an indicator that the proposed visualizations increase understandability,"One interpretation is that providing no root cause analysis support could motivate analysts to become overly suspicious which could potentially increase the likelihood to choose incorrect anomaly countermeasures. 
                       
                      
                       When preparing the visualization we assumed that the more support and information is provided by a visualization the easier, precise, and faster the identification of individual anomalies, hidden in a given execution trace, becomes","However, this assumption was only partly confirmed by the results and the oral feedback provided by the participants",In detail: for “simple” control flow related tasks such as reporting novel or missing activities the   visualization outperformed the   visualization,"We refer to such tasks as being simple as, e.g, comparing the trace with a given process model is sufficient to solve them","In comparison, resource or temporal behavior violations require to correlate multiple events at once","Based on our observations a likely reason for this is that without any support the participants (a) invested more effort and checked for anomalies more thoroughly, (b) marked observations and anomalies on the provided handouts, and (c) checked and verified more potential cases of anomalous behavior before starting to enter any information into the anomaly report website",These observations were also confirmed by oral feedback provided by the participants after completing the experiment,"Nevertheless, it was found that   outperformed both comparison approaches","This is, most likely, as it explicitly points out anomalies which, as we assume, are relatively hard to spot (i.e., anomalies related to resources and temporal behavior)","The personal preferences expressed by each participant on the master data sheet support this impression, i.e.,   dominates the categories “highest confidence” and “personal preference”","However, given the limited amount of participants and scenarios further studies must be performed to verify and expand these observations","When expanding the outlined idea of simple and more challenging tasks it could be observed that the application of supportive visualizations, i.e.,   and  , seems the more beneficial the more complex a task becomes","For example, we assume that the identification of anomalies is simpler when analyzing traces related to the “optimal documentation” than when analyzing traces related to the “typical documentation”","This assumption is motivated by the fact that the optimal documentation already describes, e.g., resource constraints in an informal manner while the typical documentation requires to deduce them from a set of execution traces",The results in   seem to support this assumption,"Hence, we found that   seems to be especially helpful if the process documentation is outdated or missing — which we assume to be a realistic and common situation for typical anomaly detection scenarios","Nevertheless, additional studies are necessary to verify if this observation holds when given, e.g., more complex process executions and process models as the models utilized here only contained   activities each and a small amount of basic gateways (parallel and XOR). 
                       The conducted   evaluation followed the assumption that organizations have to cope with a wide range of domains, process models, and anomaly reports simultaneously","Accordingly, the respective expert for each domain, process model, and execution – which was identified as being anomalous – is potentially, not available all the time","Hence, based on our observations, generic Business Process Management (BPM) experts are frequently forced into the role of “ ” domain experts and business analysts","For example, by reading up on the process model documentation before deciding on and conforming reported anomalies (as simulated throughout this experiment)","This also motivated the applied visualization styles (e.g., languages and symbols), which we found to be well known to the targeted BPM user group",For this a minimalistic visualization was implemented that represents the   (the IF part of each rule) and   (the THEN part of each rule) of each reported anomaly in a condensed form,"This enables to represent all the relevant information in close proximity to each other to support deductions on an execution’s anomaly state (e.g., to confirm an anomaly and terminate the related execution)","Accordingly, while the proposed visualization is well suited for BPM experts, less specialized stakeholders, such as, managers will likely struggle with the chosen representation without additional training","Further, preparing fundamental security related changes based on identified anomalies and root causes requires to incorporate analysts which have a   understanding of the related domains (i.e., domain experts)","This scenario was not evaluated here, but is seen as a promising extension for future work","Hence, we see   mainly as a first step towards creating awareness for supporting root cause analysis in the business process anomaly detection domain. 6 Related work (Business Process) Anomaly detection employs techniques from areas, such as, data mining and machine learning and is relevant for a number of application domains, e.g., intrusion and fraud detection  ","In particular, anomaly detection in the business process domain combines two viewpoints: Firstly, a method side which applies process-oriented techniques, such as, conformance checking along with techniques from mining cross-sectional and temporal data and, secondly, the application side, i.e., the detection of deviations in process execution behavior","Hereby, it is related and influenced by a range domains and concepts stemming from data and process mining but also security in general, see  ","This is because detecting anomalies is often connected with security issues  , for example, the detection of fraud and misuse","In the following, we discuss related approaches from the security domain and from the process domain as well as approaches on root cause analysis","The discussions also address the method point of view. 
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   6.1 Anomaly detection in the security domain In the security domain, anomaly detection and root cause analysis are major research areas","However, existing approaches are too specialized to be applied to process data, cf.  , because they focus on single unique use cases and data formats, such as, specific network protocols, e.g., the Session Initiation Protocol, cf.  ","These approaches can hardly be generalized and applied to process execution logs which hold different data, formats, and contextual attributes","In comparison, generic and more flexible anomaly detection approaches from the data analysis domain frequently show a sub par performance when being applied on process executions, cf.   and  ","One could argue that instead of applying anomaly detection the process definition could be secured by applying security focused modeling notations, cf.  ","In real world scenarios, this would require to be aware of all potential sources for security incidents during the design phase and to constantly update the processes to meet novel security challenges",In comparison the proposed anomaly detection approach is self learning and can also deal with process changes automatically,"This flexibility also differentiates the proposed approach from existing work, e.g.,  , which was found to frequently apply either overly strict signatures   (potentially resulting in false positives) or  , cf.  ",Soft matchers widen the area of behavior which a signature matches to,"Hereby, the risk of false positives can be reduced","However, as soft matchers require domain and expert knowledge and are manually defined we assume that they are hardly applicable given the huge amount of processes which are currently in use","We assume that the proposed automatic approaches, e.g., to dynamically choose a comparison trace based on its similarity or to factor in rule significance, are necessary. 6.2 Detecting anomalies in process behavior This section provides an overview on anomaly detection approaches in process behavior based on the systematic literature study in  ","One option to categorize the approaches is based on the applied method, e.g., by distinguishing supervised learning and unsupervised learning methods","For supervised learning classification is applied as follows: the process execution data, i.e., the logs, are considered as labeled training data that does not contain any anomalies","From this, a classifier is learned and new process instance data can be classified into non-anomalous or anomalous behavior","Based on the technique, the classifier varies","For   techniques, the classifier is typically a reference process model that is discovered from the training data, cf.  ","By contrast,   mines an automaton and is specifically tailored to deal with infrequent behavior (which would, elsewise, be interpreted as anomalous).   advocates an anomaly score for sliding windows on the log data","In comparison,   and   techniques, cf.  , show some resemblance, e.g., to  , as they are also capable of detecting deviations between process models and process executions, cf.  ","However, we were left with the impression that, while conformance checking and filtering show potential, related techniques are not perfectly up to the task","This is, because related conformance checking and filtering work, for example, was found to be “overly” strict which can increase false positive rates — a drawback which   process mining based approaches, which were mentioned beforehand, such as  , mitigate","For example, by precisely relaxing mined process models and related anomaly detection thresholds","For   techniques – such as, the work at hand – rules are derived in order to serve as a classifier",An approach based on Support Vector Machines (SVM) is presented in  ,The most comparable work to the approach at hand is  ,It applies association rules for anomaly detection in processes,"However, rules are largely manually generated (e.g., a user define the expected maximum activity duration) and order dependencies between activities are not verified","In  , we present an approach to detect temporal anomalies for multiple instances that is based on   mining",Unsupervised approaches are   and aim at determining clusters of non-anomalous and anomalous behavior where the latter is most likely represented by small and scattered clusters,"The clusters are based on similarity between process executions, e.g., based on the resources  , the temporal perspective  , and the control flow  ",Other control flow oriented approaches include  ,"In  , clustering is combined with likelihood graphs","Further approaches employ  , cf.  , signatures based on regular expressions  , and statistical anomaly detection  ","Currently, several shortcomings in existing work hinder the application of anomaly detection in real world applications","At first, a significant amount of process anomaly detection approaches support only single process perspectives","Secondly, existing work does hardly support the analysis of identified anomalies and mostly applies monolithic signatures which are hard to grasp, struggle with noise and ad hoc changes, but also cannot be partially updated whenever the underlying process changes. 6.3 Compliance checking and root cause analysis Compliance checking approaches such as   also utilize rule-based definitions of expected process behavior","The goal is to analyze the process definition and execution for compliance violations and their root causes, cf.  ","However, such work typically does not take noise and ad hoc changes into account, possibly resulting in false positives","Moreover, rule formalisms such as LTL enable expressive rules, but at the price of increased complexity","This work, by contrast, aims at simplicity, balanced with flexibility","Moreover, the definition of the compliance rules often requires in depth domain and process knowledge and hence results in manual effort","The work at hand, by contrast, offers automatic mining of association rules",As said before a crucial step in compliance management is the analysis and explanation of root causes,"The process of root cause analysis comprises the following steps according to  : at first, the problem should be understood",Here tools such as flow charts are proposed,"In the approach at hand, accordingly, process models can mined from the log","Secondly, a problem cause brainstorming should take place",This step can be added to the approach if desired,"Thirdly, the problem cause data is collected and analyzed",These steps are achieved by the association rule mining approach,"Subsequently, the root cause is to be identified",Here it is suggested to use cause-and-effect charts as they provide    ,"Accordingly, this work uses cause-and-effect charts based on Ishikawa (“fishbone”) diagrams","The final steps of problem elimination and solution implementation are out of scope of this work, but constitute promising research directions for future work",Alternatives for anomaly and root cause visualization are provided in literature,"In  , a visualization for anomalies in business processes is provided","For this, impact factors can be specified and related to a fraud amount","This approach can be used to further investigate factors behind the deviations, but does not analyze the process deviations directly",An approach to discover deviations between process graphs and their instance traffic is offered in  ,"Deviations can be spotted, but the root cause is neither visualized nor explained","A survey on visual analytics options based on current process mining frameworks such as ProM.  
                          is provided in   For spotting anomalies in process behavior the described visualizations using dotted charts seems particularly useful, however, it does not shed light on their root causes","Further related work can be found in the   area, cf.  ","The latter, strives to predict the outcome and execution behavior of business processes and business process changes","While such predictions provide valuable insights for business process management, they are only of limited used for anomaly root cause analysis","This is because such work typically focuses, e.g., on predicting execution costs or durations but not on describing the cause and effects of anomalous behavior. 7 Discussion and outlook The paper set out three research questions, i.e., how to detect anomalies by reducing false positives ( ), how to balance effort and flexibility for anomaly detection ( ), and how to explain the root cause for anomalies to users ( )","We conclude that the proposed approach is able to detect anomalies ( 
                      ) as the conducted evaluation showed an average anomaly detection recall of 89%","This goes hand in hand with a substantial simplification of the generated signatures compared to previous work in   (complex likelihood graphs vs. short rules) which in principle fosters the understandability of the signatures and identified anomalies ( 
                      )",Both research questions   and   require an adequate treatment of flexibility by ad hoc process changes and noise as both might lead to false positives or in other words an insufficient distinction between benign and malign behavior,"The proposed approach applies the common assumption that benign behavior is   than anomalous behavior, cf.  ","Nevertheless benign   and   still occur in the signature mining data (i.e.,  ) but also in the traces that are analyzed for anomalies, cf.  ","These kinds of behavior can, if the applied signature is too strict or overfitting, be misinterpreted as being anomalous and so result in  ","Hence, the proposed approach applies three strategies to mitigate this risk: 
                      
                   We conclude that the approach takes a next step towards the reduction of false positives by being aware of process change and noise",As the identified anomalies (and related traces) can be complex and hard to understand we argue that anomaly detection approaches should support experts when analyzing anomalies along with the related alarms,We assume that it is necessary to identify but also to understand anomalies to choose appropriate anomaly countermeasures,"For this, inter alia, the following information is required: (a) which part of an execution trace is affected by an anomaly; and (b) the anomaly severity, cf.  ","It is shown how this information is provided by the proposed rule based anomaly detection approach (  
                      )","Moreover, the proposed root cause visualization can foster the understanding and identification of root causes of anomalies",To our knowledge this is the first   approach that does so,"In future work, the performance of the proposed approach will be further optimized",This is because currently each of the three rule types is mined independently of each other,"However, we found that for example the control flow rules and the temporal rules partly have connected conditions (i.e., the respective activities they apply to)",We assume that such similarities can be exploited,"Moreover, we will analyze if the proposed approach can be applied to clean up execution logs","Hereby, if logs that are riddled with anomalous traces they could be “cleaned” by the proposed unsupervised approach so that they can be successfully utilized by existing semi- or even supervised anomaly detection approaches, cf.  ","Finally, we will extend the conducted   evaluation by incorporating domain experts and more complex execution scenarios",Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0957417414006472," 1 Introduction Text clustering is a useful technique that aims at organizing large document collections into smaller meaningful and manageable groups, which plays an important role in information retrieval, browsing and comprehension","Traditional clustering algorithms are usually relying on the BOW (Bag of Words) approach, and an obvious disadvantage of the BOW is that it ignores the semantic relationship among words so that cannot accurately represent the meaning of documents","As the rapid growth of text documents, the textual data have become diversity of vocabulary, they are high-dimensional, and carry also semantic information","Therefore, text clustering techniques that can correctly represent the theme of documents and improve clustering performance, ideally process data with a small size, are greatly needed","Recently, a number of semantic-based approaches are being developed","WordNet ( ), which is one of the most widely used thesauruses for English, has been extensively used to improve the quality of text clustering with its semantic relations of terms ( )","However, there still exist several challenges for the clustering results. (1) Synonym and polysemy problems",There has been much work done on the use of ontology to replace the original terms in a document by the most appropriate ontology concept for the solution of these problems; this process is known as word sense disambiguation (WSD),"This approach, however, has not proven to be as useful as first hoped","For example, approaches that expand the feature space by replacing a term with its potential concepts only increase the feature space without necessarily improving clustering performance ( ). (2) High-dimensional term features","High dimension of feature space may increase the processing time and diminish the clustering performance, which is a key problem in text clustering","Most current techniques usually rely on matrix operation methods such as LSI ( ), ICA ( ), and LDA ( ) to deal with this problem","Unfortunately, these models need too much computation","Although there also exist a few techniques have considered semantic information ( ), they have various weaknesses","For example, they do not explicitly and systematically consider the theme of a document. (3) Extract core semantics from texts","Existing dimension-reduced methods may remove some topic features, which results in the semantic content of a document is decomposed and cannot be reflected","It is desirable to extract a subset of the disambiguated terms with their relations (known as the core semantic features) that are “cluster-aware”, which leads to improving the clustering accuracy with a reduced number of terms. (4) Assign distinguished and meaningful description for the generated clusters","In order to conveniently recognize the content of each cluster, it is necessarily to assign concise and descriptive labels to help analysts to interpret the result","Nevertheless, good solutions of assigning topic labels to clusters for ease of analysis, recognition, and interpretation are still rare","This paper attempts to alleviate mentioned above problems, its contributions can be summarized as follows. 
                   The rest of the paper is organized as follows: Section   reviews some related works",Section   presents a modified similarity measure based on WordNet for word sense disambiguation,"In Section  , we describe how to extract core semantics by using lexical chains",Section   details the experiments that evaluate our method and the analysis of results,"Finally, we conclude this work and show its implications in Section  . 2 Related works To date, text clustering has been heavily researched and a huge variety of techniques has been proposed to deal with it",The goal of the clustering process is to group the documents which are similar in contents into a single group,"In order to understand our work better, some relevant works about several research fields related to our interests will be introduced and the limitations of the described approaches will be presented as well. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   2.1 WordNet WordNet is one of the most widely used and largest lexical databases of English","In general as a dictionary, WordNet covers some specific terms from every subject related to their terms",It maps all the stemmed words from the standard documents into their specifies lexical categories,"In this approach the WordNet 2.1 is used which contains 155,327 terms, 117,597 senses, and 207,016 pairs of term-sense","It groups nouns, verbs, adjectives and adverbs into sets of synonyms called synsets","The synsets are organized into senses, giving thus the synonyms of each word, and also into hyponym/hypernym (i.e","Part-Of) relationships, providing a hierarchical tree-like structure for each term",The application of incorporating semantic features from the WordNet ( ) lexical database has been widely used to improve the accuracy of text clustering techniques,"For example, Dave et al. ( ) employed synsets as features for document representation and subsequent clustering","However, word sense disambiguation was not performed, and WordNet synsets actually decreased clustering performance","Accordingly,   used WordNet in document clustering for word sense disambiguation to improve the clustering performance.   extended this work by exploring the benefits of disambiguating the terms using their part of speech tags",The main limitation of both approaches is the increase in dimensionality of the data.   matched the stemmed keywords to concepts in WordNet for word sense disambiguation,"Their approach improves the efficiency of the applied clustering algorithms; however, it seems to over generalize the affected keywords ( )","In the study of  , the authors accepted that the assignment of terms to concepts in ontology can be ambiguous and can lead to loss of information in their attempt to reduce dimensionality. 2.2 Semantic similarity Semantic similarity plays an important role in natural language processing, information retrieval, text summarization, text categorization, text clustering and so on",In recent years the measures based on WordNet have attracted great concern,Many semantic similarity measures have been proposed,"In general, all the measures can be grouped into four classes: path length based measures, information content based measures, feature based measures, and hybrid measures",An exhaustive overview of these approaches can be found in ( ),"Following the cited overview, we focus on measures that are related to our work. 
                          proposed an approach based on MeSH ontology to improve text retrieval",It computed semantic similarity straightforwardly in terms of the number of edges between terms in the hierarchy,"Their assumption of this approach is that the number of edges between terms in ontology is a measure of conceptual distance between terms.   defined a measure of similarity between concepts based on path lengths (in number of nodes), common parent concepts, and distance from the hierarchy root.   proposed a metric based on the count of link numbers between two set of terms or synonyms representing the same concept, and   used the same approach with Roget’s Thesaurus while   applied a similar strategy to WordNet",In this paper we utilize the Wu and Palmer measure and take into account the glosses of terms for word sense disambiguation,Some of the above described metrics also have been implemented for a comparison with our measure. 2.3 Lexical chains Lexical chains derived from the research in the area of textual cohesion in linguistics ( ),Cohesion involves relations between words that connect different fragments of the text,"A lexical chain is a sequence of related words that give important clues about the semantic content of the text, thus, computing the lexical chains allows identification of the main topics of a document",A large number of researchers have used lexical chains for information retrieval and related areas.   were the first to suggest the use of lexical chains to explore the structure of texts; they used various kinds of syntactic categories to compose lexical chains between words.   used lexical chains in the construction of both a typical IR system and a text segmentation system while   developed a technique to automatically generate hypertext links.   employed WordNet to study lexical chains for the detection and correction of malapropisms.   exploited semantic relationships between words to construct concept clusters for indexing,"However, these measures either have a poor performance in word sense disambiguation or inefficient to computation","In our work we demonstrate that the lexical chains are built in terms of disambiguated terms, which not only accurately extract core semantics but also reduce the dimension of texts","As for cluster labeling, many existing approaches generate labels with the help of external databases","For example,   proposed a WordNet-based measure that first extracts category-specific terms as cluster descriptors, and then these descriptors are mapped to generic terms based on a hypernym search algorithm to create generic titles for clusters","However, this approach is very time consuming, something that leads to high execution times in order to get the required cluster labels","In contrast, our approach can generate the clusters as well as their labels reasonably fast and the assigned labels are easier to be distinguished and interpreted. 3 Word sense disambiguation Polysemy and synonymy are two fundamental problems that affect the text representation, and they also play an important role in text clustering",Disambiguating the polysemous and synonymous nouns often yields comparable performance in document clustering ( ),Word sense disambiguation (WSD) is a process that replacing the original terms in a document by the most appropriate sense as dictated by the surrounding context of a document,"Typically, many semantic similarity measures are used for calculating the relatedness among senses",Early work varied between counting word overlaps between definitions of the word ( ) to finding distances between concepts following the structure of the LKB ( ),"As an alternative, graph-based methods have gained much attention in recent years ( )",Graph-based techniques are performed over the graph underlying a particular knowledge base; they first consider all the sense combinations of the words in a given context and then try to search for the relations among senses based on the whole graph,The main disadvantage of graph-based methods is their computational expense ( ),"However, most previous researches exploited only one type of semantic information of knowledge base such as the structural properties","In this study, we try to explore the effect of the combination of explicit and implicit semantic relationships between synsets (concepts) on WSD",The overall procedure is presented as follows,"We adopt the WSD procedure which is given by  , aim to identify the most appropriate sense associated with each noun in a given document based on the assumption that one sense per discourse",The WSD approach can be described as follows,"Let  
                      
                      
                      { 
                      ,  
                      , … , 
                      } denote the set of all nouns of a given document  ,  
                      ∊ 
                       Let  
                      = { 
                      ,  
                      , … , 
                      } denote the set of all senses associated with the noun   according to the WordNet ontology","We determine the most appropriate sense of a noun   by computing the sum of its similarity to other noun senses in   as follows. where  ( , 
                      ) is the similarity between two senses",We restrict to the first three senses for each synset to participate in this computation for several reasons as given by  ,"First, the senses of a given noun in the WordNet hierarchy are arranged in descending order according to their common usage","Furthermore, we compare the clustering results on using only the top three senses against using all senses of a noun, the former yields similar clustering results at a reduced computation cost to the latter",This result is consistent with the experimental results obtained by  ,"In this step, the one sense of a noun that is assigned the highest score is considered the most probable sense","There are many semantic measures have been proposed to compute the semantic similarity  ( , 
                      ) in formula   based on ontology hierarchy","In this work, we use the Wu–Palmer measure ( ) and extend it by incorporating the glosses of senses to improve the similarity measure","For the purpose of clearly present our proposed method, we first describe two related semantic similarity measures in detail and then lead to the connected use of these two methods in our method",Wu and Palmer computed the similarity between two senses by finding the least common subsumer (LCS) node that connects their senses,"For example, we can see from the red rectangle boxes in  
                      , the LCS of   and   is the lowest common node between the paths of these two senses from the root of WordNet hierarchy,  ","Once the LCS has been identified, the distance between two senses is computed by where   is the depth of the LCS from the root,   is the path length between   and LCS, and   is the path length between   and LCS","Based on the above Wu–Palmer similarity measure, we can calculate the similarity between each pair of synsets","However, this measure is based only on the explicit semantic relations that assuming the links between concepts represent distances; but such links do not cover all possible relations between synsets","For example, WordNet encodes no direct link between the synsets   and  , although they are clearly related","Thus, different from Wu–Palmer measure,   presented a new measure of semantic relatedness between concepts that is based on the number of shared words (overlaps) in their definitions (glosses)","When measuring the relatedness between two input synsets, this method not only looks for overlaps between the glosses of those synsets, but also between the glosses of the  ,  ,  ,   and   synsets of the input synsets, as well as between synsets related to the input synsets through the relations of  ,   and  ","For purposes of illustration, we define the description of concepts as below. 
                   Based on  , the scoring function of similarity can be defined as follows","Let  
                      = { 
                      ,  
                      , … , 
                      } be the set of synsets in a document","DES ( ) and DES ( ) are description sets of two synsets   and   ( 
                       
                      
                      ∊ 
                      ), respectively","The longest overlap between these two strings is detected first, then removed and in its place a unique marker is placed in each of the two input strings; the two strings thus obtained are then again checked for overlaps, and this process continues until there are no longer any overlaps between them","Let   be the number of continuous words that overlapped, and let   be the number of iterations they had detected","Then the similarity between two synsets is computed by 
                   The formula   is a formalized representation of the description given by  ","The score mechanism assigns an   continuous words overlapped the score of  , which gives an  -word overlapped a score that is greater than the sum of the scores assigned to those   words if they had occurred in two or more phrases, each less than   words long",This measure next assigns each possible sense a score by some other mechanisms; and sense with the highest score is judged to be the most appropriate sense for the target word,The measure of   assumes that synsets description pair with more common words and less non-common words are more similar,However it cannot work well when there is not an overlap description set,"WordNet provides explicit semantic relations between synsets, such as through the is-a or has-part links, but links do not cover all possible relations between synsets; while overlaps provide evidence that there is an implicit relation between those uncovered synsets","In order to take full advantages of the measures mentioned above, we define a new similarity measure that combines both measures as below. where  
                      = log(Score(DES( ), DES( )) + 1), and the other parameters are similar with formula  ","This method not only reflects structure information of synsets, such as distance, but also incorporates content meaning of synsets in the ontology","It integrates well with explicit and implicit semantic between synsets in ontology. 4 Core semantics extraction As we have noted, disambiguating all nouns may increase the dimensionality of the feature space since a polysemous term can be replaced by multiple word senses from WordNet",We need to seek a way to reduce the dimensionality while achieving clustering performance that is comparable to using all nouns,"Specifically, we will try to extract a small subset of semantic features (core semantics) with the help of information from WordNet","These core semantics are not only useful for clustering, but once identified, they may represent the main theme of the topics in the documents ( )",The process of core semantics extraction is similar to most taxonomy construction initiatives with the goal of finding out the representative terms and their relationships,"Recently, there have been several attempts to learn taxonomies from text","For example,   presented a framework for automatically constructing domain taxonomy from text corpora, they used a filtering method to extract terms from documents and then based on several domain-specific criteria to establish whether a term is selected as a concept","For those resulting concepts, the hierarchical relations among them are created using either the subsumption method or the hierarchical clustering algorithm.   extended this work by exploring the benefits of disambiguating the terms and concepts of a taxonomy by means of WSD","However, in both above measures, the concepts are captured with statistics and the hierarchical relations are created using the statistics-based methods, unlike the core semantic feature extraction approach used in this paper","Furthermore, they usually based on a specific domain","In order to take full use of the semantic information from WordNet, in our study, we introduce lexical chains to extract a small subset of the semantic features (core semantics) which not only represent the theme of documents but also are beneficial to clustering",It is generally agreed that lexical chains represent the discourse structure of a document and provide clues about the topicality of a document ( ),Lexical chains are identified by using relationships between word senses,"In this work, we use the approach described in   to build lexical chains, which considers only four kinds of relations –  ,  ,   ( ), and  ","Compare to other traditional lexical chains, the adopted approach highlights the semantic importance degrees of the lexical items or lexical chains within a document, and which is helpful to identify the theme of a document; however, it builds lexical chains based on nouns without considering word sense disambiguation","In a cohesive and meaningful text, the word sense that is related with more word senses should be the correct sense","Thus, the way we build lexical chains in present work is based on the previous step that has been disambiguating all the nouns in the texts","Overall, the process of extracting core semantics from texts can be decomposed into three parts","First we build lexical chains for texts based on disambiguated semantic concepts; then we adjust weights of concepts in each lexical chain by adding a weight based on the relations that this concept has with other concepts; finally, the weights of concepts in a lexical chain are added together to arrive at the score of this lexical chain, and when the score of a lexical chain pass the pre-defined requirement, the concepts in it are added to the core semantic feature sets",The three steps will be described in the subsequent sections,"Here we can view a document   as an undirected graph  
                      = ( , 
                      ) whose nodes are concepts and edges are semantic relations between concepts",Each node and edge has a weight that represents their respective degrees of semantic importance within a document,"Then, a lexical chain is defined as a connected subgraph of  ","Let RN = { ,  ,   ( ),  } be the set of semantic relations, and let  
                      = { 
                      ,  
                      
                      
                      
                      
                      
                      ,  
                      ,  
                      } be the set of the corresponding weight of relation in RN; weights of relations depending on the kind of semantic relationship","In this case,   and   are regarded as one relation because all the nouns have been replaced by the most appropriate sense based on WordNet","For a given graph   to find the semantic relations among noun senses, and if any of these senses bear some kind of cohesive relationships, we create the appropriate links in the graph","More formally, we perform the algorithm as depicted in  
                       to construct lexical chains for document  ","For the sake of intuitively illustrating, we imitate Kang’s example to show a sample of lexical chains","We apply our algorithm1 to the sample text that is extracted from reuters-21578, and we just show the generated lexical chains for simplicity","The lexical chains are shown in  
                      ","As we can see, a lexical chain in   represents semantic relations among the selected word senses of the words appearing in that lexical chain","Each node in a lexical chain is a word sense of a word, and each link can be  ,  ,   ( ) or   relation between two word senses","In  , as for the form of word# # ,   indicates the word is used under that sense marker in this text, which reflects our constraint that one sense per discourse;   is the frequencies of this word in the text, which also sums the frequencies of all words linking to this word by relation synonym","In order to extract the core semantics, the semantic importance of word senses within a given document should be evaluated first","Generally, let  
                      = { 
                      ,  
                      , … , 
                      } be the set of nouns in a document  , and let  
                      = { 
                      ,  
                      , … , 
                      } be the corresponding frequency of occurrence of nouns in  ","Let  
                      = { 
                      ,  
                      , … , 
                      } be the set of disambiguated concepts that corresponding to   Given a document  , a set of nouns  , a set of frequencies   and a set of concepts  , let  
                      = { 
                      ,  
                      , … , 
                      } as the set of corresponding weight of disambiguated concepts in  , if   ( 
                      ∊ 
                      ) is mapped from   and   ( ,  
                      ∊ 
                      ), then the weight of   is computed by 
                   Based on the weighted concepts, we give following definitions in terms of  . 
                      
                      
                   We extract the weighted concepts in the lexical chains   composing the set of core semantic features for the given document","It is these concepts can then be used to cluster the documents. 5 Experiments and analysis In this section, we experimentally evaluate the performance of the proposed method","All the experiments have been performed on an Intel I5 Processor, Windows 7 OS machine with 4 GB memory","We choose WordNet version 2.0 for our experiment. 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   5.1 Comparison of term similarity measures In order to evaluate our modified similarity measure, the measure of formula   used by   and the Pedersen’s extended gloss overlap ( ) measure which we implemented it with formula   are conducted to compare with our work","As for the experimental corpus in this section, there was an experiment carried out by   is commonly used to evaluate methods of computing the semantic similarity between words",The authors provided 30 pairs of words prior and then students were asked to rate these words for similarity in meaning on a scale from 0 (dissimilar) to 4 (highly similar),"The rating scores are shown in  
                          as the column MC","In this experiment, we focus primarily on the improvement of our method against the classical measures, and we just calculate the similarities of many noun pairs but do not take account of their context","Furthermore, as we have noted in Section  , the senses of a given noun in the WordNet hierarchy are arranged in descending order according to their common usage","Given the computational cost of using large graph, thus, for the purpose of simplification, we select the first sense of nouns in WordNet to build the on-line tree-like hierarchy for the given terms. 
                          lists the human judgments’ results and our experimental results",The column of MC is the rating scores of human,"The results of   and our modified method are listed in columns WP and WGL, respectively; and the result of extended gloss overlap is listed in column GL , which is calculated by formula  ","In GL  measure, we assign a very large number 999,999 to the score of similarity between two identical senses",We obtain a better performance than the other measures in the same experimental setups,We get a Pearson’s correlation value 0.33 between MC and GL  and a correlation value 0.536 between MC and WP,"The correlation between MC and WGL is 0.579, which indicates that our integrated measure is most coinciding with the human judgments","When we add the gloss to the path-based method, it pays a positive role to the correlation","We can see that the explicit and implicit semantic relations together can reveal hidden similarity between terms, potentially leading to better performance","Through the results, we can conclude that our method is effective. 5.2 Clustering results on text dataset In this section, we evaluate our approach by different setups and configurations, compare the results of our method with other similar measures, and discuss insights gained. 
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                      5.2.1 Dataset We use the reuters-21578 corpus in our experiments, which has been widely used for evaluating document clustering algorithms","The characteristic of the corpus reuters-21578 is that each text is labeled with zero, one, or more of the 135 pre-defined classes","However, the class distribution is not uniform","The size of some classes such as   and   is relatively large, while others such as   and   have few documents ( )","In this dataset, we discard the unlabeled and multiple labels documents","To maintain size among the classes, we sample several subsets of reuters-21578 according to the number of documents in classes","For example, if there is a subset that the minimum size of classes in it is 15 and its maximum size is 20, we label this subset as “RC-min15-max20”.  
                             summarizes the characteristic of these subsets. 5.2.2 Evaluation metrics In our experiment, cluster quality is evaluated by three metrics, purity, F1-measure, and entropy",Purity assumes that all the texts of a cluster we obtained are the members of the actual class for that cluster,F1-measure combines the information of precision and recall,Entropy is the sum of individual cluster entropies weighted by the cluster size,The detail of their definition can be seen in ( ),"Note that the values for purity and similarity are percentages, and thus limited to the range between 0 and 1","The smaller the entropy value, the better the clustering result, and the larger purity and F1-measure values the better the clustering result","In addition, in order to evaluate one method’s ability to deal with high dimensionality, we introduce another criterion from research   as follows","Given a baseline method B, the percentage of reduction in the number of input features can be computed by where   denotes a method of decreasing the number of features, and the function   denotes the total number of features which are derived from that method. 5.2.3 Clustering schemes under comparison In our experiments, we need to investigate the following aspects. 
                         In an attempt to gain insight into the above aspects, we perform our experiments on different setups as shown in  
                            . 5.2.4 Clustering results and analysis We adopt the Bisecting K-means as our underlying clustering algorithm, which has been proved to be very robust in a wide variety of experiments ( ); the clustering parameters used are the same for all methods",The parameter   is set to the known number of classes for these datasets,We evaluate the value of   in formula   using a brute force approach by incrementing it with a step size of 0.5 in the range between 0 and 2 on all datasets,"The optimal setting is  
                            = 1.5, and which is a tradeoff between clustering performance and dimensionality reduction","The weights of the relations used in the clustering are set as  
                            
                            = 
                            
                            
                            = 0.8,  
                            
                            = 0.5 and  
                            
                            = 0.3","We repeat each experiment 20 times and report their average values. 
                            
                             shows the evaluations of all clustering results on four datasets",The best result obtained in each metric is marked in bold face,"The column #Docs and #Features indicate the total number of documents and features which are derived from the corresponding method, respectively",The number of documents of CSF11 is less than the others due to there are a number of documents derived from this method that do not contain any of the core semantic features; this issue is discussed in more details later in this section,"As the experiment using CSF11 on dataset RC-min408-max3945 is very time-consuming, we only conduct this method on the other three datasets","From the  , we can make the following observations","We can see that the DC experiment results are always better than the Base, which indicates that the disambiguated concepts produced by our measure can improve the clustering performance","In addition, we note that the number of features derived from DC is higher than the Base for all datasets",The reason as we have described in Section   that is due to the disambiguation of each polysemous term into multiple word senses from WordNet,This result further supports the previous assumption that when replacing a term with its potential concepts may increase the feature space,The performance of DCS is better than the Base across all datasets and in most cases is better than the DC,"The reason for the relatively poor performance compare to DC is due to the dataset RC-min15-max500 has a wide range of topics and the size of classes in it is of large difference (as described in  ), and the core semantic features as a small portion of the total feature set might not cover all the topics in this dataset","However, we achieve a feature reduction of at least 13.5% using the DCS approach on all datasets in terms of all nouns (Base)",These results suggest that using lexical chain features (core semantics) to represent the documents not only reduce the feature set dimensionality but also improve the clustering performance for many of the datasets,"Comparing DSC, ASG03, LMJ10 and CSF11 with each other, in all cases DCS gives the best results","For datasets RC-min15-max20 and RC-min110-max150, the cluster purity using CSF11 is higher than using Baseline, ASG03 and LMJ10; but CSF11 also performs poor in dataset RCmin15-max500 as our DCS measure, the reason has been described above","Moreover, although in most cases ASG03 scheme can also improve clustering results compared to the Base, the improvement are not significant",Sometimes it even obtains poor performance than the Base,We think the reason is that it selects the first sense of concept in WordNet in case of a tie between two or more senses,"In all six schemas, the performance of LMJ10 is the worst, which is partly due to it does not remove some semantic noise but increase their weights","As for the aspect of dimensionality reduction, the number of features derived from ASG03 is slightly lower than the number of all nouns, the reduction of our DCS approach is between 10% and 40%, and the reduction of CSF11 is up to more than 74%","Although the CSF11 can greatly reduce the dimensionality, it also loses much semantic information","Moreover, the greatly dimensionality reduction may lead to only a subset of the documents will be clustered, and which we call “covered documents”","The documents that do not contain any of the core semantic features become “uncovered” ( ).  
                             lists the number of uncovered documents for three methods",The number of “uncovered” documents produced by CSF11 is higher than that produced by the other methods,"Since the CSF11 measure loses much information and does not cover all documents in a dataset during the process of core semantic features extraction, it obtains poor performance compared to our DCS approach, even though we are in the same way for WSD","This result suggests the lexical chain measure is able to identify the theme of documents for clustering without losing much semantic information, which indicates that lexical chain is effectiveness in text clustering","As for the “uncovered” documents in our approach, we map them to one of the existing core feature centroids based on “closeness” of those centroids","In short, the results in   show that the cluster quality obtained using the core semantic features is better than using all nouns and using the disambiguated concepts (or at least comparable to)","The performance of using the disambiguated concepts is better than using all nouns, which suggests that our disambiguation measure can resolve the synonymous and polysemous problems commendably and improve the quality to some extent","The lexical chains features (core semantic features) produced by our DCS approach not only reduce the number of semantic concepts without losing much information; it also sufficiently captures the main theme of a document that is helpful to clustering. 5.2.5 Identify the number of clusters In this section, we verify if our method can correctly estimate the number of clusters in a dataset by observing the experimental results with varying the number   of clusters for the parameter","As the values of purity and entropy reflect good performance of clustering with the increasing value of  , these two metrics cannot use to identify the correct number of clusters","F1-measure is a multiple evaluation method that combines recall and precision measures, ideally, the nearer the value of   to the real number of groups, the higher the F1-measure obtained, due to the higher accuracy of the clusters mapping to the original classes","Therefore, we observe the experimental results with varying the number   of clusters on F1-measure","To find the best partition, we use the Bisecting K-means algorithm with its input parameters   changes within some limit","As the number of clusters of dataset RC-min408-max3945 is too small, the experiments are conducted on the other three datasets only","The comparison results on different datasets are shown in  
                            
                            
                            ",The true number   of clusters in a dataset is shown after the corpus name (i.e,"RC-min15-max20 ( 
                            = 10))","From  , we see that the values of Base, ASG03 and DCS appear to increase with increasing the value of   and then tend to be constant after   is set equal to or greater than 9, from these results we can estimate the number of clusters is in a vicinity of 9 (the correct number is 10)","However, it is difficult to estimate the correct number of clusters from the changes of curves for CSF11, LMJ10 and DC. 
                             presents the results on dataset RC-min15-max500","In terms of the curve of our DCS approach, we pinpoint that for the case of 20 clusters, the results begin to decrease over the rest of the cases which can be interpreted as a viable indication of the actual number of clusters our data set seems to have","Indeed, the actual number of clusters in this dataset is 20","However, the results of DC, ASG03, Base and LMJ10 show stable curve in different values of  , which makes the estimation of the correct number of clusters become a difficult job",The performance of CSF11 appears unstable when the number of clusters increases,"In  , the trend of curve changes of ASG03, Base, DC and DCS is almost the same, which looks smooth when   is set between 8 and 10 and then is upward with increasing the value of  ; from the results of these four methods we can estimate the number of clusters is in a vicinity of 10 (the actual number is 9)","The results obtained by LMJ10 suggest better performance, the curve of its F1-measure values looks smooth until   is set to 9 then it declines with the increasing the value of  ",From the results we can correctly estimate the number of clusters is 9 (the actual number is 9),"However, the curve of CSF11 does not show regularity with the change of the value of  ","In all, our empirical results show that the testing curve of our DCS approach is very close to the ideal case, so we can easily identify the number of clusters and that is at least near or equal to the ground truth number","Moreover, our DCS approach always gets the best results across all datasets, regardless of the number of clusters. 5.2.6 Extract the topic labels for clusters Labeling a clustered set of documents is an inevitable task after clustering is performed",Automatic labeling methods mainly rely on extracting significant terms from clustered documents,"In this study, we extract the top-ten highest-weighted features as the cluster labels, since the weighted concepts in the extracted representative lexical chains are semantically important terms in clusters","To verify if our method can generate better description labels for derived clusters, we compare the topic labels obtained using Bisecting K-means clustering on all nouns against the topic labels obtained from core semantic features, and investigate the correlation between our topic labels and the human labels","Due to space limitations, the results are shown for dataset RC-min408-max3945 only","This dataset contains three topics, and which are described in  
                            .  
                            
                             show the list of top 10 features derived from the Base and DCS methods, respectively","In both tables, the columns 1, 2 and 3 correspond to the class  ,   and   mentioned in  , separately","In addition, the features in each column are listed in decreasing order of their weights which is obtained by using formula  ","From the  , we observe that the features in first group (group 1) and the third group (group 3) either the same (e.g., common features ‘share’) or with similar meaning (e.g., ‘stake’ in group 1 and ‘profit’ in group 3, ‘exchange’ in group 1 and ‘sale’ in group 3), which make us difficult to distinguish between them","On the other hand, the top features of the group 2 have nothing to do with the topic of   as which has been described in  ","Using the core semantic concepts as features ( ), most of the top features clearly identify the topic of the class","For example, the first group contains concepts, such as ‘share’, ‘company’, ‘stock’, and ‘stockholder’, which all related to the ‘acq’ class; and the concepts in the group 3 are also consistent with the ‘earn’ class (‘net_income’, ‘loss’ and ‘wage’ are related to ‘revenue’; ‘prior’, ‘year’ and ‘April’ are commonly presented in finance report of companies)","Although the top features of group 2 do not include any concepts that directly related to ‘crude’, there still exist several concepts such as ‘Uruguayan_peso’ (which relies on crude oil import), ‘Delaware’ (its petroleum chemical industry occupies the first place in the USA), ‘panel’ and ‘compromise’ have something to do with international oil","From the above comparison and analysis, we see that despite its simplicity, our DCS method produces labels that are almost coincide with the original given labels, and yields better results than the Base method, showing that the effectiveness of the DCS method in generating the meaningful topical labels",These topical labels have good indicator of recognizing and understanding the content of clusters,"Importantly, we also record the senses along with their corresponding definitions in WordNet for ease of analysis and understanding",We therefore argue that our extracted topical labels are feasible in recognizing and interpreting the main topics of clusters. 6 Conclusions This paper presents a methodology for clustering using WordNet and lexical chains,"A modified WordNet-based semantic similarity measure is proposed for word sense disambiguation, and lexical chains are employed to extract core semantic features that express the topic of documents",We have mainly solved four problems in document clustering,"The problems are disambiguating the polysemous and synonymous words, overcoming high dimensionality, determining the number of clusters, and assigning appropriate description for the generated clusters",Most previous researches tried to address only one of these four problems,"But, we study on a hybrid method for solving these problems in text clustering at the same time",We show that the combination of explicit and implicit semantic relationships in WordNet pays a positive role to the assessment of word sense similarity,"Furthermore, our proposed approach can estimate the true number of clusters by observing the obtained results, which is valuable for deciding the value of   in K-means clustering algorithms","In addition, we can use the top ranked concepts of each cluster to define the clusters for ease of human recognition and analysis","More importantly, we show that the lexical chain features (core semantics) can improve the quality significantly with a reduced number of features in the document clustering process","Although lexical chains have been widely used in many application domains, this study is one of the few researches which try to investigate the potential impact of lexical chains on text clustering","However, there are still some limitations in our research",Some important words which are not included in WordNet lexicon will not be considered as concepts for similarity evaluation,"In addition, the proposed method can obtain better clustering results only if the explicit and implicit relationships between words are thoroughly represented in WordNet","In future work, we would like to perform our method on a larger knowledge base, such as Wikipedia","Moreover, since we have demonstrated that the lexical chains can lead to improvements in text clustering, the next work we plan to explore the feasibility of lexical chains in the text mining task.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0925231217309864," 1 Introduction With sensors pervading our everyday lives, we are seeing an exponential increase in the availability of streaming, time-series data","Largely driven by the rise of the Internet of Things (IoT) and connected real-time data sources, we now have an enormous number of applications with sensors that produce important data that changes over time",Analyzing these streams effectively can provide valuable insights for any use case and application,The detection of anomalies in real-time streaming data has practical and significant applications across many industries,"Use cases such as preventative maintenance, fraud prevention, fault detection, and monitoring can be found throughout numerous industries such as finance, IT, security, medical, energy, e-commerce, agriculture, and social media","Detecting anomalies can give actionable information in critical scenarios, but reliable solutions do not yet exist","To this end, we propose a novel and robust solution to tackle the challenges presented by real-time anomaly detection","Consistent with  , we define an   as a point in time where the behavior of the system is unusual and significantly different from previous, normal behavior","An anomaly may signify a negative change in the system, like a fluctuation in the turbine rotation frequency of a jet engine, possibly indicating an imminent failure","An anomaly can also be positive, like an abnormally high number of web clicks on a new product page, implying stronger than normal demand","Either way, anomalies in data identify abnormal behavior with potentially useful information","Anomalies can be  , where an individual data instance can be considered anomalous with respect to the rest of data, independent of where it occurs in the data stream, like the first and third anomalous spikes in  
                      ","An anomaly can also be  , or  , if the temporal sequence of data is relevant; i.e., a data instance is anomalous only in a specific temporal context, but not otherwise","Temporal anomalies, such as the middle anomaly of  , are often subtle and hard to detect in real data streams","Detecting temporal anomalies in practical applications is valuable as they can serve as an early warning for problems with the underlying system. 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   1.1 Streaming applications Streaming applications impose unique constraints and challenges for machine learning models",These applications involve analyzing a continuous sequence of data occurring in real-time,"In contrast to batch processing, the full dataset is not available",The system observes each data record in sequential order as they arrive and any processing or learning must be done in an online fashion,"Let the vector  
                          represent the state of a real-time system at time  ","The model receives a continuous stream of inputs:
 
                      Consider for example, the task of monitoring a datacenter","Components of  
                          might include CPU usage for various servers, bandwidth measurements, latency of servicing requests, etc",At each point in time   we would like to determine whether the behavior of the system is unusual,"The determination must be made in real-time, before time  ","That is, before seeing the next input ( ), the algorithm must consider the current and previous states to decide whether the system behavior is anomalous, as well as perform any model updates and retraining","Unlike batch processing, data is not split into train/test sets, and algorithms cannot look ahead",Practical applications impose additional constraints on the problem,"Typically, the sensor streams are large in number and at high velocity, leaving little opportunity for human, let alone expert, intervention; manual parameter tweaking and data labeling are not viable","Thus, operating in an unsupervised, automated fashion is often a necessity","In many scenarios the statistics of the system can change over time, a problem known as   
                         ",Consider again the example of a production datacenter,"Software upgrades and configuration changes can occur at any time and may alter the behavior of the system ( 
                         )","In such cases models must adapt to a new definition of “normal” in an unsupervised, automated fashion",In streaming applications early detection of anomalies is valuable in almost any use case,Consider a system that continuously monitors the health of a cardiac patient's heart,An anomaly in the data stream could be a precursor to a heart attack,"Detecting such an anomaly minutes in advance is far better than detecting it a few seconds ahead, or detecting it after the fact","Detection of anomalies often gives critical information, and we want this information early enough that it's actionable, possibly preventing system failure","There is a tradeoff between early detections and false positives, as an algorithm that makes frequent inaccurate detections is likely to be ignored","Given the above requirements, we define the ideal characteristics of a real-world anomaly detection algorithm as follows:
 
                      Taken together, the above requirements suggest that anomaly detection for streaming applications is a fundamentally different problem than static batch anomaly detection","As discussed further below, the majority of existing anomaly detection algorithms (even those designed for time-series data) are not applicable to streaming applications. 1.2 Related work Anomaly detection in time-series is a heavily studied area of data science and machine learning, dating back to  ","Many anomaly detection approaches exist, both supervised (e.g. support vector machines and decision trees  ) and unsupervised (e.g. clustering), yet the vast majority of anomaly detection methods are for processing data in batches, and unsuitable for real-time streaming applications",Examples from industry include Netflix's robust principle component analysis (RPCA) method   and Yahoo's EGADS   both of which require analyzing the full dataset,"Likewise, Symbolic Aggregate Approximation (SAX)   involves decomposing the full time series to generate symbols prior to anomaly detection",Other recent techniques include  ,"Although these techniques may work well in certain situations, they are traditional batch methods, and the focus of this paper is on methods for online anomaly detection",For reviews of anomaly detection in general we recommend  ,For prior work on data stream mining and concept drift in general see  ,Some anomaly detection algorithms are partially online,"They either have an initial phase of offline learning, or rely on look-ahead to flag previously-seen anomalous data",Most clustering-based approaches fall under the umbrella of such algorithms,"Some examples include Distributed Matching-based Grouping Algorithm (DMGA)  , Online Novelty and Drift Detection Algorithm (OLINDDA)  , and MultI-class learNing Algorithm for data Streams (MINAS)  ",Another example is self-adaptive and dynamic k-means   that uses training data to learn weights prior to anomaly detection,Kernel-based recursive least squares (KRLS) proposed in   also violates the principle of no look-ahead as it resolves temporarily flagged data instances a few time steps later to decide if they were anomalous,"However, some kernel methods, such as EXPoSE  , adhere to our criteria of real-time anomaly detection (see evaluation section below)","For streaming anomaly detection, the majority of methods used in practice are statistical techniques that are computationally lightweight","These techniques include sliding thresholds, outlier tests such as extreme studentized deviate (ESD, also known as Grubbs’) and k-sigma (e.g.,  ), changepoint detection  , statistical hypotheses testing, and exponential smoothing such as Holt–Winters  ",Typicality and eccentricity analysis   is an efficient technique that requires no user-defined parameters,"Most of these techniques focus on spatial anomalies, limiting their usefulness in applications with temporal dependencies",More advanced time-series modeling and forecasting models are capable of detecting temporal anomalies in complex scenarios,ARIMA is a general purpose technique for modeling temporal data with seasonality  ,It is effective at detecting anomalies in data with regular daily or weekly patterns,Extensions of ARIMA enable the automatic determination of seasonality   for certain applications,A more recent example capable of handling temporal anomalies is the technique in   based on relative entropy,"Model-based approaches have been developed for specific use cases, but require explicit domain knowledge and are not generalizable","Domain-specific examples include anomaly detection in aircraft engine measurements  , cloud datacenter temperatures  , and ATM fraud detection  ","Kalman filtering is a common technique, but the parameter tuning often requires domain knowledge and choosing specific residual error models  ",Model-based approaches are often computationally efficient but their lack of generalizability limits their applicability to general streaming applications,"There are a number of other restrictions that can make methods unsuitable for real-time streaming anomaly detection, such as computational constraints that impede scalability","An example is Lytics Anomalyzer  , which runs in  ( 
                         ), limiting its usefulness in practice where streams are arbitrarily long",Dimensionality is another factor that can make some methods restrictive,"For instance online variants of principle component analysis (PCA) such as osPCA   or window-based PCA   can only work with high-dimensional, multivariate data streams that can be projected onto a low dimensional space","Techniques that require data labels, such as supervised classification-based methods  , are typically unsuitable for real-time anomaly detection and continuous learning",Additional techniques for general purpose anomaly detection on streaming data include  ,Twitter has an open-source method based on Seasonal Hybrid ESD  ,"Skyline is another popular open-source project, which uses an ensemble of statistical techniques for detecting anomalies in streaming data  ","We include comparisons to both of these methods in our Results section. 1.3 Outline The contributions of this paper are twofold: a novel anomaly detection technique built for real-time applications, and a comprehensive set of results on a benchmark designed for evaluating anomaly detection algorithms on streaming data",In   we show how to use Hierarchical Temporal Memory (HTM) networks   to robustly detect anomalies on a variety of data streams,"The resulting system is efficient, extremely tolerant to noisy data, continuously adapts to changes in the statistics of the data, and detects subtle temporal anomalies while minimizing false positives","The HTM implementation and documentation are available as open-source. 
                          In   we review the Numenta Anomaly Benchmark (NAB)  , a rigorous benchmark dataset and scoring methodology we created for evaluating real-time anomaly detection algorithms","In   we present results comparing NAB on ten algorithms, many of which are commonly used in industry and academia.   concludes with a summary and directions for future work. 2 Anomaly detection using HTM Based on known properties of cortical neurons, Hierarchical Temporal Memory (HTM) is a theoretical framework for sequence learning in the cortex  ",HTM implementations operate in real-time and have been shown to work well for prediction tasks  ,"HTM networks continuously learn and model the spatiotemporal characteristics of their inputs, but they do not directly model anomalies and do not output a usable anomaly score","In this section we describe our technique for applying HTM to anomaly detection. 
                      
                      (a) shows an overview of our process","At each point in time, the input data  
                       is fed to a standard HTM network",We perform two additional computations on the output of the HTM,"We first compute a measure of prediction error,  ","Then, using a probabilistic model of  , we compute  , a likelihood that the system is in an anomalous state",A threshold on this likelihood determines whether an anomaly is detected,"In the following subsections, we provide an overview of HTM systems and then describe our techniques for the additional steps of computing the prediction error and anomaly likelihood","Taken together, the algorithm fulfills the requirements for streaming applications outlined in  . 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   2.1 Overview of HTM 
                         (b) shows the core algorithm components and representations within a typical HTM system  ","The current input, x , is fed to an encoder   and then a sparse spatial pooling process  ","The resulting vector, a(x ), is a sparse binary vector representing the current input",The heart of the system is the sequence memory component,This component models temporal patterns in a(x ) and outputs a prediction in the form of another sparse vector  (x ).  (x ) is thus a prediction for  ,"HTM sequence memory consists of a layer of HTM neurons organized into a set of columns ( 
                         )",The network accepts a stream of inputs encoded as sparse vectors,It models high-order sequences (sequences with long-term dependencies) using a composition of two separate sparse representations,"The current input,  
                          and the previous sequence context,   are simultaneously encoded using a dynamically updated sparse distributed representation",The network uses these representations to make predictions about the future in the form of a sparse vector.  (c) shows how the sparse representations are used to represent temporal patterns and disambiguate sequences with long-term dependencies,"When receiving the next input, the network uses the difference between predicted input and the actual input to update its synaptic connections",Learning happens at every time step but since the representations are highly sparse only a tiny percentage of the synapses are updated,The details of the HTM learning algorithm and the properties of its representation are beyond the scope of this paper but are described in depth in  ,"In our implementation we use the standard HTM system   and a standard set of parameters (See Supplementary Section S3 for the complete list). 2.2 Computing the prediction error Given the current input, 
                         ,  ( 
                         ) is a sparse encoding of the current input, and   is the sparse vector representing the HTM network's internal prediction of  ( 
                         )",The dimensionality of both vectors is equal to the number of columns in the HTM network (we use a standard value of 2048 for the number of columns in all our experiments),"Let the  , be a scalar value inversely proportional to the number of bits common between the actual and predicted binary vectors:
 where | ( 
                         )| is the scalar norm, i.e. the total number of 1 bits in  ( 
                         )","In   the error   will be 0 if the current  ( 
                         ) perfectly matches the prediction, and 1 if the two binary vectors are orthogonal (i.e. they share no common 1 bits).   thus gives us an instantaneous measure of how well the underlying HTM model predicts the current input  
                         ",Changes to the underlying statistics are handled automatically due to the continuous learning nature of HTMs,"If there is a shift in the behavior of the system, the prediction error will be high at the point of the shift, but will automatically degrade to zero as the model adapts to the “new normal”.  
                          shows an example stream and the behavior of the prediction error  ",Shifts in the temporal characteristics of the system are handled in addition to spatial shifts in the underlying metric values,An interesting aspect of this metric is that branching sequences are handled correctly,"In HTMs, multiple predictions are represented in  ( 
                         ) as a binary union of each individual prediction","Similar to Bloom filters, as long as the vectors are sufficiently sparse and of sufficient dimensionality, a moderate number of predictions can be represented simultaneously with exponentially small chance of error  ",The above error handles branching sequences gracefully in the following sense,"If two completely different inputs are both possible and predicted, receiving either input will lead to a 0 error",Any other input will generate a positive error. 2.3 Computing anomaly likelihood The prediction error described above represents an instantaneous measure of the predictability of the current input stream,"As shown in  , it works well for certain scenarios","In some applications however, the underlying system is inherently very noisy and unpredictable and instantaneous predictions are often incorrect","As an example, consider  
                         (a)",This data shows the latency of a load balancer in serving HTTP requests on a production web site,"Although the latency is generally low, it is not unusual to have occasional random jumps, leading to corresponding spikes in prediction error as shown in  (b)","The true anomaly is actually later in the stream, corresponding to a sustained increase in the frequency of high latency requests",Thresholding the prediction error directly would lead to many false positives,"To handle this class of scenarios, we introduce a second step","Rather than thresholding the prediction error   directly, we model the distribution of error values as an indirect metric, and use this distribution to check for the likelihood that the current state is anomalous",The   is thus a probabilistic metric defining how anomalous the current state is based on the prediction history of the HTM model,To compute the anomaly likelihood we maintain a window of the last W error values,"We model the distribution as a rolling normal distribution 
                          where the sample mean,  , and variance,  , are continuously updated from previous error values as follows:
 
                         
                      We then compute a recent short term average of prediction errors, and apply a threshold to the Gaussian tail probability (Q-function  ) to decide whether or not to declare an anomaly. 
                          We define the   as the complement of the tail probability:
 where:
 
                      
                         ′ here is a window for a short term moving average, where  ′ ≪  , the duration for computing the distribution of prediction errors. 
                          We threshold   based on a user-defined parameter   to report an anomaly:
 
                      Since thresholding   involves thresholding a tail probability, there is an inherent upper limit on the number of alerts and a corresponding upper bound on the number of false positives",With   very close to 0 it would be unlikely to get alerts with probability much higher than  ,"In practice we have found that   works well across a large range of domains and the user does not normally need to specify a domain-dependent threshold. 
                         (c) shows an example of the anomaly likelihood,  , on noisy load balancer data",The figure demonstrates that the anomaly likelihood provides clearer peaks in extremely noisy scenarios compared to pure prediction error,"It is important to note that   is based on the distribution of prediction errors, not on the distribution of underlying metric values  
                         ","As such, it is a measure of how well the model is able to predict, relative to the recent history",In clean predictable scenarios   behaves similarly to  ,"In these cases, the distribution of errors will have very small variance and will be centered near 0",Any spike in   will similarly lead to a corresponding spike in  ,"However, in scenarios with some inherent randomness or noise, the variance will be wider and the mean further from 0",A single spike in   will not lead to a significant increase in   but a series of spikes will,"Importantly, a scenario that goes from wildly random to completely predictable will also trigger an anomaly. 2.4 Extensions Modeling multiple streams simultaneously can enable the system to detect anomalies that cannot be detected from one stream alone","In Supplementary Section S4, we discuss this scenario and describe an extension for performing anomaly detection across multiple streams",We show how to combine independent models while accounting for temporal drift,This is particularly useful when there are many sensors and the combinations that enable detection are unknown,"Our algorithm is agnostic to the data type, as long as the data can be encoded as a sparse binary vector that captures the semantic characteristics of the data","We present an interesting extension with streaming geospatial data in Supplementary Section S2, demonstrating the applicability in diverse industries. 3 Evaluation of streaming anomaly detection algorithms Numerous benchmarks exist for anomaly detection   but these benchmarks are generally designed for static datasets",Even benchmarks containing time-series data typically do not capture the requirements of real-time streaming applications,It is also difficult to find examples of real-world data that is labeled with anomalies,"Yahoo released a dataset for anomaly detection in time-series data  , but it is not available outside of academia and does not incorporate the requirements of streaming applications","As such we have created the Numenta Anomaly Benchmark (NAB) with the following goals:
 
                   We briefly describe each of these below (full details can be found in  ). 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   3.1 Benchmark dataset The aim of the NAB dataset is to present algorithms with the challenges they will face in real-world scenarios, such as a mix of spatial and temporal anomalies, clean and noisy data, and data streams where the statistics evolve over time","The best way to do this is to provide data streams from real-world use cases, and from a variety of domains and applications","The data currently in the NAB corpus represents a variety of sources, ranging from server network utilization to temperature sensors on industrial machines to social media chatter","NAB version 1.0 contains 58 data streams, each with 1000–22,000 records, for a total of 365,551 data points","Also included are some artificially-generated data files that test anomalous behaviors not yet represented in the corpus's real data, as well as several data files without any anomalies","All data files are labeled, either because we know the root cause for the anomalies from the provider of the data, or as a result of the well-defined NAB labeling procedure.  These labels define the ground truth anomalies used in the NAB scoring process.   is an example of noisy sensor data with spatial and temporal anomalies.   shows two related anomalies preceding a shift in the underlying statistics of the stream.  
                          shows several data streams from the benchmark dataset, sourced from a variety of domains and exhibiting diverse characteristics such as temporal noise and short and long-term periodicities. 3.2 NAB scoring The NAB scoring system formalizes a set of rules to determine the overall quality of streaming anomaly detection relative to the ideal, real-world anomaly detection algorithm that we defined earlier","There are three key aspects of scoring in NAB: anomaly windows, the scoring function, and application profiles","These are described below, and detailed discussion can be found in  ","To incorporate the value of early detection into scoring, we define   that label ranges of the data streams as anomalous, and a   that uses these windows to reward early detections (and penalize later detections)","When an algorithm is tested on NAB, the resulting detections must be scored","After an algorithm has processed a data file, the windows are used to identify true and false detections, and the scoring function is applied relative to each window to give value to the resulting true positives and false positives","Detections within a window correctly identify anomalous data and are true positives (TP), increasing the NAB score","If a detection occurs at the beginning of a window, it is given more value by the scoring function than a detection towards the end of the window","Multiple detections within a single window identify the same anomalous data, so we only use the earliest (most valuable) detection for the score contribution","Detections falling outside the windows are false positives, giving a negative contribution to the NAB score",The value of false positives is also calculated with the scoring function such that a false positive (FP) that occurs just after a window hurts the NAB score less than a FP that occurs far away from the window,"Missing a window completely, or a false negative (FN), results in a negative contribution to the score",Refer to Supplementary Section S1 for details on scoring equations,"All the scoring code and documentation is available in the repository. 
                         
                      How large should the windows be? Large windows promote early detection of anomalies, but the tradeoff is that random or unreliable detections would be regularly reinforced","Using the underlying assumption that anomalous data is rare, the anomaly window length is defined to be 10% 
                          the length of a data file, divided by the number of anomalies in the given file","This scheme provides a generous window to reward early detections and gives partial credit for detections slightly after the true anomaly, yet small enough such that poor detections are likely to be counted negatively ( 
                         )",Note that the streaming algorithms themselves have no information regarding the windows or the data length,Anomaly windows are only used as part of the benchmark and scoring system to gauge end performance,NAB also includes a mechanism to evaluate algorithms on their bias towards false positives or false negatives,"Depending on the application, a FN may be much more significant than a FP, such as detecting anomalies in ECG data, where a missed detection could be fatal",These scenarios are formalized in NAB by defining   that vary the relative values of these metrics,"For example, a datacenter application would be interested in the “Reward Low FP” profile, where false positives are weighted more heavily than in the other profiles","The combination of anomaly windows, a smooth temporal scoring function, and application profiles allows researchers to evaluate online anomaly detector implementations against the requirements of the ideal detector","The NAB scoring system evaluates real-time performance, prefers earlier detection of anomalies, penalizes “spam” (i.e","FPs), and provides realistic costs for the standard classification evaluation metrics TP, FP, TN, and FN. 3.3 NAB is open-source With the intent of fostering innovation in the field of anomaly detection, NAB is designed to be an accessible and reliable framework for all to use",Included with the open-source data and code is extensive documentation and examples on how to test algorithms,"The NAB repository contains source code for commonly used algorithms for online anomaly detection, as well as some algorithms submitted by the community. 4 Results & discussion In this section we discuss NAB results and the comparative performance of a collection of real-time anomaly detection algorithms drawn from industry and academia","Our goals are to evaluate the performance of our HTM anomaly detection algorithm and, through a detailed discussion of the findings, to facilitate further research on unsupervised real-time anomaly detection algorithms. 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   4.1 Tested algorithms and parameter tuning We considered a number of anomaly detection methods but the list was heavily filtered based on the criteria discussed in   and  ","The algorithms evaluated include HTM, Twitter's Anomaly Detection, Etsy's Skyline, Multinomial Relative Entropy   EXPoSE  , Bayesian Online Changepoint detection  , and a simple sliding threshold",Some of these algorithms have open-source implementations and we implemented the rest based on their respective papers,We performed extensive parameter tuning for each algorithm; the resulting parameters are optimal to the best of our knowledge,Most of the algorithms also involve setting thresholds,"The parameters were kept constant across all streams, and a single fixed threshold for each algorithm was used for the entire dataset, as required by NAB",Additional implementation details for these algorithms are included in Supplementary Section S5,"In addition, during the summer of 2016 we ran a NAB competition in collaboration with IEEE WCCI 
                          to encourage additional algorithm testing",We include the result of the three competition winners below,We use the HTM algorithm as described in  ,The core HTM algorithm by its nature is not highly sensitive to parameters,We used the architecture shown in   and the standard HTM parameter set (see Supplemental Section S3),"The parameters specific to anomaly detection, ɛ,   , and  ′ , were set to 10 , 8000, and 10, respectively",Timestamps and metric values in the NAB dataset were encoded using standard HTM datetime and scalar encoders  ,"In the results below, we show two variations",We show NAB results obtained by using prediction error only (i.e. thresholding   directly),"We also show results obtained by using anomaly likelihood, as defined in  ","For transparency and reproducibility, we have incorporated the source code and parameter settings for all of the above algorithms into the NAB repository. 4.2 Comparison of results 
                         
                          summarizes the NAB scores for each algorithm across all application profiles including the three NAB competition winners","In addition to the algorithms described above, we also use three control detectors in NAB","A “null” detector runs through the dataset passively, making no detections, accumulating all false negatives","A “perfect” detector is an oracle that outputs detections that would maximize the NAB score; i.e., it outputs only true positives at the beginning of each window",The raw scores from these two detectors are used to scale the score for all other algorithms between 0 and 100,"We also include a “random” detector that outputs a random anomaly probability for each data instance, which is then thresholded across the dataset for a range of random seeds",The score from this detector offers some intuition for chance-level performance on NAB,"Overall we see that the HTM detector using anomaly likelihood gets the best score, followed by CAD-OSE, nab-comportex, KNN CAD, and the Multinomial Relative Entropy detector","The HTM detector using only prediction error performs moderately well, but using the anomaly likelihood step significantly improves the scores",Most of the algorithms perform much better than a random detector,"There is a wide range of scores but none are close to perfect suggesting there is still significant room for improvement. 
                         
                          summarizes the various algorithmic properties of each of the algorithms we implemented","Each algorithm is categorized based on its ability to detect spatial and temporal anomalies, handle concept drift, and automatically update parameters; these characteristics are based on published information, which may or may not reflect the actual performance",We also list the measured latency of processing each data point,Several algorithms claim to have all of the listed properties but their actual anomaly detection performance on the benchmark varies significantly,"In general there is a rough correlation between the number of properties satisfied and the NAB ranking (with the exception of EXPoSE, see discussion below)","Based on a more detailed analysis of the results we highlight four factors that were important for achieving good performance on NAB: concept drift, ability to detect temporal anomalies, assumptions regarding distribution, and assumptions regarding the number of data points",We discuss each of these qualitatively below as well as more detailed file-level comparisons,"The ability of each algorithm to learn continuously and handle concept drift is one of the contributing factors to obtaining a good NAB score.  
                         (a) demonstrates one example of this","This file shows CPU usage on a production server over time, and contains two anomalies",The first is a simple spike that is easily detectable by all algorithms,"The second is a sustained shift in the usage, where the initial change in sequence is an anomalous changepoint, but the new behavior persists",Most algorithms detect the change and then adapt to the new normal,"However, the Twitter ADVec algorithm fails to detect the changepoint and continues to generate anomalies for several days","The NAB corpus contains several similar examples, where an anomaly sets off a change in sequence that redefines the normal behavior",This is representative of one of the key challenges in streaming data,"An inability to handle concept drift detections effectively results in a greater number of false positives, lowering the NAB score","The ability to detect subtle temporal anomalies, while limiting false positives, is a second major factor in obtaining good NAB scores","In practical applications, one major benefit of detecting temporal patterns is that it often leads to the early detection of anomalies.   shows a representative example",The temporal anomaly in the middle of this figure precedes the actual failure by several days,"In  , the strong spike is preceded by a subtle temporal shift several hours earlier.  (b) shows detection results on that data stream","The Twitter ADVec, Skyline, and Bayesian Online Changepoint algorithms easily detect the spike, but there are subtle changes in the data preceding the obvious anomaly",The Multinomial Relative Entropy and HTM detectors both flag anomalous data well before the large spike and thus obtain higher scores,It is challenging to detect such changes in noisy data without a large number of false positives,Both the HTM and Multinomial Relative Entropy perform well in this regard across the NAB dataset,The third major factor concerns assumptions regarding the underlying distribution of data,A general lesson is that algorithms making fewer assumptions regarding distribution perform better,"This is particularly important for streaming applications where algorithms must be unsupervised, automated, and applicable across a variety of domains",Techniques such as the sliding threshold and Bayesian Online Changepoint detectors make strong assumptions regarding the data and suffer as a result,"Note that the Gaussian used in our anomaly likelihood technique is used to model the distribution of prediction errors, not the underlying metric data",As such it is a non-parametric technique with respect to the data,Another interesting factor is demonstrated by the performance of EXPoSE,"Theoretically EXPoSE possesses all the properties in  , however it performs poorly on the benchmark",One of the reasons for this behavior is that EXPoSE has a dependence on the size of the dataset and is more suitable for large-scale datasets with high-dimensional features  ,The technique computes an approximate mean kernel embedding and small or moderate data sets do not provide a sufficiently good proxy for this approximation,The average NAB data file contains 6300 records and is representative of real streaming applications,"This issue highlights the need to output reliable anomalies relatively quickly. 4.3 Detailed NAB results A breakdown of the algorithms performance on the benchmark is shown in  
                         ","Results have been aggregated across data sources ranging from artificially generated streams to real streams from advertisement clicks, server metrics, traffic data and twitter volume","Data streams are characterized by spatial anomalies, temporal anomalies or a combination thereof",Grouping the streams by their anomaly types in   helps us inspect the characteristics of the algorithms identified earlier in  ,"Results show that both HTM and CAD-OSE yield the best overall aggregate scores on almost all data sources and anomaly types, with the exceptions of Twitter AdVec on artificial temporal streams and KNN CAD on miscellaneous known causes",The difference between aggregate scores for HTM and CAD-OSE for the majority of the data streams is less than 0.20,"For some stream types, HTM significantly outperforms CAD-OSE such as spatial advertisement streams, temporal server metric streams and spatial/temporal miscellaneous streams","In particular, the results show HTM performing well on server metrics and online advertisements data while CAD-OSE performs well on traffic and twitter streams","In addition, the results in   also demonstrate that statistical techniques with assumptions on data distribution such as sliding threshold, Twitter AdVec and Skyline may be able to capture spatial anomalies (e.g",Skyline on spatial traffic streams) but are not effective enough for capturing temporal anomalies,This is reflected by the negative scores for most temporal and spatial/temporal anomaly streams for these algorithms,"This further reinforces the correlation between non-parametric techniques and detection of temporal anomalies. 5 Conclusion With the increase in connected real-time sensors, the detection of anomalies in streaming data is becoming increasingly important",The use cases cut across a large number of industries,We believe anomaly detection represents one of the most significant near-term applications for machine learning in IoT,In this paper we have discussed a set of requirements for unsupervised real-time anomaly detection on streaming data and proposed a novel anomaly detection algorithm for such applications,"Based on HTM, the algorithm is capable of detecting spatial and temporal anomalies in predictable and noisy domains","The algorithm meets the requirements of real-time, continuous, online detection without look ahead and supervision","We also reviewed NAB, an open benchmark for real-world streaming applications",We showed results of running a number of algorithms on this benchmark,"We highlighted three key factors that impacted performance: concept drift, detection of temporal anomalies, and assumptions regarding distribution and size of data",There are several areas for future work,The error analysis from NAB indicates that the errors across various algorithms (including HTM) are not always correlated,An ensemble-based approach might therefore provide a significant increase in accuracy,The current NAB benchmark is limited to data streams containing a single metric plus a timestamp,"Adding real-world multivariate data streams labeled with anomalies, such as the data available in the DAMADICS dataset  , would be a valuable addition","Supplementary materials Supplementary material associated with this article can be found, in the online version, at  ","Appendix Supplementary materials 
                      
                  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0736584515000666," 1 Introduction Non-destructive testing (NDT) is a highly multidisciplinary group of analysis techniques used throughout science and industry to evaluate the properties of materials, and/or to ensure the integrity of components/structures, without causing damage to them  ","In civil aerospace manufacturing, the increasing deployment of composite materials demands a high integrity and traceability of NDT measurements, combined with a rapid throughput of data",Modern components increasingly present challenging shapes and geometries for inspection,Using traditional manual inspection approaches produce a time-consuming bottleneck in the industrial production   and this limitation provides the fundamental motivation for increased automation,Modern Computer-Aided Design (CAD) is used extensively in composite manufacture,"Additionally, where it was once necessary to construct large items from many smaller parts, Computer-Aided Manufacturing (CAM) now allows these large items to be produced easily from one piece of raw material (through traditional subtractive approaches, or built up using more recent additive manufacturing processes  )","As a result, large components with complex geometries are becoming very common in modern structures, and the aerospace industry is a typical field, where wide complex shaped parts are very frequently used","Moreover the use of composite materials, which are notoriously challenging to inspect  , is becoming widespread in the construction of new generations of civilian aircraft","To cope with future demand projections for these operations, it is therefore essential to overcome the current NDT bottleneck, which traditionally can be the slowest aspect in a production process",A fundamental issue with composites manufacturing compared to conventional light alloy materials lies in the process variability,"Often parts that are designed as identical, will have significant deviations from CAD, and also may change shape when removed from the mould",This presents a significant challenge for precision NDT measurement deployment which must be flexible to accommodate these manufacturing issues,"For these reasons, NDT inspection is often performed manually by technicians who typically have to position and move appropriate probes over the contour of the sample surfaces",Manual scanning requires trained technicians and results in a very slow inspection process for large samples,"The repeatability of a test can be challenging in structures where complex setups are necessary to perform the inspection (e.g. orientation of the probe, constant standoff, etc.)  ","While manual scanning may remain a valid approach around the edges of a structure, or the edges of holes in a structure, developing reliable automated solutions has become an industry priority to drive down inspection times","The fundamental aims of automation within the inspection process are to minimise downtimes due to the higher achievable speed, and to minimise variability due to human factors","Semi-automated inspection systems have been developed to overcome some of the shortcomings with manual inspection techniques, using both mobile and fixed robotic platforms","The use of linear manipulators and bridge designs has, for a number of years, provided the most stable conditions in terms of positioning accuracy  ","The use of these systems to inspect parts with noncomplex shapes (plates, cylinders or cones) is widespread; typically, they are specific machines, which are used to inspect identically shaped and/or sized parts","More recently, many manufacturers of industrial robots have produced robotic manipulators with excellent positional accuracy and repeatability","An industrial robot is defined as an automatically controlled, reprogrammable, multipurpose manipulator, programmable in three or more axes  ","In the spectrum of robot manipulators, some modern robots have suitable attributes to develop automated NDT systems and cope with the challenging situations seen in the aerospace industry  ","They present precise mechanical systems, the possibility to accurately master each joint, and the ability to export positional data at frequencies up to 500 Hz",Some applications of 6-axis robotic arms in the NDT field have been published during the last few years and there is a growing interest in using such automation solutions with many manufacturers within the aerospace sector  ,"Exploring the current state of the art, RABIT is a group of systems developed by TECNATOM S.A., in collaboration with KUKA Robots Ibérica, that first approached the possibility of incorporating the use of industrial robots in NDT applications  ","These systems boast the capability of using the potential of industrial robots and integrating them in an overall inspection apparatus, bringing together all the hardware and software required to plan and configure ultrasonic inspections","Off-the-shelf robotic arms were also used in the Laser Ultrasound for Composite InspEction (LUCIE) system, addressed to inspect large curved surfaces such as the inside of aircraft fuselage, by means of ultrasound generated by laser  ",Genesis Systems Group has developed the NSpect family of Robotic Non-Destructive Inspection cells,"Incorporating the FlawInspecta technology, developed by Diagnostic Sonar in conjunction with National Instruments, the NSpect systems employ a KUKA 6DOF robot arm to perform ultrasonic inspection using either an immersion tank, or a recirculating water couplant",General Electric (GE) has also investigated the integration of phased array UT with off-the-shelf industrial robots for the inspection of aerospace composites  ,"Despite these previous efforts, there remain challenges to be addressed before fully automated NDT inspection of complex geometry composite parts becomes commonplace","The key challenges include generation and in-process modification of the robot tool-path, high speed NDT data collection, integration of surface metrology measurements, and overall visualisation of measurement results in a user friendly fashion","Collaborations driving this vision include TWI Technology Centre (Wales), which is currently carrying out a 3-year project, called IntACom, on behalf of its sponsors; its objective is to achieve a fourfold increase in the throughput of aerospace components  ",Additionally the UK RCNDE consortium conducts research into integration of metrology with NDT inspection  ,"Both these consortia have identified the requirement for optimal tool path generation over complex curved surfaces, and the current article describes joint work between these groups to develop a novel approach to a flexible robotic toolpath generation using a user friendly MATLAB toolbox","This new toolbox provides a low cost research based approach to robot path planning for NDT applications, and provides a platform for future development of highly specific NDT inspection challenges. 2 Approaches to robotic path planning for NDT applications 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   2.1 Existing robotic path-planning software Six-axis robotic arms have traditionally been used in production lines to move the robot end-effector from one position to another for repetitive assembly and welding operations","In this scenario, where the exact trajectory between two points in the space is not too important, the teach pendant of a robot is used to move manually the end-effector to the desired position and orientation at each stage of the robot task",Relevant robot configurations are recorded by the robot controller and a robot programme is then written to command the robot to move through the recorded end-effector postures,"More recently, accurate mechanical joints and control units have made industrial robotic arms flexible and precise enough for finishing tasks in manufacturing operations  ",Robotic manipulators are highly complex systems and the trajectory accuracy of a machining tool has a huge impact on the quality and tolerances of the finished surfaces,"As a result, many software environments have been developed by manufacturers, academic researchers and also by the robot manufacturers themselves, in order to help technicians and engineers to programme complex robot tasks  ",The use of such software platforms to programme robot movements is known as off-line programming (OLP),"It is based on the 3D virtual representation of the complete robot work cell, the robot end-effector and the samples to be manipulated or machined","Although some limited applications for inspection delivery have been demonstrated  , in general conventional OLP is geared towards manufacturing applications where the task is the production of a specific component using conventional milling/ drilling / trimming operations","In contrast, the result of an automated NDT inspection requires a flexible and extensible approach that has the flexibility to allow future changes in the path planning to accommodate requirements of future NDT inspections","Building the NDT toolbox in MATLAB provides an easy route to such future adaptation by the research community (e.g. fluid dynamics modelling of water-jets, compensation for part variability and conditional programming approaches)","Using current OLP software to generate appropriate tool-paths for NDT purposes can appear quite straightforward at first inspection; however it is possible to list a series of serious inadequacies: 
                      Contrary to currently available OLP software, our new MATLAB toolbox provides a capability for full synchronisation (at all points on the path) with external instrumentation systems (in our case an ultrasonic NDT inspection system)",Such synchronisation is fundamental   to building an accurate map of NDT results on an inspected part with the accuracy required (typically sub-millimetric). 2.2 Robot programming & simulation in MATLAB MATLAB ® is a common platform for the modelling and simulation of various kinds of systems,"It is a flexible programming software environment extensively used for matrix manipulations, plotting of data, implementation of algorithms and creation of user interfaces","It can also be interfaced with programs written in other languages, including C, C#, C++, Java and Fortran","As such, MATLAB is a popular choice for the simulation of robotics systems","Specific toolboxes (collections of dedicated MATLAB functions) have been developed in the past few years for research and teaching in almost every branch of engineering, such as telecommunications, electronics, aerospace, mechanics and control",Several toolboxes have been presented for the modelling of robot systems  ,"These software tools have been inspired by various application scenarios, such as robot vision  , and space robotics  , and have addressed both industrial   and academic/educational   targets",Off-line programming has been investigated by   using a combined Simulink/SimMechanics approach,"To date, no NDT specific path-planning software has been presented in the literature","The following section describes the architecture of a new MATLAB toolbox, developed to specifically address the current needs of robotic NDT related to effective tool-path generation taking into account the deficiencies of existing off-line programming as outlined above. 3 RoboNDT software Originally a MATLAB toolbox for robotic path planning targeted to ultrasonic NDT inspection was developed",A modular approach to the toolbox development was adopted throughout to allow for growth and progressive validation of a large-scale project,"The toolbox was based on 4 main modules:  ,  ,   and  ","The latest developments have led to a full software application, named RoboNDT, equipped with a Graphical User Interface (GUI) to enhance ease-of-use","The latest executable version of the application can be downloaded from http://www.strath.ac.uk/eee/research/cue/downloads/. 
                      
                   
                       shows the schematic architecture of RoboNDT",Traditional commercial path-planning software generates specific robot language programmes that need to be transferred to the robot controller to be executed,RoboNDT generates output files suitable to be used through a C++ server application that has been developed to achieve external control of KUKA robots,This is a novel approach,The command packets of coordinates are sent from an external computer in real-time to the robot controller via Ethernet,"Working with KUKA Robots, the external control is enabled by the KUKA Robot Sensor Interface (RSI) software add-on installed into the robot controllers  ","The C++ application manages the reception of robot feedback positions and NDT data, whilst commending the tool-path to the robot manipulating the NDT probe",This approach allows sending of command coordinates to multiple robots from the same external server computer and enables the path synchronization mismatch to be maintained within the distance covered by the robots in a single interpolation cycle,"For example for robots controlled in a 12 milliseconds interpolation cycle running at 100 mm/s, the maximum path mismatch would be equal to 1.2 mm",This worst case scenario is much improved over commercial solutions that use digital I/O signals for synchronization purposes,In addition this approach is more sophisticated than simple master–slave synchronisation approaches developed by robot suppliers,"Our solution allows for a constant change in the relative path to be encoded with ease, such a situation arises in ultrasonic inspection when considering through transmission of ultrasound through materials with constantly changing thickness. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   3.1 Libraries It was deemed that, in order to develop a flexible platform, the easy and appropriate definition of all elements involved in the path-planning operations has to be guaranteed",Five libraries have been implemented to allow the user to easily reproduce the real working environment for the robotic NDT inspection and use virtual models of the real equipment,"The Libraries menu, accessible from the menu bar ( 
                         a), provides access to these important modules of the GUI","They are the robot library, the tool library (probes and sensors) the environment library and the appropriate contexts to manage the robot cells and the samples of interest",The list of available items is placed on the left hand side of each library,The user can select any of the items to display the related CAD model,"The push buttons on the bottom left corner can be used to remove, edit or duplicate the selected robot model or to create a new item","All libraries, except the cell management context, allow loading of STL (Standard Tessellation Language) CAD files and the specification of key properties for robots, environments, samples and tools (kinematic features, coordinate reference systems, etc.)","The cell management context allows the user to create a new robotic working environment through assembling one or more robots into one selected environment. 3.2 Start-up module The triangular mesh of the sample, imported from the STL file, needs to be placed in the correct position within the virtual robotic cell","Existing software usually considers the CAD models strictly correspondent to the real parts; whilst this can be tolerated for well machined metallic samples, it is sometimes the source of unacceptable errors for large composite components",Therefore the sample's position calibration mode implemented in RoboNDT uses a positioning algorithm originally proposed for 3D point cloud data registration  ,It calculates the optimum position of the STL mesh within the virtual robot cell in order to minimise the square errors of the distances between at least four points selected in the real sample and the relative points in the CAD model,"The  -th point whose coordinates are measured through jogging the robot arm in the real environment is herein named as  
                         ; the relative point selected from the virtual CAD model of the sample is named as  
                         .  
                          shows locating the four required reference points of a complex curved aerofoil sample","For the sake of describing the operation of the software, the carbon fibre composite component, shown in  , is used for the experimental validation described in this paper","The sample's wide surfaces, spanning across most of the available robot working envelope and curving in different directions, were chosen for testing and validating the path-planning software","The centroids of the two datasets ( 
                         and  
                         ), named  
                         and  
                         , are given by 
                         
                      Therefore the point clouds with the centroids removed ( 
                          and  
                         ) are 
                         
                      A correlation matrix (H) is calculated as 
                      The singular value decomposition (SVD) function, applied to the correlation matrix, produces a diagonal matrix S of the same dimension as H, with nonnegative diagonal elements in decreasing order, and unitary matrices U and V so that  . 
                      The rotation matrix ( ) and the translation vector ( ), necessary to match the reference cloud ( 
                         ) with the experimental cloud of points ( 
                         ), are then calculated from the SVD output. 
                         
                      The calculated rotation matrix and translation vector are used to locate the meshed sample in the best position","The result of this operation is then displayed, so that the user can verify and accept before proceeding.  
                         a shows a picture of the real setup; it shows the robotic hardware of an automatic inspection prototype system developed within the TWI led project IntACom","The system utilises two KUKA KR16 L6-2 robotic arms, with controllers that run KUKA System Software (KSS) 8.2","Mounted on the robot end-effectors are 3 -printed water jet nozzles, which encapsulate phased array ultrasonic probes",The water jets are used to transmit the ultrasonic waves to the specimen under inspection,The specimen under test in this work is a curved composite sample with 1.6 m  surface inspection area.  b is a screen shot of the sample model in the virtual robot environment at the end of the calibration. 3.3 Path planning module The easiest way to generate a tool-path following the contour of a meshed CAD surface would consist of approximating the mesh with a polynomial analytical surface,"However, during initial development, this approach revealed its limitations",The approximation introduces an error by definition,"The error can obviously be decreased by increasing the order of the polynomial fitting function, but at the expense of increased computation time","More importantly, the approximation of a meshed surface with a polynomial surface is only possible when the surface can be mathematically described by a surjective function","A surjective function,  
                         with  –  domain and codomain in  , is surjective (or a surjection) if every element   in Z has a corresponding  –  couple such that  
                         ","The function   may map more than one couple of  –  to the same element of  , but not the opposite",The inverse of a surjective function is not surjective,"As a result, the approximation of a meshed surface fails if the surface is not surjective and it is influenced by the orientation of the surface in the 3D Cartesian space",Therefore a new path-planning approach was developed,"Since the CAD files are imported as meshed objects, the basic idea is to compute the tool-path directly on the triangular mesh without need for an approximating analytical surface","For the sake of presenting the algorithm, let us consider the circumstance where we need to create a curve parallel to one edge of a given surface edge, maintaining the distance   from the edge and laying down on the same surface","Since the surface is constituted by a triangular mesh, the surface edge is formed by segments, whose extremities coincide with two corners of the adjacent triangles","For each segment of the surface edge it is possible to find the intersection points between the plane perpendicular to the segment for its middle point and the edges of the triangles of the surface mesh.  
                         a shows the intersection points relative to the first segment of the surface edge","Starting from the point on the surface edge and following the succession of the intersection points, the curvilinear distance from the reference edge is calculated",The resulting distance is the proper distance along the surface contour,The integration of the distance stops when the distance exceeds the set distance  ,The farther intersection points are ignored,"Since the last remaining point is further than the set distance and the second-last point is closer, a point that is standing exactly at the set distance is calculated through interpolation between the two points",This point lies on one of the mesh triangles,The process is repeated for all the segments in the reference edge ( b),"The final net result is a curve, parallel to the selected reference edge ( c)","Since all of the points of the curve lay on the mesh triangles, the accuracy with which the curve follows the contour of the surface is equal to the maximum deviation between the mesh and the sample surface","Since every point of the obtained curves lay on one (and only one) of the triangles of the mesh, the perpendicular direction associated to each point is given by the vector normal to the relative triangle","For the creation of a raster scan tool-path, the algorithm described so far is iterated to generate other parallel lines, equally spaced, covering the entirety of the meshed surface","The row of crossed triangles shown in  
                         a divides the mesh in two regions: the region that has already been swept, between the reference edge and the generated curve, and the remaining part of the mesh",The former region is identified ( b) and excluded from the domain of interest for the iteration of the algorithm,"The generated curve is then replaced to the reference surface edge and a new parallel curve is computed.  c shows the result of the iteration of the algorithm to achieve 100% coverage of the test surface, where three irregular shaped holes were introduced to test the algorithms under more challenging circumstances","It is important to optimise the NDT coverage around surface voids and obstacles, and rule out any risk of collisions",The software is able to recognise holes and obstacles in the surface of interest,The footprint of the ultrasonic probe active area and its casing is definable in the GUI,"The user can specify the footprints during the creation of a new inspection tool-path ( 
                         )","If the desired tool-path type is a raster, the individual generated curves have to be linked to generate a single scanning raster path","The end of each line is linked with the first point of the next line, inserting a connecting path","During this step, the software adds the kinematic features to the tool-path",Acceleration and deceleration ramps characterize the robot end-effector speed pattern at the start and at the end point of each continuous portion of the tool-path,"If  
                         is the duration of the acceleration and deceleration ramps in a normalised time scale ( ) and   is the normalised speed as a function of time, the following conditions are applied to obtain a continuous speed pattern: 
                      The typical speed pattern is given in  
                         a","It is described by the function: 
                      When the tool-path continuous portion is not long enough to allow reaching of the regime speed, e.g. for short distances between two consecutive parallel curves, the following conditions replace the former ones: 
                      where   is a percentage of the target speed used for the raster scan",This parameter spans between 0 and 1 according to the length of the trajectory linking two consecutive lines of the scan path,"Small values of   are used for short trajectories, to let the robot quietly abandon the end point of the finished line and reach the starting point of the next line","The speed function results: 
                      The typical pattern of the speed is given in  b. 3.4 Evaluation and output 
                         
                          shows the inspection tool-paths generated through RoboNDT for the experimental tests described in  ",The tool-paths and the approaching and retracting trajectories are displayed relative to the virtual model of the robot arm,"For the sake of testing the software with surfaces curving in different directions, the main skin of the winglet and the top surface of one of its back wall beams ( b) were considered for path-planning",The main skin surface has an area of 1.6 m ; the beam surface has an area of 0.5 m ,The generated tool-paths are raster scans with a 29.4 mm raster step,"This step is suitable for phased-array ultrasonic inspection (PAUT) when 64 elements, 0.6 mm pitch phased-array probe is employed and its elements are fired with focal law that uses a sub-aperture of 14 elements",The output function of the software translates the generated tool-path into a set of command coordinates packets that can be interpreted by the robot controllers,"Each robot pose is represented by a vector,  
                         , containing the three Cartesian coordinates of a given position and the roll ( ), pitch ( ) and yaw ( ) angles of the end-effector orientation for that position","The conversion of the normal vector components ( 
                         
                         
                         
                         
                         ) into the angular coordinates is based on the following rotational matrix: 
                      The normal vector components populate the third column of the matrix","The second column contains the tangential vector, representing the direction of travel calculated as 
                      where  
                         ,  
                          and  
                          are the gradients of the trajectory in the three dimensions","The first column contains the bi-normal vector: 
                      Thus the angular coordinates ( ,  ,  ) are calculated through the following general formulation: 
                         
                         
                      The software generates two output text files: the first contains all command coordinates the robot needs to receive to inspect the target surface, and a second short log file containing the points to set the initial and final motion to approach the starting point of the inspection and to abandon the endpoint","These two files have very simple syntax; each line merely contains 6 coordinates ( ,  ,  ,  ,  ,  ) to drive the robotic arm to a specific pose","The two text files can be used by the aforementioned C++ server application and the packets of coordinates are sent one by one to the robot controller via Ethernet communication. 4 Validation experiments – path accuracy and NDT results Tests were carried out to validate the accuracy of the tool-paths generated through RoboNDT and prove the reliability of the new approach, based on external control of the robot motion and simultaneous collection of feedback coordinates and NDT data carried out by the C++ server application","Since one single application manages the command and the feedback packets of coordinates, the new approach allows monitoring of the dynamic accuracy of the tool-paths",This type of investigation is not possible with the traditional approach,Two sets of tool-paths were created to execute the NDT inspection at 100 mm/s and 300 mm/s path speed and maintaining the same robot acceleration equal to 500 mm/s ,The positional error is calculated as the distance between the commanded tool centre points (TCPs) and the reached points as measured by the robot encoders,"The orientation error is calculated as the mismatch angle between the commanded rotation matrix and the rotation matrix computed from the feedback roll, pitch and yaw angles. 
                      
                       shows the maps of position and orientation errors for all tool-paths generated through RoboNDT","For the sake of helping the comparison, the same colour scale has been maintained where possible. 
                      
                       reports the maximum and Root Mean Square (RMQ) errors",The maximum position error is equal to 2.70 mm and the maximum orientation error is equal to 0.29°,"As it was expected, faster speeds produce bigger errors because of the inertial effects affecting the robotic motion","The variability of the standoff between the probe and the surfaces is shown in  
                       with Time-Of-Flight (TOF) maps of the ultrasonic wave reflected from the scanned surface to the probe",The TOF values have been divided by the speed of sound in the sample to quantify the standoff variability in millimetres,The standoff relative to the RoboNDT tool-path is compared to that relative to the tool-path generated through leading aerospace commercial path-planning software based on the Dassault Delmia V5 platform,The comparison is made for the same travelling speed of 300 mm/s and acceleration of 500 mm/s ,The phased array probe focal law and the setting of the ultrasonic receiver (a Micropulse 5 PA from PeakNDT) were set to acquire C-scans with resolution of 1.2 mm in all directions,The tool-paths accuracy is evaluated through comparing the feedback coordinates received from the robot encoders and the command coordinates,The variability of the standoff is within 10 mm for the tool-paths created with the commercial software and within 4.5 mm for the RoboNDT tool-paths,"The experimental data demonstrates that the path errors, achievable through externally controlled robots, are lower than the errors given by the traditional approach",The development of RoboNDT has enabled a new viable and innovative approach for robotic NDT; however the software is not yet optimised in terms of computation times,The path-planning tasks executed by RoboNDT take around 5 times longer than the time taken by the commercial software,"An Intel® Xeon® CPU computer with 24 Gb of RAM, running the 64- bit Windows 7 operating system, was used to test both approaches. 
                      
                      
                      
                      
                   4.1 NDT results Using the RoboNDT tool-paths, the raster scan of the main skin and of the beam surface took respectively 205 s and 38 s respectively","For the commercial software generated tool-paths, the raster scan was 200 s and 35 s respectively","Previous manual scans of the same surfaces were respectively completed in 2 and 0.4 h; this results in robotic inspection being around 40 times faster than manual inspection (in addition to being much more reliable, repeatable and accurate)","It is clear from  
                          that the path accuracy of RoboNDT derived tool-paths exceeds those obtained from the commercial software","For the current application, the level of accuracy for both approaches is sufficient as the intended NDT delivery is accomplished using a water jet coupling approach  ",The water path from water nozzle to sample surface can easily accommodate such tool-path inaccuracies,The bottom row of   shows the close-up of an array of artificial squared delaminations embedded within the thickness of the winglet main skin,The smallest delaminations have a size of 3 mm and are visible in both cases,For other NDT inspection applications the improvements in path accuracy are more significant,"For example if implementing eddy current inspections, then a tight control of standoff distance is required throughout the path to avoid false defect indications. 5 Conclusions In modern aerospace manufacturing, the increasing role of composite materials is introducing new challenges to the inspection and verification procedures employed to ensure safe deployment of the components in the finished structure",Traditional NDT methods such as ultrasonic testing are fundamental for such inspection,"However, the complex part shapes employed in aerospace structures, combined with complex material properties of composites, present significant challenges",Traditional manually delivered NDT is time consuming and manufacturers are increasingly demanding decreased cycle times for the inspections undertaken,"Although some part geometries lend themselves to bespoke Cartesian or Cartesian plus rotation stage mechanical scanners, there are many instances of complex geometry that make the use of 6 axis robot positioners highly attractive","Most existing commercial off-line programming approaches are geared towards manufacturing processes, and lack the required flexibility for application to delivery of NDT measurements","In particular the lack of full point by point synchronisation, between multiple robots and the external measurement system, has been identified as one of the key shortcomings in existing software","Future flexibility to accommodate part variability through conditional programming approaches, and ability to build additional path modification due to effects such as water jet orientation are also key attributes of the new software tool developed","The software, named RoboNDT, has been tailored to the generation of raster scan paths for the inspection of curved surfaces by 6-axis industrial robots, and in its current form represents the first iteration of a system designed to overcome the issues with current OLP packages",RoboNDT is intended to be flexible and extendable to accommodate future system and robot developments,"It has been explained how the execution of the calculated path by a robotic arm, externally controlled through a C++ server application, can be beneficial for NDT inspections","The developed NDT robot toolbox will ultimately assist NDT technicians to move from a component CAD file to the actual physical inspection, without the need to use multiple pieces of software not optimised for robotic NDT inspections",The commercial driver for this work is the need to decrease NDT inspection times – this has been identified clearly as a bottleneck in existing composite parts manufacture in aerospace industries,"The software contains specific functions tailored to generate a tool-path able to follow the contours of curved surfaces, according to a raster scan","The features of this type of path (surface reference edge, offsets, raster step, speed, acceleration, etc.) are fully customizable by the user",Tool paths were generated directly on the triangular mesh imported from the CAD models of the inspected components without need for an approximating analytical surface,Comparative metrology experiments were undertaken to evaluate the real path accuracy of the toolbox when inspecting a curved 0.5 m  and a 1.6 m  surface using a KUKA KR16 L6-2 robot,The results have shown that the deviation of the distance between the commanded TCPs and the feedback positions is within 2.7 mm,The variance of the standoff between the probe and the scanned surfaces was smaller than the variance obtainable via commercial path-planning software,"In the future, more versatile versions of the software with additional features could be realised","The ultimate goal of the authors remains the simultaneous management of command coordinates, robot positional feedback and NDT data by an integrated server application running on a single dedicated PC","This paves the way to introducing intelligent novelty factors to the robotic NDT inspections; on-line monitoring and data visualisation, real-time path correction and versatile path amending approaches are just some of the possible opportunities",The current version of the software is available for download for research purposes from  .,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s1071581917300988, 1 Introduction Humans are equipped with multiple senses to perceive and interact with their environment,"However, in HCI, vision and hearing have been the dominant senses, and our sense of touch, taste, and smell have often been described as secondary, as the lower senses ( )","HCI researchers and practitioners are however increasingly fascinated by the opportunities that touch, smell, and taste can offer to enrich HCI","Recent examples of such experiences include the novel olfactory display by  , taste-based gaming by  , olfactory in-car interaction by  , digital flavour experiences by  , and the added value of haptic feedback for audio-visual content by  ","In particular, there has been a growing interest in uncovering the specificities of haptic experience design ( ) and the unique features of haptic stimulation that would allow the creation of emotionally engaging and meaningful experiences ( )","With the advent of novel touchless technologies that enable the creation of tactile stimuli without physical contact (e.g., ( ), a novel design space for tactile experiences has been opening up ( )","Most notably, it has been demonstrated that mid-air haptic stimulation can be used to convey emotions to the user ( )",This research has motivated further investigations of the design possibilities for creating novel mid-air haptics experiences ( ),"Here we extend the use of mid-air haptics stimulation in the context of a museum, moving beyond a controlled laboratory environment to investigate the effect of multisensory stimulation on users’ experience of art","Museums and art galleries have always been in the forefront of integrating and stimulating multiple human senses, not only to explore new ways of representing arts, but also to increase the wider public interest in the artifacts being displayed.   showed that the use of touch specimens, sounds, and smells to complement the object along with interactive components (e.g., role playing induction device) and dynamic displays can have a strong influence on visitors’ experiences, especially creating a strong sense of flow – being fully immersed and focused in a task ( )","Another intriguing work that relates to multisensory museum experiences is the Jorvik Viking Centre ( ), where multisensory stimuli were used to enrich the experience of a tour concerning the Viking past of the city of York","This experience allowed visitors to touch historical objects (Viking Age artefacts), taste the unsalted, dried cod of the Viking diet, smell the aroma of the corresponding displayed objects, see the animals and inhabitants of the Viking city, and listen to the Viking sagas","More focused on the sense of touch,   presented how visitors could see and feel virtual 3D artworks (e.g., statues) using a haptic device that was connected to the user's right index finger to provide haptic feedback",This use of technology enabled users to touch and feel the contours and stiffness of the artwork,"Despite the increasing interest in the different senses as interaction modalities in HCI and related disciplines and professions (e.g., art curators, sensory designers), there is only a limited understanding of how to systematically design multisensory art experiences that are emotionally stimulating","Moreover, there also seems to be a lack of understanding on how to integrate different sensory stimuli in a meaningful way to enrich user experiences with technology ( ), including art pieces.   replicated the work of   and pointed out the mismatches in the amount of time and space people spent in viewing artworks in a laboratory versus a museum context","Specifically, museum visitors had longer viewing time than was mostly realized in lab contexts, as well as longer viewing time when attending in groups of people","Additionally, this work uncovered a positive correlation between size of artwork and the viewing distance",These findings emphasize the fact that there is a need to carry out museum related investigations in the actual environment of a museum,"Only through an in-situ approach, the intended users who have an intuitive interest and knowledge about art environments, are reached and can provide valuable feedback on the multisensory design and integration efforts","Building on these prior works, in this paper, we present research and design efforts carried out as part of a six-week multisensory art display – Tate Sensorium – in an actual museum environment (i.e., Tate Britain art gallery)","For the first time, mid-air haptic technology was used in a museum context to enhance the experience of a painting (i.e., the   by John Latham) through its integration with sound",The multisensory integration of touch and sound aimed to aid the communication of emotions and meaning hidden in the painting:   (see  b),"In collaboration with a creative team of art curators and sensory designers, the specific experience for the   painting was created","A total of three variations of the experience were created, keeping the sound the same but changing the mid-air haptic pattern to investigate the effect of the sense of touch on the visitors’ art experience (see illustrated in   and described in  )","We hypothesized that museum visitors would enjoy more experience involving the pattern specifically designed for Tate Sensorium (Tate pattern, the most sophisticated and purposeful designed experience), followed by the experience involving the Circle pattern (congruent with the visual appearance of the painting) and finally the Line pattern (incongruent with the visual appearance of the painting)",Visitors’ experiences were assessed through a short questionnaire at the end of the Tate Sensorium experience and through interviews to deepen our understanding on the subjective differences of sensory enhanced art experiences,"In the following sections, we first provide a review of related work on multisensory research and design in museums, followed by a general overview on the multisensory art display – Tate Sensorium in the Tate Britain art gallery",We include the description of the exhibited art pieces and sensory design space,We then focus on the work around the   painting and the design and development of the mid-air haptic patterns as part of the specific touch-sound integration,We provide a detailed description of the data collection process and the insights from the analysis of 2500 questionnaires and 50 interviews,"We conclude with a discussion of our findings with respect to the lessons learnt, limitations and future opportunities for designing multisensory experiences outside the boundary of a laboratory environment. 2 Related work Museums are public places that contain a collection of artifacts that hold values in artistic, historical, and cultural contexts ( )","Importantly, museums offer “a multi-layered journey that is proprioceptive, sensory, intellectual, aesthetic and social” ( )","Given the experiential aspect of museums, they (and exhibitors) have always been looking for new ways to diversify and enrich the experiences that they deliver to the visitors","Therefore, there have been examples and efforts of enhancing art objects through sensory stimuli to engage visitors and convey meaning. 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   2.1 Multisensory interaction in the museum Museums are a forerunner in harnessing new ways of interacting with public users","Therefore, they are recognized within the field of HCI as relevant places for designing interactive systems to reach out to the public","An example is   ( ), an exhibition that explored the transcend between physical and conceptual boundaries (e.g., elements from one work can fluidly interact with and influence elements of the other works exhibited in the same space) via visual, auditory, and tactile interactions","In addition, there are various cases in which the integration of multiple senses has been explored in museums","For example,   explored the “Universal Scent Blackbox”, an artwork composed of boxes emitting five smells: grass, baby powder, whiskey tobacco, dark chocolate, and leather",Visitors to the installation could trigger an odour emission in another area for other visitors and vice-versa,This olfactory interaction attracted much interest from the visitors and became an inspirational probe for exploring olfactory interfaces for communication,"Based on those prior explorations, it has been suggested that multisensory design in a museum may enhance the richness, and even the memorability, of the visitor's experience ( ), due to the emphasis on the multisensory nature of our everyday life experiences",Work by   has shown that auditory and visual modalities mutually influence each other during motion processing of external events so that the brain obtains the best estimates of such events ( ),"Within HCI, we can additionally observe various efforts of integrating interactive technologies (e.g., touch screens, multi-touch tabletop, see ( ) into a museum context to make artworks more accessible and enjoyable","In particular,   used a multi-touch tabletop for multimedia interaction in museums, allowing visitors to access artworks’ details and to assign tags to artworks","Among the implementations of multisensory integration in museums, the integration of touch, together with vision and hearing, are the most frequent senses to be stimulated","For example, the Victoria and Albert Museum in London ( ) provided visitors “touch objects” (e.g., a wise owl supervising the Sculpture Galleries and carved examples of different woods types) to experience the displayed artifacts",Visitors were also able to press a button next to an object to hear related audio descriptions,"Another example is   who presented a sandbox used in an archaeology workshop to recreate an archaeological scene for the attending children to enjoy “playing the archaeologist”.   designed three interactive prototypes of prayer-nuts in an effort to convey and contextualize the historical, sensory, and its embodied information","These 3D printed tangible prototypes offered visitors sensory interactions of smell, touch, and sound with visual and audio feedback, which was relevant to the historical, social, and cultural context of the artifact.   created a virtual environment where visitors could see virtual 3D artworks (e.g., statues) and experienced an associated haptic feedback",A two-contact-point haptic device was linked to the right index finger of each visitor enabling them to touch and feel the contours and stiffness of the artworks through haptic feedback,"However, the authors also pointed out that asking visitors to wear an exoskeleton, to enable the haptic feedback, is contradictory to the idea of free exploration in a museum","Thus, any devices designed for museum visitors should be as little invasive as possible","From the artistic side, new technologies have been used as innovative means for creating art pieces","For example,   created an interface for drawing using a stylus that provided different haptic feedbacks depending on the colours used to paint (e.g., participants experienced dark colours as heavy in weight and light colours as light in weight)","In this work, the attachment of vibrotactile feedbacks to different colours created a novel experience for the creators of those digital/ media artworks","However, the authors did not investigate further the visitor's user experience once presented with these artworks",Another work explored the creation process of art integrating vision and touch ( ),The authors ran one-on-one guided design sessions where visual artists created tactile design prototypes augmenting an existing work in their portfolio as a visual context,They analysed the creation following two rationales: (1) the tactile construct (a set of attributes that define its physical characteristics) and (2) the tactile intent (the variety of meaning assigned to a tactile feature),"This analysis provides insights on how to design creativity tools for artists, but does not further investigate the museum visitors’ experience",The above examples show the interest and growing attention from various stakeholders in exploiting the human senses in the experience of artwork,"In particular, the proliferation of haptic technologies creates a new space for experimentations for both researchers and artists alike",All prior work around the sense of touch is however so far limited to actual physical contact between visitors and the artifacts,"Consequently, it does not yet exploit the use of novel contactless technology","This consequently raises the question of what user experiences around art can be created through the use and integration of mid-air haptic feedback in a museum context, in particular given recent evidence suggesting that mid-air haptic feedback can convey emotions ( ). 2.2 Haptics as an aid in communicating emotions Recent developments of novel haptic technology, such as focused ultrasound ( ), air vortex ( ), and PinPad ( ), aim to create new forms of tactile experiences","These works highlight the design opportunity of creating tactile sensations in mid-air, without requiring the user to physically touch an object, a surface or wear an attachment such as a glove or exoskeleton","Such experiences are of great interest when it comes to augmenting the experience of artworks, which are often fragile and would decay through multiple exposure to human touch","Yet, these new haptic technologies are intriguing to engage people with art emotionally, and to inspire artistic explorations and create memorable experiences",Here we focus on communicating and mediating emotions through touch as a research area that allows the design of new emotion-related interactions ( ),This is demonstrated in a recent work of   on the integration of touch during phone conversations in order to enhance emotional expressiveness in long-distance relationships,"Moreover, there is a growing number of wearable systems that allow different types of social touch and an increasing number of studies demonstrating the rich expressiveness of tactile sensations derived from novel haptic systems ( )","Previous work has showed that participants used weak touches for positive emotions, and hard, fast, and continuous touches for negative emotions ( )","Others identified different types of touch for each emotion (e.g., stroking for love, squeezing for fear), but also reported participants’ difficulty in differentiating the intensity of the expressions when applied through a wearable system on the forearm ( )","Altogether, these results promote the potential for communicating affective information through touch","Most recently, this potential has been established for mid-air haptic technology using a haptic device that uses focused ultrasound to create one or multiple focal points on the human hand",A focal point is created using a fixed pressure (physical intensity) in mid-air using 40 kHz ultrasound waves and by applying the correct phase delays to an array of ultrasound transducers ( ),"This focal point of pressure can then be felt when modulating the ultrasound waves within the frequency range of the mechanoreceptors of the human hand (i.e., Meissner corpuscle and Pacinian corpuscle ( )","Using this mid-air haptic device,   created haptic emotional descriptions and identified a specific set of parameters (combining spatial, directional, and haptic characteristics) with respect to the two-dimensional emotion framework of valence and arousal","Based on this, the authors concluded that it is possible to communicate emotions through mid-air tactile stimulation in a non-arbitrary manner from one user to another","This work was a major inspiration for the team of practitioners, curators, and researchers working on the Tate Sensorium. 3 Tate Sensorium Tate Sensorium was a six-weeks multisensory exhibition in Tate Britain, an internationally recognized art gallery in London, UK","In this section, we provide a general overview and background on the project, the overall ambition, and the specific aims for the multisensory augmentation of artwork through the use of mid-air haptic technology","Tate Sensorium was the winning project of the 2015 Tate Britain IK Prize award that is specifically designed by Tate to support innovative installations using cutting-edge technologies that enable the public to discover, explore, and enjoy art in new ways","The ambition of Tate Sensorium was to enable museum visitors to experience art through all senses (vision, sound, touch, smell, and taste)","This was achieved through the joint efforts of a cross-disciplinary team of collaborators from the art gallery, creative industries, sensory designers, and researchers (see details in the Acknowledgments)","Flying Object ( ), a creative studio based in London, led the project and coordinated the activities across the various stakeholders",Below we will first describe the setup of Tate Sensorium in the Tate Britain gallery (for an overview),"We then provide the details on the artwork selection process and the design of the sensory stimuli for the finally selected art pieces (i.e., four paintings, see  ), their integration and deployment in the museum, so that visitors were able to experience the different art pieces in a novel way","We will describe in even more detail the design of the haptic feedback using mid-air haptic technology and the scientific approach to collect user feedback (both led by the research team at the University of Sussex). 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   3.1 Overview on the setup in the museum A large dedicated room inside the Tate Britain art gallery was used for Tate Sensorium.  
                          shows the layout of the room divided into four areas specifying the final set up for the four selected paintings including details on the painting locations, lighting, senses used, etc","Each painting had a dedicated space and was hung on a wall in each section of the room (marked 2, 3a, 3b, 4)",Visitors first entered the room and were welcomed just inside the entrance (in front of the point marked 1 in  ),"At that point, visitors put on headphones and listened to a welcome message, which briefly introduced the event and gave some general instructions",Visitors entered in a group of four at a time and viewed one painting at a time during the tour,"After viewing the first painting, the group of four people split when reaching the second painting, so that two people continued with the second painting and the other two went to the third painting","These groups swapped afterwards, before moving forward all together to the fourth painting","The split was necessary due to the setup of the mid-air haptic technology for the second painting, which could only be used by two people at a time. 3.2 Artwork selection and sensory design The selection of the artworks was a collaborative process between gallery professionals and external experts from different fields (at Flying Object, University of Sussex, and other independent sensory experts)","At first, not only paintings but also sculptures were part of the pool of potential artworks",The list of potential artworks was compiled by Flying Object and included suggestions from the team at Tate Britain as well,This resulted in an initial pool of potential artworks consisting of 60 paintings,"The selection criteria for the paintings focused on non-representational (or abstract) paintings, as it was agreed that they would leave more room for viewer interpretation","In other words, without any clear visual identity of objects within the painting, the non-visual stimuli would potentially have a stronger impact on how the artwork would be perceived","Additionally, the not-so-clear visual identity would give room for other sensory stimuli to guide the interpretation of the experience, given that sensory information can prime specific notions in users ( )",The availability of the artwork for the exhibition and the preparation phase (∼2 months) was also a key criterion considered in the selection process,"The final decision as to what artworks to select was made by the creative project team led by Flying Object, with sign-off by Tate Britain's management, in June 2015","Tate Britain's staff provided advice on the selection of artworks, based on their availability and suitability for inclusion (in terms of conservation, safety, and other artistic considerations)","Further guidance on developing content (selecting appropriate interpretive/contextual information relating to each work) for the display, eventually translated into “sensory form” (e.g. audio material), was provided by Tate","Four paintings were selected based on their potential for interpretation through different senses, as well as their availability at the museum for the duration of the display in August and September","The four selected paintings were:
 
                      
                         
                          shows the illustration shots of a participant experiencing the four selected paintings","Original copies of the paintings can be accessed via the Tate Britain website. 
                          The details of each painting are in the next section alongside the description of the sensory stimuli","The suitability of the sensory stimuli was decided by considering the literature on multisensory perception and experiences (by the university research team), suggestions from sensory professionals, and based on an iterative creative process","To do this, an on-site visit to the art gallery by the whole team was arranged","During the visit, the team experimented with the different senses in front of the artwork (e.g., using scented paper strips), as well as experiencing the mid-air haptic technology at the University with the project team","The methodology for designing the sensory stimuli was as follows: (1) The team (of all people in the project) generated ideas for each of the four paintings selected, as well as a fifth reserved painting, prototyping them where possible (i.e. selecting actual scents or food ingredients, creating audio samples). (2) The team assigned a leading sense to each painting, along with a secondary sense (in the case of the painting   by Francis Bacon, a tertiary sense to accompany the taste). (3) The designers of each of those senses formed, with Flying Object, sub-teams to collaborate on the experience for each painting. (4) Through iterative discussions with experts and professionals between the teams, these sensory ideas were refined","Below, we present a detailed description of the “ ”, which was selected for the present study, where we utilized mid-air haptics to design the experience of such a painting. 3.3 Sensory design for the “Full Stop” painting Here we provide details on the specific design for the second painting (  by John Latham), which was augmented through the integration of sound with mid-air haptic stimuli using the mid-air haptic device described by ( ) and developed by  . 
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                      3.3.1 Background about the painting The   painting by John Latham is an acrylic paint on canvas from 1961, with the size 3015 × 2580 × 40 mm","It was presented in the room marked 3a in   and can be described thus: 
                            ”. ( 
                         3.3.2 Sensory augmentation Participants experienced this painting through the integration of sound and touch features","The sound was presented via headphones supplied by Polar Audio (manufactured by Beyer Dynamic) and which were worn by participants while in the room (see  
                            )","The sound was created by a sound expert accentuating the interplay between the positive and negative space in the artwork, especially emphasizing the painting's duality of black and white","The audio was also designed to create a sense of scale, of roundness and reference to Latham's use of spray paint, which was resembled in the mid-air haptic feedback","Participants stood in front of a plinth box and put one hand, with the palm facing down, inside the top part of the plinth to have the haptic feedback delivered to their palm (see  
                            )","The haptic device was placed inside the plinth, with the specifications shown in  
                            ",A speaker gauze was placed 50 mm above the device to prevent participants touching the device,The haptic feedback was presented through the gauze when participants put their hand on top of it ( ),"The height of the plinth was calculated so that it fitted comfortably with adults, children, and disabled visitors in wheelchairs. 3.3.3 Mid-air haptic pattern design Synchronization between the sound and the mid-air haptic sensation was handled by self-developed software that could read Musical Instrument Digital Interface (MIDI) inputs (using RtMidi 2.1)","Thus, the mid-air haptic patterns could be synchronized automatically with the sounds created by the sound designer","In other words, the sound designer could control the mid-air haptic patterns (frequency, intensity, and movement paths) to create a desired experience for the   painting","The final version of the sound file also synchronized with the desired mid-air haptic feedback sensation (as depicted in  
                            , left)",This sensation had the “ ” feature to enhance the visitor's experience of the painting,"Specifically, it was created by a round-shape haptic sensation synchronized with the sound","The circle shape was composed of 16 points of varying size (having an increase/decrease in diameter), and was integrated with the rain pattern created by using one point at random positions on the whole hand","Importantly, we further investigated the impact of the mid-air haptic stimulation on visitor's experiences","To do so, we created a set of seven alternative haptic experiences using three sources of inspiration: (1) the painting itself, trying to emphasize its visual properties (rounded), (2) contradicting the visual appearance of the painting (not rounded) and (3) emotional haptic stimuli based on the findings from  ","These seven patterns were:
 
                         Eight participants volunteered to evaluate these seven patterns alongside the main haptic pattern","Participants experienced each haptic pattern in a counterbalanced order, and then rated both the valence and arousal of each pattern on a Likert scale (1–9)",Participants were also encouraged to describe what they felt and how meaningful they perceived the sensory integration for the   painting (which was represented by an A3 poster on the wall),"The results showed that “Circle” (pattern #4) and “Line” (pattern #3) patterns were the most distinctive ones for the   painting in terms of valance and arousal, accordingly","In specific, the Circle pattern had the highest valence ratings (6.43 ± 2.15) among all the patterns (averaged 5.02 ± 0.65) and an arousal average rating of 4.14 (±2.48)",The Line pattern had the highest arousal rating (5.86 ± 2.48) among all the patterns (averaged 5.11 ± 0.59) and a valence average rating of 5.71 (±2.48),"Notably, the Line pattern has a contradicting shape with the painting (showing a circle shape)","Therefore, it was expected to have lower ratings in valence and liking as well during the science days","The two patterns chosen are described below:
 
                         The three patterns (named Tate, Circle, and Line) were alternated during the Science days before closing the exhibition (see  
                            )","In contrast, on the other days of the exhibition, only the Tate pattern was shown. 4 Procedure and method In this section, we provide a detailed description of how the Tate Sensorium visitors experienced the multisensory installation and our method for capturing their experiences through questionnaires and interviews","Additionally, we explain the difference between Standard days and Science days (as depicted in  )","Overall, the exhibition opened to the public for 1 month and 8 days","As mentioned before, the purpose of Science days was to investigate the impact of different parameters of mid-air haptic stimulation on visitors’ experience","The three patterns were alternated at different times on each Science day (on the other days of the exhibition, only the Tate pattern was shown)","Additionally, on Science days, we collected visitors’ perceptions through questionnaires on the relative importance of each sense (vision, auditory, smell, touch, and taste) when experiencing the paintings at Tate Sensorium","On the final day of the display, visitors were also asked to take part in a short audio-recorded interview lasting for 10 minutes (see below). 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   4.1 Step-by-step procedure Participants entered Tate Sensorium in groups of four","This group size was to allow Tate Sensorium visitors a truly immersive multisensory experience, as well as to separate visitors to attend different paintings in a smooth traffic",Another purpose was to mimic a common group visit to a museum,"Moreover, a group of four people was a manageable group per session (15 min) allowing each participant to enjoy the artwork with the multisensory experience","After entering the main door, participants were welcomed and then guided by a member of staff until the end of the tour","First, participants stopped at the point marked 1 in  ","Here they were instructed to put on the headphones to hear a short introduction about Tate Sensorium (see  
                         ), as follows: 
                         
                      
                         
                      
                         
                      Additional audio guidance for each painting was provided, giving some details about the painting itself (by whom it was painted), and the accompanying multisensory stimulation (e.g., walk around the room to explore the different smells)","Participants also received a wristband to capture their skin conductance response, which was used to create a personalized printout at the end of the tour",This data is not included in this paper as it was not the focus of the study led by the University team,"After the short introduction, participants removed their headphones and continued walking to the first painting (  by Richard Hamilton, as marked 2 in  )","Here, they stood in front of the painting and were instructed (through the speakers in the room) to experience it as naturally as possible, and to move around the room to explore the three different scents (see  a)",Three minutes were given to all four participants to experience the painting,"After that, participants were instructed by the staff to separate into two pairs of two participants to continue to the next painting",Pair #1 went to the room marked 3a in   and view the   painting,Participants were asked to put on the headphones provided,"Following the audio guidance, each participant was asked to put their hand into the empty space in the plinth to experience the mid-air haptic feedbacks (see   for an example and   for the plinth specifications)","The mid-air haptic feedback was provided on the participant's palm, and was synchronized with the sound provided through the headphones","After the sound-haptic stimulus finished (1 minute), the second participant took a turn in experiencing the mid-air haptic stimulus for the   painting",Participants were instructed to enjoy viewing the painting while experiencing the sound and touch integration,The total duration given for participants to be in this room was 3 minutes,Pair #2 went to the room marked 3b in   and viewed the   painting,There were two plinths in this room,On top of each plinth are two 3D printed scent objects,Participants were encouraged to experience the painting and the scents by picking up the scented object and smelling it (see  c),Participants were given 3 minutes to explore the painting in association with the sound and smell stimuli in this room,"After, Pair #1 finished experiencing Room 3a, and Pair #2 went through room 3b, they switched roles","Pair #1 now moved on to room 3b and Pair #2 moved to room 3a, following the same procedure as described above for each of the two paintings","Once both pairs completed Room 3a and 3b, all four participants moved to the final room (marked 4 in  )","Here, each participant put on the headphones again",They all stood in front of the   painting with a plinth in between,On top of the plinth was a box with 4 pieces of chocolate,Participants were encouraged to pick up a piece of chocolate and eat it (see  d),"Three minutes were given to participants to experience the painting and its associated taste and sound. 4.2 Methods used: questionnaire and interview Once participants had finished visiting all four rooms, they were requested to move to the exit point","Just before exiting, participants were encouraged to complete a short questionnaire about their experience of Tate Sensorium","The questionnaire consisted of three questions for each painting: (1) visual liking (of the painting itself); (2) multisensory experience liking (the sensory stimuli integrated into the painting); and (3) emotional reaction (arousal) (see  
                          for an illustration)",These questions were used to quantify the added values of the designed sensory augmentation added to the experience of the paintings,Participants answered using 5-point Likert scales (where 5 is the highest rating ( ),"Participants were also asked to respond to some demographic questions (i.e., age, gender), and to report whether they would be interested in visiting such a multisensory experience again in the future (yes/no/maybe)",This information was used in the analysis to explore differences between the experience ratings and users’ personal backgrounds,"Moreover, the curator of Tate Sensorium was interested in the age and gender distribution attracted by the multisensory display and if people would be interested in future events","For the dedicated Science days, participants had an additional question on the importance of each individual sense (see  
                         )",Participants signed a consent form before answering the questionnaires,"On the last day of the display, visitors of Tate Sensorium were also invited to take part in a short audio-recorded interview lasting about 10 minutes","The interviews aimed to explore: (i) the overall experience of the multisensory display, and (ii) gain specific insights on the experience created for the   painting, which integrated mid-air haptic feedback with sound","Here, we were particularly interested in understanding any qualitative differences in the perception of the three haptic patterns (the Tate Sensorium, Circle, and Line patterns as illustrated in  ), which were alternated between groups of participants","An interview guide was defined based on those two main areas of interest and included the following eight questions for each interview session:
 
                      In each interview session, between two and four users participated at a time","Each participant was encouraged to express her/his opinion one after another, as well as to react to each other's responses to allow some discussion and reflection on the multisensory experiences",This could help to obtain further insight about the visitor experiences in their own words,"Participants signed a consent form before taking part in the study, which was approved by the University of Sussex Science and Technology ethics committee. 5 Results In total, we collected data from 2500 participants (1700 females, 800 males, mean age 36.00 SD 16.11)","We analysed participants’ visual liking, multisensory experience liking, and emotional reaction (arousal) ratings using a mixed effect design, ANOVA, where painting was considered a within-participants factor, and gender were considered between-participant factors",We used age to investigate how different age groups perceived the sensory augmentation of the paintings and to calculate correlations with the participant's ratings,We added ‘haptic patterns’ as between factor in the analysis in order to investigate any differences across the three haptic patterns used in relation to the participant's ratings,Full interactions were considered in each ANOVA model we used,"Overall, ANOVA's assumptions were tested on all the combinations of between and within factors","The Saphiro-Wilk test indicated the normal distribution of the data ( > 0.05 in all cases), Mauchly's test of sphericity was used to assess the sphericity of the data (again,  > 0.05 in all cases), and Levene's test the homogeneity of the data ( > 0.05 in all cases)","When ANOVAs showed significance, Bonferroni-corrected pairwise comparisons were performed","Moreover, given the high number of participants, Cohen's d was used on each significant comparison as an index of the effect size","Note that the effect size was not computed at the ANOVA level, given the fact that the power analysis of multiple way mixed effect experimental designs can lead to negative values and difficult interpretation, and it is still an active field of research ( )","In addition to the questionnaire data, we collected qualitative data from 50 participants through conducting interviews on the last day of the multisensory display",All the interviews were transcribed and analysed by one researcher (who conducted the interviews) based on the main areas of interest defined above (see  ),"Based on repeated readings of the transcripts and discussions in the group, we clustered the findings into three main themes, which we present in the following sections after the quantitative results gained from the questionnaire. 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   5.1 Effect of the different mid-air haptic patterns With the aim of investigating the add-values of mid-air haptic in a museum context, we were particularly interested in evaluating the effect of mid-air haptic feedback on participants’ experiences","For that purpose, three variations of haptic patterns were created for the   painting and alternated during the dedicated Science days (see   for illustrations of the haptic patterns).
 
                         . 
                          
                          summarizes the numbers of participants that experienced the different mid-air haptic patterns (Tate, Circle, and Line)","Please note that the alternation between patterns was constraint to the dedicated Science days, hence there is a different number of participants experiencing each pattern","The expectation was that participants would like the main pattern purposely designed for Tate most, followed by the Circle pattern, and the Line pattern being the least liked due to its incongruence with the visual appearance of the painting (rounded shape of the Full Stop on a large canvas)","To test this hypothesis (that is: whether the different patterns influenced the ratings of the participants), three multiple way ANOVAs were used to analyse the visual liking, multisensory experience liking, and arousal ratings, having as independent variables the age of the participants, the viewing order of the paintings, and the different haptic patterns into the model","The analysis showed that the different mid-air haptic patterns only had an effect on the reported arousal (  = 4.129,   < 0.01)","No statistically significant interaction was observed ( > 0.05 in all cases).  
                          shows the averaged ratings for each pattern","Pairwise comparisons, using the Bonferroni correction, showed that pattern 1 and pattern 2 (Tate 3.77 ± 1.04 and Circle 3.90 ± 0.96) were found to be more arousing compared to pattern 3 (Line 3.50 ± 1.13, Cohen's d to the closest value = 0.38)","These results are in line with our expectation of the Line pattern being the least appropriate sensation in mid-air as it does not resemble the rounded characteristic of the painting. 5.2 Importance of haptic experience Specific to the Science days (as described above and shown in  ), participants were asked one additional question designed to assess the perceived importance of each sense in each of the multisensory experiences (e.g.,  )","This was inspired by previous work assessing the relative importance, to people, of the five senses in a given experience ( ). 
                         
                          and  
                          show the average participants’ ratings (with standard deviation) of the importance of haptic for the   painting",A repeated measure ANOVA and post-hoc pairwise comparisons with Bonferroni correction were used to assess which senses were considered more important for the painting,We found that ratings of touch as rated significantly more important (  < 0.001) compared to the ratings of scent and taste,This is as expected for this painting as it was designed with the mid-air haptic (the sense of touch),"Multiple way ANOVAs were also conducted to assess any differences in gender, haptic patterns, on the relative importance of the different senses in their experience",No significant effect of any of these factors was found ( > 0.05 in all cases),"That means that participants rated the added experiences of the associated sense similarly, regardless of their gender and haptic patterns. 6 Interview findings As mentioned before, the aim of the interviews was to gain more insights into participants’ overall experience of the multisensory installation, and more specifically to obtain qualitative feedback on their experience for the   painting","Below we summarise the main findings, further illustrated through quotes from participants (n = 50)","We first present the qualitative findings of the overall experience of the multisensory exhibition (  and  ), followed by the findings that focus on the experiences of the   painting, with the mid-air haptic feedback (  and  ). 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   6.1 Overall multi-faceted experiences: immersive vs distracting Participants described their experience of Tate Sensorium as “stimulating”, “interesting”, “mind blowing”, “incredible, I really enjoyed it”, “something new, unusual”",While their feedback was overwhelmingly positive – which also fits the quantitative results – there were also some more critical voices,"These critics were mainly based on different expectations, such as those expressed by some participants as  , and   Some participants literally expected a complete full body immersion in the painting through the stimulation of all senses",One participant was even ready to take off their shoes in expectation to be stimulated on the feet,"All participants strongly acknowledged that stimulating all the senses added another layer, dimension, and perspective to the experience of the paintings and thus opened new ways of thinking and interpreting art, in particular abstract art, which sometimes leaves people wondering how to interpret the work",One participant said:   The majority of participants stated that additional sensory stimuli did not change their initial liking of the artwork,"However, some participants highlighted the potential of multisensory stimuli to turn their attention toward painting.   Full Stop   The interviews brought to the fore the general feeling that sensory augmentation can awaken a museum visitor's imagination, make the visit to the museum or art gallery more engaging, and has the ability to elicit strong reactions, establish a connection to, and build a narrative around the art","The multisensory layers on top of the visual appearance of the paintings was described to allow stronger emotional reactions, such as empathy, being immersed, or even scared in front of the artwork","One participant described it as follows:   For the   painting, the sensory experience was described as very intense due to the integration of mid-air haptics and sound","While one participant stated that   another participant focused on the sensation on the hand   [from the plinth]  
                      In addition, participants highlighted the opportunity and danger of multisensory stimuli","For example, it could either   on the particularities of an artwork or   from the artwork itself","Involving all the senses, when experiencing an artwork for the first time in such a setting could cause distraction, which was, however, not always described as negative distraction","Instead, it was sometimes a welcomed distraction, as the following statements represent:   versus   For the   painting, one participant pointed to the positive emphasis of the haptic stimulus on the hand which made her notice the particularities of the artwork:  … [without the feeling on the hand] 
                      6.2 Balance in sensory design: curated vs. explorative The impact of the sensory stimuli on each individual's experience was not always straightforward and sometimes bipolar in the sense that multisensory augmentation of art can either open up opportunities for interpretation, but can also narrow down the visitor's perspective","On the one hand, participants described the multisensory experience as supportive in understanding art, creating a story, elevating the visual experience through touch, taste, and smell and sound","While on the other hand, the experience was described as too prescriptive, orchestrated, and shepherded","One participant stated:   Another participant made the following statement:   There seemed to emerge, although only from a handful of participants, a feeling of not being in control, and maybe not being able to follow their own exploration of the senses alongside the art, but then again being excited about the novelty of the engagement","This leaves space for other ways of designing future multisensory experiences and creating an interactive setting in a museum serving the varying expectations of visitors: being guided or allowing for surprise. 6.3 New mid-air sensation: feeling without touching Overall, the   painting emerged as the most liked painting, not just from the questionnaire data, but also from the interview responses",The combination of mid-air haptic (a new technology not yet available for the end user market) with sound was perceived as immersive and really opened up a new way of experiencing art,"Participants described the multisensory experiences as follows:  
                      Participants also stressed the uplifting experience of touching without touch, just feeling air and variations of air patterns on the hand:   and the associated uncertainty introduced through the new mid-air haptic technology:   The familiarity with a sensory stimulation and consequently the predictability of the experience was an interesting topic that emerged in the interviews and opens up the question for future investigations of its long-term impact","Moreover, participants expressed the potential of this technology for artists themselves, providing them with a new opportunity to paint, create art, and provide people with new experiences. 6.4 Integration of touch and sound: three experiences As explained above we were able to vary the mid-air haptic feedback for the   painting on dedicated Science Days, including the day we conducted the interviews","Thus, we were able to collect qualitative feedback on the experience for each of the three haptic patterns: Tate, Circle, and Line","First, it is worth noting that the role of the sound in the combination of each of the three haptic patterns was described as very important","While the sound was dominant across all three haptic patterns, there was, however, a notable difference in the description of the experience between the three conditions","For the Line pattern, participants described the sound as very dominant, even more so than in the two other conditions","The Line pattern was perceived as less meaningful, as expected from our setup","The pattern was, moreover, described as distracting, random, and did not live up to the integration of a powerful painting and sound","Participants said:   Full Stop  Whereas others said:  
                      In contrast, participants who experienced the Tate pattern described the experience as much more balanced between touch and sound","One participant said:   Full Stop   The Tate pattern was well integrated with the sound and emphasized the physicality of the painting, thus creating an affordance for touch","The Circle pattern was still meeting the expectations of roundedness inherent in the visual appearance of the painting, but in contrast to the Tate pattern it introduced movement in the form of a clockwise rotation on the palm, though synchronized with the sound","Participants neither particularly liked nor disliked the pattern or the sound, but interestingly shared a lot of stories evoked through the sensation","One participant said:   Another participant become agitated when talking about the sensation:   It almost seemed that due to the slight deviation from a perfect design, participants were looking for explanations and coming up with their own narratives and short stories about the meaning of the experience. 6.5 Summary Overall, all participants reported that they were looking forward to seeing more of this kind of multisensory installation in a museum in the future","Among the five senses stimulated, sound, and taste signals were described as the most intensively experienced","Taste was either described as scary, invasive to put something in your body, or comforting","The latter was however not often mentioned, as the stimulus itself (chocolate soil) was not as pleasant as usual chocolate but mixed amongst others with charcoal, sea salt and cacao as reference to the darkness of the painting ( )","With respect to the three different haptic patterns for the   painting, it became clear that participants wished for more time and another try to fully grasp the experience conveyed with the novel mid-air haptic device","One participant said:   That suggests the need for further explorations into users’ experiences over time. 7 Discussion Tate Sensorium, a multisensory art exhibition, was designed to enable museum visitors to experience art through all their traditional senses: vision, hearing, touch, smell and taste","Overall, Tate Sensorium attracted over 4000 visitors over a six-week period, out of which 2500 gave feedback via questionnaires and a sub-set of 50 participants took part in a short interview, sharing their experience of the multisensory display","Our work presents the design and implementation of Tate Sensorium, with a specific focus on the use and integration of mid-air haptic stimulation as part of the experience of a painting",Below we discuss our findings and lessons learnt from this unique case study in particular from the perspective of exploiting a novel haptic technology beyond a controlled laboratory environment,"We highlight opportunities and limitations for multisensory experience design when creating emotional engaging and stimulating art experiences. 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   7.1 Mid-air haptic design space to enhance art Our results showed that different haptic patterns could selectively influence the reported degree of arousal of users",The original Tate pattern and the Circle pattern elicited significantly more arousal compared to the Line pattern,"The higher arousal of these two patterns might be, as hypothesized, due to the geometric similarity between the   painting and the haptic patterns","In contrast, the Line pattern was described as “distracting” due to the confliction between what was being seen and what was being experienced through touch","This finding is in line with what ( ) previously reported for a lab setting, and extends their results for mid-air haptic stimulation ( )","In addition, while the differences of liking between the three haptic patterns remained non-significant based on the questionnaire, the qualitative data suggests that the participant's subjective experience changed depending on the used pattern",The sound integrated with the haptic pattern became more important when the haptic pattern was not considered as meaningful in relation to the visual appearance of the painting (in the case of the Line pattern),"That might indicate a specific case of sensory dominance of sound over touch (e.g. ( ), but also that minimal changes in the stimuli can change the meaning of the conveyed experience","That was particularly interesting for the Circle pattern, which was rated in the middle of the liking scale (better than the Line pattern, but worse than the Tate pattern)","Presented with the Circle pattern, participants seemed to be most stimulated in their imagination and expression of narratives","It is, however, an interesting question for further research to investigate what kind of paintings that mid-air haptics lends itself to (e.g., busier paintings with more details than the  )","Those insights into the subtle differences of haptic experiences and subjective perception of integrated sensory stimuli (i.e., sound and touch) can provide designers as well as curators and artists with a distinct opportunity to intentionally design for variation from the visual stimulus to create friction that leads to stronger engagement",This can be further facilitated through the development of new design creativity tools for artists by the HCI community ( ),"In addition, visitors of Tate Sensorium were asked about their experience of the multisensory experience of the artwork (with the question “ ”)","Our results show that high liking was elicited in all three mid-air haptic patterns for the   painting, with no significant difference between them","This might be due to the novel experience when visitors first encountered with mid-air haptic, designed for the artwork","Future investigation specifically to regular visitors might reveal the differences in more details between different mid-air patterns. 7.2 Design considerations for a multisensory art By integrating mid-air haptic technology into a real-world environment, which has not been done before, the design team had to decide about the form of multisensory presentation that accounts for the experimental integration of this new technology in a museum context over an extensive period of time","From the visitor's feedback, we know that there was a high level of appreciation and liking for the multisensory experiences designed for the selected paintings","However, some visitors perceived Tate Sensorium as too pre-designed (choreographed) and somehow limiting the space for an individual journey (exploration)","While this is an important point to keep in mind for future explorations, it is worth noting that it was a conscious decision by the project team to guide the museum visitor in a coherent and complete way through their experience of art enhanced through a new technology they have never experienced before (please note that this mid-air device was not available on the consumer market at that time)","Alternative designs can be imagined, where the visitor is not even aware of the multisensory augmentation of an art piece and stays embedded in the natural flow of a museum visit","In conclusion, the insights gained from this research are clearly staged outside a controlled laboratory environment and still embedded in a semi-controlled set up in a dedicated area in the museum","That allowed us to collect relevant first hand experiences from the intended target users, just like suggested by recent work by  , who highlighted the fact that there is a need to carry out museum related investigations in the actual environment of a museum","Based on those design decisions, relevant follow up research and design questions emerge, such as  ; and   These are only some questions that come to mind that require further explorations and are ultimately a balance between the advanced state of a technology, and the ambition and requirements of the involved stakeholders","For Tate Sensorium, the purpose was clearly the augmentation of existing painting experiences via multisensory design","However, the interviews showed that there was an interest for exploration as well as for allowing artists themselves to create sensory experiences for their own artwork","This is in line with recent efforts described by ( ), where visual artists created a tactile design prototype that augmented one of their existing works","A major challenge identified by the authors was the need to provide the artist with tools that allow them to express their imagination without reducing it due the technical limitations. 7.3 Opportunities for HCI research and design Based on the involvement of curators, sensory designers, and creative businesses in this design and research project, it became clear to us that there is an immense need for tools and interfaces to facilitate the work and practices of sensory designers (e.g., sound designer)",This consequently allow the meaningful exploitation of new technologies such as the mid-air haptic device used in this project,Such devices are often not easily accessible for designers or artists due to the requirements of specific programming skills (in our case C++),"Although a collaboration across disciplines and areas, as demonstrated in this project, can overcome those technical challenges, it limits the creative exploration and exploitation of new technologies","Hence, it is great to see current developments around the latest version of the mid-air haptic device, that comes with a graphical user interface that allows designers and artists to freely explore different patterns and parameters (see   for their touch development kit)","On top of this, there is still an enormous opportunity for the design of new interfaces and tools to support the engagement of artists and designers with technologies such as mid-air haptics","As stated by   and emphasized by  , there is a need for these tools to be designed with “ ”","In other words, the designed tools should be easy for novices to begin using them, yet provide ambitious functionalities to scale up for the expert user and their needs, and hence support a wide range of design opportunities","In our research, we aim to push solutions using multilayer interface design, which provide users with different ways of interacting with the tool (e.g., the user interface of the tool is adaptive to the user's skills using it)","Some examples of this are video games, search engines (e.g., Google, Yahoo), and video editing tools (e.g., Adobe Premier) with various workspaces to accommodate the user's expertise","As mentioned before,   analysed the creation of tactile feedback for visual arts and used the gained insights from this collaboration to guide the design of dedicated creativity tools for artists","Accordingly, tactile constructs and tactile intents define the “form” and “meaning” components of each tactile feature, respectively","Their findings indicate associations among the identified categories and between the two components, leading to design implications for expressive tactile interfaces","They also propose a user interface architecture, based on a design space for an expressive tactile augmentation design tool","This idea can be further extended and applied for other senses in the future. 7.4 Design trade-offs and limitations Although this project revealed several insights into immediate reactions and reflections on the multisensory experience (overall very positive), it is certainly a challenge to draw on generalizations about the individual effect of the senses on the overall experience of art and its possible impact on art preference",Conducting research in a typically noisy real-world context that has several stakeholders involved makes it difficult to generalize,"Nonetheless, the different lessons learned here might facilitate large-scale studies involving multiple sensory signals in highly ecological contexts","Moreover, given the nature of Tate Sensorium, there is a limitation in terms of the amount of questions that we could include in the questionnaire, giving us only a snapshot of the users’ experiences","In particular, we would have liked to expand on the questions related to the overall experience of the sound-touch integration for the   painting",This would help to understand better the influence of the augmentation of mid-air haptic on top of the visual appearance of the painting (akin to ( ) who previously investigated the added value of sound),"Based on the interviews, we know, however, that participants usually used the visual characteristics of a painting to explain their experience with the other sensory stimuli","Studying multisensory experiences outside a controlled laboratory environment comes with challenges and although our research took place in the field, it was controlled to a certain extent","Participants were guided through the different sections of the room but were still given freedom to experience the artwork (e.g.,  ) and the associated multisensory design (e.g., mid-air haptic feedback)",Doing this ensures a valid background for comparing different conditions of mid-air haptic stimulation while providing participants the same experience as they normally have in a museum,Our results indicate that the use of technology should not limit visitor's freedom in exploring the space in the exhibition,This was reflected in their qualitative feedbacks and must be considered by designers in their follow-up installations,"Yet, it is limiting a completely free exploration one can have in a museum environment",It is up to the researcher and stakeholder to find the right balance between design and research,"Furthermore, we did not explore the aesthetics and culture in museum as it is beyond our core expertise in HCI","Instead, we focused on exploiting the potential of novel haptic technology to create emotionally engaging and stimulating experiences in particular through its integration with other senses, in our case with sound","Nevertheless, it would be an interesting research topic for future investigation, from the perspective of aesthetic science, to study multisensory art appreciation ( )","Finally, the interviews revealed the need for more time to explore and experience this new type of experience","One of the two couples who visited Tate Sensorium twice said:   This demonstrates huge potential for further exploration of experiences and engagement over time. 8 Conclusions and future work Traditionally, museum attendees tend to experience art mostly through vision","Tate Sensorium allowed us to reflect on the process of enhancing art by considering all our major senses, particularly the sense of touch using novel mid-air haptics",The degree of success of this initiative depends on who one asks,"From the point of view of the art gallery, the results of Tate Sensorium exceeded their initial expectations",The one-month exhibition was extended for two additional weeks given the massive interest from the public,"From the creative team's point of view, it was also a success despite small technical problems with lightning and sound at the beginning","Overall, the whole installation ran smoothly and attracted media interest within the UK and worldwide such as the  ), the  , and The Wall Street Journal ( )","From a research point of view, this project provided a unique opportunity to collect user data on multisensory art experiences and in particular on mid-air haptic experiences from a large user group","However, that opportunity also comes with practical constraints such as negotiating the integration of the data collection in the overall display design and timing, compromising the design of the haptic feedback and limited control over the artwork selection","While the HCI research team contributed to the design and integration of the multisensory stimuli and materials, the final decision was mainly made by the creative team and curator of the art gallery",Balancing the different stakeholders’ requirements and thoughts on the project could be challenging,"However, at the same time, this environment encouraged the team to think beyond their traditional ways and methods of designing experiences and studying them","Museum visitors were not recruited for an experiment, but they came to enjoy art, new ways of experiencing paintings, and to engage their senses in a new exciting way","Therefore, the experience they received needed to be interesting and memorable","Despite compromises (finding the right balance between the various stakeholder requirements) and potential limitations, we believe that our work allows a glimpse of how to create, conduct, and evaluate multisensory experiences in a museum","With projects such as Tate Sensorium, we are convinced that our understanding of multisensory signals in relation to art, experiences, and design, based on novel interactive technologies, can be advanced","In particular, we hope that this case study will inspire other researchers and professionals in the creative industry, to explore new ways of engaging people and exploiting all human senses in the design of new multisensory interactive experiences in the museum.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0888613x14000206, 1 Introduction There are strong historical links between probability theory and mathematical logic,"In the last decades more and more areas of these subjects have been very closely connected in investigations of logical systems called probabilistic logics with a broad range of possible application areas (learning from data  , causal reasoning  , multi-agent systems  , robotics  , logic programming  , etc.)","There are numerous proposals for probabilistic logics  , mainly based on the standard Kolmogorov's (measure theoretical) approach to probabilities which are real-valued measures that are typically required to be  -additive","However, a number of alternatives to the probability measures have been studied (for reference see  )",Many of them differ from the standard probability measures in two respects,"First, the ‘infinitary’ condition of  -additivity is replaced by finite-additivity","Second, ranges of probabilities are extensions of the rational numbers different from the field of reals  ",It turns out that non-Archimedean fields are particularly useful since they contain infinite and infinitesimal numbers,"The two most important examples of non-Archimedean fields are the fields of hyperreal numbers   and the fields of  -adic numbers  , for any prime number  ","While   is a non-Archimedean ordered field, the  -adic field cannot be ordered in an appropriate way, but in some sense, one can say that it contains infinite and infinitesimal elements","Namely, one can define infinitesimal in the following way: if   is a field of characteristic 0 equipped with the norm  , then   infinitesimal if for every positive integer  ,  ","According to this approach, each  -adic number   such that  ,   and   for   is infinitesimal","In addition to these alternatives it is worth to mention an approach (that goes back to de Finetti, Renyi and Popper among others) which considers conditional probability and conditional events as basic notions, not derived from the notion of unconditional probability","Coletti and Scozzafava's book   includes a rich elaboration of different issues of reasoning with coherent conditional probability, i.e. conditional probability in de Finetti's sense",Recently a lot of research has been done by using probabilistic constraints based on the coherence principle and has got remarkable influence in the field of reasoning with uncertainty,We may cite   as some of the relevant references,Khrennikov presents  -adic probability theory and discusses its applications in physics (especially quantum mechanics),"Namely, it is well known that any non-trivial norm on the field of rational numbers   is equivalent to either the usual real absolute value or a  -adic norm  , for some prime   
                      ",By completing the field of rational numbers we obtain the field of real numbers or a field  ,"On the other hand, values of relative frequencies of random experiments are rational numbers","Therefore, to estimate the probability of the corresponding event we can use frequencies in the field of real numbers, but we can also use numbers in  , for some prime number  ","So, we can choose   as the range of the corresponding probability measure","Thus, the  -adic approach to probability theory is an alternative to the standard real-valued probabilities which gives the opportunity to work: 
                   Following the concepts of Khrennikov's approach to probability and techniques that are developed in a range of papers on probabilistic logic  , in   the authors have developed the propositional probability logic   which is an extension of classical propositional logic with modal-like operators  , where the intended meaning of   is that the probability of   is in the ball  ","As the corresponding semantics, probabilistic Kripke-like models are introduced","These models are based on a  -adic probability space  , where   is a nonempty set,   is a subalgebra of subsets of   and   is an additive function normalized by the condition  ","Now, we introduce two probabilistic logics by extending classical propositional logic with probability operators of the form  , with the intended meaning of   that the conditional probability (of truthfulness) of   given   is in the  -adic ball  ",As the corresponding semantics we use models introduced for the logic  ,"One of the essential conditions for  -adic measures is the  : If   is a field of subsets of some set   then, for every  
                       In   this condition is ensured by reducing the range of probabilities to an arbitrarily large (but fixed) ball  , where   is a fixed integer",When handling conditional probabilities there is a need for multiplying  -adic numbers,"Since arbitrary ball   is not closed under multiplication, these balls are no longer useful for logics with conditional probabilities",Here we might proceed in two ways,"First, we can choose the unit ball   as the range of probabilities since it is closed under multiplication","The main part of this paper (Sections  ,  ,  ,  ) concerns the logic  , built on this strategy (  denotes the ring of  -adic integers)","In the second approach, also presented in this paper, we can build formulas over a finite set of propositional letters, and retain   as the range of probabilities","In this case, we compute the supremum of finitely many numbers of the form  ,  , which is again a finite number",The corresponding logic   is presented in Section  ,"Following the strategy of the companion work  , we give axiomatic systems for the logics   and   which are sound and strongly complete with respect to the proposed semantics","Since this is one of the first papers that discuss  -adic probability logics, we would like also to present our motivation to introduce this formalism","There are numerous fields where  -adic numbers and analysis are useful: for example in mathematics in  -adic probability  , in physics  , in biology in modelling of the genom and the genetic code  , and in cognitive processes  , etc","Thus, our aim is to try to address some of the related issues and to illustrate the ways in which the proposed logics can be used to represent uncertain knowledge and to perform the corresponding inferences","Below, in Section  , we analyze several examples",The first one is related to formalization of brain working with information coded by  -adic numbers,"Then, using our logic, we describe an experiment in which relative frequencies oscillate with respect to the real, but converge with respect to the  -adic, metric","Finally, we consider the possibility to express in our framework different properties of default reasoning systems",The rest of the paper is organized as follows,In Section   we present syntax and semantics of  ,Section   presents axioms and inference rules of  ,In Section   we prove the corresponding soundness and completeness theorems,Decidability of   is discussed in Section  ,"In Section   we consider a modification of the logic  , denoted  , which might be interesting for practical purposes","Some possible applications of the presented logics are discussed in Section  , while concluding remarks are given in Section  ","Finally, a short introduction into fields of  -adic numbers is given in  , while more comprehensive overview can be found in  . 2 Syntax and semantics of the logic  
                   Throughout the paper,   is a fixed prime","Let us introduce the following sets: 
                   The language of the logic consists of a countable set   of propositional letters, classical connectives ¬ and ∧, and a list of probabilistic operators  ","We will denote the set of all propositional formulas over   by  , while propositional formulas will be denoted by  ,   and  , indexed if necessary","The set   of all probabilistic formulas is defined as the least set satisfying the following conditions:  Elements of   will be denoted by  ,   and  , indexed if necessary",The set   of all  -formulas is the union of   and  ,"Formulas from   will be denoted by  ,   and  , indexed if necessary","According to these definitions, neither mixing of pure propositional formulas and probabilistic formulas, nor nested probabilistic operators are allowed",We use the usual abbreviations for the other classical connectives  ,"Both,   and   will be denoted by ⊥, letting the context to determine the meaning","Similar, we use ⊤ for   and  .  
                   If   is a  -model, then  ",The subscript   will be omitted in   whenever the context is clear,A  -model   is   if   for every formula  ,"Throughout this paper we focus on the class of all measurable  -models.  
                   A probabilistic formula   is   if there is a  -model   such that  , and   is   if for every  -model  ,  ",A set of probabilistic formulas   is satisfiable if there is a  -model   such that   for every  ,"According to  ,  ","Therefore, from   we obtain   iff  , i.e., iff  ","In the sequel, we will denote   by  ",Let us consider in more detail the definition of satisfiability of formulas of the form  ,"If  ,  , and  , then   means that the quotient   which represents conditional probability of   given  , belongs to the  -adic ball with the center   and the radius  ",Note that the case when the measure of the condition is   is formulated on the useful assumption that the conditional probability is by default 1,It means that the corresponding probability belongs to each  -adic ball  ,"Since each point of  -adic ball is its center, the probability belongs to each ball   where  ","Finally,   means that   belongs to the  -adic ball with the center   and the radius  ","If  , then we obtain that the (conditional) probability is equal to  . 3 Axiomatization of the logic  
                   The  -valid probabilistic formulas can be fully characterized by the following axiomatic system  . 
                      
                       
                      
                      
                   Let us briefly discuss the meaning of the axioms and inference rules",Axiom A1 provides validity of all classical tautologies,Axiom A2 corresponds to the additivity of measures and it also reflects an important property of the  -adic norms (strong triangle inequality),Note that the unit  -adic ball   is closed under addition (if   then   according to the strong triangle inequality for the  -adic norm),Axiom A3 reflects the obvious property of  -adic balls: a ball of smaller radius is contained in a ball of larger radius provided the balls are not disjoint,Axiom A4 says that the conditional probability cannot belong to two disjoint balls,Axiom A5 allows that any point of a  -adic ball can be its center,Axioms A6–A8 express the definition of conditional probability,"Note that in Axiom A6 we estimate the conditional probability of   given   using the ball with the appropriate center, or more precisely with the center that is obtained as the quotient   where   belongs to the ball with the center   and   belongs to the ball with the center  ","Since   belongs to the ball with the center   and   belongs to the ball with the center  , we have to restrict   with the appropriate radius","Using the properties of  , we obtain the radius   (for details see the proof of  )","In Axiom A7, we apply similar considerations","Also, note that there are limitations in the application of Axiom A6","Namely, since   it follows that   
                      ","Therefore, we cannot apply this axiom with hypothesis  , if  ","Thus, the logic   does not answer the question what is the conditional probability of   given   for this pair of formulas","This situation is somehow inevitable, because otherwise  -adic norm of probability would be greater then 1",In the sequel we discuss the inference rules,Rule R1 is Modus Ponens,"Rule R2 can be considered as the rule of necessitation in modal logic, but it can be applied on the classical propositional formulas only",Rule R3 provides that for every classical formula   and every radius   there must be some   such that the measure of   belongs to the ball  ,Rule R4 guarantees that a contradiction has the measure 0,"Rule 5 expresses the following property: if the quotient of   and   (which corresponds to the conditional probability) is arbitrary close to some rational number  , then this quotient is equal to  ","Finally, Rule R6 says that equivalent classical formulas have the same measures",Note that Rules R3 and R5 are infinitary,"A formula   is deducible from a set   of formulas ( ), if there is an at most countable sequence of formulas  ,  , such that every   is either an instance of some axiom, or it is a formula from  , or it can be derived from the preceding formulas by some inference rule",The length of a proof is a successor ordinal.   means that   does not hold,"We say that   is a theorem (denoted by ⊢ ), if  ","A set   of formulas is consistent, if there are   and   such that   and  ","A consistent set   of formulas is said to be maximal consistent if it has the following properties:  A set of formulas   is deductively closed if for every  , if   then  . 4 Soundness and completeness of the logic  
                   In this section we prove that the axiomatic system   is sound and strongly complete","Proofs of the statements that are omitted here can be found in  .  
                   
                      
                   
                       
                      
                   
                       
                      
                   Let   be a maximal consistent set obtained from a consistent set   by the construction from  ","According to the step 3,   has the next property: For every formula   and every   there is at least one   such that  ","Since   is deductively closed, using Axiom A5, we can obtain countably many rational numbers   such that  ","Thus, for each formula   we make a sequence of rational numbers   in the following way:  In this way we obtain the sequence  ,  ,   where  ","The sequence   satisfies the properties described in the following two lemmas.  
                   
                      
                   The canonical model  , is defined as follows: 
                   Note that   is well defined: by Axiom A4 it cannot happen that  ,  . 
                      
                   
                      
                   
                      
                   5 Decidability of the logic  
                   In this section we analyze decidability of the satisfiability problem for  -formulas","Since there is a procedure for deciding satisfiability of classical propositional formulas, we will consider only  -formulas",A complete procedure for deciding satisfiability of formulas of the form   is given in  ,"We recall the main steps of this procedure, and then, adapt it to prove decidability of the satisfiability problem for all  -formulas","If   are all propositional letters appearing in  , then an atom of the formula   is a formula of the form  , where   is either   or  ","It can be shown, using classical propositional reasoning, that   is equivalent to the formula  where   
                       denotes either   or  ",The formula   is satisfiable iff at least one disjunct from   is satisfiable,Fix some   and consider disjunct  from  ,Let   be all propositional letters appearing in  ,"Every propositional formula   is equivalent to the full disjunctive normal form, denoted  ","If  , then according to Rule R6, for every model   and every  ,  ,   iff  ","Similar, if   and  , then  , and so   and  ","Therefore, for every model   and every  ,   iff  ","Thus, the disjunct   is satisfiable iff the formula  is satisfiable","Since for different atoms   and  ,  , for every model  , we have  ","Hence, the disjunct   is satisfiable iff the following system is satisfiable: 
                      
                      
                      
                      
                      
                       where   denotes that the atom   appears in  , while   denotes that the atom   appears in   and in  , and  ","We replace   with   and put it in inequalities  ,  ","In this way we obtain the system  : 
                      
                      
                      
                      
                      
                       where   denotes   or  ",Let us consider the inequalities that appear in this system,"Let  , and  ","In the sequel, we will also use the short  -adic representation  .  According to these considerations, for each variable, we focus only on a finite part in  -adic representation","The length of this part depends on the radius, which appears in the corresponding inequality","Let  Since each  , there are   possibilities for each representation of the form  ","The system   has at most   variables  , so there are at most  ways to choose representations of the form  for all variables appearing in the system",We enumerate these representations (potential solutions) by  ,"More precisely:  Thus, we assign the potential solution   to the variables and check whether the system is satisfiable","If   does not satisfy the system, we can try with   and so on","Finally, after a finite number of steps, we will find a representation which satisfies the system, or we can conclude that no representation satisfies the system  ","Note that each   that satisfies the system is a “finite part” of infinitely many solutions, thus it is actually particular solution","Thus, we have shown the following theorem.   
                      
                   6 The logic  
                   We introduce a modification of the logic  ","This logic generally retains properties of the logic  , with the fact that there are two key differences: the set of propositional letters is finite, while the range of probability is  ","The main reason for introducing such a logic is an attempt to ensure   for every propositional formula   in a model  :  but to avoid syntactical restrictions in logic  , where it is not always possible to express conditional probabilities of some formulas","However, if the set   of propositional letters is finite, then, for every propositional formula  , there exist finitely many logical inequivalent formulas  , such that   is tautology, i.e., such that  ","Thus, if we allow   to be an arbitrary  -adic number, then in the above boundedness condition, we compute supremum of finitely many numbers of the form  ,  , which is again a finite number","In the sequel, we present the logic   and emphasize differences from the logic  . 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   6.1 Syntax and semantics The language of the logic   consists of the finite set of propositional letters  , classical connectives ¬ and ∧, and a list of probabilistic operators   for every  ,  ","Sets of classical and probabilistic formulas are defined as in the logic  . 
                         
                      Thus, in contrast to  -model, the measure of a  -model is allowed to take values in the whole  ",The satisfiability relation remains the same. 6.2 Axiomatization The axiomatic system   for the logic   is the same as   with the only difference in Axiom A6,"Namely, since we allow centers of balls to be arbitrary rational numbers, there is no longer the requirement  ","Thus, the new Axiom A6′ states: 
                      6.3 Soundness and completeness 
                          
                         
                      The construction of a maximal consistent set and the corresponding canonical model is derived in the same way.   
                         
                      6.4 Decidability In order to check satisfiability for  -formulas, we need to make several changes in the procedure represented in Section  ","The main difference corresponds to the fact that rational numbers appearing in these formulas have representations of the form  , where   is an arbitrary positive integer","Thus, we cannot consider an initial piece beginning with the position  , but we have to “enlarge” these representation to the size that is appropriate for all numbers appearing in a formula for which we examine the satisfiability","Thus, we consider inequalities   and   where   and  ","Let  ,  ","Similarly, as in the case of the logic   it is obvious that: if   then   iff   and   for  ; if   then   iff   for  ","For inequalities of the form   we use the following result: 
                          
                         
                      Let   be a probabilistic  -formula","Using the considerations from Section  , we consider one disjunct   from the  , and then, replacing   with   in the system corresponding to   we conclude that   is satisfiable iff the system of the following form is satisfiable: 
                         
                         
                         
                         
                         
                          where   denotes   or  ","Now,  's are arbitrary rational numbers and  's (from  ) are from  ","Suppose that   and that   have maximal  -adic norm (among all  's), while   have minimal  -adic norm","Let  ,  ,  ,  ",We put 0 in these estimations because 1 appears in the inequalities,"Now, we have to test each variable on its finite part:   (it is assumed that   for  )","According to   and considerations above, this part is large enough to check satisfiability of all inequalities from the system","There are   variables in the system  ,  , thus there are   different representations   for all variables","We examine these representations, until we find one solution for the system  ,  , or we test all of them and conclude that the system is unsatisfiable. 
                          
                         
                      7 Applications Although the main subject of this work is a proof-theoretical development of  -adic valued conditional probabilistic logics, we shortly discuss three possible applications of them: in modeling of human thinking and of a mental experiment where relative frequencies do not converge w.r.t. the real metrics, and in representing non-monotonic reasoning",Further and more detailed investigation is left for a forthcoming paper,In   the process of human thinking is designed as a nonlinear function   where   is a space of information,It is assumed that pieces of information are close to each other if they have a sufficiently long common prefix,One can measure nearness of information   and   using the corresponding  -adic distance   in   which is suggested as the space of information  ,One of the key concepts in this formalism is the   of the function  ,"A   
                       of the function   is called   if there is a neighborhood   such that for every  ,  ","For an attractor  , we define a   
                      ","A relevant result which considers the dynamical system   from   is:  
                   Using this statement we represent below an example from   which illustrates thinking in   with the function  . 
                       In this model a  -adic number is attached to each piece of information","In a “logical environment” we consider each piece of information as a proposition, and we want to represent it by a classical propositional formula",Then we have to define a coding function which associates a  -adic number to that formula (information),The logic   seems to be a good candidate to formalize this procedure,"However, in Khrennikov's approach coding functions might not be additive, which motivated us to weaken the logic so that additivity is not required",It can be done by withdrawing Axiom 2 which corresponds to additivity,In that way we obtain a sublogic of   which is suitable to represent the process of thinking,"For this sublogic all theoretical results, stated above for  , can be similarly proved","Note that now   is no longer a probability, but it is a coding function that associates a  -adic integer to each item of information","In order to describe some special properties (if there are any) of coding functions, we use particular theories in the logic",In this formalism   means “the  -adic code of the information   belongs to the  -adic ball with the center   and the radius  ”,Assume that the proposition   means: the tourist will go to the county   and let  ,"Based on  , we use the coding process  ","Following that example, we suppose that the code of   is given, and below we construct appropriate  -theory such that the code of  , is a deductive consequence of that theory","Let  If she can go to her favorite country ( ), then: 
                      ; i.e., if the code of   is equal to 1, then the code of   belongs to the ball with the center 1 and the radius  , 
                       for every  ; i.e., for every  , the code of   belongs to the ball with the center 1 and the radius  , and therefore, according to Rule 5 
                      ","Thus,  , which we interpret as her decision to go to the first country  ","In the same way we obtain 
                      , i.e., if she cannot go to   ( ), she will go nowhere",The paper   discussed several statistical models where relative frequencies oscillate with respect to the real metric and converge with respect to the  -adic one,"Therefore, in such situations,  -adic-valued probabilities appear to be more useful then the standard real-valued counterparts","Below, we consider the model of a  -adic coin, and introduce a  -theory in which the experiment is expressed, while the result of the experiment is a deductive consequence of this theory",A  -adic coin is a metal disk with a label   on one side and   on another,"There is an electric apparatus, which can generate a negative electric charge on the side   or on the side  ",A charge can be generated only on one side at a time,"There is also a digital computer with a generator   of binary random numbers, i.e.,  ",Here we consider the case of equal probabilities of realizations of 0 and 1,"The digital computer works following the algorithm  : 
                   A statistical experiment with the  -adic coin is performed in the following way",An observer throws the coin on the table,There is one and only one tossing of the coin every second,The values of charges are such that the coin always falls in such way that its top side has not a charge,The algorithm   described above begins to work after the first coin-tossing,"We want to calculate the probabilities of the realizations of the side  ,  , and the side  ,  ",Let   be a sequence of independent random variables   with probabilities  ,"We introduce the sums of these random variables: 
                       Set 
                       Introduce relative frequencies   and  ",Note that   and  ,"The following theorems show different behaviors of the real and  -adic valued probabilities:  
                   
                      
                   Let   represent a statement “The side   is realized in the experiment”","Then, there is a  -theory T which deduces the formula   which means “the probability of a realization of the side   is equal to  ”","Suppose that after every series of 0 and 1 someone writes the corresponding relative frequencies:  ,  ",These frequencies will be the centers of some  -adic balls,"In   the probability of a realization of the side   is calculated as a ( -adic) limes of  , and we have  ",We form a sequence of sets of  -formulas:  and define the theory  ,Then we can deduce:  and obtain the same result for the probability of the considered statement as in  ,The significance of the presented logics might be also reflected in the fact that one of the most important bridges between probability and logic is the relationship between the conditional probabilities and the notion of implication,"Namely, the notion of conditional probability is a natural generalization of the implication  ","For instance, in   Adams suggested a logic of infinitesimal probabilities as a good basis for non-monotonic reasoning and indeed the core properties of a non-monotonic consequence relation   are presented in such logics","Based on these ideas, in   the authors investigate a probability logic assuming that probabilities may have values in the unite interval of the Hardy field  . 
                       The corresponding probability logic has the ability to combine uncertain knowledge and defaults","As some elements of   can be considered as infinitesimals, we believe that similar research that is based on the logics   and   might be interesting",We also consider another very general approach to modeling uncertainty,It is presented in the paper   where the authors introduce so-called plausibility measures and show that almost all approaches for dealing with uncertainty can be viewed as plausibility measures,"Drawing on this work, one can see that the monotonicity is an essential property in reasoning with uncertainty","A plausibility space is a tuple  , where   is an algebra of subsets of  , and   is a plausibility measure on  , i.e., a function   that maps sets in   to elements in some partially ordered set  , and satisfies the only one condition: if  , then  ","Some special types of  -models can be viewed as plausibility spaces and, therefore, be used in appropriate context","Although there are several possibilities to restore a plausibility space from a special  -adic probability space, we will present only one","A  -adic probability measure   is  
                       if   implies  ","A  -model is   if its measure is  -monotone. 
                      
                   One important task for some future research is to give an axiomatic system which is sound and complete with respect to the class of monotone  -models, since these models reflect some features that are important in fuzzy logic as well as non-monotonic reasoning  ","If   is a monotone  -model, then it gives the following interpretation of a default   (‘if  , then typically/likely/by default  ’):  Note that if   is a monotone  -model and  , then   implies  ","We show ( ) that the relation   satisfies all KLM properties, i.e., the core of inference rules for default reasoning","First, we prove the following technical lemma. 
                      
                   
                       
                      
                   
                       Note that for every   we can define the corresponding relation  ",It is easy to prove that these relations satisfy all the rules listed in the previous Lemma,Studying their properties shows that they may be useful in characterizations of various types of weakened monotonicity that are significant in nonmonotonic reasoning,"A more detailed research is left for a future work. 8 Conclusion In this paper we have defined two  -adic valued conditional probabilistic logics, provided strongly complete axiomatizations and proved their decidability",Some of possible applications were outlined in the previous section,Detailed considerations of possible applications (in particular to reasoning with uncertainty) are left for a companion paper which will follow,Another interesting issue for further investigation concerns a possibility to consider conditional probabilities as the primary notion in the sense of de Finetti,It seems that it can be achieved along the ideas presented by two of the authors in   where a real-valued conditional probability logic of that sort is described,Appendix A A short introduction to  -adic numbers In 1897 the German mathematician Kurt Hensel introduced the field of  -adic numbers   (for each prime number  ) as a number theoretical analog of power series in complex analysis,"In fact,   was the first example of a number field different from the fields of real and complex numbers",Besides the fact that  -adic numbers occupy the central position in Algebraic number theory   interest in  -adic numbers has been extended far beyond number theory,In the last thirty years  -adic numbers have been used in applications to quantum physics,The first models of  -adic quantum physics were the  -adic strings and superstrings  ,Attempts of physicists to create new models of space–time for the description of the Planck distance have motivated their interest in  -adic numbers,There are evidences that the standard model based on real numbers is not adequate for the Planck's domain,On the other hand some properties of fields of  -adic numbers are more suitable to be used in that area,"The investigations in the  -adic string theory induced investigations in  -adic quantum mechanics, field theory   and also development of  -adic mathematics in many directions: theory of distributions  , differential and pseudodifferential equations  , spectral theory of operators in  -adic analogue of Hilbert space  , and of the particularly interesting field – probability theory  ",It is well known that negative probabilities are one of the issues appearing in quantum physics,"Namely, they arise in many quantum models: the most famous are Wigner distribution on the phase space and Dirac's negative probability distributions in the formalism of relativistic quantization.  -Adic valued probabilistic measures allow us to work in a rigorous way with negative probabilities  ",The field   of  -adic numbers can be constructed as a completion of the field   in a similar way as the field of real numbers  ,Let us briefly recall how we obtain the field   from the field  ,"We introduce the notion of Cauchy sequence in   in the following way: sequence   is Cauchy if  , where   is the absolute value","It is well known that the limit of a sequence in   may not belong to  , i.e.   is not   with respect to  ","Thus, for  , we add all possible limits of all possible Cauchy sequences that can be constructed and in this way we obtain  ","We obtain the field   in the same way, but this time with respect to the  -adic norm","In order to explain this completion procedure, we introduce the  -adic norm and the notions of limit and Cauchy sequence with respect to this norm",Then we continue by presenting important features of  -adic numbers and  -adic probability,Let   be a fixed prime number,"We define the function   in the following way:  The function   satisfies the following properties (i.e., it is a norm on  ): 
                   Let   be a sequence of rational numbers.  
                   
                      
                   
                      
                   
                      
                   As we already mentioned, we obtain the field   by completing   with respect to  ","Therefore   is complete with respect to the norm  , i.e., every Cauchy sequence in   has a limit in  . 
                      
                   According to this result, by completing the field of rational numbers we obtain the field of real numbers or some field  ",Note that values of relative frequencies of random experiments are rational numbers,"Therefore, to calculate the probability of an event, we usually evaluate probabilities by using observed frequencies in the field of real numbers, but we can also use numbers in  , for some prime number  ",These are the only two possibilities,"The fields   are uncountable and unordered  , although it is possible to construct several partial orders on  ",Now we will discuss some basic topological notations,"Let  ,  .  ,   are open and closed balls in   while   is a sphere in   with the center   and the radius  ","Any  -adic ball   is an additive subgroup of  , while the ball   is also a ring which is called   and is denoted by  ","Balls and spheres in   have some unusual properties:  For instance we will prove the third property. 
                      : Let  ","If there is   such that   then  , a contradiction. 
                      : Suppose that  ",We show that  ,"Take, for instance that  ","Precisely,  .  □ Next, we discuss representations of  -adic numbers",Every  -adic number   can be uniquely written in the form  where   and   is such that  ,"If   is any positive integer, then   can be uniquely written in the form:  For example,  ","Thus, every natural number can be easily transformed to the form (1)",The procedure for transforming an arbitrary rational number to the form (1) is a little more complex and it can be found in  ,There is a one-to-one correspondence between the power series  and the short  -adic representation  where only the coefficients of the powers of   are written,It is possible to use the  -adic point to separate coefficients with negative indices from the others:  If   is a rational number then its  -adic expansion is of the form:  where  This representation is called periodic,"For example the 5-adic representation of   is  More precisely, the following statement holds. 
                      
                   Addition, subtraction, multiplication and division of  -adic numbers are explained in  ","For instance, here we will describe addition","Let  ,   be two  -adic numbers,   and   their  -adic representations, where   is the first nonzero digit in the expansion of   while   and there is no nonzero digits before   in the expansion of  ","Addition is done from left to right and we add digits at the same position,  ","Also, the addition is done modulo   and the remainder is transferred to the next position","We give one example.  
                   Note that we can describe  -adic balls using  -adic representations of their members",Let   and  ,"Then:  Thus, for  ,   there are countably many   such that  ","In   several mathematical probabilistic formalisms are presented: Kolmogorov's measure theoretical approach, and those based on ensembles and frequencies",For each of these formalisms the corresponding theory with  -adic valued probabilities is developed,Possible ways to avoid some paradoxes (like Bell's inequality) are discussed,"In   Khrennikov presents some experiments where  -adic-valued probabilities appear to be more useful then the standard real-valued counterparts since the corresponding relative frequencies oscillate with respect to the real metric, but converge with respect to the  -adic norm","Since in this paper our focus is on the measure theoretical approach to  -adic probabilities, we give here only the following definition of the  -adic probability space from  : 
                      
                   The  -adic probabilities are defined as bounded  -valued additive measures, normalized by 1","In the  -adic frequency theory, values of probabilities always belong to a  -adic ball   (  for some   or  ), where   plays the role of the segment   for real-valued probabilities",That motivated Khrennikov to define  -adic probability as a  -valued measure,"On the other hand, there naturally arises the question: what are mathematical restriction on   that would induce a fruitful integration","In the case of real-valued probabilities, Kolmogorov considered  -additivity on measures and the  -structure of the field of events","In the  -adic case there is the following result. 
                      
                   Moreover, it is possible to construct a measure, which cannot be extended from the field   to the  -field generated by  ","Thus, in the  -adic case  -additivity cannot be chosen as the basic integration condition","In   Monna and Springer introduced the  : if   is a field of subsets of some set   then, for every  
                       as a necessary condition (and in some cases sufficient) to obtain a fruitful integration theory","Thus, if a  -adic probability is defined as a  -valued function, with  , the boundedness condition is satisfied, which explains  .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0950705118301394," 1 Introduction Creativity is the intellectual ability to create, invent, and discover, which brings novel relations, entities, and/or unexpected solutions into existence  ","Creative thinking involves cognition (the mental act of acquiring knowledge and understanding through thought, experience, and senses), production, and evaluation  ","We first become aware of the problems with which we are confronted, then produce solutions to those problems, and finally evaluate how good our solutions are","Each act of creation involves all three processes—cognition, production, and evaluation  ","Guilford, who first introduced the terms convergence and divergence in the context of creative thinking, productive thinking can be divided into convergent and divergent thinking; the former which can generate one correct answer, and the latter which goes off in different directions without producing a unique answer  ","Although currently there is no general consensus on the definition of convergent and divergent thinking, modern theories of creativity tend to have the following perspectives","Convergent thinking is regarded as analytical and conducive to disregarding causal relationships between items already thought to be related, whereas divergent thinking is viewed as associative and conducive to unearthing similarities or correlations between items that were not thought to be related previously  ",Both convergent and divergent thinking are used to model the structure of intellect  ,"With regard to the nature of intelligence and originality, two general problem-solving behaviors were identified, those of the converger and those of the diverger, who exhibit convergent and divergent styles of reasoning/thinking, respectively  ",The distinction between convergent and divergent thinkers is done based on the dimensions of scoring high on closed-ended intelligence tests versus scoring high on open-ended tests of word meanings or object uses  ,The converger/diverger distinction also applies in cognitive styles and learning strategies  ,"Dual-processing accounts of human thinking see convergent and divergent styles as reflective/analytic and reflexive/intuitive, respectively  , which is in line with current theories of creative cognition involving generation and exploration phases  ","The convergent thinking style is assumed to induce a systematic, focused processing mode, whereas divergent thinking is suspected to induce a holistic, flexible task processing mode  ","Psychological accounts that consider convergent and divergent production as separate and independent dimensions of human cognitive ability allow one to think of creative problem solvers as divergers rather than convergers  , and to associate creativity with divergent thought that combines distant concepts together  ","Focusing only on either convergent or divergent thinking, however, may inhibit the full understanding of creativity  ","Viewing convergent production as a rational and logical process, and divergent production as an intuitive and imaginative process, creates the danger of oversimplification and confusion between intelligence and creativity","Instead, it should be recognized that there are parallel aspects or lines of thought that come together toward the end of the design process, making the design а matter of integration  ","Since convergent and divergent thinking frequently occur together in a total act of problem solving  , creativity may demand not only divergent thinking, but also convergent thinking  ","For example, deliberate techniques to activate human imagination rely on the elimination of criticism in favor of the divergent generation of a higher number of ideas","The process of deferred judgment in problem solving defers the evaluation of ideas and options until a maximum number of ideas are produced, thereby separating divergent thinking from subsequent convergent thinking  ","This sequence of divergent and convergent thinking is classified as ideation-evaluation, where ideation refers to nonjudgmental imaginative thinking and evaluation to an application of judgment to the generated options during ideation  ","Such accounts of creativity treat divergence and convergence as subsequent and iterated processes  , particularly in that order","More recent accounts of creativity, however, highlight the interwoven role of both convergent and divergent thinking  ",This interweaving has been identified in two ways,The analytic approach to creative problem solving based on linkography showed that convergent and divergent thinking are so frequent at the cognitive scale that they occur concurrently in the ideation phase of creative design  ,"The computational approach demonstrated that a computer program (comRAT-C), which uses consecutive divergence and convergence, generates results on a common creativity test comparable to the results obtained with humans  ","Hence, the creative problem solver or designer may need to learn, articulate, and use both convergent and divergent skills in equal proportions  ",The concurrent occurrence of convergent and divergent thinking in creative problem solving raises several important questions,"Is it possible to evaluate convergence and divergence in problem-solving conversations in an objective manner? How do convergence and divergence relate to different participants in a problem-solving activity? Are there particular moments in the process of real-world problem solving where a definitive change from convergence to divergence, or vice versa, occurs? How do convergence and divergence relate to the success of different ideas that are generated and developed in the process of problem solving? Could semantic measures predict the future success of generated ideas, and can they be reverse-engineered to steer generated ideas toward success in technological applications, such as in computer-assisted enhancements of human creativity or implementations of creativity in machines endowed with artificial intelligence? We hypothesized that semantic measures can be used to evaluate convergence and divergence in creative thinking, changes in convergence/divergence can be detected in regard to different features of the problem-solving process, including participant roles, successfulness of ideas, first feedback from client, or first evaluation by client or instructor, and semantic measures can be identified whose dynamics reliably predicts the success or failure of generated ideas","To test our hypotheses we analyzed the transcripts of design review conversations recorded in real-world educational settings at Purdue University, West Lafayette, Indiana, in 2013  ","The conversations between design students, instructors, and real clients, with regard to a given design task, consisted of up to 5 sessions ( 
                      ) that included the generation of ideas by the student, external feedback from the client, first evaluation by the client or instructor, and evaluation of the ideas by the client","The problem-solving conversations were analyzed in terms of participant role, successfulness of ideas, first feedback from client, or first evaluation by client or instructor using the average values of 49 semantic measures quantifying the level of abstraction (1 measure), polysemy (1 measure) or information content (7 measures) of each noun, or the semantic similarity (40 measures) between any two nouns in the constructed semantic networks based on WordNet 3.1  . 2 Materials and methods 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   2.1 Design review conversations Real-world conversations are an outstanding source to gain insights into the constructs of problem solving and decision making","To study human reasoning and problem solving, we focused on design review conversation sessions in real-world educational settings","The conversation sessions were between students and experienced instructors, and each session was used to teach and assess the student's reasoning and problem solving with regard to a given design task for a real client",The experimental dataset of design review conversations employed in this study was provided as a part of the 10th Design Thinking Research Symposium  ,"Here, we analyzed two subsets, with participants (students) majoring in Industrial Design: Junior (J): 1 instructor, 7 students (indicated with J1–J7), and 10 other stakeholders (4 clients and 6 experts) and Graduate (G): 1 instructor, 6 students (indicated with G1–G6), and 6 other stakeholders (2 clients and 4 other students)",The experimental dataset included data collected either from the same students and teams over time (although not always possible) or from multiple students and teams  ,"In addition, efforts were made to be gender inclusive",All data were collected   in natural environments rather than controlled environments,"In some cases, the design reviews were conducted in environments well insulated from disruptive noises, surrounding activities, and lighting changes; in other cases, these conditions were not possible to achieve","When disruptions occurred, most were less than a minute in duration","Because English was a second language for a number of the participants, there were some light accents in the digital recordings  ",The purpose of the conversations was for the instructor to notice both promising and problematic aspects in the student work and to help the students deal with possible challenges encountered  ,"At the end of these conversations, the students developed a solution (design concept for a product or service) that answered the problem posed in the task given initially",Computational quantification of the results was based on the digital recordings and the corresponding written transcripts of the conversations,"Because our main focus was on studying ideas in creative problem solving, we had explicitly defined the term   as a formulated creative solution (product concept) to the given design problem (including product name, drawings of the product, principle of action, target group, etc.)  ","As an example, on the graduate project “Outside the Laundry Room,” some of the generated ideas were “Laundry Rocker,” “Clothes Cube,” “Drying Rack,” “Tree Breeze,” “Washer Bicycle,” etc",Our criterion for a minimal conversation was a conversation containing at least 15 nouns,"Since on average 13.4% of the words in the conversation were nouns, an average minimal conversation contained ≈ 110 words","The reported results were per student and solution (idea). 
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                      2.1.1 Comparison between student thinking and instructor thinking On the basis of the participant roles, the speech in the conversations was divided into speech by students or speech by instructors","Instructors were defined as those giving feedback or critique that were not only persons directly appointed as instructors in the particular setting, but also clients, sometimes other students acting or criticizing as instructors, or other stakeholders present on the intermediate or the final meetings","If there were several instructors in a conversation, their speech was taken together","For this comparison, the J and G subsets contained 7 and 6 subject cases, respectively, for a total of 13 cases","For both students and instructors, 39 conversation transcripts were each analyzed ( ). 2.1.2 Comparison between successful ideas and unsuccessful ideas Conversations were divided into 2 groups: those related to unsuccessful ideas and those related to successful ideas","The unsuccessful were ideas that had not been developed to the end or had been disregarded in the problem-solving process, whereas successful ideas were those that had been developed to the end",The final evaluation of successful ideas was performed by the clients,"For each student, only one of the generated ideas was the successful one","The same conversation was divided into a part or parts that concerned one or more unsuccessful ideas, and a part that concerned the successful idea",These divisions were made on sentence breaks,"When two ideas were compared in one sentence, the sentence was considered to belong to the idea that was main for the comparison","In rare cases, if the main idea could not be identified, the sentence was not included in the analysis","The division of the text in the conversation transcripts between different ideas was assisted by the available slides in the dataset containing drawings of the generated ideas (product concepts), product names, principle of action, etc","For this comparison, the J and G subsets contained 7 and 5 subject cases, respectively, for a total of 12 cases",One case in the G subset was omitted because of missing data (slides with design sketches for client review) pertaining to unsuccessful ideas,"For the 12 subject cases, the J subset contained conversations pertaining to 22 unsuccessful and 7 successful ideas; the G subset contained conversations pertaining to 19 unsuccessful and 5 successful ideas","In total, conversations pertaining to 41 unsuccessful ideas and 12 successful ideas were analyzed ( ). 2.1.3 Comparison of ideas before and after first feedback Conversations were divided into 2 groups: containing ideas before and after first feedback","The division was based on a predefined point, which was the first feedback from the client (a stakeholder that was not a student or appointed as an instructor)","For this comparison, the J and G subsets contained 7 and 5 subject cases, respectively, for a total of 12 cases",One case in the G subset was omitted due to missing data for ideas after the first feedback,"For the 12 subject cases, the before first feedback group contained 25 conversation transcripts, whereas the after first feedback group contained 24 conversation transcripts ( )","The effect of first feedback on the time dynamics of successful ideas was assessed on 7 successful ideas (G4, G5, G6, J3, J5, J6, and J7) that had sufficiently long conversations to allow for the division into 6 time points comprising 2 sets of 3 time points before and after the first feedback. 2.1.4 Comparison of ideas before and after first evaluation Conversations were divided into 2 groups: containing ideas before and after the first evaluation","The division was based on a predefined point, which was the first evaluation performed by the instructor (for the J subset) or the client (for the G subset)","At the time of first evaluation, some of the generated ideas were discarded as unsuccessful","Those ideas that passed the first evaluation were developed further, mainly with a focus on details rather than on change of the main characteristics","In the G subset, two or more ideas passed the first evaluation, whereas in the J subset, only the successful idea passed the first evaluation","For this comparison, the J and G subsets contained 6 and 5 subject cases, respectively, for a total of 11 cases",One case of the J subset and one case of the G subset were omitted because of missing data for ideas after the first evaluation,"For the 11 subject cases, the before first evaluation group contained 22 conversation transcripts, whereas the after first evaluation group contained 13 conversation transcripts ( )","The effect of first evaluation on the time dynamics of successful ideas was assessed on 8 successful ideas (G4, G5, G6, J1, J3, J4, J6, and J7) that had sufficiently long conversations to allow for the division into 6 time points comprising 2 sets of 3 time points before and after the first evaluation. 2.2 Modeling with semantic networks In psychology, semantic networks depict human memory as an associative system wherein each concept can lead to many other relevant concepts  ","In artificial intelligence, the semantic networks are computational structures that represent meaning in a simplified way within a certain region of conceptual space",The semantic networks consist of nodes and links,"Each node stands for a specific concept, and each link, whereby one concept is accessed from another, indicates a type of semantic connection  ",Semantic networks can be used to computationally model conceptual associations and structures  ,"In this study, to construct semantic networks of nouns used in the conversations, we first cleaned the transcripts of the conversations for any indications of non-verbal expressions, such as “[Laughter],” speaker names and all the time stamps","As a second step, we processed the textual data using part-of-speech tagging performed by the Natural Language Toolkit (NLTK)   with the TextBlob library  ","Then, we extracted only the nouns, both singular and plural","With the use of Python scripts, we processed all the nouns by converting the plural forms to singular and by removing nouns that were not listed in WordNet","In total, only 8 nouns were removed, which comprised 0.2% of all nouns that were analyzed","Finally, we analyzed the constructed semantic networks using WordNet 3.1. 2.3 Analysis of time dynamics of semantic measures For graph analysis, we used Wolfram Mathematica, a mathematical symbolic computation program developed by Wolfram Research (Champaign, Illinois)","The average level of abstraction, polysemy, information content and semantic similarity in the semantic network were computed using WordGraph 3.1, a toolset that implements the WordNet 3.1 is-a hierarchy of nouns as a directed acyclic graph, allowing for efficient computation of various graph-theoretic measures in Wolfram Mathematica","The is-a relationship between noun synsets (sets of synonyms) organizes WordNet 3.1 into a hierarchical structure wherein if synset A is a kind of synset B, then A is the hyponym of B, and B is the hypernym of A","As an example, the synset {cognition, knowledge, noesis} is a kind of {psychological_feature}","The level of abstraction is negatively related to the depth of the noun in the taxonomy in a way that the root noun “entity” is the most abstract, whereas the deepest nouns in the taxonomy are least abstract  ",The complement of the level of abstraction to unity is a measure of word concreteness,"The polysemy counts the number of meanings of each word, and its log-transformed value measures the bits of missing information that are needed by the listener to correctly understand the intended meaning of a given word","The information content (IC) of nouns was calculated using seven IC formulas by Blanchard et al.  , Meng et al.  , Sánchez  , Sánchez–Batet  , Seco et al.  , Yuan et al.  , or Zhou et al.  ","The semantic similarity of pairs of nouns was calculated using five path-based similarity formulas by Al-Mubaid–Nguyen  , Leacock–Chodorow  , Li et al.  , Rada et al.  , or Wu–Palmer   and five IC-based similarity formulas by Jiang–Conrath  , Lin  , Meng et al.  , Resnik  , or Zhou et al.  , each of which could be combined with any of the seven IC formulas, thereby generating 35 IC-based similarity measures","Because WordNet 3.1 as a database is much richer than a mathematical graph, we created and employed WordGraph 3.1, a custom toolset for Wolfram Mathematica that allows for fast and efficient computation of all graph-theoretic measures related to the is-a hierarchy of nouns","To test whether convergent or divergent thinking could be quantified through convergence or divergence of semantic similarity, we assessed the change of the average semantic similarity in time","Convergence in the semantic networks was defined as an increase in the average semantic similarity in time (positive slope of the trend line), whereas divergence as a decrease in the average semantic similarity in time (negative slope of the trend line)","To obtain 3 time points for analysis of time dynamics for each subject ( ), we joined the conversation transcripts pertaining to each group or idea and then divided the resulting conjoined conversations into 3 equal parts based on word count",This division was made into whole sentences in such a way that no time point of the conversation contained less than 5 nouns,"Then, we assessed the time dynamics using linear trend lines","Because only nouns in the conversations were used for the construction of semantic networks, each time point had to contain at least 5 nouns to obtain a proper average semantic similarity. 2.4 Semantic measures based on WordNet 3.1 The calculation of semantic measures based on WordNet 3.1 ( ) was performed with the WordGraph 3.1 custom toolset for Wolfram Mathematica","The structure of WordGraph 3.1 is isomorphic to the is-a hierarchy of nouns in WordNet 3.1, implying that all mathematical expressions defined in WordGraph 3.1 also hold for WordNet 3.1","The nouns in WordGraph 3.1 were represented by 158,441 case-sensitive word vertices (including spelling variations, abbreviations, acronyms, and loanwords from other languages) and 82,192 meaning vertices, in which each word could have more than one meaning (polysemy) and each meaning could be expressed by more than one word (synset)","WordGraph 3.1 consists of two subgraphs, subgraph  , which contains 84,505 hypernym → hyponym edges between meaning vertices, and subgraph  , which contains 189,555 word → meaning edges between word vertices and each of their meaning vertices","Several graph-theoretic functions were used as follows: With the use of the above graph-theoretic functions, semantic functions were constructed that take words as arguments and return values that depend only on the relationship between the word arguments and the meanings subgraph   ( 
                         )","Two graph operators were used:  ( ) reverses the direction of all directed edges in the graph  , and  ( ) converts all directed edges in the graph   into undirected (bidirectional) edges. 
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                      2.4.1 Information content (IC) measures The intrinsic information content (IC) of a word   in WordNet 3.1 was computed using seven different formulas: IC by Blanchard et al.  , normalized in the interval [0,1], is 
                         IC by Meng et al.  
                            
                         IC by Sánchez et al.  , normalized in the interval [0,1], is 
                         IC by Sánchez–Batet  , normalized in the interval [0,1], is 
                         IC by Seco et al.  , normalized in the interval [0,1], is 
                         IC by Yuan et al.  
                            
                         IC by Zhou et al.  
                            
                         2.4.2 Path-based similarity measures The semantic similarity between a pair of words   and   such that   ≠   was computed using five different path-based similarity formulas: Al-Mubaid–Nguyen similarity  , normalized in the interval [0,1], is 
                         Leacock–Chodorow similarity  , normalized in the interval [0,1], is 
                         Li et al. similarity  , normalized in the interval [0,1], is 
                         Rada et al. similarity  , normalized in the interval [0,1], is 
                         Wu–Palmer similarity  , normalized in the interval [0,1], is 
                         2.4.3 IC-based similarity measures The semantic similarity between a pair of words   and   such that   ≠   was computed using five different IC-based similarity formulas, each of which was combined with every of the seven IC formulas thereby generating a total of 35 different IC-based similarity measures: Jiang–Conrath similarity  
                            
                         Lin similarity  
                            
                         Meng similarity  
                            
                         Resnik similarity  
                            
                         Zhou similarity  
                            
                         2.5 Statistics Statistical analyses of the constructed semantic networks were performed using SPSS ver. 23 (IBM Corporation, New York, USA)","To reduce type I errors, the time dynamics of semantic measures were analyzed with only two a priori planned linear contrasts   for the idea type (sensitive to vertical shifts of the trend lines) or the interaction between idea type and time (sensitive to differences in the slopes of the trend lines)","Because semantic similarity was calculated with 40 different formulas and information content with 7 different formulas, possible differences in semantic similarity or information content were analyzed with three-factor repeated-measures analysis of variance (rANOVA), where the idea type was set as a factor with 2 levels, the time was set as a factor with 3 levels, and the formula type was set as a factor with 40 or 7 levels, respectively","Differences in the average level of abstraction, polysemy, or individual measures of information content or semantic similarity were analyzed with two-factor rANOVAs, where the idea type and time were the two only factors","The implementation of the repeated-measures experimental design controlled for factors that cause variability between subjects, thereby simplifying the effects of the primary factors (ideas and time) and enhancing the power of the performed statistical tests","Pearson correlation analyses and hierarchical clustering of semantic similarity and IC measures were performed in R ver. 3.3.2 (R Foundation for Statistical Computing, Vienna, Austria)","For all tests, the significance threshold was set at 0.05. 3 Results 
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                   3.1 Student and instructor thinking are similar in terms of semantic measures With regard to creative thinking, our primary interest was focused on semantic similarity because as a two-argument function, it is able to evaluate the relationship between pairs of vertices in the constructed semantic networks","In addition, the average of semantic similarity is more informative than is the average of single-argument functions, such as information content, polysemy, or level of abstraction, because there are   pairs of vertices versus only   vertices in the semantic network","A comparison between the student and instructor speech in the problem-solving conversations did not show significant differences in semantic similarity (three-factor rANOVA:  
                         
                         < 0.3,   > 0.58;  
                         (A)), information content (three-factor rANOVA:  
                         
                         < 0.2,   > 0.65;  (B)), polysemy ( 
                         
                         < 0.6,   > 0.46;  (C)), or level of abstraction ( 
                         
                         < 0.9,   > 0.38;  (D)); this could be because all of the ideas originating from the student or the instructor were commented upon by both participants","To reduce Type II errors, we also confirmed that the linear contrasts in individual two-factor rANOVAs were not significant for each of the 40 semantic similarity measures ( 
                         
                         < 0.9,   > 0.37) and each of the 7 information content measures ( 
                         
                         < 0.8,   > 0.40)","These results justify our decision to further analyze both student and instructor speech jointly with regard to different types of ideas contained in the conversations. 3.2 Divergence of semantic similarity predicts the success of ideas Creative ideas should be novel, unexpected, or surprising, and provide solutions that are useful, efficient, and valuable  ","The success of generated ideas in creative problem solving depends not only on the final judgment by the client who decides which idea is the most creative, but also on the prior decisions made by the designer not to drop the idea in face of constraints on available physical resources","Thus, while success and creativity are not the same, the ultimate goal of design practice is to find solutions that are both creative and successful","To determine whether different types of thinking are responsible for the success of some of the generated ideas and the failure of others, we have compared the time dynamics of semantic measures in the conversations pertaining to successful or unsuccessful ideas","Three-factor rANOVA detected a significant crossover interaction between idea type and time ( 
                         
                         = 11.4,   = 0.006), where successful ideas exhibited divergence and unsuccessful ideas exhibited convergence of semantic similarity ( 
                         (A))","The information content manifested a trend toward significant crossover interaction ( 
                         
                         = 4.0,   = 0.072), where successful ideas increased and unsuccessful ideas decreased their information content in time ( (B))","The polysemy exhibited crossover interaction decreasing in time for successful ideas ( 
                         
                         = 12.8,   = 0.004;  (C)), whereas the average level of abstraction decreased in time but with only a trend toward significance ( 
                         
                         = 4.6,   = 0.055;  (D))","Because design practice usually generates both successful and unsuccessful ideas, these results support models of concurrent divergent ideation and convergent evaluation in creative problem solving. 3.3 IC-based semantic similarity measures outperform path-based ones The majority of 40 different semantic similarity formulas generated highly correlated outputs, which segregated them into clusters of purely IC-based, hybrid path/IC-based, and path-based similarity measures ( 
                         )","Motivated by the significant difference detected in the time dynamics of semantic similarity between successful and unsuccessful ideas, we performed post hoc linear contrasts in individual two-factor rANOVAs and ranked the 40 semantic similarity measures by the observed statistical power ( 
                         ;  
                         )","The best performance was achieved by purely IC-based similarity measures using the formulas by Lin ( 
                         
                         > 10.6,   < 0.008, power > 0.84), Jiang–Conrath ( 
                         
                         > 10.2,   < 0.008, power > 0.83), and Resnik ( 
                         
                         > 8.3,   < 0.015, power > 0.75;  ), all of which rely on the calculation of the lowest common subsumer of pairs of nouns","Hybrid path/IC-based similarity measures had a weaker performance as exemplified by the Meng formula for all IC measures ( 
                         
                         > 6.6,   < 0.026, power > 0.65), and the Zhou formula ( 
                         
                         > 6.0,   < 0.031, power > 0.61) for all IC measures except IC Sánchez for which there was only a trend toward significance ( 
                         
                         = 4.3,   = 0.063, power = 0.47)","Path-based similarity measures underperformed ( 
                         
                         > 4.4,   < 0.059, power = 0.48), even though the Wu–Palmer ( 
                         
                         = 5.7,   = 0.035, power = 0.59) and Li ( 
                         
                         = 5.1,   = 0.045, power = 0.54;  ) measures were statistically significant","Among the IC formulas, the best overall performance was achieved by the cluster of Sánchez–Batet, Blanchard and Seco, which exhibited highly correlated IC values (  > 0.93,   < 0.001;  
                         )","Having ranked the IC formulas ( ), we also performed individual two-factor rANOVAs for each of the 7 IC measures","The information content of nouns increased/decreased in time for successful/unsuccessful ideas exhibiting a crossover interaction as shown by IC Sánchez–Batet ( 
                         
                         = 6.2,   = 0.03), with 4 other IC measures by Blanchard, Meng, Seco and Zhou manifesting a trend toward significance ( 
                         
                         > 3.8,   < 0.076)","Because the first-ranked IC measure by Sánchez–Batet was significantly changed in the post hoc tests, we interpreted the trend-like significance from the corresponding three-factor rANOVA as a Type II error due to inclusion in the analysis of IC measures that compound the word information content with path-based information (such as the depth of the word in the taxonomy). 3.4 Effect of first feedback on creative problem solving Further, to test whether the first feedback from the client influences problem solving, we compared the conversations containing ideas before and after first feedback","Apart from polysemy, which exhibited an interaction between idea type and time ( 
                         
                         = 6.1,   = 0.031;  
                         (C)), none of the other 40 semantic similarity measures (two-factor rANOVAs:  
                         
                         < 2.7,   > 0.13;  (A)), 7 information content measures (two-factor rANOVAs:  
                         
                         < 1.6,   > 0.23;  (B)), or the level of abstraction ( 
                         
                         < 0.1,   > 0.78;  (D)) differed before and after first feedback when both successful and unsuccessful ideas in the conversations are analyzed together","However, when only the time dynamics of successful ideas is considered, the first feedback led to a divergence of semantic similarity (three-factor rANOVA:  
                         
                         = 22.8,   = 0.003;  
                         (A)) and enhanced the information content (three-factor rANOVA:  
                         
                         = 6.5,   = 0.044;  (B))","Post hoc two-factor rANOVAs found significant differences in 36 of 40 semantic similarity measures ( 
                         
                         > 6.2,   < 0.047 for 36 measures;  
                         
                         > 12.6,   < 0.012 for 33 measures excluding Zhou similarity) and in 4 of 7 IC measures by Sánchez–Batet ( 
                         
                         = 25.2,   = 0.002), Meng ( 
                         
                         = 11.3,   = 0.015), Zhou ( 
                         
                         = 10.4,   = 0.018), and Yuan ( 
                         
                         = 6.6,   = 0.042), with a trend toward significance for 2 other IC measures by Blanchard ( 
                         
                         = 5.4,   = 0.059) and Seco ( 
                         
                         = 5.6,   = 0.056)","The first feedback also decreased polysemy ( 
                         
                         = 8.2,   = 0.029;  (C)) and the average level of abstraction ( 
                         
                         = 16.8,   = 0.006;  (D))","These results show that the first feedback from the client has positively altered the process of problem solving and suggest that creativity benefits from external criticism obtained during the time in which the generated ideas are still evolving. 3.5 Effect of first evaluation on creative problem solving Ideas before first evaluation are subject to change, with new features added and initial features omitted, whereas ideas after first evaluation do not change their main features, only their details","Considering this, we also tested the effects of first evaluation by client or instructor upon problem solving","Conversations containing both successful and unsuccessful ideas before and after first evaluation did not exhibit different time dynamics in any of the 40 semantic similarity measures (two-factor rANOVAs:  
                         
                         < 2.7,   > 0.14;  
                         (A)), in any of the 7 information content measures (two-factor rANOVAs:  
                         
                         < 0.9,   > 0.38;  (B)), polysemy ( 
                         
                         < 3.8,   > 0.08;  (C)), or the average level of abstraction ( 
                         
                         < 0.1,   > 0.76;  (D))","Analyzing the time dynamics of only successful ideas also showed a lack of effect upon 39 of 40 semantic similarity measures (three-factor rANOVA:  
                         
                         = 2.9,   = 0.131; two-factor rANOVAs:  
                         
                         < 4.9,   > 0.063;  
                         (A)), 7 information content measures (three-factor rANOVA:  
                         
                         = 3.1,   = 0.124; two-factor rANOVAs:  
                         
                         < 4.7,   > 0.067;  (B)), polysemy ( 
                         
                         = 3.8,   = 0.093;  (C)), and the average level of abstraction ( 
                         
                         = 5.0,   = 0.06;  (D))","Only the semantic similarity measure by Rada showed an enhanced divergence after first evaluation ( 
                         
                         = 6.0,   = 0.044), but we interpreted this as a Type I error since the path-based similarity measures were the weakest in terms of statistical power ( )","These results suggest that the first evaluation had a minimal effect upon those ideas that were not dropped but developed further. 4 Discussion 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   4.1 Implications for cognitive science of creativity The presented findings advance cognitive science by showing that convergence and divergence of semantic similarity, as well as time dynamics of information content, polysemy, and level of abstraction, could be evaluated objectively for problem-solving conversations in academic settings and be used to monitor the probability of success of different ideas that are generated and developed in the process of problem solving in view of improving student training, creative thinking and skill acquisition",The observed convergence of semantic similarity for unsuccessful ideas and divergence for successful ideas parallel the psychological definitions of convergent/divergent thinking that associate creativity with divergent thought  ,"Thus, the convergence or divergence of semantic similarity in verbalized thoughts could be interpreted as a faithful reflection of the underlying cognitive processes, including convergent (analytical) or divergent (associative) thinking","Given the correspondence between convergence/divergence of semantic similarity and convergent/divergent thinking, our results, with regard to successful/unsuccessful ideas, provide extra support to recent accounts of concurrent occurrence of convergent and divergent thinking in creative problem solving  ","Psychological accounts of creative thinking and problem solving describe divergent generation of novelty and convergent exploration, evaluation or elimination of the introduced novelty  ",The opposite trend line slopes for successful and unsuccessful ideas found in the studied design review conversations can be well explained by difference in the rates of divergent production and convergent elimination of novelty,"Thus, convergent (analytical) and divergent (associative) cognitive processes, quantified through time dynamics of semantic similarity, appear to be the main factors that shape the evolution and determine the outcome of generated ideas","Language is a powerful data source for the analysis of mental processes, such as design and creative problem solving","Extracting meaningful results about the cognitive processes underlying human creativity from recorded design conversations, however, is a challenging task because not all aspects of human creative skills are verbalized or represented at a consciously accessible level  ","Semantic networks address the latter problem by providing a structured representation of not only the explicitly verbalized concepts contained in the conversations  , but also of the inexplicitly imaged virtual concepts (connecting the verbalized concepts), which are extracted from available lexical databases that are independent of the designer's background  ","In our methods, we have used WordNet 3.1 as a lexical database and have constructed semantic networks containing only nouns","Working with a single lexical category (nouns) was necessitated by the fact that WordNet consists of four subnets, one each for nouns, verbs, adjectives, and adverbs, with only a few cross-subnet pointers  ","Besides nouns being the largest and deepest hierarchical taxonomy in WordNet, our choice to construct semantic networks of nouns had been motivated by previous findings that showed how: noun phrases are useful surrogates for measuring early phases of the mechanical design process in educational settings  , networks of nouns act as stimuli for idea generation in creative problem solving  , noun–noun combinations and noun–noun relations play essential role in designing  , and similarity/dissimilarity of noun–noun combinations is related to creativity through yielding emergent properties of generated ideas  ","Noteworthy, disambiguation of noun senses is not done for the construction of semantic networks because nouns used to describe creative design ideas may acquire new senses different from dictionary-defined ones and polysemy may be responsible for the association of ideas previously thought to be unrelated  ",The effectiveness of semantic networks of nouns for constructive simulation of difficult-to-observe design-thinking processes and investigation of creativity in conceptual design was validated in previous studies using different sets of experimental data  ,"The temporal factor is not a prerequisite for applying semantic network analysis to text data, however, determining the slope of convergence/divergence is essential if the objective is to understand dynamic processes or to achieve dynamic control of artificial intelligence applications",The temporal resolution of the method for studying cognitive processes in humans is limited by the speed of verbalization and the sparsity of nouns in the sentences,"A possible inclusion of more lexical categories in the semantic analysis would increase the temporal resolution by allowing verbal reports to be divided into smaller pieces of text, but for practical realization this will require further extensive information theoretic research on how semantic similarity could be meaningfully defined for combinations of lexical categories, such as verbs and nouns, which form separate hierarchical taxonomies in WordNet. 4.2 Implications for artificial intelligence research Implementing creativity in machines endowed with artificial intelligence requires mechanisms for generation of conceptual space within which creative activity occurs and algorithms for exploration or transformation of the conceptual space  ","The most serious challenge, however, is considered not the production of novel ideas, but their automated evaluation  ","For example, machines could explore structured conceptual spaces and combine or transform ideas in new ways, but then arrive at solutions that are of no interest or value to humans","Since creativity requires both novelty and a positive evaluation of the product, the engineering of creative machines is conditional on the availability of algorithms that could compute the poor quality of newly generated ideas, thereby allowing ideas to be dropped or amended accordingly  ","Linkography is a method for analyzing decisions and activities that occur during a design work session by parsing the design conversations into a large number of small steps called design moves, some of which are then interrelated through backlinks to previous moves or forelinks to future moves","The most significant elements in a linkograph are critical moves, which are particularly rich in links",The percentage of critical moves and the link index (the ratio between the number of links and the number of moves) are positively correlated with creativity,The ideas considered most meaningful (successful ideas) have a significantly higher number of links than other ideas  ,"Information theoretic approach to measuring creativity in linkography has further shown that the Shannon entropy   of the linkograph is not directly correlated to the design outcome, however, the slope of the rate of change in entropy (second derivative in time of the entropy curve,  ) for high-scoring design sessions (successful ideas) is positive, whereas for low-scoring design sessions (unsuccessful ideas) is negative  ","Here, we have analyzed design review conversations at the level of individual words and extracted nouns from the corresponding text transcripts through computer automated natural language processing","With the use of semantic networks of nouns constructed at different times, we studied the time dynamics of 49 semantic measures that quantitatively evaluated the content of generated ideas in creative problem solving","We found that the creative ideas, which are judged as successful by the client, exhibit distinct dynamics including divergence of semantic similarity, increased information content and decreased polysemy in time",These findings are susceptible to reverse-engineering and could be useful for the development of machines endowed with general artificial intelligence that are capable of using language (words) and abstract concepts (meanings) to assist in solving problems that are currently reserved only for humans  ,A foreseeable application would be to use divergence of Lin/Sánchez–Batet semantic similarity in computer-assisted enhancement of human creativity wherein a software proposes a set of possible solutions or transformations of generated ideas and the human designer chooses which of the proposed ideas to drop and which to transform further,"As an example, consider a design task described by the set of nouns {bird, crayon, desk, hand, paper} whose average semantic similarity is 0.39","The software computes four possible solutions that change the average similarity of the set when added to it, namely, drawing (0.40), sketch (0.39), greeting_card (0.35), origami (0.29), and proposes origami as the most creative solution as it is the most divergent","If the designer rejects the idea, the software proposes greeting_card as the second best choice, and so on",Divergence of semantic similarity could be monitored and used to supplement existing systems for support of user creativity  ,Accumulated experience with software that enhances human creativity could help optimize the evaluation function for dynamic transformation of semantic similarity and information content of generated ideas up to the point wherein the computer-assisted design products are invariably more successful than products designed without computer aid,"If such an optimized evaluation function is arrived at, creative machines could be able to evaluate their generated solutions at different stages without human help, and steer a selected design solution toward success through consecutive transformations; human designers would then act as clients who run design tasks with slightly different initial constraints on the design problem and at the end choose the computer product that best satisfies their personal preference. 4.3 Future work Having established a method for the quantitative evaluation of convergence/divergence in creative problem solving and design, we are planning to utilize it for the development of artificial intelligence applications, the most promising of which are software for the computer-assisted enhancement of human creativity and bot-automated design education in massive open online courses (MOOCs), wherein a few instructors are assisted by artificial agents that provide feedback on the design work for thousands of students","We are also interested in cross-validating our results with the use of conversation transcripts from the design process of professional design teams in which the instructor-student paradigm is not applicable, and testing whether semantic measure analysis of online texts in social media or social networks could predict future human behavior",Ethics statement The authors have signed Data-Use Agreements to Dr,"Robin Adams (Purdue University) for accessing the Purdue DTRS Design Review Conversations Database, thereby agreeing not to reveal personal identifiers in published results and not to create any commercial products","Supplementary materials Supplementary material associated with this article can be found, in the online version, at  ","Appendix Supplementary materials 
                      
                  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s1875952116300040, 1 Introduction The pre-show period in the film industry is widely recognized as a good opportunity for advertising,"Theatregoers often arrive early to see a movie and furthermore, there are few distractions once they are settled in their seats",This captive audience setting is what advertising companies aim to capitalize on to reach and influence as many attendees as possible  ,"However, an increasing number of moviegoers are becoming dissatisfied and disengaged during the pre-feature period, partly due to the fact that they are aware of their captive status  ","For example, patrons may choose to use their smartphones instead of watching the pre-show or decide to not even go to the theatre at all",Patrons are very aware of the diversity of entertainment options available,"In fact, many theatregoers have home theatre environments that negatively influence their movie-going behaviour  ",Movie theatres have a challenge to provide additional value that leverages the social properties of cinema attendance  ,"However, now is the right time for the pre-show period to harness the opportunity of social networking and personal interactive technologies  ","The pre-show period can be reshaped to provide a socially enticing and personally engaging experience instead of driving theatregoers away  . 
                      
                      
                      
                   1.1 Research goal The goal of this research was to create a multiplayer real-time socially engaging game for cinema theatres",An important design requirement was to support all of the major smartphone models currently on the market to facilitate accurately reflecting the reality of the movie-going population,"The research plan included the creation of a smartphone app (client) and game server using a rigorous design, development and agile refinement process",The final research objective was to assess the effectiveness of this game using a rigorous scientific method involving a quasi-experimental mixed method study with repeated measures. 2 Background The section provides a review of related work in the area of multi-player games using handheld devices and large public displays,Schminky   is a multi-player game involving players using their smartphones in a café,A large public display was used to show the social network that resulted from users playing the game,"The degree of collaborative interaction around the large display was minimal and the display was used only as a large scoreboard; furthermore, the game was played exclusively on smartphones as opposed to a client–server model  ","In a client-sever model some game aspects run on the smartphone, while other aspects run on the large display",MobiLenin   is an entertainment system that allows people to use smartphones to vote on music video clips to be played on a large public display,"It was tested in a restaurant, and elicited interesting social behaviours such as spectatorship  ","In both Schminky and MobiLenin, the majority of interaction was on the handheld devices and between small groups of people who sat next to each other, rather than with the public display and the larger public","FishPong   is a ball- and-paddle style game played on a tabletop display using augmented coffee cups, designed to serve as an  ","Similar to Schminky and MobiLenin, the form factor of FishPong’s tabletop display makes it more suited to small groups around the table than the larger public display  ","Flashlight Jigsaw   developed an interactive game that is played exclusively on a highly-visible public display, using wireless handheld controllers (not smartphones)",The public display also served as the sole shared focus of attention for all players and spectators in a larger public space,"While there were many participants (239) involved over the course of the study, relatively few (10) were engaged concurrently",This is quite different than in a cinema theatre environment where significantly more people would be concurrently engaged,"Furthermore, the handheld device used was extremely limited compared to the capabilities of current day smartphones",Eriksson and Jeon et al.   explored using handheld devices such as mobile phones equipped with cameras to interact with public displays by direct pointing,Belinky et al.   studied collaborative planning of a museum visit over a large display using handheld devices,"Ballendat   studied information exchange between different devices, such as digital cameras, large displays and personal tablet computers in his thesis: “Visualization of and Interaction with Digital Devices around Large Surfaces as a Function of Proximity.” In all of these studies however, the number of concurrent users interacting with the large display was very low (less than 8) and the design of the systems were such that interaction amongst the users was not implicitly supported or encouraged",A study by Ballagas et al. in 2005 focused on enabling interactions with large public displays using mobile phones  ,They explored various interaction techniques such as the camera’s   technique and the   technique using the smartphones’ built-in accelerometer  ,They built some prototypes to explore these features but did not create a game nor did they provide any empirical results,"Ballagas et al. postulated that their prototypes would inspire and enable new classes of large public display applications 
                      ",One notable recent study by Gruntjens et al. in 2013 showed that social interaction was higher when people are in the same physical area interacting with the same public display,"Gruntjens et al. explored two different games for social interaction: First, all players interact at one place  ","Second, players are spread all over the world while playing the same game","The researchers asked their participants their preference of playing games. 78% of the participants answered they like playing with others using a single large display, while only 50% like playing games while being at different places  ",The game Gruntjens et al. developed used a public screen showing a virtual shooting range scene while the smartphone touchscreens of all participants displayed an interactive slingshot providing a draggable animated rubber strip,"As the taut rubber strip is released, a projectile is shot into the 3D scene onto the public screen (please see  
                      )","Unfortunately, this game may be only be played by up to four players",This is a severe limitation in terms of scalability and social interaction when designing games for cinema theatre environments where up to 100 players may be engaged—which is the goal of this proposed research,"Another study by Centieiro et al. in 2014 involved creating a mobile multiplayer game called Gaea that explore interaction with public displays stimulate engagement, persuasion and social interaction  ","Gaea is a persuasive location-based multiplayer mobile game, which prompts people to recycle virtual objects within a geographical area","Players use a smartphone to locate and collect the virtual litter in their surroundings, which should then be dropped into the correct virtual recycle bin, available for selection when approaching the public display  ",Gaea raises users’ awareness to the impact of their actions on our planet’s natural resources  ,"It also promotes users’ physical activity, social interaction and environmental behaviour changes","While this research showed encouraging results through user studies that were performed, the purpose is quite different than the research conducted in our work: (1) participants are situated in their theatre seats and do not move; (2) the interaction with the large screen is paramount and central to the game play; and (3) the design for multiplayer interaction amongst the users facilitated by a large display is key in our game but was not a focus in Gaea’s design",A thorough literature review of multiplayer real-time games for movie theatre environments was also conducted,"However, extremely few studies were found that could be drawn upon for direct comparison purposes","This may in part be due to the fact that virtually all mainstream cinema theatres have very closed and proprietary systems (e.g., Doremi, Christie cinema servers)  ","Furthermore, there are strict legal agreements that bind the movie theatre company (and, by extension, their display servers) to their movie suppliers in order to show the movies to the public  ",Such systems by design are very secure and closed meaning there are no openly accessible frameworks for application developers to build games,"In short, to create any game using these proprietary systems is involved and complex, and in some situations simply not permitted",The closest notable comparator to the proposed research is Cineplex’s TimePlay:  )  ,"Unfortunately, while there are ad hoc reviews on the games on this platform, there are no scientifically rigorous reviews available at this time","In summary, these studies indicate that with the increasing prevalence of smartphones, the techniques and approaches discussed could make public display interaction more accessible to the general public, and scale to significant numbers of concurrent users  . 
                      
                      
                      
                      
                      
                      
                      
                   2.1 Frameworks for the creation of multiplayer real-time socially engaging games for movie theatre environments A framework is required to create multiplayer real-time socially engaging games for movie theatre environments","In 2011 Deller and Ebert proposed a framework called   
                         ","ModControl is a configurable, modular communication structure that enables large screen applications to connect with personal mobile devices  ","Unfortunately, it is entirely theoretical and was not implemented such that application developers could use it  ",Another framework developed by KPicture Productions Inc. is an open implemented set of libraries for application developers to create interactive content for the pre-movie period,This framework enables any theatregoer with a web-enabled smartphone to quickly and seamlessly participate in big-screen social activities,"The framework offers both functional and social benefits such as:  : The cross-platform framework enables any device that can browse the web to fully participate in the activity;  : since the interactive content is provided by HTML, the time to launch and run the app is at most 1–2 s;  : the goal is to engage moviegoers in interactive activities that create excitement, attentional draw and social synergy during the pre-feature presentation","By engaging the audience with mobile devices and the big-screen, theatres can provide an interactive holistic social movie experience for each patron; and  : Most moviegoers have a smartphone that is Internet enabled","Consequently, the interactive content will reach the largest possible number of theatregoers",KPicture’s framework is an audience interaction technology that allows participants to have an immediate effect on the content with which they are presented,The research in this area is clear: audiences engaged with the content are much more receptive to that content  ,"Furthermore, audiences that engage in interactive behaviour with a common goal feel a greater sense of belonging to that community, and are more invested in the experiences of those around them  ",Movie theatres already leverage these benefits of interactive behaviour by providing added excitement to the movie-going experience  ,"Audience interaction offers many potential benefits in the pre-show event   including, an interactive, engaging and exciting environment; improved movie-going experience; and greater receptivity and impact of pre-movie content and/or advertising","In designing the game, several desirable characteristics were identified—characteristics that are well-defined and have been used in other related works  ; they are:   (multiplayer real-time interaction);   (movie-goer must be able to learn the game quickly and easily);   (leverage movie-goer’s familiarity with their own mobile device);   (game needs to appeal to a wide audience to generate crowd engagement);   (collaboration) and   (team competition);   (game must be able to be created within a 6-month period [external requirement], and   [via smartphone’s browser]); and   from 2 to 100 movie-goers","The outline of the remainder of this paper is as follows: candidate games for the theatre environment, game design, methodology (including the method used for game refinement and the scientific method used to evaluate the game), findings and analysis, discussion and conclusion",The findings and analysis are based on sound empirical results and the conclusion includes guidelines for generalizable contribution. 3 Candidate games for this research The desirable characteristics (design requirements) for the game were used in assessing the suitability of candidate games,"These design requirements are presented in  
                      ",The following candidate games were considered on the outset before commencing prototype design and development,Candidate Game #1: A Racing Car Game in which the design of the game involves cars and racing lanes shown on the cinema screen,Each player would see two buttons to control his/her speed of the car on his/her smartphone,Each player would be able to see their own lane and the hurdles that they will need to pass through to reach the finish line,Racers have the ability to either aid or impede other racer’s progress (depending on if they are a member of a team or not),"When a hurdle is reached, a brief quiz is presented on the user’s mobile device that they must answer in order to proceed","If the question is not answered in a timely fashion or not answered correctly, their progress is impeded","Candidate Game #2: A Maze game in which the cinema screen displays the entire maze, including obstacles, collectables, and all the players in the game","On the player’s smartphone, the player can also see the entire maze on their mobile device but only a neighbourhood section at a time—due to the limited screen real estate",Each participant controls his/her character and attempts to move that character from a starting point to a finishing point in the maze,"Candidate Game #3: A Visual (musical) Performance in which the game initially starts as a solid coloured screen (black, white, etc.)",Rhythmic music starts to play after a short instruction display,A random geometric shape in a random position on the theatre screen is assigned to each of the participant’s mobile device,"When the participant taps the screen of his/her device, the shape will visually pulsate","With the introduction of the music, each participant will be responsible for tapping his/her smartphone’s screen to the correct tempo",This will create a visual presentation of every one’s shape pulsating to the beat of the music,Candidate Game #4: Paths: This game involves a rectangular playing field with players from each team positioned along the opposite sides,Each player is represented by a round coloured piece with their player number displayed on it,"Players will traverse a pre-defined curved path from one side to the other (e.g., from the left to the right)","On the participant’s device, players will be presented with an interface that allows them to join the game, trace a path with their fingers, speed up or slow down while their piece traverses their path, and throw a ball to another player on their team",Players are prompted to draw a path on their mobile device from one end of their screen to the other,This path will be translated to the big screen where each player’s piece will traverse this path until it reaches the opposite side of the playing field without colliding with a player from the opposing team,The player uses the mobile device’s game interface to control the speed of their piece during each round,"Also, during each round, a ball is given to one player from each team, which is to be carried through to the opposite side by a successful combination of passing and dodging manoeuvres. 4 Game selected for this research Detailed sketches and storyboards were created for each of the candidate games","A focus group was established that involved four people with game critiquing experience (median age of 25 [mean 25.5, min. 22, max. 30]), to evaluate each of the proposed games against the seven criteria identified in  ","Each of the members of the focus group was independently asked to complete a survey consisting of a description of the candidate games followed by a set of questions (7 questions 
                      
                      4 games = 28 questions in total per survey)","The survey responses were based on a 5-point Likert scale (1 = very unsuitable, meaning the candidate game would very unlikely satisfy that characteristic vs. very suitable = 5, meaning the candidate game shows great promise to satisfy that characteristic)",Each of the members in the focus group were asked to carefully weigh each of these candidate games against the design requirements when completing the survey,"The results from each survey were aggregated together using standard mean calculations that are summarized in  
                       (the means are presented in “()”)",The results showed that the   game had the greatest potential from the focus group’s perspective to most effectively satisfy the desirable characteristics,The most significant reason why the other games were unsuitable is because they did not satisfactorily meet the   desirable characteristic,"For example, in the Racing Car Game, it was envisioned that the when 20 or more cars move around a racetrack on large display it would become very confusing and hard to distinguish between cars","This would be particularly so if the cars were bunched up together, as typically happens in races of this nature","Similar conclusions were drawn from the analysis of the other games (i.e., Maze Game, Musical Performance) in terms of poor scalability potential","The following section presents the features of   relative to the desirable characteristics: Ease of Learning, Ease of Use, Crowd Engagement, Creation of Team Spirit, Feasibility and Scalability","Ease of Learning  Frequent instructions on the big screen will guide the player through the learning process, including path drawing,   and  ",The game itself progresses through rounds of increasing activity and complexity to ensure that players are eased into learning the game without difficulty,"For instance, to facilitate ease of learning, the first several rounds of the game simply involve the users to watch the big screen","After these rounds,   is introduced and then for the final set of rounds   is presented as an option during game play",This approach is based on reducing the cognitive load of users while they are acquire new knowledge of game play  ,Ease of Use: Players know how to interact with their own mobile devices,This game relies on the player’s knowledge of interacting with the touch screen in a web interface on his/her device,The client controller is a web browser built using HTML and JavaScript technologies,Frequent prompts and instructions are presented to the user in order to facilitate ease of use and intuitive game interaction,The Ease of Use desirable characteristic is a reflection of the fine-motor skill acquisition activity to play the game,Studies show that the ease with which one interacts with his/her device (and/or app) is a unique and determining factor in the overall user experience and satisfaction in using that app  ,Crowd Engagement: The game requires modest player attentiveness throughout each round in order to have successful results,Players are encouraged to keep their eyes on the big screen during round play to interactively engage their fellow teammates as well as those from the opposing team,The speed control and ball passing features are gradually introduced to the player during later rounds to ensure appropriate and incremental increase in difficulty level,This design of difficulty level is aimed to be proportional to the level of fun and engagement the player experiences,"Creation of Team Spirit: To encourage team spirit, ball-passing rounds were introduced in the game",One player of each team is randomly given a ball with the objective to pass it to another teammate (and then potentially to others on the team) and successfully reach the end zone,"With each pass, the team earns points","If the team successfully gets the ball to the end zone, additional team points are awarded","Feasibility: From the outset, the game design was fundamentally sound in that it could accommodate many smartphone devices and was not overly complex (easily fit within 6-months period to build, test, evaluate and refine)","The communication framework (client–server), Unity3D framework (game server) and mobile device interface (HTML/Javascript client controller) were all well understood by the team",Scalability: The game can be easily scaled from 2–100 players,"This is accomplished in three ways: (1) reducing the size of the player’s piece on the theatre screen to comfortably fit all the players; (2) lining up the pieces on each team vertically on each side of the screen, column by column until all the pieces are placed (see  
                      ); and (3) to avoid confusion, particularly for large groups of 50 or more participants, identification is accomplished by showing the player’s number and colour on each piece on the cinema screen (which is also shown on the player’s mobile screen). 5 Game design This section discusses the game design and contributing factors that led to the development and refinement of the game",The game area consists of a rectangular space representing the large cinema screen and this view is represented on the smartphone in landscape orientation,"There are two teams – Team A, located on the left side of the game area and Team B located on the right side",Each player is represented by a unique number and coloured circle,"The player’s pieces are randomly positioned and lined up vertically in teams.   depicts the layout of the game with 14 players.  
                       presents the smartphone screen for when a player is asked to draw a path by using their finger from their piece (circle) to the opposite edge of the game area (end zone)","Because of the gross scale differences between the smartphone and cinema screen, the position of the piece on the smartphone is relative to the player’s position on the large screen","As shown in  , player #1 is on Team A (left side of the screen) and located near the top of the cinema screen","Once all players have finished drawing their paths (each player can only see their own path while they are drawing it on their smartphone), the game begins and the player’s pieces start moving along their pre-determined paths at a fixed speed (see  
                      )",The object of the game is for players to build paths that they can hope will not cause a collision with another player’s piece during the execution of the round,This will be mostly guesswork because players will not know beforehand how the other players will drawn their paths nor will they have any control over the speed,This design was purposeful so that during the initial rounds players would get a feel for the game and the audience’s behaviour,Players can decide what constitutes a better and safer path to take in future rounds and thus reduce the feeling of randomness when playing,"When a collision occurs, popcorn pops out from that location and quickly disappears, the players involved will automatically lose the current round and be temporarily stuck together in place allowing the possibility for collision with other players who are still active. 
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   5.1 Scalability In an effort to accommodate for the variable nature of movie-goers participating in the game, for each new player that joins the game, the player’s pieces decrease in radius to accommodate these new arrivals","This also ensures that when there are only a handful of players, the level of challenge is maintained (as bigger pieces increase the chance for collisions to occur during gameplay)",The potential issue of identification when accommodating large number of players (say 50+) was mitigated by a well-defined identification strategy,"Players are identified by their unique coloured piece and their unique number shown on the piece on the smartphone (see  ) and on the cinema screen (see  ). 5.2 Game modes There are four modes in the   game; they are: 
                      In cinema theatres, it is possible to have very few patrons attending the pre-screen event before the movie starts","Consequently,   was designed to offer a simulation mode during which the game runs if no player is present or not participating in the game",We felt this design offered visually stimulating entertainment in such circumstances,"In   mode, 20 AI players are created and traverse random paths at a constant speed",The result is a unique colourful collage of various intertwining paths and is entertaining to watch,"In   mode, each player’s piece moves at a constant speed for the duration of the round","Players draw their paths on their smartphones,  , and watch the show","This is the design of the first few rounds to help players become accustomed to the game and its underlying concept.  
                          presents the smartphone screen while the round plays out on the cinema screen",This mode also provides players with an opportunity to become familiar with the playing style of other participants,"In the second half of the game, it enters a new mode called   in which players will have the ability to strategically increase or decrease their speeds along their drawn path to further influence the outcome of the game.  
                          presents the interface for this mode showing the discrete course-grain speed intervals facilitating minimal attentional focus on the smartphone since the player is watching the action on the cinema screen","After several rounds of   and  , the game changes to the   mode","During these rounds, a ball is randomly given to one player on each team","The goal is to pass it to another player on the same team (who in turn may pass it to another teammate etc.), and reach the end zone","Incentives are built into the game to encourage passing as it builds team and individual points.  
                          depicts the interface for the   mode",Note the speed control on the left is essentially the same as the   mode to provide continuity in learned skills for the game,On the right is a dial control that allows the player to aim and then press the   button (centre of dial control) when ready to pass the ball. 5.3 Scoring There are five events that award points to players,Points are awarded to players who reach the end zone and they receive additional points if they have a ball in hand,"Beyond simply trying to just get across, we incentivized the players to pass the ball around to their teammates in an effort to generate team spirit","Therefore, points are also awarded for successfully passing the ball as well as receiving it","Lastly, points are awarded to players when they collide with an opponent","The more opponents a player collides with and hence takes-out of the round, the more points that player receives","Team scores show the total points earned by players on their respective team (see  ).  
                          presents the scoring and point system in  . 6 Methodology The methodology employed in this project is supported by two distinct research components",The first component is related to the manner in which   was designed and developed,"In this component of the research methodology, the results from pilot studies and experimentation provided insight for the improvement of the game",The new knowledge was fed back into the refinement of  ,"Beyond the initial development of  , an agile development process was used: design, develop, test, modify, redesign, redevelop, retest, etc",The second component of the methodology is related to the manner in which   was evaluated from a scientific rigorous approach,The research methodology for this section involved a quasi-experimental mixed method study with repeated measures,This research study was a quasi-experimental design because we could not guarantee random selection  ; it was a mixed method study because both qualitative and quantitative instrumentation was used  ; it was a repeated measures study since there were several phases of evaluation (  was revised and updated after each iteration)  ,"This section describes various aspects of the methodology including: Game Design; Pilot studies; Participants; and Experiment Procedure. 
                      
                   This section presents the initial design of   and the methodology for improving the game along the seven desirable characteristic dimensions (see  )","This involved both qualitative instrumentation (i.e., researcher observation and note taking) and extensive client–server log data that was captured throughout the experiments including: 
                   The remainder of this section presents the  ; the  ; the  ; and the  . 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                   6.1 Game design – architectural model This section presents the architectural model of   One the of overarching design considerations for the architecture was to create a model that was independent as much as possible from any game specific considerations","For example, the model provides fundamental support for communication between the Game Server and Mobile Device clients and abstracts as much as possible details surrounding game specifics",It also provides the framework by which scalability is an inherent design requirement theoretically supporting up to 500 concurrent players (based on the computational resources required and the network bandwidth throughput to support intense client and server communication),This architecture provides the foundation for a whole host of other cinema-based games using smartphones may be created—all using this common architecture,"Our framework is publically available via BitBucket and can be easily downloaded from the Internet for anyone use in developing games for cinema theatres 
                         ","In this way, generalizability and appropriate reuse may be achieved.  
                         
                         
                          visually present this architecture and are explained below. 
                          presents the client-sever architectural model designed and developed to support the   game",The Game Server and Mobile Device client core communication components,The main class that handles all outgoing and incoming communications is AIPEventDispatcher,This class interfaces with the AIP network to send and receive data from the mobile clients,"We also implemented an internal event messaging system that acts as a “post office” for events that are raised by a sending object and listened for by a receiving object instead of direct messaging, thus decoupling some the game objects from each other during runtime",Two types of events were designed to be used with this system: game events and network events,"Game events are strictly used for internal communication (i.e., within the server)","When a Unity object raises a network event, important data is packaged into the event object so that the event dispatcher that is listening for these events can then extract their contents before communicating with the client",A typical example of outgoing communication occurs when a player is positioned on the screen at the beginning of a round,A   would be raised by the   at this point and it will contain the player’s client ID and normalized   and   position coordinates,The event dispatcher receives the event (via the event messaging system) and then sends a   message to a specific client with their position information so that the client can position the player’s piece on the mobile device screen,The dispatcher also handles incoming communication via the callback method   which handles incoming messages from all clients,Messages are identified by name and client ID and then added to a queue which is then processed sequentially in the game’s   loop,"Then, the event dispatcher will pass information to the appropriate game entity. 
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                      6.1.1 Game server (unity server design) The main classes that support the game logic are described below and depicted in  . 
                         6.1.2 Client-side design The mobile device client for   was designed as an HTML5 canvas web app",The architectural model is presented in  ,Perhaps the most significant component of the client is the  ,This module is responsible for all incoming and outgoing messaging between the client and the server,"Information, such as where a player is positioned initially, which player has the ball, the player’s score, is collected and sent to the client","The client mainly sends player control events, but also sends   packets at certain points in the game to let the server know that they are still active, as to not be placed out of the next round","The client has a scheduler like the server, but its role is different",One of its purposes is to anticipate state changes from the server,"Knowing the time period of each state, it can conclude that a network connectivity issue is present if a state change does not happen when expected","When this happens, it will trigger a connectivity issue modal telling the user of the problem",Another purpose for the client’s scheduler is to display the time a latecomer has to wait before entering the game,The total time to wait is known by summing the time periods for all the game states until the next round starts,The web app also manages the display and removal of several different Interfaces,Interfaces are activated depending on the state the game is in currently by the  ,"At any given point in time, the server informs the   what Interface to present to the player","On top of Interfaces, the   can also display   that block interaction with the Interface on the layer below",Modals are used to indicate waiting times for latecomers to the theatre (as can be expected for typical for movie-goers),"Modals are also used to indicate connectivity issues with the server. 6.1.3 Game server path construction From the user’s perspective, a path is simply a continuous smooth curve with two endpoints","From a development perspective, a path is nothing more than a sequence of points","The problem was how to capture this sequence from gesture input, transport this data across the network, and unpackage it on the game server so that it could be rendered on the cinema screen",The following section describes our solution,Determining the most efficient and effective way to communicate the sequence of points across the network required a significant amount of research in this project,The number of points to be sent and received for potentially hundreds of clients in movie theatres could put a heavy load on the network and server,A design focus was to reduce this communication to a minimum and transmit only the necessary information to be able to fully reconstruct the curve,The Bézier curve algorithm was used for this purpose,"A Bézier curve is a parametric curve frequently used in computer graphics and related fields   (please refer to  
                            )","In vector graphics, Bézier curves are used to model smooth curves that can be scaled indefinitely  . “Paths,” as they are commonly referred to in image manipulation programs, are combinations of linked Bézier curves  ","A Bézier curve is defined by a set of control points  
                             through  , where   is called its order ( 
                            = 1 for linear, 2 for quadratic, 3 for cubic, etc.)  ","As the curve is completely contained in the convex hull of its control points, the points can be graphically displayed and used to manipulate the curve intuitively  ",Quadratic and cubic Bézier curves are the most commonly used (as shown in  ),The control points uniquely determine the curvature of the path – all other aspects of the paths are extrapolated by using the algorithm  ,"In our solution, the coordinates of the Bézier control points are normalized before being sent to accommodate for the differences in screen dimensions between various mobile devices and the cinema screen",The points are then de-normalized and scaled appropriately upon receipt,"On the client side, points are captured via a custom touch handling mechanism and rendered to the mobile screen using a JavaScript line rendering framework that uses a Bezier curve algorithm behind the scenes","The same technique was used on the game server for rendering on the cinema screen. 
                            
                         6.2 Pilot studies The pilot study was a complete representation of the whole experiment—that is, each participant in the full experiment was exposed to the same treatment from introduction to debriefing as participants involved in the pilot study","The rationale for conducting a pilot study is explained below: 
                      A video of one of the researchers testing   before the pilot study is found here:  
                      6.3 Participants A total of 48 volunteers were participants in this experiment (10 involved in the pilot study, 6 in main experiment #1 (median age of 20.5 [mean 22.3, min. 18, max. 32]), 16 in main experiment #2 (median age of 25 [mean 25.6, min. 18, max. 54]), and 16 in main experiment #3 (median age of 19 [mean 19.3, min. 18, max. 22])","All participants were sampled from the general population of Sheridan Institute of Technology and Advanced Learning, Oakville, Ontario, Canada",Participants were recruited by a set of posters at various locations throughout the campus,"Furthermore, an email broadcast was also sent to all the faculty members in the School of Applied Computing in the Faculty of Applied Science and Technology to let students know of this opportunity","The recruitment message did not disclose the purpose of the experiment, but described the task as fun and similar to a video game",The message also indicated that each volunteer would receive compensation for his or her time and that volunteering would be a significant contribution to the advancement of science,This method for population sampling is less than random (thus is classified as a quasi-experimental design),"However, it was deemed adequate because of the diversity of the Sheridan College student population, the location of the college with respect to the Greater Toronto Area, and because of the motivation of the compensation upon completion of the experiment. 
                         
                      6.4 Experiment procedure Participants were coordinated to meet at a specific time and theatre location on the Sheridan campus",A script was followed to ensure that each of the experiment was treated in a consistent fashion,"This script is shown in  
                         ",Each participant was expected to take between ½-h and 1-h to participate in the experiment,"The Closing Questionnaire was the main instrument for collecting qualitative information in this study and is shown in  . 7 Findings and analysis 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   7.1 Pilot study findings At the time of the pilot study, the   game was a beta version in that it had all of the game modes implemented: (1) Simulation, (2) Sit-back and Relax, (3) Varying Speed, and (4) Varying Speed and Ball-Throw, however, there were some limitations","In this version,   did not track or present individual scores and there were no onscreen instructions",One of the purposes of the pilot study was to determine the appropriate number of and timing of rounds within the game,We discovered that there were too many rounds and the timing of rounds was slightly too long,"We also found that since there were no onscreen instructions, some participants were confused at the beginning","However, after a couple of rounds, all of the participants appeared to be engaged and interacting comfortably with the game",A second purpose of the pilot study was to set the game difficulty to the appropriate level for the population base,"The difficulty of   is directly proportional to the number of players (human and non-human), the size of the pieces and the speed with which they move",The number of AI players can be easily adjusted and directly influences the difficulty and strategy human players exhibit,AI players during the pilot study followed a random path and traversed it at a steady and predicable speed,"As mentioned earlier,   was designed to function in simulation mode if no player were present",This mode provided great opportunities to conduct extensive experimentation without involving human participants,"Consequently, we were able to conduct a variety of experiments to determine the impact of adjusting the number of AI players with: (1) the number of collisions that occurred and where, and (2) the number that successfully reach the end zone. 
                         
                          presents a summary of the success rate statistics for AI and human players",We felt the game was sufficiently challenging yet not overly so to cause despair or frustration,Another factor that we considered during these simulation experiments was to determine the appropriate number of AI players to ensure the game remained challenging and interesting when scaled,"For example, the game should remain engaging and interesting if only 2 people are playing and equally so when 100 human players are playing",These findings provided guiding principles to set the game difficulty level when human players are involved in the game,Another purpose for the pilot study was to evaluate   from a stability and reliability perspective,"We wanted to make certain that once the pilot studies were completed, the experiment would run consistently and the results would be as reliable as possible","For example, initially, we followed a more naive approach for Bézier curve traversal","We traversed the points on the curve at a constant rate, not taking into account that points were not laid out equidistantly on the curve","There were more points around areas of high curvature and because the randomly generated paths constructed for AI players were different than human created ones, there were significant differences between the rate at which human players and AI players moved",We solved this problem by a   object (see  ),"This object manages its position along the curve and the index of the point it is on, and by keeping track of the distances of each line segment between the points, it can move along those segments and make sure it moves exactly the amount of distance allowed",The following section provides selected comments from participants in an effort to uncover common elements regarding benefits and/or problems with playing   The pilot study also served as a means to uncover low-level technical issues and high-level game logic issues in an effort to fully reach all of the desired characteristics (see  ),"Positive comments: 
                      Negative comments: 
                      Beyond the comments gathered from the participants, statistical analysis based on the closing survey was also performed.  
                          depicts the summary statistics of this qualitative survey",There were a number of interesting observations that result from the analysis of this data,"The following are the most significant ones. 
                      These findings provided opportunity and justification to explore how to refine   in very focused ways to ensure that the next version would address the negative comments and attempt to enhance the positive ones too","Furthermore, numerous technical issues were identified by the researchers captured in researcher logbooks that facilitated cross-referencing with server and client logs that were recorded during the execution of the pilot study",This approach facilitated very accurate analysis of the technical issues and their resolution,"Some of the issues identified at this stage were: 
                      These problems were diligently worked on, refined and tested in the laboratory until we felt the game was sufficiently improved and stable for another participant based experiential study (i.e., main experiment #1)","Specifically, we worked on: 
                      7.2 Main experiments Three main experiments were conducted in this study",Each of these main experiments was a complete representation of the whole experiment as in the pilot study,"There were approximately 1-month time gaps between these experiments to ensure that participant feedback, researcher’s logbooks, and client–server low-level data logs were diligently analyzed","This analysis informed the refinement of   for the next experiment. 
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                      7.2.1 Main experiment #1 Summary of Results – Researchers’ observations and perspectives 
                         The following section provides representative comments from participants of the main experiment #1","Positive comments: 
                         Negative comments: 
                         Beyond the comments gathered from the participants, statistical analysis based on the closing survey was also performed.  
                             depicts the summary statistics of this qualitative survey",There were a number of interesting observations that result from the analysis of this data,"The following are the most significant ones. 
                         The areas that were identified for improvement were: 
                         7.2.2 Main experiment #2 Summary of Results – Researchers’ observations and perspectives 
                         The following section provides representative comments from participants of the main experiment #2.  
                             depicts the summary statistics of this qualitative survey","Positive comments: 
                         Negative comments: 
                         The areas that were focused on for improvement after this experiment were: 
                         7.2.3 Main experiment #3 Summary of Results – Researchers’ observations and perspectives 
                         The following section provides comments from participants of the main experiment #3.  
                             depicts the summary statistics of this qualitative survey","Positive comments: 
                         Negative comments: 
                         After this experiment the following areas were focused on the following: 
                         8 Discussion In this paper we presented the design, development, refinement and evaluation of  , a multi-player real-time cinema based game.   was created using an agile development process",We shared the results of a rigorous scientific evaluation of the game created,Another contribution of this work is through an architectural model for others who wish to create cinema games using mobile devices,"The generalizability of the work spans the following areas: (1) a common generic architecture for cinema based games that use smartphones; (2) a common generic scientific methodology for the evaluation of these types of games; and (3) visual scalability. 
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   8.1 A common generic architecture for cinema based games that use smartphones In Section  , an architectural model was presented from which   was created",One of the overarching goals in this work was to design and develop an architecture that supports multiple clients (using any mobile device) and a large cinema screen (game server),This architectural model was designed in such a way that game specific details were abstracted away as much as possible,This is a common design approach in computer science and software engineering  ,"As a result, by design, there is great flexibility, and ease with which, to swap in a new game using this framework","For example, the model provides a communication mechanism between the Game Server and Mobile Device clients that is mutually exclusive and independent of game specific details","As with most architectural models, an inherent part of this architectural model is scalability  ","It is an inherent design criteria in which several hundred (up to 500 theoretically) concurrent players may fully participate in the game. 
                          This architecture provides the foundation for a host of other cinema-based games—all using this common architecture",In this way we believe that generalizability and scalability have been achieved,"Furthermore, our framework has been fully implemented and the source code is publically available via BitBucket  ","The implemented framework may be easily downloaded from the Internet for anyone interested in developing games for cinema theatres (please see:  ). 8.2 A common generic scientific methodology for the evaluation of cinema based games To a somewhat lesser degree, we have also shared a generic scientific approach that may be used as a blueprint for setting up a scientific experiment to evaluate a cinema based game","For instance, the same experiment design, instruments, and analysis techniques may be used in other similar studies",This blueprint would serve as platform on which similar studies could be conducted to validate (or invalidate) the findings from this and other related research,"This platform would be especially useful at this nascent stage because there are so few of these types of games currently available and even fewer rigorous scientific studies that have been conducted. 8.3 Visual scalability of Paths Beyond these contributions, we also explored the degree to which scalability of   is supported from a low-level game specific perspective",One of the decisions made early on in the development of   was the requirement that simplicity and clarity of the user’s representation were paramount for both the smartphone and the large display,This is one of the key reasons why we choose very plain and simple round pieces,They are psychologically familiar and simple  ,"They are also based on pieces used in extremely popular games such as Checkers, which is a game very familiar to a vast number of people  ",We discovered that this uncomplicated graphical representation is important when engaging concurrent participants in gameplay,We did not receive any criticisms from participants regarding their ability to immediately track to his/her piece on the smartphone and also on the cinema screen,All participants seemed very comfortable and capable of tracking their piece regardless of the number of concurrent players,"In our design of  , as the number of concurrent players increases, the size of the pieces decreases yet the important identifiable number remains proportionally larger","This facilitates quick and easy identification of the piece on the cinema screen even when there are many concurrent players (e.g., 50+). 
                         
                         
                         
                         
                         
                         
                          show simulated runs of 50, 70 and 100 players",Each of these figures are paired showing the initial layout of the game before any piece has moved and a screen capture of gameplay with that number of pieces in motion,"It is important to note that even with these numbers of players, in each scenario the pieces are all easily identifiable and during gameplay it was quite easy to track any of the individual pieces as they travelled their path","Furthermore, as shown in the case with 100 players ( ), the trailing path for the pieces that are ahead of other pieces was significantly reduced to avoid visual interference with other pieces",This assists in providing visual clarity for all participants,"There is however, an upper bound to the number of people that can concurrently participate in the game","For example, it is not likely feasible for 500 people to play   even though theoretically the architectural model could support it","In its present form, the number of players that   can accommodate is limited by the following constraints: 
                      We also explored ways in which marketing, advertisement and monetization models could be incorporated into the game, but it was not a focus of this research","For instance, as shown in  , when a collision occurs between two pieces, popcorn flies up and out to the sides of the screen and down to the bottom using the inspiration of actual popcorn machines","At the same time, a loud “pop” is heard resembling the sound of popcorn “popping.” We thought this was creative, light and entertaining","We certainly did not over-analyze this component of the game, however, one could argue that this may contribute to some level of subliminal marketing to encourage patrons to purchase popcorn at the concession stands before the movie starts",We also explored to a small degree a financial model that could be incorporated into  ,The monetization model could include a combination of a platform license fee as well as participation in playing the game in movie theatres,The platform license costs could be supported through subsidization by the host movie theatre and/or individual patron’s purchasing and using the app,"An advertising model could promote the entertainment offering to attract patrons, increase frequency, create promotional opportunities and deliver incremental revenues","Content in   could be customized and personalized with sponsorship that would be tied to offers (e.g., movie coupons, concession product vouchers, etc.), thus creating revenue opportunities. 9 Conclusion The pre-show period is becoming an increasingly important part of the movie-going experience and the film industry itself","However, an increasing number of moviegoers are finding the pre-feature period disengaging",The time is right for the pre-show period to harness the opportunity of social networking and personal interactive technologies,"Since 2005, the pre-movie in-theatre experience has grown to over a half-billon dollar industry and this growth has shown no signs of subsiding","Consequently, there is an industry-wide demand for innovation in the pre-movie area","In this paper, we presented   an innovative multiplayer real-time socially engaging game that we designed, developed and evaluated",An iterative refinement application development methodology was used to create the game,The game may be played on any smartphone and group interactions are viewed on the large theatre screen,The design of the game was guided by the desirable characteristics:  ;  ;  ;  ;   (collaboration) and   (team competition);  ; and  ,This paper also reports on the quasi-experimental mixed method study with repeated measures that was conducted to ascertain the effectiveness of this new game,"The results show that the game is very engaging with elements of suspense, pleasant unpredictability and effective team building and crowd-pleasing characteristics","The contributions of this work include: 
                   Two videos of the final version of   being played in a theatre environment by 22 and 28 participants respectively are found here:  
                   
                      
                      
                      
                      
                   9.1 Future work We have additional experiments scheduled for later in 2015 and others in 2016",We are excited to continue this research to determine how to improve   so that the participant engagement and satisfaction is further increased,The same survey design would be used to ensure consistency in the experiment,Our ultimate goal is to test   with 100 or more human players,Using a sophisticated tool like Unity makes it quite straightforward to provide a visually beautiful and eye-catching experience for the user,One idea that we explored was a prototype to showcase   in 3D,We discovered that we could reuse much of the existing code base and just change the various game artefacts to render everything in 3D instead of 2D,The main challenge here was figuring out how to generate the paths in 3D,"Whereas previously, we had been using a   to draw our paths in “billboard” fashion (i.e. without any possibility of adapting to a 3D perspective), now we had to come up with a new rendering process",We decided to experiment with procedural mesh rendering where we build the path by surrounding each 3D coordinate within a square and then connecting the corners of these squares to produce a 3D mesh,"The results were spectacular as can be seen in the screenshot of the prototype ( 
                         )","Appendix A Closing questionnaire survey sheet 
                      
                      
                   Title of study: Mobile devices at the cinema theatre This survey is used to determine the effectiveness of the   game – Mobile Devices at the Cinema Theatre","For each question, select the most appropriate response based on the following scale: 
                         
                      Appendix B Supplementary material Supplementary data associated with this article can be found, in the online version, at  ","Appendix B Supplementary material 
                      
                      
                  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0888613x14000851,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0740818818301828," 1 Introduction A public university library is usually open to everybody, not only to students, and users may access the library in its traditional or electronic (open access) forms","There are many institutions of this type and user choice is determined by many factors, the most important being the richness, variety, and size of resources at their disposal","These key attributes create competition among libraries, who apply for various donations and grants, including governmental ones, both in the domestic and international markets","Maintaining an advanced and modern information and communication infrastructure is crucial to a library's development, and this would not be possible without external finance","Because a library plays a vital role in the educational development of society, the way it functions is of primary importance too","As it constitutes an organizational entity, at its essence, as in the case of any other organization, is the library's socio-technical system, the complexity of which, and its corresponding business processes, make analysis interesting","This socio-technical system is composed of people (staff), whereas the technical system covers processes, tasks, knowledge and resources used by the employees in their work","So, although a library is a public organization, it fits in well with the general conditions of market competitiveness. 2 Problem statement The effectiveness of a library is analyzed in this study from the perspective of the network of relations between people (social system) and knowledge, resources, and tasks (technical system)","The study uses a unique approach called organizational network analysis (ONA) (e.g.,  ;  ;  ;  )","The organizational network in this approach is made up of knowledge networks, task networks and resource networks and their combinations","These features distinguish this approach from the traditional, and most frequently explored, one used in research, social network analysis (SNA), which constitutes a part of ONA as it particularly exposes the relations between people","The use of network analysis has become a very popular field of research, especially in an inter-organizational context (e.g.,  ;  ;  ;  )","This network approach covers intra-organizational relations, not only between people (in this case university library staff) but also between knowledge, resources, and tasks, which are integrally connected with the way a library functions","Analyzing a library from the perspective of the network of relations and ties which exist between social and technical network elements (called nodes), contributes to a more nuanced assessment of the effectiveness of university library operations, by means of some dedicated network measures, such as: 
                   These measures allow a library to be viewed from a slightly different perspective, which goes beyond such classic measures as network centrality and nodes centrality","Redundancy assesses the excess or deficit of knowledge, resources, and tasks; whereas congruence assesses the level of needs (the technical elements of the network) and the potential level of waste (the degree to which they are not used)","Looking at a library (but also at any other organization) from the perspective of a relations network makes it a living organism in which relations (connections) and nodes are constantly appearing and disappearing, thus affecting the general effectiveness of the whole network (a library)","Viewing the library as a network, and evaluating both whole network redundancy as well as node-level congruence, provides researchers with a unique and more sophisticated method for assessing optimal library effectiveness","The findings of this study extend the knowledge and understanding of library effectiveness; provide evidence for library managers to improve their socio-technical system; and benefit library staff by indicating how tasks and resources should be accomplished or used. 3 Literature review 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   3.1 Network approaches in an organizational context There are a number of theories related to the network approach which allow the interactions occurring in the organizational environment to be understood",Interactions entail the influence exerted by people (called actors in network theory) on each other or exerted by technical elements (called non-human actors in the actor-network theory),"Network theory, which includes tasks and resources as nodes as well as people, is an emerging ( ), but by no means consolidated, approach to organizational research observed from the perspective of a network of ties and relations ( ;  )",The theory of a social network ( ;  ;  ;  ) is the dominant approach and is distinguished from other networks by the intentionality of network actors' actions,"The subjects of network research are usually social relations (e.g.,  ), network structure, and the place of nodes in the network, which can be quantitatively estimated on the basis of the complex instruments within SNA ( ;  ;  )","Scientists use social networks and SNA to analyze a selected fragment of organizational reality, and the organization as a whole, in which phenomena are perceived through the prism of network relations","Organizations are created by expanding relations between people and their networks, generating social capital, aligned with having greater or lesser institutionalized relations ( ) and resources ( )","The multimodal approach to the organization as a socio-technical network and the study of its effectiveness from the perspective of information, knowledge, and task management is visible in the public health sector","Hospitals, like libraries, are mainly public and they offer their services to clients (patients)","The interactions occurring in these networks are poorly understood and, generally, unmanaged.   explain the impact of the level of knowledge development on ego network redundancy in the community of hospital doctors","The level of knowledge development, and the extent to which knowledge is uniformly distributed among cooperating physicians, is related to the reduction of their advisory networks",The impact of these links on network redundancy is moderated by belonging to different professional groups,"In the context of sharing knowledge,   discovered that weak ties are not effective when it comes to transferring complex information",Strong ties are necessary for the transfer of complex knowledge based on mutual connections,"Weak relationships provide non-redundant knowledge, and strong ones provide incentives for others to share knowledge ( )","In other studies,   used organizational network analysis to support the decision-making process for public health managers",The analysis allowed the authors to gain an insight into organizational processes that informed public health management on solving problems and using the strengths of the network,The skills that public managers need to cope with the limitations of their roles can be improved by carefully supervising the task environment,"In turn,   used organizational network analysis to document the relations between employees, tasks, knowledge, and resources in so-called local health departments, which may exist regardless of the formal administrative structure","These studies established basic network parameters that can serve as a comparative basis for local management decisions in the areas of communication, integration and resource allocation","Organizational network analysis, with a particular emphasis on congruence and the multimodal approach to network research, has found application in the process of software development ( )","Here, software development is based on the alignment or consistency between values, beliefs, standards, practices, skills, behaviors, knowledge, and goals of stakeholders.   extended the scope of the analysis of the organizational structure of a construction project to the knowledge that is understood as the ability to fulfill the tasks of the project, and other entities","A well-functioning project organization has a high adjustment between the organization of projects and the allocation of tasks. 3.2 Knowledge, resource, and task networks in an organizational context The knowledge network (AK) comprises relations between the actor (A) and knowledge (skills) (K) possessed by the actor, or used by them, in an organization",The ties (AK ) in the AK network indicate that the actor is connected with knowledge   if the actor possesses and/or uses knowledge in his/her work,Knowledge concerns the proper execution of organizational tasks,Knowledge networks enable one to determine the flows and bottlenecks of knowledge inside the organization from a network perspective ( ),Both the creation and the use of knowledge is undoubtedly a social process ( ;  ),"A knowledge network is defined as a collection of individuals and teams who meet in various organizations in order to create and share knowledge, coordinate, learn, create innovations, and support individual members inside and outside the organization ( )","In such networks, nodes are at the same time the sources and the recipients of information and knowledge, and employees are not isolated from others in the organization, except for some isolated cases","An individual may receive access to valuable information and knowledge, as every employee is a part of the network and occupies a different position in the network","This provides them with different possibilities for accessing new knowledge, which, in turn, is required to perform tasks ( )","This allows one to determine what knowledge each person in an organization possesses and also how inter-personal network structures affect the degree to which knowledge is used, and means that actors share knowledge resources and relations between them in the process of knowledge use","Since AK is constantly expanded to incorporate the knowledge gained in the learning process, it should be treated as a dynamic structure combining different levels and fields of knowledge","It is essential, therefore, to facilitate the creation of networks between particular kinds of knowledge (for example individual, group, or organizational) and fields of knowledge (for example knowledge of the market, customers, or products)","In the knowledge network, each actor has a subordinated domain of knowledge and also the ultimate relation—namely that the tie between the actor and the knowledge depends on whether the actor uses this specific knowledge in tasks (activities)",The actors' ability to use their knowledge in relation to a task will be affected by how this knowledge is distributed among organization members,"The degree of knowledge and actors' isolation will hinder the team's ability to coordinate their tasks, as it is difficult to involve everybody needed to perform the task when some actors possess knowledge which is inaccessible to others","Common knowledge helps teams since it enables them to understand expectations, explain the task to all, and plan activities in agreement with other members of the team ( )","Just as in the case of the knowledge network, the resources network is also bi-modal and may determine who (A) possesses and uses what resources (R)","Resources in the network constitute passive organizational elements, which are tools used for performing tasks and can be controlled by actors",The resources network (AR) is defined respectively by access to resources and their use by particular actors,"The tie of AR  in an AR network indicates that actor   is tied with resource  , if actor   has access to the resource and/or uses it in his/her work","The use of knowledge is perceived as a process related to a specific task (the knowledge to perform the task network – KT), in which knowledge can be applied to the task",The relation between knowledge and task KT  in a KT network exists if knowledge   is required to perform task   Task elements reflect resources which are organized in order to perform a specific business process,"Each task and allocated resource becomes a distinguished class of node in the meta-network model ( ;  ;  ), in which relations are modeled by means of graphs",The relation between the actor and the task AT  exists in an AT network if actor   is able to perform task  ,"Intra-organizational relations (determined by means of a matrix) must be coordinated in order to achieve the specified levels of information and knowledge sharing, and using resources or performing tasks","In order to perform a task, an actor who has knowledge of how the task should be completed is required","The actor allocated to this task may only have partial knowledge, or no knowledge, and therefore, in this case, knowledge would not be used or would only be used fragmentarily",The degree of using knowledge is thus a function of subordinating tasks to actors who possess the relevant knowledge,"By understanding the complexity of the flow of information, knowledge, resources, and tasks between employees in the organization, it is possible to determine more precise ways of improving access, information use and availability, with a view to increasing productivity and creating value. 
                          brought an essential contribution to the differentiation between resources (R) and tasks (T)","Resources constitute a potential for activities, creating a network of resources for performing tasks (RT) which may generate various streams of services or activities","Without subordinating particular resources (both material and non-material), an activity (service) cannot be performed","The relation between resources and tasks (RT )   in the RT network appears when resource or resources   are required to perform task  . 4 Methodology 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                   4.1 Research questions and instruments The study looks at the university library in Warsaw from the perspective of the knowledge network, the task network and the resources network, and the effectiveness of their use","In particular, this study focuses on the following questions: 
                      These research questions were reflected in two applied research methods: an interview and a questionnaire","In order to identify areas of knowledge, resources (material and non-material), and performed tasks, the interview was conducted with the director of the library, who is usually the person with the most extensive knowledge of how an organization functions ( )",The director of the library who was responsible for its development was chosen for the interview as the person with the longest work experience in the organization,The interviewee was advised of the scope of the interview in advance and they were advised again at the beginning of the conversation,"A semi-structured interview was used as it affords a less scripted method, allowing the interviewee to feel free to discuss constructively the interview goals","The interview focused on discussing the opportunities and threats to the library's external environment; discussing the business model and identifying basic business processes from the perspective of accomplishing strategic goals; and determining the required knowledge, resources, and tasks necessary to implement particular business goals","The main criteria for the selection of processes were direct influence on the library mission and vision; generating revenues and general success of the library; creating added value for the library; satisfying customers' needs (service recipients); and relying on valuable human, technological and information resources","The interview revealed a deeper understanding of the business processes, the knowledge needed and created by those processes, the resources needed and used in the processes, and the needed and performed tasks",The transcript of the interview and coding was performed using descriptive codes ( ) and a glossary of the library's own terms was used,"Using the interview results, a category of typical knowledge and skills, resources, and tasks used in business processes was developed, and this was used as an answer option in the survey questionnaire","The following five questions and statements were used in the survey questionnaire: 
                      The answers served to build five bimodal matrixes, in which the value 1 was allocated to the existence of strong relations between actors and knowledge (matrix AK); actors and tasks (AT); actors and resources (AR); and when the knowledge or resource proved necessary to perform a given task (matrixes KT and RT)","The questionnaire was pilot-tested on a sample of two employees, and the resultant effect of this pilot was, inter alia, a simplification of the survey, and the choice of a matrix construction of questions and answers (multi-grid) instead of the repeated roster","This significantly shortened the time needed to complete the questionnaire and decreased the respondents' involvement, as recommended by  ","All library employees, both librarians and full-time administrative employees, were invited to take part in the survey. 24% of employees have work experience of up to 5 years, 20% up to 10 years, 13% up to 15 years, 13% up to 20 years, and 30% of employees have an internship of over 20 years in this library","The staff were invited (via e-mail) to participate in the questionnaire, which was available online from 7th November to 4th December 2015","In the first week, 76 employees filled in the questionnaire, after which a reminder was sent out and a further 6 employees filled in the questionnaire","This meant that, in total, the survey was conducted on 82 library employees, of whom 88% were women and 12% were men, and gave a response rate of 93% from this target population, which can be considered highly satisfactory for network research ( )","In the library, apart from administrative employees who make up only 10% of the staff, the team of employees consists of librarians, junior and senior librarians, and custodians, including two persons as senior custodian","The organizational structure is hierarchical with a director, then, successively, a deputy director, executives, librarians and administrative employees","Regarding this study, it was not possible to select other observations as only the entire population (82 actors) were sampled, which automatically eliminates the use of any selection to the sample, because all library's employees were invited to take part in the study","Despite such a limited population of A (one library), the distribution of the quantities of K, R, and T assigned to each A is close to normal","Therefore, it can be concluded that, in the three categories of association, group A is the “standard”; there are no atypical A subgroups. 4.2 Organizational network analysis techniques 
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      4.2.1 Redundancy as a measure of network effectiveness Redundancy is expressed by the number of actors who gain access to the same resources, perform the same task, or use the same knowledge",Excess exists only when more than one actor meets the condition,"Redundancy determines the distribution of knowledge, resources, and tasks within the organization, according to the   below ( ): 
                            
                            
                         where: |K| - the number of elements in set K (knowledge), |R| - the number of elements in set R (resources), |A| - the number of elements in set A (actor), |T| - the number of elements in set T (tasks)","When computed on an actor (A) and knowledge (K); or resource (R); or task (T) network, this is called knowledge redundancy (KR), resource redundancy (RR), or task redundancy (TR) and is the average normalized number of redundant actors per K, or R, or T. 4.2.2 Congruence as a measure of network effectiveness Congruence denotes a relation of fit between the way in which the library is organized and its ability to perform tasks",The measure is expressed as a relation of the share of knowledge (resource) required to perform the task relative to the total knowledge (resource) of a given actor—  below,It summarizes the necessary but unavailable knowledge,"Full congruence appears when an actor possesses knowledge (resources) which is perfectly matched to the performance of the task ( ). 
                            
                         Actor knowledge (resource) waste compares the knowledge (resource) of the actor with the knowledge (resource) he or she actually needs to do his or her task","Any unused knowledge (resource) is considered wasted— ) below, ( ;  ): 
                            
                         4.3 The university library in Warsaw The library is part of one of the biggest universities in Poland and, accompanied by its high ranking as a university, enjoys a great reputation in its environment, not just in its domestic education and research market","This can be attributed to many factors, inter alia: the growing importance of technical education observed in the past few years; the openness of graduates, regular users of the library, and knowledge; the strengthening of the trend combining science and technology with economy in which knowledge and its use is the main resource","Moreover, an efficient flow of information and the emerging tendency to provide full access to the results of scientific research (so-called open access), which help identify the gap in research result commercialization, are further factors promoting the institution","In addition, the location of the library in the capital of Poland encourages all kinds of cooperation and helps the library to be a leading institution, providing high-quality services. 5 Results Since business processes reflect the influence of people, resources, knowledge and information on each other, their identification has become the most important area for analyses (see  
                      )","The identification of knowledge and skills, tasks, and resources are closely related to specific business processes indicated by the library's director and ordered by him or her in accordance with the logic of library operation","Each business process consists of knowledge, performed by the actors of tasks, and the resources used to carry out the tasks",Resources can be interchangeably called types of things or tools necessary to perform tasks,The following business processes were identified: providing access to library resources; documenting library resources; information services detailing the library's collections available to users; participating in education services; storing and protecting resources; and cooperating with the university and other libraries,"Then, the required knowledge and skills, tasks, and resources necessary to implement the business processes defined in this way were attributed","For the network analysis, those processes which are of vital importance when creating value for users and for the library were chosen","Some elements needed combining (for example, marketing and promotional activities) or moving from the task area to the knowledge and skills area (for example, knowledge of gathering collections). 
                      
                       presents the selected and grouped elements, divided into knowledge and skills (K), tasks (T) and resources (R)","In total, 24 elements of knowledge/skills (K), 31 tasks (T) and 26 resources (R) were defined","To facilitate the checking of the popularity distribution of individual K, R, and T among A, a division into 10 categories was applied (sten scores according to the formula S = 5.5 + 2∙Z, where Z is the normed value of the variable)","Firstly, every K, R, and T were counted with the number A, and then for such a variable (SUM of AK, SUM of AR, SUM of AT - the sum in the columns of these matrices “01”), Z = (X was normalized) -μ)/σ (where X is the variable value, and μ and σ are its average and standard deviation, respectively) and S values (SAK, SAR, SAT) were determined based on the sten formula","In spite of the existence of low and highly popular Ks, there are no extreme outliers among them (Grubbs Test Statistic = 2.07716,  -value = 1)","Therefore, the K group is also consistent in terms of connections with A","The majority of Ks is of average popularity, and to maintain the reality of research, in the group there are both less and more popular Ks - of course in the minority, as it is for Ts and Rs","Despite the existence of low and highly popular Rs, there are no extreme outliers (Grubbs Test Statistic = 1.819563,  -value = 1)","Therefore, the R group is consistent in terms of AR relationships","Most Rs have average popularity, and to maintain the reality of research, the R group is both less and more popular, of course in the minority","In spite of the existence of low and highly popular Ts, there are no extreme outliers (Grubbs Test Statistic = 2.22828,  -value = 1)","Therefore, the T group is consistent in terms of AT connections","The majority of Ts is of medium popularity, but to maintain the reality of research, in the T group there are tasks that are both less and more popular - of course in the minority, as presented above","All three variables (SAK, SAR, SAT) are characterized by a distribution with a light right-sided asymmetry, which would indicate the predominant low values of these variables (links with less than half A)","Nevertheless, in the AK, AR, AT distributions there are no atypical values (in particular too large) or extreme outliers","Yes, there are both low and high popular Ks, Rs, and Ts, but they can be considered typical for the studied population (Grubbs and boxplot tests)","Having identified the areas of knowledge, resources, and tasks, the redundancy level in the library was calculated","The redundancy indicator is global (it applies to the entire network) as opposed to the congruence index, which measures this value for the individual actors",Hence the level of detail in the measurement will be different here,"Knowledge redundancy (KR) = 0.430; task redundancy (TR) = 0.314, whereas resource redundancy (RR) = 0.165","Knowledge redundancy shows that 43% of the employees in the library use the same knowledge and skills in their work, 31% perform the same tasks and 16% use the same resources. 
                      
                       below presents the result of congruence for all the library staff","The minimal values for the congruence of knowledge needs and waste were MIN = 0, whereas the maximum values for knowledge congruence were, respectively, MAX = 1 (needs) and MAX = 0.667 (waste)",The maximum values for congruence of resources are MAX = 0.759 (needs) and MAX = 0.500 (waste),"The average results were, respectively, for knowledge congruence AVG = 0.348 (needs) and AVG = 0.061 (waste), and for resource congruence AVG = 0.252 (needs) and AVG = 0.177 (waste). 6 Discussion The obtained results constitute some interesting research material that enables one to look at the library through its fundamental business processes, specific knowledge, unique resources, and tasks which are typical for organizations of this type","The identified business processes provide a context for knowledge, resources, and tasks, and for employees involved in particular processes, even if such involvement results from the performance of a particular action or the possession of a fragment of the specified knowledge",The analysis concentrated mostly on network elements and their interactions,"Without defining the basic business processes for an organization, the determination of knowledge or skills, resources, and tasks would be polarized, and it would not be known how knowledge, resources, and tasks could be incorporated into the research","In an ever-changing and stormy environment, the scope of business processes will evolve and, in time, the required knowledge, resources, and tasks will also be updated","Redundancy does exist in the library, though it is difficult to assess whether it is optimal due to the lack of research in this area","Redundancy allows the level of required specialization (related to knowledge, resources, and tasks) to be determined, and the level of knowledge specialization in the library may be higher than is actually necessary to perform tasks","With a high specialization of knowledge, resources, and tasks, each excessive relation concerning this knowledge, resource, or task, will not be beneficial","Although many library employees have the same knowledge, a smaller percentage of them perform the same tasks or use the same resources","However, from the library's perspective, this may be justifiable, taking into account the inability to outsource tasks that must be performed within the library; or maybe knowledge redundancy is necessary in order to coordinate library tasks","It should be remembered though that a redundancy of knowledge, resources, or tasks which is too high leads to similar tasks being performed and not to cooperation","In the analyzed library the level of indicators seems optimal, taking into account its size (measured by the number of employees) and given the possibility of replacing absent workers with others who have similar knowledge, use similar resources, or perform similar tasks","The profit derived from additional, excess contact (concerning the same information or knowledge) will be minimal, contrary to establishing new relations between actors","According to the concept of strong and weak ties developed by  , people with whom we have weak ties have unique experience and access to new and valuable information","Therefore, it is more likely that non-redundant information will come from weak relations between actors",Non-redundancy in the library is not a feature of information (knowledge) but the fact that the exchange is very limited,"A network which is composed of non-redundant ties accounts for better use of limited resources and is more efficient, especially concerning the benefits of timing and access to information, as suggested by  ","Knowledge in dense relations is redundant, and the risk of excess (similar) knowledge in the knowledge network (AK) of the library exists, which may affect the creation of new knowledge and innovative services (see more  )","As suggested by  , individual redundancy gives an organization (in this case the library) flexibility, and mitigates against risk in a small network (library) by limiting the harmful effects connected with the unavailability of an actor.   admits that knowledge and resources are unevenly distributed in most organizations, and this is also the case in this library","The loss of redundancy, however, may limit learning, and the adaptive and flexible reactions available to this library",Moving an actor into a network of relations brings about a significant change in risk management and requires a timely reaction to changes when strategic decisions are made by library managers,Understanding the changes occurring in a network is as equally important for a library manager as understanding the structure at a given moment,"Seeing the bigger picture of the network of relations between actors, knowledge, resources, and tasks allows managers to introduce strategic interventions aimed at anticipating changes and limiting risk through, inter alia, the effective use of excess knowledge and the re-identification of key actors in the library","The role of a library manager is to identify the needs and deficits concerning knowledge and resources, and ensure access to them is provided","The congruence measure is a useful tool in this respect, as it allows the knowledge and resource needs, and the level of knowledge and resources not used by particular employees, to be assessed","The level of knowledge needs for the first ten library employees oscillates in the range 1–0.597, which may mean a high and average knowledge need, and this is higher than the resource need, which is in the range 0.759–0.448 for the first ten employees","With the measures of imbalance of knowledge and resources to workers being slightly better, and the level of unused knowledge not being high, the library can be considered efficient in this respect","A well-functioning library should have a high match between knowledge, resources, and assigned tasks ( ), which could create the optimal congruence without unnecessary waste of knowledge and resources or demand for knowledge, resources, and tasks in managing the knowledge network, the resource network, and the task network in the library",The experience of library employees and the positions held by them has become the context for the analysis of congruence results at the individual level,"The actors A22, A62, A81 and A82 have over 20 years of tenure in this library","Actor A80 has 20 years, and actors A50 and A58 have 15 years of tenure in the institution","Two actors (A58 and A80) are in management positions, and in both cases the demand for knowledge in the implementation of tasks is high",The individual results obtained for each actor in the survey encourage a more detailed analysis to be conducted,"For example, actor A80 does not have strong relations with the knowledge used but the required knowledge needs are full from the perspective of task completion","Actor 58 uses only knowledge K08 and K21 on tasks, but the required needs are much greater according to the tasks performed","Actors A58, A81, A62 also have a high degree of knowledge needs but only possess approximately 80% of the requirement","On the other hand, knowledge waste congruence is demonstrated by actors A22 and A58, who have a relatively high degree of unnecessary (excess) knowledge from the perspective of the tasks performed, at 67% and 50% respectively","In the case of resources, actor A82 uses 76% of the resources required to accomplish the tasks and a similar situation can be observed when analyzing actors A62, A81 and A58 (around 70%)","The level of resource waste is relatively lower than the congruence of wasted knowledge but, nevertheless, it still amounts to 50% for actors A22 and A50. 
                      
                      
                      
                      
                   
                      
                      
                      
                   6.1 Further research In the future, it could be worth investigating in which cases redundancy is needed due to the volume of some work areas in the library","It may also be helpful to discuss redundancy in the short, medium and long-term needs of the library",This is already done to the extent that it is mentioned that the staff may need to replace each other,"One could also examine how much time employees spend on different types of tasks, especially because some jobs require the staff to perform multiple tasks in a specific domain or in many different domains","In that case, redundancy would be needed",It may also be helpful to take into account the bottlenecks that can be reflected in the network analysis using betweeness centrality,The current study design allows cross-sectional analysis but does not take into account unmet needs,"Longitudinal studies would allow one to show the size of the gap in the studied areas of knowledge, resources, and tasks, and how it changes over time (growing, decreasing)","On the other hand, something different and requiring further research, is the existence of dependence between the occurrence of individual Ks, Ts or Rs, as well as the correlation between the quantities of K, T, R assigned to a particular A. 6.2 Limitations The measures used in this study have made it possible to assess the efficiency of the library through the redundancy and congruence of network nodes (knowledge, resources, and tasks)",These network measures are useful techniques for measuring network efficiency and their analysis provides the tools for managing the library with a view to using and sharing of resources or knowledge,"At this stage, however, it is impossible to determine the required level of redundancy that defines the efficiency of the library, since the redundancy level in other libraries operating in the same sector is not known, due to a lack of research","Therefore, as the optimal level of redundancy in libraries is not known, this would suggest that the analyzed library's excess knowledge, resources and tasks is an element protecting it against the organizational risk associated with unavailability of staff","This provides the university library, however, with some flexibility in relation to excess knowledge, resources, and tasks, in the case of the loss of a given employee or employees","It is apparent that it is necessary to conduct broader, and more extensive, research to formulate any clear conclusions here. 7 Conclusion Traditional tools for measuring an organization's efficiency are becoming inadequate and cannot grasp the dynamic nature of organizational knowledge, resources, and tasks, and associated risk, since these elements are becoming more and more potent, are sometimes hidden, and are often based on experience and interpersonal relations","This rapidly changing environment forces managers to adopt an anticipatory approach to management but, so far, the subject measures have not been widely used in organizations or network research","Nonetheless, they provide an excellent starting point for analyzing the efficiency of knowledge use, resource use, and task performance","This study constitutes a unique approach to the analysis of the efficiency of the university library through the inter-dependencies and interactions between human nodes (actors) and non-human nodes (knowledge, resources, and tasks)","By using bimodal matrices, it was possible to determine the relations and ties between particular nodes",The analysis of the network of relations and the influence exerted by particular network nodes on each other give a different perspective—a network perspective—through which it is possible to visualize the inter-dependencies between the network elements that make up the socio-technical system of the library,"The surveys give an opportunity to identify deficits in the library, identify areas of knowledge, the tasks to be carried out, or knowledge about resources in the library which might be inadequate","Additionally, this research is a source of information for the training of librarians, and can highlight the demand for knowledge and skills in this profession and the diversity of knowledge, resources, and tasks needed in library practices.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s2590188519300083," 1 Introduction The evolution of communication technologies, such as Facebook, Google+, Twitter, Instagram, Flicker and WhatsApp, help people to interconnect quickly ( )",One such example is photo-sharing services for social networking,"By taking advantage of the advancements in mobile digital camera technologies, people can easily take photos when they find something interesting and upload them to a social media platform to share exciting moments with their friends, families and colleagues ( )","As a result, one can expect large collections, which is evident, as the uploaded photo count was “about 4.5 million daily” according to the report in  ","In addition, the development of multimedia technologies and cost effective CCTV cameras for surveillance applications produce diversified images or videos at a larger scale",This leads to a huge collection with a high degree of diversity and unstructured data ( ),"For instance, some sample images of family and non-family photos chosen from our dataset are shown in  
                      (a) and (b), respectively, where we can see each image has its own variety of foreground (face regions) and background information","In this context, face recognition alone may be insufficient to identify family or non-family photos","This is because the recognition methods developed might not work well for images which contain faces with multiple emotions, postures and actions",This makes the problem of finding photos that belong to the same family complex and challenging,"As a result, family photo classification/identification can play a vital role in finding a solution to unsolved issues such as human trafficking, kinship recognition, and the problem of identifying/locating refugees ( )","Hence, there is an urgent need for developing an intelligent expert system for tackling the above-mentioned challenges","There are methods for identifying humans, facial expressions and emotions based on biometric features, which can be used for family and non-family image identification ( )","However, one major challenge of biometric systems is the variability in characteristics of the biometric of each individual","For example, the human face is complex, with features that change over time","In addition, facial features change due to variations in illumination, head pose, facial expression, cosmetics, aging, and occlusion because of beards or glasses ( )","In addition, most of the methods require cropped face images for achieving better results ( )","Therefore, recognition-based systems may not be suitable for family and non-family photo classification because the images can have unconstrained backgrounds and multiple faces with numerous emotions or expressions ( )","Hence, we can conclude that we need an expert and robust system that can cope with background complexities and issues of multiple faces with different emotions and expressions","In this work, we propose to find a solution for family and non-family photo classification based on the characteristics defined below for family and non-family images in  ","In the case of family photos, it is expected that 
                   In the case of non-family photos, it is expected that 
                   2 Related work To overcome the limitations of recognition-based systems, methods which use unsupervised features such as clustering, grouping, and similarity between the parents and children's faces, as well as personal traits such as age, race and gender ( ) have been developed. 
                       proposed social relationship discovery and face annotations for personal photo collections",This method explores the combination of ensemble RBFNN with pairwise social relationships as context for recognizing people,"However, the method requires face annotations and parameter tuning for social relationship identification","In addition, the focus of the method does not relate to family and non-family image classification; rather it explores general social relationships. 
                       proposed verification of family relationships from parents’ and children's’ facial images",The method uses local binary pattern features and degree of similarity between the faces of children and parents,The method follows conventional feature extraction and classifiers for achieving results,"However, the method is good for cropped face images but not those with multiple faces, emotions, expressions and complex backgrounds","In addition, the main target of the method is to match childrens’ faces with parents’ faces but not finding group images. 
                       proposed face clustering in a photo album, where the method explores spectral features, similarity features, minimum cost flow and clustering",The proposed features are extracted from cropped face images,The main objective of the method is to find images which share the same faces,"This idea is good for grouping personal collections but not family and non-family image classification. 
                       proposed tri-subject kinship verification for understanding the core of a family","The method proposes a degree of similarity between children and parents, resulting in a triangular relationship","To achieve this, the method uses a relative symmetric bilinear model for estimating similarity","To improve the results, the method takes spatial information into account","This method is good as long as the recognition approach provides successful results; however, recognition-based methods may not be robust for the images affected by severe illumination, postures and actions. 
                       proposed family member identification from photo collections",The method explores an unsupervised EM joint inference algorithm with a probabilistic CRF,The proposed model identifies role assignments for all detected faces along with associated pairwise relationships between them,"The performance of the proposed model depends on the success of face detection and recognition; however, the extracted biometric features used to find relationships may not be sufficiently robust when images are exposed to an open environment","In addition, the main target is to identify relationships between members of a family but the approach does not focus on family and non-family classification. 
                       proposed visual kinship recognition of families in the wild","This method explores deep learning for face verification, clustering and boosted baseline scores",The method involves multimodal labeling to optimize the annotation process,This includes information of faces and metadata collected from family photos,"It is noted that although the method explores recent powerful deep learning approaches for kinship identification, it is still limited to family photos but not non-family photos. 
                       proposed leveraging geometry and appearance cues for recognizing family photos",The methods identify facial points for each face in an image,"Based on facial points, the method constructs polygons to study geometric features of faces in the image","Due to the height difference of persons and the arrangement of faces in family and non-family images, the method gets different polygons to study geometric features","It estimates pairwise relationships like kinship recognition, and generates a codebook using k-means clustering","Furthermore, the degree of similarity of each group is extracted for classifying family and non-family photos with the help of an SVM classifier","However, classification may not be accurate when the heights of persons in an image do not follow a hierarchical arrangement","In addition, one might expect that non-family members could have the same arrangements and heights","In light of the above discussions, we can assert that a few methods have addressed family and non-family photo classification or identification, but most of the methods focus on kinship recognition based on face detection and recognition","These methods may not work well for images where we can see faces with multiple emotions, postures and actions",The methods which addressed family and non-family classification explore only foreground information (facial information) for achieving their results,"This is good for images with simple backgrounds but not images that have complex backgrounds, where we can expect open scenes and outdoor environments in the case of non-family photos","Therefore, we can conclude that there is a critical need for an accurate method to classify family and non-family photos","Hence, we propose a novel method which explores the advantages of spatial and angle information of facial key points and fractional entropy features for classification of family and non-family images","As noted from related work, facial points and geometric features for faces play a vital role in identifying members of a family, including kinship/relationships ( )","Motivated by this argument, we propose spatial and angle features in a new way to study geometric structures of faces, which captures the spatial and directional coherence of the face regions","Furthermore, to improve the discriminative power of the features, the propose method explores regular patterns in images","It is observed that in general, persons’ standing or sitting arrangements in family photos follow regular patterns such as particular orders, while non-family photos may not follow these","To extract such observations, we propose a novel fractional entropy feature to study the texture of facial regions as well as the background (other than facial region) of images","The combination of spatial information, angles that extract the geometric structure of faces, and fractional entropy that extracts the texture of facial and background regions, produces a feature vector","Furthermore, the feature vector is passed to a Convolutional Neural Network (CNN) to overcome the above-mentioned challenges","The contributions of this work are two-fold. (1) Exploring spatial and angle features for extracting the spatial and directional coherence through the geometric structure of face regions. (2) Introducing fractional entropy for extracting the texture of facial and background regions, which extracts regular patterns in the images. 3 Proposed method We noted from the Introduction and Related Work sections that facial features are important for discriminating between family and non-family persons","As a result, we propose to explore the same for finding facial key points (mouth, nose, left and right eyes and eyebrows) for the input of family and non-family images ( )",The spatial relationship and angles between facial points provide unique cues for identifying a member of the same family or to distinguish between non-family members,"Motivated by this observation, we propose to extract spatial and angle features for facial key points in a new way based on major and minor axes","It is stated in   that facial appearance in family images has a high degree of similarity with the unique pattern of spatial arrangement of persons (regular patterns), while in the case of non-family, one cannot expect such a high degree of similarity between faces and regular patterns in arranging persons (irregular patterns due to randomness in the ordering of persons)","To extract such an observation, we propose to estimate the distance between facial key points with respect to major and minor axes of the respective face images, which gives spatial coherence","In the same way, we also estimate the angle between facial key points of the respective face images, which gives directional coherence",Spatial and directional coherence together extracts geometric properties of face images,"However, the geometric features are limited to facial regions","In order to extract regular patterns from both the foreground and background (other than face regions), we further explore fractional entropy which extracts texture properties in the regions","In this way, the proposed method combines the strengths of geometric features and fractional entropy for classifying family and non-family images successfully",The proposed method extracts 8 distances and 24 angle features using facial key points and two features from fractional entropy for face regions and background information (other than face regions),"Therefore, for each input image, it gives a feature vector containing 26 features (8 + 16 + 2)","Furthermore, the feature vector is fed to a Convolutional Neural Network (CNN) for classification ( )","The overall steps of the proposed method are shown in  
                      ","In  , P  to P  are the points given by the face detection method ( ), and based on those points, the same method detects five facial key points, namely, left Eyebrow ( ), right Eyebrow ( ), left Eye ( ), right Eye ( ), Nose ( ), Mouth ( ) and the centroid, using all the 68 points","The distances are estimated between facial key points (d) for each face and finally the proposed method computes the mean of all the 8 features of all the faces ( ) in the image ( ), which gives a vector of 8 features","Similarly, angles ( ) are estimated between the facial points, and we compute the mean of all the angles of all the faces in image ( ), which gives a vector of 16 features","For faces and background regions, which are other than face regions, the proposed method extracts fractional entropy for each non-overlapping block ( )",The mean ( ) and variance ( ) of the fractional entropy of all the blocks are considered as a feature vector containing features,"The above observations are illustrated in  
                      , where we draw line graphs for distance/angle features   variances of distance/angle values for family and non-family images shown in  (a)",It is noticed from  (b) and (c) that the line behavior which represents families is smoother than that representing non-family for both spatial and angle features,"This confirms that the appearance of faces in a family image does not have many variations, while non-family have high variations",The same conclusion can be drawn from the illustration of the angle feature shown in  (c),This motivates us to use spatial and angle features for family and non-family image classification,"Detailed explanations for each step of the spatial and angle feature extraction process is discussed in subsequent sections. 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   3.1 Geometric features for facial key points For a given input image, the proposed method uses face alignment via regression of local binary features for detecting facial key points, namely, mouth, noise, left and right eyes, and eyebrows ( )",The method basically proposes a better learning-based approach,It works based on learning with a “locality” principle,"The principle is defined as: for locating a certain landmark at a given stage, the most discriminative texture information lies in a local region around the estimated landmark from the previous stage","Shape context, which gives locations of other landmarks and local textures of this landmark, provides sufficient information","With these observations, the method first learns intrinsic features to encode local textures for each landmark independently; it then performs joint regression to incorporate shape context",The method first learns a local feature mapping function to generate local binary features for each landmark,"Here, it uses a standard regression random forest to learn each local mapping function",Then it concatenates all the local features to obtain the mapping functions,It learns linear projections by linear regression,This learning process is repeated stage by stage in a cascaded fashion,"After that, a global feature mapping function, and a global linear projection and objective function are used to incorporate shape context",This process can effectively enforce the global shape constraint to reduce local errors,"In the case of the testing phase, shape increment is directly predicted and applied to update the current estimated shape",More details regarding implementation can be found in  ,"The reason to choose this method is that it is said to be generic, efficient and accurate for finding facial key points","In addition, it can cope with issues of partial occlusion and distortion",This is justifiable because the proposed work considers family and non-family images with complex backgrounds and diversified content,"The sample results of the above method are illustrated in  
                         , where (a) gives the results of candidate point detection for the input image, while  (b) shows samples of facial key points for family and non-family images","It is noted from  (b) that although the images are affected by distortion and poor quality, the method finds key points successfully","Let  
                         ,  
                         ,  
                         ,  
                         ,   and   be center points given by the method ( ), which denote left and right eyebrows, eyes, nose and mouth, respectively","These points are marked manually in  
                          for the family and non-family faces chosen from the images shown in  ","To extract spatial features to study geometric characteristics, the proposed method finds the centroid using candidate points of the face region as defined in  , where   is the number of candidate points given by the method ( ). 
                      With the help of the centroid ( ,   ), the proposed method draws an ellipse to find the major and minor axis as shown in  
                         (a) and (b) for family and non-family faces, respectively","The proposed method moves in a perpendicular direction to each key facial point ( 
                         ,  
                         ,  
                         ,  
                         ) of family and non-family images until it reaches pixels of the major axis as shown in the second illustration in  (a) and (b), respectively","Similarly, the proposed method moves in a perpendicular direction to each key point of family and non-family images until it reaches pixels of the minor axis as shown in the third illustration in  (a) and (b), respectively","Then the method finds the distance between facial key points   =  
                         ,  
                         ,  
                         ,  
                         } and respective pixels of the major and minor axes in  ′ = {major, minor} defined in  , which outputs 8 distances   = {1,2,…,8}for each face  : 
                      The distance features are extracted with respect to the major and minor axes to make the features robust to different rotations","In other words, if the input image is rotated in different directions, the feature still works well","For this step, we consider only four facial key points (that is,  
                         ,  
                         ,  
                          and  
                         ) for distance calculation because Mouth ( ) and Nose ( ) do not contribute much to classification because the   and   points lie on the minor axis",Note that the perpendicular distance is calculated by finding the smallest distance between facial key points and the pixels of major/minor axes,The step finds many distances by considering a few left and right pixels of major and minor axes to the key points,Then it chooses the pixel which produces the smallest distance between the pixels of the major/minor axis and key points,We believe that the smallest distance is the same as the perpendicular distance,"Since the input image contains many faces and the number of faces is not predictable, the proposed method computes the mean of the 8 respective distances   of all faces in the input image as defined in  , resulting in an average distance vector   for each input image, where   is the number of faces: 
                      To make the geometric features robust, we also propose to calculate the angles between facial key points to study the structure of the face region","This is because, as the face shape changes, the angle between facial key points also changes","To extract such observations, we construct a rectangle using   as shown in the first image in  (c), which gives four angles","In the same way, the proposed method forms triangles using   as shown respectively in  (c), which gives twelve angles","In total, the proposed method obtains 8 spatial + 16 angles = 24 geometric features for family and non-family image classification","Let  ( ),  ( ),  ( ) be the coordinates of the ABC triangle","The inner angles   for the ABC triangle can be calculated as defined in   and  .   computes a vector between B and A called   and the vector between C and B similarly called  Angle   is driven by   by computing the four-quadrant inverse tangent, where   is determinant, while   is the scalar dot product of the two vectors","Similarly, the proposed method estimates angles for the rectangle and the other triangles in this work. 
                         
                      Since we can expect many faces in a single input image, we propose to consider the average of the angles of the respective 16 angles","In order to average the respective angles of   faces, the circular mean is computed","First, since the angles { },   = 16 are defined on a circular coordinate system, the coordinate system should be changed to a rectangular one according to  , where   is the  th angle   of the  th face in the image","Afterwards,   the resultant vector and its direction are calculated as defined in   and  , respectively","Finally,  mean of the  th angle for all the   faces is computed as defined in  . 
                         
                         
                         
                      The proposed method computes the mean of distances to extract spatial features and the mean of angles for extracting angle features for each image",The reason for computing the average is to widen the difference between family and non-family images,"As discussed in the Introduction Section, family images have persons with almost the same facial appearance, while non-family images have persons with different facial appearances",This is valid because one can expect a high degree of similarity between the appearances of faces from the same family,It may not be true for non-family images,"In addition, family and non-family images can have any number of faces, which should be more than 3 persons in the images","In this situation, the average features for a family does not make much difference, while for non-family, the average makes a vast difference","Since the appearance of faces in a family have a high degree of similarity compared to those in non-family images, it is expected that the average gives almost the same values for family images while for non-family, we cannot predict the same values always","Besides, to make the spatial and angle features invariant to the number of faces, the proposed method considers the average for achieving better results. 3.2 Fractional entropy feature extraction As mentioned in the Introduction Section, it is found that the other than face region also provides cues for discriminating family and non-family images","However, the previous step does not explore other than face region","Therefore, inspired by the method in   where fractional calculus has been used for studying texture in splicing images, this section explores a new Tsallis fractional entropy-based texture ( ) for studying variations in background as well as facial regions in family and non-family images",An overview of the Tsallis fractional entropy is presented in the following,The Tsallis fractional entropy ( ) measures the amount of uncertainty acting in the valuation of a random variable or the consequence of a random process,"The general discrete form of this entropy is given in  . where ρ is the  -Gaussian  probability of pixel  , q ≠ 1 and   ≠ 1 are the fractional powers of the entropy, and the quantity 1/(  − 1) is the capacity of the image",The q-Gaussian is a probability distribution ascending from the growth of the Tsallis entropy under suitable restrictions,"It has the formal function as defined in  , where C  is a normalization factor. 
                         
                         
                      Since the variable is the pixel which has a positive value in the maximum entropy procedure, the q-exponential distribution is derived","Applying   in  , we have the following generalized formula of the fractional entropy: 
                      In our discussion, let  , then we conclude where   is the total number of pixels in the image","The proposed method calculates the above Tsallis fractional entropy based on frequency details of the input image, which gives a texture property to study the structure of it",The advantage of Tsallis fractional entropy is that it is sensitive to non-textured regions (low frequency),"In addition, it sharpens any changes in texture details in the regions, where pixel values are changing sharply (high frequency)","The sample illustration for Tsallis fractional entropy for family and non-family images is shown in  
                         , where we can see all the dominant information represented by edges in the background and the facial regions are highlighted.  
                          shows the clear discriminating power of Tsallis fractional entropy texture features for family and non-family images","Therefore, for the feature matrix given by the Tsallis fractional entropy texture, we first split the input image into blocks with a size of   pixels, then the Tsallis fractional entropy for each block is computed","For all the blocks of the input image, the “mean” and the “variance” are computed and saved as the output texture features   and  , respectively","The pseudo-code for the proposed Tsallis fractional entropy algorithm is described as follows: 
                      Feature distributions of the spatial, angle and texture features for family and non-family images are shown in  
                         (a)–(c), respectively, where one can see that the feature distributions of geometric and Tsallis fraction entropy provides a clear distinction between family and non-family images in terms of histogram behavior",The concatenated features are then passed to a fully connected Convolutional Neural Network (CNN) for classifying family and non-family images ( ),"Inspired by the method ( ), where it is mentioned that the combination of handcrafted features and the ensemble of CNNs give better results than deep learning tools such as GoogleNet, ResNet50 that use raw pixels of the input images for bioimage classification, we explore the same idea of combining the proposed features with the CNN for family and non-family image classification in this work","Since the proposed work does not provide a large number of samples for training and labeling samples, we prefer to use the combination of the proposed features and the CNNs rather than raw pixels with the recent deep learning models",The main objective of the proposed work is to propose features that can classify the family and non-family photos,"Thus, the proposed features are fed to a pre-defined CNN classifier which is available online ( ) for classification in this work","For learning parameters of the classifier, we follow a 10-fold cross-validation procedure, which splits the dataset into training and testing components",The training samples are used for learning and adjusting the parameters of the classifier and the testing samples are used for evaluation,"The complete algorithmic steps of the proposed method for classifying family and non-family images are presented below. 
                      4 Experimental results For experimentation, we created our own dataset by collecting images from social media, such as Facebook, Flickr, Instagram and from our own camera",This dataset includes indoor/outdoor scenes and images with 3–25 people,"In addition, the dataset involves family and non-family photos of different cultures, such as Hindu and Chinese, and modern styles of family/non-family photos",This makes the dataset challenging for experimentation,"For labeling the data as either family or non-family, we followed the instructions suggested in  ,  ","Furthermore, the dataset includes one photo for one family","In other words, the dataset does not have multiple photos of the same family","In total, our dataset consists of 388 family images and 382 non-family images, which gives a total of 770 images","To demonstrate that the proposed method is effective, we also considered the benchmark dataset collected from publicly available data in  ,  ","This public data provides a large number of images, which include many groups of photos and images containing both family and non-family categories","As a result, we chose the relevant family and non-family images and labeled these manually according to the instructions in  ,  ","We consider this dataset as the benchmark dataset, which consists of 1790 family and 2753 non-family images","In total, there are 4543 images, which is larger than the dataset considered in  ,  ","Overall, we considered 5263 (770 from our dataset and 4543 from benchmark dataset) images for experimentation in this work","Sample images of family and non-family photos for ours and the benchmark dataset are shown in  
                      (a) and (b), respectively, where we can see intra- and inter-class variations",It is also observed from   that family and non-family images have both indoor and outdoor scenes as backgrounds,It is also true that height distribution of persons in a hierarchical order for family and a non-hierarchical order for non-family is not necessarily true as shown in  ,"The detailed statistics of ours and the benchmark dataset are listed in  
                      , where we calculate the ratios (E  and E ) as respectively defined in   and   using the count images with indoor and outdoor scenes, and hierarchical or non-hierarchical persons’ height orders",The ratio in   indicates that our dataset is much more complex than the benchmark dataset because the ratio with respect to indoor backgrounds and the hierarchical order of our dataset are greater than those of the benchmark dataset,"Note that in   and  ,   denotes the size of the dataset as given in   in the bracket. 
                      
                   To show that the proposed method is superior in comparison to existing methods, we implemented two state-of-the-art methods, namely,  , which explores facial geometric features and facial appearance model-based features",The features are passed to an SVM classifier for family and non-family image classification,"Please note, the same idea is extended and the results are improved in   for the purpose of family and non-family image classification","However, both the ideas focused only on facial regions for achieving results; these also ignored background clues","To measure the performance of the proposed and existing methods, we generate confusion matrices for family and non-family classification and the classification rate",The Classification Rate (CR) is defined as the number of images classified correctly by the proposed method ( ) divided by the total number of images in the class ( ) as defined in  ,The Average Classification Rate (ACR) is calculated for diagonal elements of the confusion matrices to evaluate the overall performance of the proposed and existing methods,"In this work, we undertake 10-fold cross-validation for choosing the number of training and testing samples",The criteria divides the whole dataset into 10 equal-sized sub-folds,"For each iteration, images from each sub-fold are considered as testing samples, while images from the other sub-folds are considered as training samples for classification, which results in a confusion matrix for one sub-testing fold out of 10 sub-folds",This process indicates that the chosen training samples are used for training the classifier and the testing samples are used for evaluation,"In this way, the process considers every sub-fold as testing samples at each iteration, which results in 10 confusion matrices i.e. 10-fold","The average of all the 10 confusion matrices are considered as the final confusion matrix for evaluation in this work. 
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   4.1 Evaluating the proposed classification The proposed method consists of three key steps, namely, extracting spatial/angle-based geometric features and fractional entropy-based texture features for classifying family and non-family images","In order to assess the contribution of each key step, we conducted experiments on both our dataset and the benchmark dataset individually to calculate average classification rates","The results reported in  
                          show that the combined Spatial + Angle achieves the best results compared to the individual features for both our dataset and the benchmark dataset","It is also noted from   that the ACR of angle-based features is better than Spatial, but lower than Spatial + Angle for both the two datasets","This shows that angle-based features are better than spatial-based features, and the combination is better than both individual features",This is understandable because the spatial structure alone is not sufficient for handling the problem of complex backgrounds as it only extracts 8 features,"However, the improvement is marginally different","Therefore, we can conclude that spatial and angle-based features contribute equally for achieving the best results","In this work, we extracted fractional entropy-based texture features for the whole image, which includes facial regions and background information","We conducted experiments for calculating classification rates only for the Facial region (FEF), Background (FEB), and the whole image (FEW) to identify the effectiveness of the facial region and background information, individually","Note: the facial regions detected by facial point detection are considered as foreground, and the rest of the region is considered as the background for experimentation","The results of FEF, FEB and FEW are reported for both our dataset and the benchmark dataset in  
                         ",It is observed from   that the FEF is the best at ACR compared to FEB for both the datasets,This shows that facial regions contribute more compared to the background,"This is justifiable because sometimes, family and non-family photos may share the properties of the background","It is evident from the results of FEB for the benchmark dataset in  , where most family images are misclassified as non-family",This shows that the features of the background of family images overlap with the features of the background of non-family images,"However, facial regions alone are not sufficient to achieve the best ACR compared to FEW","Therefore, we can conclude that the features of the foreground and background are important to achieve the best results for classification",It is noted from   and   that Spatial + Angle and FEW are better compared to individual features on both our dataset and the benchmark datasets for family and non-family image classification,"In order to decide the best combination, we conducted experiments for the following combinations: Spatial + FEF, Spatial + FEB, Spatial + FEW, as reported in  
                          and Angle + FEF, Angle + FEB and Angle + FEW, as reported in  
                         ","When we look at the ACR of all the combinations in   and  , Spatial + FEW and Angle + FEW are the best compared to the other combinations for both our dataset and the benchmark dataset",It is justifiable because Spatial + FEW and Angle + FEW include features of facial regions and background information,"Therefore, to achieve the best results, we propose the combination of Spatial + FEW and Angle + FEW, which is the proposed method and the results are reported in  
                          for our dataset and the benchmark dataset","When we compare ACR of Spatial + FEW and Angle + FEW with the results of the proposed method (Spatial + Angle + FEW), ACRs for the respective three experiments on our dataset are almost the same",This is because the proposed method has been developed based on our dataset,"However, when we compare the ACR of Spatial + FEW, Angle + FEW, and the proposed method on the benchmark dataset, there is a significant improvement for the proposed method compared to Spatial + FEW and Angle + FEW","Hence, we can conclude that the proposed method is capable of handling complex datasets",It is observed from the ACR of the proposed method on our dataset and the benchmark dataset reported in   that the proposed method scores highly on the benchmark dataset compared to our dataset,"The reason is that our dataset includes diverse images such as those of different culture, modern families and non-family photos","At the same time, the benchmark dataset provides a large number of images for training, i.e., 1790 for family and 2753 for non-family compared to 388 for family and 382 for non-family images of our dataset",This is the advantage of the benchmark dataset for achieving the best results compared to ours,"This is because when we feed a large number of training samples to the classifier, it covers more possible variations in images","Therefore, a large number of training samples and more variations led to achieving the best results for the benchmark dataset by the proposed method compared to our dataset","In the case of spatial and angle feature extraction discussed in the Proposed Methodology Section, the proposed method computes the mean for distances and angles of all the faces in the respective images separately","To assess the influence of averaging (the mean), we conduct experiments for calculating the classification rate using the proposed method without averaging","In other words, the proposed method considers all the distance and angle features of faces in images as distance and angle feature vectors respectively for classification","The results are reported in  , where one can see the proposed method without averaging the scores providing very poor results compared to the proposed method with averaging for both the datasets","This shows that the operation, namely, averaging, plays a vital role in achieving better results for family and non-family classification","Since we use the CNN for classification, to show its effectiveness compared to the SVM and the use of raw pixels with the CNN, the proposed method is used for experimentation of the proposed features with an SVM as well as feeding raw pixels to a CNN for ours and the benchmark dataset","For experiments using raw pixels of the images, the proposed method considers each pixel value as a feature and it converts a two-dimensional image matrix to single-dimensional feature vector in a row-wise fashion",The converted single-dimensional feature vector is passed to a CNN for classification,"This experiment does not involve the proposed distance, angle and fractional entropy-based features for calculating the measures",The results are reported in  ,It is noted from   that the results of feeding raw pixels directly to a CNN performs poorly in terms of classification rate compared to the proposed features with a CNN,"The main reason for the poor results is that since the number of samples for the training set is small, it may not cover all the possible variations of images when we feed raw pixels to the CNN directly","For experimentation with an SVM and for a fair comparative study with the CNN, the proposed method uses a polynomial kernel as it is non-linear like the CNN classifier","When we compare the results of the proposed features with an SVM and the proposed features with a CNN, the proposed features with an SVM achieve poorer results compared to the proposed features with a CNN",It is justifiable because the SVM does not have a generalization ability as is the case with a CNN,"In addition, the performance of the SVM depends on the kernel type and size","On the other hand, the CNN can learn complex non-linear input and output relationships","Therefore, for the proposed problem, which is complex in terms of foreground and background variations according to the statistics reported in  , the proposed features with a CNN perform better than the proposed features with an SVM","We also report the results of two existing methods on our dataset and the benchmark datasets in  
                         ","Since ( ) is the improved version of  , whereby   gives better results in terms of ACR","When we compare the ACR of the proposed method with two existing methods, the proposed method gives better results than ( ) and (Want et al., 2017)","This is understandable as both the existing methods use only facial regions for classification, while the proposed method uses both facial and background regions","In addition, the proposed method extracts geometric features based on spatial and angle information, and the new fractional entropy feature are an enhancement on existing methods and hence it makes a positive difference","Sample qualitative results of the proposed method on our dataset and the benchmark dataset are shown in  
                         .   also includes the results of misclassifications by the proposed method on our dataset and the benchmark dataset","The reason for misclassification is that when the images of family and non-family images share geometric structures of the faces and the properties of backgrounds, the proposed method fails to perform correct classification","Therefore, there is scope for improvement in the future","The existing methods ( ) that work based on the fact that the height distributions of persons in images should satisfy a hierarchical order for family, while it does not for non-family","In the same way, according to the statistics in  , the benchmark dataset contains more images with indoor scenes for the family class, and more images with outdoor scenes for the non-family class","However, the proposed method does not consider these two constraints for the classification of family and non-family images","It is evident from the statistics reported in   for our dataset, where it can be seen that the ratio of hierarchical to the total number of family images and non-hierarchical to the total number of non-family images is greater compared to that from the benchmark dataset",The same thing is true for images with indoor scenes for family and outdoor scenes for non-family images,"To validate the statement, we conducted experiments for Family-Hierarchical   Non-family-Non-hierarchical and Family-Non-hierarchical   Non-family-Hierarchical on both ours and the benchmark dataset, and the results are reported in  
                         ",It is observed from   that the classification rate for the expected order is higher than that of the other order,"Therefore, we can conclude that there is not much influence on the overall performance of the proposed method","Similarly, images with indoor and outdoor scenes do not have much of an effect on the overall performance of the proposed method",It is evident from the results reported in   for Family-Indoor   Non-family-Outdoor and vice versa,"In summary, for all the experiments listed in   for both ours and the benchmark dataset, the proposed method achieves almost consistent average classification rates",This demonstrates that the proposed method works well irrespective of the background complexities and hierarchical distribution of heights,"However, when we compare the results of the proposed method on the whole dataset ( ) without separation and the results in  , the results of the proposed method in   are higher than those in   due to fewer training samples which represent the variations in the case of individual experiments listed in  . 5 Conclusions and future work In this paper, we have proposed a new idea for classifying family and non-family photos by combining facial structure and background texture",The proposed method explores distances between facial key points for extracting spatial features,"In addition, angles between facial key points are also explored for studying the structures of faces, which are called geometric features","To make use of the background information and textural properties of facial regions, we have proposed novel Tsallis fractional entropy-based features","Furthermore, the proposed method combines spatial, angle and fractional entropy features to obtain the feature vector",The feature vector is applied to a conventional convolutional neural network for classification,Experimental results on our own dataset and the benchmark datasets show that the proposed method is better than two state-of-the-art methods in terms of average classification rate,The main contributions are the following,It is inherent that facial regions are the key factor for family and non-family photo image classification,"Based on this observation, we explore distance features for facial key points as spatial features to study the structure of facial regions",We have used angle information for facial key points to make spatial features robust to extract the detailed structure of facial regions,The way we combine spatial and angle-based features as geometric ones is novel and an interesting approach to tackle the issues of family and non-family photo classification,"To extract regular patterns in facial and background regions (other than facial region), we propose a novel idea of introducing Tsallis fractional entropy for extracting texture properties of facial regions and other background regions","Furthermore, the proposed method combines geometric and fractional entropy features in a different way for achieving the best results","Despite having proposed a new idea for family and non-family images classification, there are some limitations to the proposed approach","Sometimes, when family and non-family image share the same properties of facial regions with the background, the proposed method fails to yield good results",This is understandable because one can expect similar patterns of foreground and background for both family and non-family images,"In this case, we need a method, which can work irrespective of background and facial regions","One way is to introduce context features using foreground and background information to find a solution regarding context, which can be independent of facial regions and the background","When photos contain both family and non-family members, the proposed method may not work well",It is beyond the scope of the proposed work as it is hard to separate family or non-family members in the same image,"To find a solution, one possible way is to bring multimodal concepts, such as face, skin, dress, and structure of the body",This is due to the potential of sharing personal traits and habits with members belonging to the same family,"If individuals do not belong to a particular family, we can expect different habits, structures (apart from the face), skin, etc","In summary, this paper presents a new idea for finding a solution to family and non-family photo classification","The proposed work demonstrates a promising direction for solving a number of issues, including human trafficking","There are several potential concepts, which can be considered as new research directions for future study","In order to support reproducible research, the dataset and code will made available to readers upon request","CRediT authorship contribution statement 
                       Implementing Methodology, data curation.   Conceptualization, Formal analysis, Supervision, Draft writing, Investigation.   data curation, Validation.   Formal analysis, Modeling.   Formal analysis, Modeling.   Review editing.   Validation and reviewing.   Validation and reviewing","Supplementary materials Supplementary material associated with this article can be found, in the online version, at doi: ","Appendix Supplementary materials 
                      
                  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s1875952117300952," 1 Introduction Stroke, brain injury and multiple sclerosis are the leading causes of most disabilities in adults which result in neuro-motor deficits",This can affect postural control and balance and cause difficulties in independent daily life  ,"Physiopathologic research has demonstrated that such difficulties include utilising hands for timed grasp, holding, buttoning, reaching, balancing or/and walking  ",It is of vital importance to provide opportunities for patients to relearn or improve basic skills by doing exercises that help them to restore appropriate physical functionality,"The recent availability of inexpensive off-the-shelf sensors (such as the Apple iPad, Nintendo Wii, Nintendo DS, Microsoft Kinect and balance board) have opened up new exciting perspectives to assess the practical capabilities for home-based rehabilitation and to improve exercise capacity  ","These devices have received attention from the academic community in many disciplines including; health, robotics, biomechanics, and engineering  ","Some of these devices have been used by researchers to develop rehabilitation tools, but they lack sufficient data acquisition capability   or are expensive, time-consuming and require extensive technical expertise  ","The Kinect v2 however is a relatively cheap, easily configurable off-the-shelf device capable of accurately tracking gestures and joint positioning  ","It detects the position, orientation and angular velocity of a players’ 25-joints through use of an infrared emitter and a colour camera which forms part of a skeleton tracking system to mirror the location of the player’s joints.   report that those body parts that are obstructed from the Kinect’s direct line of vision cannot be tracked, whilst   conclude that the Kinect v1 system can accurately measure gross unique characteristics.   have conducted a comparative study on motion tracking between Kinect and the OptiTrack optical systems and their work shows that Kinect can achieve a comparable motion tracking performance","Previous work exclusively using the Kinect in a rehabilitation context includes its use in the recovery of spinal muscular atrophy   who report a significant improvement in patient motivation, and   who devised a quantitative assessment of exercises performed by trauma brain injury patients",A number of researchers have utilised the Kinect in conjunction with other devices,These (expensive) studies include use of MoCap and treadmill system for rehabilitation and gesture recognition   and integration with a multiple camera 3D-motion analysis system  ,The latter study demonstrated that the Kinect can validly assess postural control in a clinical setting,In the present study we aim to bring a novel solution to the problems related to traditional physiotherapy and rehabilitation by using a cost-effective serious game where the core device is the Kinect v2 but integrated with a Myo armband,"The wireless Myo armband (Thalmic Lab) 
                       is a motion capture device that collects inputs from the user’s skin",The Myo is made of eight medical grade stanless steel electromyography (EMG) sensors that detect the electric impulses in the muscles,The armband is connected via a Bluetooth USB adapter that records/collects real-time data with a high accuracy and precision,"The Kinect-Myo apparatus is also linked to a Saitek FootPedal device 
                       which is connected to the computer via a USB port and a seated or standing player in order to simulate walking via the avatar; foot resistance is adjustable/configurable according to the required level of difficulty",Use of the FootPedal is reported here solely for information as it will be the subject of future development and is not the focus of the present study,This multi-input system outputs high quality data and provides the convenience of wireless transfer to provide a superior clinical grade source of medical data for muscular performance compared to alternative hardware studies,The players’ input is simultaneously transferred into a simulated virtual 3D-park via the Unity game engine with all files and data stored locally on a hard drive,"A Monte Carlo Tree Search algorithm (MCTS) generates virtual objects in the 3D-space and adapts game difficulty to the player’s ability in real-time.  
                       illustrates the architecture, peripherals, and algorithm used to design the system","Four game scenarios are developed; “Fruit-Collection” to grasp/ release virtual fruits in a virtual basket, “Button-Press” to reach/press virtual buttons for 3-s (the duration reflects an appropriate balance between playability and difficulty), “Sling-Shot” to knock-down virtual boxes and “Fruit-Collection with the FootPedal” to navigate between pre-established key points and collect virtual objects  ",The frequencies of the entire data collection were normalised to facilitate comparison between Kinect and Myo when continuously estimating and comparing arm orientation,The reliability and accuracy of the devices in measuring functional and clinically relevant movements of the upper limb were also investigated,"Use of a FootPedal is not significant to this study and only reported as work in progress for future developments to the game. 2 Methods 
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   2.1 Monte Carlo Tree Search (MCTS) MCTS is a probabilistic algorithm based on the random simulations of paths taken by an upper limb (combination of joint positions) to grow the tree (path) structure","It uses the “upper-confidence-bound-for-trees” (UCT) selection strategy to pick the highest victory ratio and construct confidence intervals  .  
                         (a) shows the four-stage algorithm that is broken down into  ,  ,  , and   and described in detail by  .  (b) shows a branch of the tree of the rehabilitation game structure represented by a ‘right hand’ with its child nodes",The algorithm iteratively builds a search tree until a predefined number of evaluations is reached,The search stops and the best performing root-action returned,The next action is chosen according to the stored statistics according to a balance between exploitation and exploration,"If the selected action is less promising, it continues exploration",Child nodes are added to grow the tree according to the available satisfactory weighting of actions,"A roll-out is performed when a predefined stop criterion is met, the score is backed-up to the root node, and the reward is saved","The pseudocode is presented in  . 
                      2.2 Game design In designing the game scenarios, a broad spectrum of rehabilitation exercises were devised using advice obtained through collaboration and consultancy from physiotherapists",The game was adjusted continuously throughout the development process in accordance to the experts’ feedback.  (a) shows a screenshot of the “Fruit-Collection” game; a 3D-virtual-park with virtual-fruits that are generated based on the MCTS algorithm,"The player interacts with fruits by grabbing them (show the palm to the Kinect, open/close the fingers) and then holds on to that fruit and releases it when the reach is above the virtual basket",A valid release condition is flagged to the player by a change from a flashing red to flashing green bottom surface,"Once released, gravity pulls the fruit down, it hits/lands in the basket, the score is achieved and recorded, and the fruit disappears  .  (a) compares the normalised elbow data taken from the Myo and the Kinect playing the “Fruit-Collection” game. 
                      
                         
                         (b) shows a screenshot of the “Button-Press” game",Virtual-buttons are generated in the 3D-park,"The player reaches and presses a button steadily for 3-s, and a virtual ring with green dots appears (one per second) for visual feedback",The 3-s steadiness is a default value based on our observation throughout the study that no frustration or fatigue was reported by the participants whilst still being challenging to achieve,"This sensitivity duration can however be is however be adjusted through the game menu. 
                         (c) illustrates the “Sling-Shot” game","It is made of a virtual-elastic-sling and a virtual ball that is controlled by the player, who pulls the sling with the ball and releases the ball to fly",The sling’s reaction force fires the virtual ball into the virtual space,Based on the amount of force applied and the direction pulled the sling’s colour changes (from yellow to red) to provide feedback,"If the combination of applied forces and the pull directions are appropriate, it hits the virtual boxes and scores. 
                         (d) illustrates the “Fruit-Collection and FootPedal” game where the avatar walks into the park to collect fruits spawned in various locations","Walking is managed by the FootPedal, and the player reaches the highlighted (dark yellow) locations in the virtual world to collect the fruits",When the player reaches a highlighted spot a virtual basket appears that is used to collect the rewards,The same rules are applied for grasp and release actions,The FootPedal algorithm measures the amount of pressure applied to the footrests and determines if the steps are taken in the right order (otherwise the forward movement would not take place),"The left/right feet push takes the avatar forward, and toe presses enable turning left/right",The footrest sizes are adjustable and have non-slip materials to hold the foot steady,"A left foot pressure is  , neutral is 0, and a right foot pressure is  ","If the average is negative the left foot is dominant if it is positive the right foot is dominant, and zero means both feet apply equal pressures.  (e) plots the trajectory of the FootPedal taken from a healthy subject",It shows that the wave oscillates symmetrically around zero and the mean is zero. 2.3 Subjects Twenty-three participants volunteered for this study (approved by the researchers’ affiliated institutional Research Ethics Committee),"Of this cohort 10 healthy subjects (with no known motor defects) were selected to form two Control Groups (CG:5 males, 5 females); 2 participants were post-stroke (PS: 2 males); 2 participants had Traumatic Brain Injury (TBI: 1 male, 1 female) and 9 participants had multiple sclerosis (MS: 3 male, 6 female)",The cohort had mean age of 37 (range is 25–64 years),In preparation for game play a Myo armband was located on the participant’s arm above and below the elbow joint,"To simplify the comparison of the two systems, the range of motions (ROM) and two-dimensional kinematics of the sagittal and frontal planes were considered where appropriate",The mean ROM and timing were collected based on the frames per second,"Subjects were also equipped with the FootPedal as required by the game. 2.4 Materials and data collection A 3D-avatar was designed with the skeleton joints using Fuse and Mixamo animation software. 
                          Before data collection, a camera was placed on a tripod at   above the floor to track timing and record players activities from behind",The Kinect was placed on a stand located on top of a   Curved-Ultra-Wide screen,The players performed the exercises in sit or stand positions within 1.5–2.0 m from the screen,"Movements in the games are the combination of abduction/adduction, flexion/extension, pronation/supination, shown in  (a)",The timing is measured based on wrist joint orientation for a proxy measure of functional reach tests,The Kinect calibration was done via the skeleton tracking system with slight modification to capture pronation/supination through wrist joint orientation,The associated algorithm adapts the avatars’ dimension to each player’s physical proportions,"The Myo data was obtained using Samy-Kamkars-myo-osc application 
                          with some modification to access the raw EMG data","The joint coordinate systems, linear/angular velocity, and orientations were collected from the devices anatomical landmarks  ","The orientation and rotation of joints are calculated based on the pitch (rotation about the Kinects’ x-axis), yaw (rotation about the y-axis) and roll (rotation about the z-axis)",Thus the orientations are determined based on the parent joints and the supporting joints depicted in  (b),Data were averaged from the Myo light emitting diode (LED) light positioned above and below the elbow joint facing the same direction,"The Myo armband device streams the accelerometer, gyroscope and magnetometer data with nine degrees of freedom (DOF) comprising a three-axis accelerometer, a three-axis gyroscope and a three-axis magnetometer","The gyroscope measures angular velocities which can be integrated to obtain the orientation, however this method accumulates an exponential error over time so that in this case the gyroscope can only offer an estimation of the attitude",To mitigate this effect the gyroscope data must be adjusted by the accelerometer and magnetometer to measure the orientation of a wearer’s arm and hand gestures  ,The Myo is made of eight medical grade stainless steel EMG sensors that detect the electric impulses in the muscles,The armband is connected via a Bluetooth USB adapter that records/collects real-time data with high accuracy and precision,Apart from the rules of the game no other particular instruction was given,"The type of the game, the initial difficulty level, the number of trials and timing were configurable via the main menu","The games were developed based on the requirements of compatible modules execution treatment, repetition of tasks, progressive assessment, and considering the needs and viewpoints of a number of involved stakeholders. 2.5 Data processing and statistical analysis Anatomical frames and associated joints are illustrated in  (b)",The joints are defined by a single point,"SR/SL (Shoulder Right/Left), ER/EL (Elbow Right/Left), WR/WL (Wrist Right/Left), FR/FL (Fingers), TR/TL (Thumb)",A hand’s angle is measured from the WR/WL to FR/FL and TR/TL,The shoulder angle is defined by the SR/SL and the SP (Spine) line that connects CS (Central Spine) to the CH (Central Hip),"At rest, the shoulders are perpendicular to the CS, SP and CH and the neck is aligned with the SP line",Muscle signals were collected by the Myo armband,To increase the fidelity of the data the maximum amount of noise was filtered using a fifth-order FIR smoothing Savitzky-Golay filter (Signal Processing Toolbox™) in Matlab  ,"The normalised original and filtered EMG data are illustrated in  
                         ",It compares the data taken from a healthy subject  (a) with a post-stroke subject  (b) while both were playing the “Fruit-Collection” game,The oscillation of the healthy subjects’ muscle signals shows the muscle activity while collecting the fruit in  (a).  (b) shows the signals occasionally reach a high value and then drops due to patients’ muscle fatigue,The ROMs and the kinematic mean were calculated for statistical analysis using the SPSS 24 package,The average bias between the devices was determined using two-sided  -tests over the repeated measurements,"Normality tests were conducted to measure the significant differences between the means for different sessions, games and groups","In particular, a Shapiro-Wilk’s normality test was calculated  ",Visual inspection of histograms and normal Q-plots were also performed,The relative agreement between the sensor devices was measured using Pearson’s correlation,The absolute accuracy was determined for sessions 1 and 2 using intra-class correlation coefficients ( ) based on a two-way ANOVA  ,Angular variation was assessed using limits of agreement analysis (LoA) with Bland–Altman plots  ,The coefficients of change of the error ( )   were determined to assess angular discrepancies,"Hierarchical regressions were performed to guide interpretation and calculation of the significant predictor using  - . 3 Results and discussion Analysis of data for CG and experimental group participants were conducted separately and tabulated in  
                      ",Bland and Altman plots show that there is no significant bias between the data taken from different games,The average ROM and standard deviation (SD) differences between various groups are due to the limited range of motion of patients compared to the CG and is expected,"The timing measurement data and motion of clinically relevant functional movements taken from the Kinect, Myo and also camera footage data are comparable",The reliability   is in a range from moderate to good [ ],The results from the Kinect relate strongly to those obtained from the Myo with a high Pearson correlation ( ),"However, intra-session reliability discloses some discrepancy in two devices’ capturing the grasp and release actions when the joints are covered by other body parts and not detected by the Kinect",Root mean square error (RMSE) of the Kinect intra-sessions were 11° and 9° for session one and session two respectively,In all cases the values measured by Kinect were overestimated compared to the Myo,The Bland Altman method was used to calculate the mean difference between the measurement of the two devices with   limits of agreements (LoAs) above  ,There was no significant inter-session differences in ROM and the  -  is   apart from the “Fruit-Collection-with-FootPedal” game,The   for ROM is in the range of 1.4–5.5 for all the games,"Comparison of the camera footage with the Kinect’s data showed that if players show the palm to the Kinect device while grasping, releasing or collecting virtual objects, Kinect measures and detects real-time data with a negligible error",Some occasional delays (mismeasurements) were detected with an error of less than 2.7%. 4 Conclusion The rehabilitation serious game is a feasible and safe system that could be used to enhance upper extremity limb function in patients with motor impairment,This system is developed to help patients to improve motor limitations and inspire physical rehabilitation,"Our results suggest that low-cost home-based devices (Kinect, Myo, and also FootPedal) can accurately measure the timing of movement repetition, Kinematic activity, and ROM",This arrangement can facilitate an inexpensive and home-based assessment and treatment strategy,"The statistical analysis has shown a significant improvement in participant performance throughout the trial period, reflected in response times and range of motion","Overall, the results of the current study are encouraging for the next generation of rehabilitation games for gait and balance",All participants with motor impairment showed high interest and engagement during the activities,The players demonstrated the positive feeling and improved moods while playing the game and afterwards because of the scores they achieved,"Activities were transferred to the 3D world via the Kinect and FootPedal, and the Myo armband was used for validity and correcting the hand’s pronation and supination",The activities seem to have helped the patients to gain a significant benefit by enabling the players to interact with virtual objects without requiring any head-mounted display,It enables them to achieve visual and real-time feedback on the screen,The designed algorithms allow data to be collected and transferred into the avatar,The MCTS configuration algorithm monitors and progressively corrects the abnormalities of the upper limb kinematic movement by expanding the tree,Overall there is some bias between the calculated ROM by the Kinect and Myo armband,"That is, the Kinect has slightly overestimated the ROM values compared to the Myo device",This is because the armband’s calculation is based on the peripheral measurements whereas the Kinect tracks single and central joint positions,"Although the differences exist, the applicable agreement was good","In this study, the Kinect’s inherent inaccuracy with hand gestures is resolved when the player interacted with virtual objects through hand’s palm and finger’s open/closing gesture","The study indicates that the Myo can be coupled with the Kinect to detect and track hand gestures with high accuracy, and the Kinect skeletal model’s limitation for assessment and data conveyance of hand’s pronation and supination can be corrected through the Myo gesture control armband",This study suggests that players could use the Kinect as the only single device to interact with the game and perform the rehabilitation activities,It also can be used in conjunction with the FootPedal without using Myo armband,The results showed a comparable inter-session reliability (acceptable to good)   over two repeated sessions,The Pearson correlation ( ) was high enough to determine the validity and reliability of using Kinect and Myo devices in assessing the clinically relevant movement of the upper limbs,Players reported that the 3D visualisation technique combined with the real-time mirrored and visual feedback helped them to correct themselves as well as improve their ROM,"They stated that training the physical functions through the system was stimulating, exciting and they could translate the skills learned in such therapy-like activities to everyday life",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0888613x15000857," 1 Introduction Humans mostly use words (in natural languages), which are inherently vague and qualitative in nature, to describe real world information, to analyze, to reason and to make decisions",Human knowledge is often expressed linguistically and thus involves fuzzy predicates,"Also, if human knowledge is expressed linguistically, different linguistic hedges are commonly used to express different levels of emphasis","For example, different degrees of tallness can be expressed using terms such as “very tall” and “rather tall”","In addition, in the real world applications, there are situations in which information may not be assessed in a quantitative form (i.e., in terms of numbers), but rather in a qualitative one (e.g., in terms of linguistic terms)",This may arise for different reasons,"In some cases, due to its nature, the information may be unquantifiable and thus can be stated only in linguistic terms","For example, when evaluating the “comfort” or “design” of a car, we may be led to use linguistic terms such as “good”, “medium” and “bad”","In other cases, precise quantitative information may not be stated since either it is unavailable or the cost of computation is too high, so a linguistic “approximate value” may be acceptable","For instance, when the rotation speed of an electric motor is evaluated, linguistic terms, e.g., “very large”, “large” and “medium”, may be used instead of numerical values","Therefore, there is a natural demand for formalisms that can directly work with linguistic terms and make use of linguistic hedges since such systems make it easier to represent and reason with linguistically-expressed human knowledge","Logic programming (LP) is a well-established, coherent formalism for knowledge representation and reasoning since it has a semantics in the sense of Tarski and sound and complete (or weakly complete) proof procedures, which are understandable by humans in terms of problem reduction and are computer-implementable  ",Traditional LP has a serious limitation in that it cannot cope with the issues of vagueness and uncertainty into which fall most modes of human reasoning,Fuzzy set theory   and its derived disciplines such as fuzzy logic   and possibilistic logic   are well known for providing the most widely-adopted techniques for managing vagueness and uncertainty,There is substantial literature on LP frameworks extended by these techniques to handle vagueness and uncertainty in knowledge representation and reasoning  ,"Fuzzy linguistic logic programming (FLLP), introduced in  , is an LP framework for managing vagueness in linguistically-expressed human knowledge, where truth of vague sentences is given in linguistic terms, and linguistic hedges can be used to express different levels of emphasis",It is an LP framework without negation under fuzzy logic in the narrow sense (FLn)  ,"In FLLP, each fact or rule is graded to a certain degree specified by a linguistic truth value, and linguistic hedges can be used as unary connectives in rule bodies","For example, a statement “(A car is considered good if it is   reliable and consumes   little fuel) is  ” can be represented by the following rule:  where  ,  ,  ,  ,  , and   stand for  ,  ,  ,  ,  , and  , respectively",Most concepts and results of traditional definite LP can have a counterpart in the framework,"The linguistic truth values are generated from primary terms   and   using hedges (e.g.,   and  ) as unary operations  ",The linguistic truth values and hedges have several intuitive natural semantic properties such as  ,Some approximate semantic equivalence in natural language holds in the framework,The procedural semantics of FLLP can directly manipulate linguistic terms to compute answers to queries,"Thus, the framework can provide a computational approach to human reasoning in the presence of vagueness","According to Zadeh  , linguistic hedges play a twofold role: in the generation of values of a linguistic variable from primary (atomic) terms and in the modification of fuzzy predicates","In FLLP, hedges are used to generate linguistic truth values from the primary terms","Also, since hedges are allowed to be unary connectives in rule bodies, they can play the role of predicate modifiers","In FLLP, up until now, there have been only two methods to compute answers to a query w.r.t. a logic program: (i) by bottom-up iterating the immediate consequence operator  ; or (ii) by using the procedural semantics","Nevertheless, on one hand, the former is exhaustive and not goal-oriented","Indeed, it requires computation of the whole least Herbrand model despite the fact that not all the results are required to determine the answer of the query","On the other hand, although the latter is goal-oriented, it may lead to an infinite loop (if the program is recursive) and may recompute atoms (subgoals) in rule bodies","Moreover, it typically does not give a most general answer to a given query","In the literature of logic programming, the   (also called   or  ) technique can be utilized to overcome these problems  ","The underlying idea is that subgoals and their associated answers are stored in an appropriate data space, called the  , so that they can be reused when a repeated subgoal appears during the resolution process","Whenever such a repeated call occurs, the associated answers of the subgoal are retrieved from the table instead of being re-evaluated against the program clauses","Tabulation-based query answering procedures are able to reduce the search space, avoid looping, and have better termination properties than SLD-based procedures  ","There have been several attempts at utilizing the tabulation technique to improve the efficiency and termination properties of the procedural semantics for logic programming dealing with vagueness and uncertainty, resulting in so-called  ","The procedures generate a set of  , called a  , and each tree computes answers for an atom","The procedures in   and   deal with propositional residuated logic programs and propositional multi-adjoint logic programs, whereas the one in   is for first-order residuated logic programs",The procedure in   is non-deterministic and not very efficient since it still creates redundant trees and nodes,"Moreover, its termination remains open","In this paper, we first adapt the non-deterministic tabulation proof procedure in   for FLLP",The purpose of this is to ease the subsequent proofs of soundness and completeness,We show the termination of the non-deterministic procedure and prove its soundness and completeness,"In addition to still generating answers to queries that are less general, the non-deterministic procedure also creates redundant trees and nodes","To overcome these problems, we then propose a deterministic procedure which is more efficient than the non-deterministic one in terms of the number and size of trees created","Moreover, the non-deterministic procedure gives all and only the most general answers to a given query",The tabulation rules of the deterministic procedure are an improvement of those of the non-deterministic one,We determine the priority orders in which the tabulation rules are selected to apply and the nodes are selected for application of each tabulation rule,The deterministic procedure is proved to terminate and to be sound and complete,"We also discuss the applicability of the deterministic procedure in problems such as threshold computation and top-  retrieval which can be used, for example, in a case when the number of answers becomes large due to a very large set of facts in the logic program","This paper is a substantially extended version of a conference paper  , in which we propose the deterministic procedure, show its termination, and prove its soundness",The remainder of the paper is organized as follows,Section   gives an overview of FLLP,"Section   presents the non-deterministic procedure, defining non-deterministic tabulation rules and the non-deterministic procedure, showing its termination, and proving its soundness and completeness","Section   presents the deterministic procedure, defining deterministic tabulation rules and the deterministic procedure, showing its termination and soundness, proving its completeness, giving an example of how it works, and discussing its applicability in threshold computation and top-  retrieval",Section   discusses related work,"Section   concludes the paper.   gives all proofs of lemmas and theorems of the paper. 2 Preliminaries In this section, first, the structure of linguistic truth domains utilized for FLLP is presented",A   formula is built from its constituents using a logical connective,"FLLP is  , i.e., the truth value of a compound formula is a function of the truth values of its constituents",The function is called the   of the connective,"Thus, then, operations, which can be truth functions of connectives, are defined on the truth domains","Finally, the language, declarative semantics, procedural semantics, and fixpoint semantics are presented. 
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   2.1 Linguistic truth domains and truth functions of hedges 
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                      2.1.1 Hedge algebras and linguistic truth domains In the theory of hedge algebras (HAs)  , values of the linguistic variable  , e.g.,   and so on, can be regarded as being generated from a set of primary terms   using linguistic hedges from a set   as unary operations","There exists a natural ordering among them, e.g.,   <  , where   means that   indicates a degree of truth less than or equal to  , and   if   and  ","Hence, a set   of values of   is a partially ordered set (poset) under the   (SOR) ≤","Let  , and   stand for  , and  , respectively","There are natural semantic properties of linguistic hedges and terms that can be formulated by means of the SOR as follows: (i) Hedges either increase or decrease the meaning of terms they modify, so they can be regarded as  , i.e.,  ","It is denoted by   if the hedge   modifies terms more than or equal to another hedge  , i.e.,  ,   or  ","Since   and   are disjoint, the same notation ≤ can be used for different order relations on   and   without confusion","For example, one has   (  if   and  ) since, e.g.,   and  . (ii) A hedge has a semantic effect on others, i.e., it either strengthens or weakens the degree of modification of other hedges","If   strengthens the degree of modification of  , i.e.,  ,   or  , then it is said that   is   w.r.t.  ","If   weakens the degree of modification of  , i.e.,  ,   or  , then it is said that   is   w.r.t.  ","For instance,   is positive w.r.t.   since, e.g.,  ;   is negative w.r.t.   since, e.g.,  . (iii) An important semantic property of hedges, called  , is that hedges change the meaning of a term, but somewhat preserve the original meaning","Thus, if  , where  , then  , where   denotes the set of all terms generated from   by means of hedges, i.e.,  , where   is the set of all strings of symbols in   including the empty one","For example, since  , one has  ","Also, if   and   are incomparable, all terms generated from   are incomparable to all terms generated from  ","For instance, since   and   are incomparable, so are   and  ",Two terms   and   are said to be   if   and  ,"For example,   and   are independent. 
                            
                         Axioms (A2)–(A4) are a weak formulation of the semantic heredity","Given a term  , the expression  , where   for  , is called a   of   w.r.t.   if  , and, furthermore, it is called a   of   w.r.t.   if  ","Let   be an artificial hedge called the   on   defined by  ,  ",The following proposition shows how to compare any two terms in  ,"The notation   denotes the suffix of length   of a representation of   w.r.t.  , i.e., for  , we have  , where  , and  . 
                            
                         A set of primary terms of a linguistic variable usually consists of two comparable ones with one being an antonym of the other","For the variable  , one has  ",HAs with such a set   are called   ones,"For a symmetric HA, the set of hedges   can be divided into two disjoint subsets   and   defined by   and  ","For example, given a set  ,   can be decomposed into   and  ","Two hedges in each of sets   and   may be comparable, e.g.,   and  , or incomparable, e.g.,   and  ","Thus,   and   become posets","Since  , the identity   is the least element in each of the sets   and  ","In  , an   (EHA) is an HA augmented by two artificial hedges Φ and Σ defined as   and  ,  ","Also, an HA is said to be   if   and  ,  ","For a free EHA with a non-empty set of hedges, we have   (the  ),   ( ) and   ( )","Furthermore, 0, W, and 1 are fixed points, i.e.,   and  ,  ","A symmetric HA   is called a   HA (lin-HA, for short) if   is decomposed into   and   that are linearly ordered","For a lin-HA  ,   is linearly ordered.  
                         An   
                             on   is defined based on those of  ,   as follows  :  ,   if: (i)  ; or (ii)   and  ; or (iii)   and  ","It is denoted by   if   and  . 
                            
                         A   
                             taken from a lin-HA   is the set  , which is linearly ordered  ","To have well-defined operations, we consider only finitely many truth values","An   HA, where   is a positive integer, is a lin-HA in which canonical representations of all terms w.r.t. primary terms have a length of at most  ","A linguistic truth domain taken from an  -limit HA is finite. 2.1.2 Truth functions of hedge connectives 
                            
                         In natural language, the following assessments can be considered to be approximately semantically equivalent: “It is   that Lucia is  ” and “It is   that Lucia is  ”",This result is also obtained when the fuzzy predicates and truth values are expressed by fuzzy sets  ,"This means that if the truth value of the atom  ( ) is  , then the truth value of  ( ) is  ","In our formalism,   is regarded as a unary connective applied to  ( )","By Condition  , given   being the truth value of  ( ), the truth value of  ( ) is  ","In other words, Condition   generalizes the above case for other hedges as well",Condition   expresses that   is non-decreasing,"By Condition  , given the set of hedges in  , since  , we have  ,   This is also in accordance with fuzzy-set-based interpretations of hedges  , in which   and   are called   while   and   are called  , and they satisfy the so-called   property 
                            :  where   is a fuzzy predicate","The models for fuzzy-set-based representations of linguistic hedges and terms proposed in   also satisfy the hypotheses imposed on hedges in  . 
                            
                         2.2 Operations In many-valued logics/FLn  , there are several prominent sets of connectives called Łukasiewicz and Gödel ones. Łukasiewicz t-norm, its residuum, and Łukasiewicz t-conorm can be, respectively, defined on a linguistic truth domain   with   as  : 
                         
                          Gödel t-norm, its residuum, and Gödel t-conorm can be, respectively, defined as: 
                         
                          All operations other than the residua are non-decreasing in all arguments",The residua are non-decreasing in the first argument and non-increasing in the second,They are used as truth functions of implication connectives in rules,"The t-norms are used to evaluate many-valued modus ponens, stating that from   and   (i.e., truth values of   and   are   and  , respectively) infer  ","Also, the t-norms (resp., t-conorms) can be used as truth functions of conjunction (resp., disjunction) connectives","Each t-norm and its residuum satisfy the   
                         :  and the following also holds: 
                      2.3 Fuzzy linguistic logic programming 
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                      2.3.1 Language The language is a many-sorted (or typed) predicate without function symbols with   denoting the finite set of all attributes",The reason why function symbols are not allowed in the language is that we want to make FLLP implementable,"With no function symbols, Herbrand universes of all sorts of variables of a finite logic program   are finite, and so is the Herbrand base of  . 
                             This, together with a finite linguistic truth domain, allows us to obtain the least Herbrand model of a logic program by finitely iterating the immediate consequence operator from the least Herbrand interpretation","Furthermore, thanks to this, the two tabulation proof procedures proposed in this paper always terminate","Connectives consist of the following:   (Gödel and Łukasiewicz conjunctions);   (Gödel and Łukasiewicz disjunctions);   (Gödel and Łukasiewicz implications); and hedges as unary connectives. 
                             For a connective  , its truth function is denoted by  ",A   is either a constant or a variable,"An   (or  ) is of the form  , where   is an  -ary predicate symbol, and   are terms","A   is defined inductively as follows: (i) an atom is a body formula; (ii) if   and   are body formulae, then so are  ,  , and  , where   is a hedge connective","A   is a graded implication  , where   is an atom called  ,   is a body formula called  , and   is a truth value different from 0 
                            ;   is called the   of the rule","A   is a graded atom ( ), where   is an atom called the logical part of the fact, and   is a truth value different from 0","In a graded formula  ,   is understood as a lower bound to the exact truth value of  . 
                             A   (program, for short) is a finite set of rules and facts, and there are no two rules (facts) having the same logical part, but different truth values","Therefore, a program   can be represented as a partial mapping:  where the domain of  , denoted  , is finite and consists only of logical parts of rules and facts","For each formula  ,  . 
                            
                         We refer to the   of a program  , which consists of all ground atoms, by   
                            . 2.3.2 Declarative semantics A   (interpretation, for short)   of a program   with a linguistic truth domain   is a mapping  ","The ordering ≤ in   is extended to interpretations pointwise as follows: for any interpretations   and   of  ,   iff  ,  ","An interpretation   extends to all ground formulae, denoted  , as follows: (i)  , if   is a ground atom; (ii)  , where   are ground formulae, and   is a binary connective; and (iii)  , where   is a ground body formula, and   is a hedge connective","For non-ground formulae, since all variables in formulae are assumed to be universally quantified,   is defined as:  where ∀  denotes the   of  ","An interpretation   is a   (model, for short) of a program   if for all formulae  , we have  ",A   (or  ) is an atom used as a question ?  prompting the system,"A pair  , where   is a truth value, and   is a substitution, is called a   for a query ?  w.r.t. a program   if, for every model   of  , we have  ","In fact,   is a correct answer for ?  w.r.t.   iff  , where   denotes the least model of  . 
                            
                         2.3.3 Procedural semantics Given a program   and a query ? , the procedural semantics uses   to compute a lower bound to the truth value of   under any model of  ","Admissible rules are defined as follows: 
                            
                         
                            
                         
                            
                         
                            
                         
                            
                         
                             
                             
                            
                         2.3.4 Fixpoint semantics Let   be a program","The immediate consequence operator   mapping from interpretations to interpretations is defined as  : for an interpretation   and every ground atom  ,  The Herbrand base   is finite, and for each  , there are a finite number of ground instances of rule heads and logical parts of facts which match  ","Thus, both   operators in the definition of   are, in fact, maxima.   is shown to be continuous",The bottom-up iteration of   is defined as follows:  where ⊥ denotes the least interpretation mapping every ground atom to 0,"The least model   is exactly the least fixpoint of  , denoted  , and can be obtained by finitely bottom-up iterating  . 
                            
                         3 A non-deterministic tabulation procedure We recall several basic notions of LP",An atom   (  or  ) an atom   if there exists a substitution   such that  ,Let   and   be expressions,"Similarly, it is said that   is   if   for some substitution  ","Also, it is said that   and   are   if there exist substitutions   and   such that   and  , i.e.,   and   are identical up to renaming of variables","We can say   is a variant of  , and   is a variant of  ","We now define the notion of an   for our tabulation procedures.   
                      
                   To compute answers to a query w.r.t. a program, the tabulation procedures generate a set of  , called a  ",Each tree computes answers for an atom  ,"Answers computed for   are stored in the   attached to the root node of the tree (for short, we can say the answer list of  )","Every node other than the root is of the form  , where   is a substitution, and   is an   consisting of t-norms, truth functions of connectives, atoms, and truth values in order to compute a truth value for  ","A formula   can be denoted by   where   are truth values, and   are atoms appearing in  . 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   3.1 Non-deterministic tabulation rules 
                         
                      As usual, variables occurring in program clauses used in   or   are renamed (standardized) apart so that the clauses do not have any variables which already appear up to the current step","This process is also applied to variables in answers used in  . 3.2 A non-deterministic procedure 
                          
                         
                      The notion of a   is redefined as follows.  
                      For the rest of the paper, we use only this definition of a computed answer. 3.3 Termination of the non-deterministic procedure Given any program   and a query ? , since   is finite and the language of   is function-free, there are a finite number of different atoms up to renaming of variables, which can be formed out of a finite number of predicate symbols and constants appearing in   and variables","Due to this, the following can be seen: First, thanks to the variant check in  , the number of created trees in the forest is bounded by the number of different atoms (up to renaming of variables) and is thus finite","Second, the number of child nodes (branches) of the root node of a tree for an atom is finite since it is, in fact, the number of program clauses whose rule heads or logical parts are unifiable with the atom","Third, due to a finite number of instances (up to renaming of variables) of an atom and a finite number of truth values, the number of different answers (up to renaming of variables) for any atom is finite","Also, thanks to the variant check of  , each answer cannot appear more than once in an answer list, so the number of times for   to add an answer is finite","Fourth, by  , the number of child nodes of any non-root node is finite due to the finite number of atoms occurring in it and the relevant answers","Fifth, also by  , the depth of each branch starting from a child of the root node of a tree is bounded by the number of atoms appearing in it and is thus finite","In summary, we can say that the numbers of trees, nodes, and answers in the forest are finite, and there is always the situation in which no rules can make any modification to the forest, i.e., the non-deterministic procedure terminates. 3.4 Soundness of the non-deterministic procedure 
                         
                      3.5 Completeness of the non-deterministic procedure A tabulation rule is said to   to the computation of an answer   in the answer list of the tree for an atom   if: (i) it is applied in the path from the root to the leaf generating the answer  ; or (ii) it directly contributes to the computation of the answers used by  's along the path",We can say that the tabulation rule directly contributes to the answer,"A forest created by the non-deterministic procedure is called a  .  
                      The following theorem shows the completeness result for the case of ground queries.  
                      Now in order to prove completeness of the non-deterministic procedure, we define several more notions","The   is the non-deterministic procedure in which the substitutions   in  ,  , and   do not need to be mgu's, but are only required to be unifiers","An   is a forest created by the unrestricted non-deterministic procedure. 
                         
                      
                         
                      
                         
                      
                         
                      A   forest is a forest to which no tabulation rules can make any modification","The following theorem shows that all complete non-deterministic forests for a query w.r.t. a program give the same set of answers, up to renaming of variables.  
                      The following theorem, which is a stronger completeness result for the non-deterministic procedure, follows immediately from  .  
                      This theorem, together with  , essentially states that the non-deterministic procedure gives correct answers, including the most general and less general ones, to a given query w.r.t. a logic program. 4 A deterministic tabulation procedure As shown in  , the non-deterministic procedure still generates redundant answers to a given query w.r.t. a logic program and redundant trees and branches as well","Therefore, in this section, we propose a deterministic procedure which is more efficient than the non-deterministic procedure in terms of the number and size of trees created","Moreover, it gives all and only the most general answers to a given query w.r.t. a logic program. 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   4.1 Deterministic tabulation rules and a deterministic procedure 
                         
                      The underlying idea behind   is that   is the greatest truth value that the expression   can give, and since there exists a more general answer in the answer list, the branch can be stopped","Moreover, with the subsumption check instead of variant check in  , it will create fewer trees than the one of the non-deterministic procedure","Also, the subsumption check in   and   will eliminate all less general answers and thus prevent   from generating redundant branches","Hence, we can say that the tabulation rules of the deterministic procedure is an improvement (optimization) of the ones in the non-deterministic procedure","In order to make the deterministic procedure more efficient, we try to further reduce the number and size of trees in the forest","To that end, we will determine the following aspects of the deterministic procedure: (i) the priority order in which the tabulation rules are selected to apply to the forest; and (ii) the priority order in which the nodes of the forest are selected for application of each tabulation rule","For the former,   is just applied once at the beginning of the procedure, and the remaining tabulation rules can be divided into two groups: (1) Group 1: { ,  ,  }, and (2) Group 2: { ,  }","First, the rules in Group 1 should have a higher priority than those in Group 2 since they do not create any more nodes","Next, it can be seen that in order for   and   to take effect, we need some general answers in the answer lists as soon as possible","Thus, in Group 1,   should have the highest priority","Also, since   stops the expansion of branches of trees, it should have a higher priority than  ","Finally, regarding the rules in Group 2, since   is needed for   to take effect and creates only one new node, not one new tree like  , it should have a higher priority","Therefore, the   (from the highest to the lowest) in which the tabulation rules should be selected to apply to the forest is  ,  ,  ,  , and  ","At any step, if a higher priority rule is not applicable to the forest, the next priority rule will be considered","For the latter, the idea is that we try to pursue the most promising branch first in order to obtain a general answer as soon as possible before considering other branches, so: 
                         
                      
                         
                      
                         
                      
                         
                      4.2 Termination and soundness of the deterministic procedure Clearly, once an answer has been removed from an answer list by  , it cannot be added again by  ","With this, the termination of the deterministic procedure follows immediately from that of the non-deterministic procedure","Soundness of the deterministic procedure can be proved similarly to that of the non-deterministic procedure, and, in fact, it has been proved in Le  .  
                      4.3 Completeness of the deterministic procedure A forest created by the deterministic procedure is called a   forest.  
                      The following corollary immediately follows from  .  
                      The following theorem immediately follows from   and  .  
                      This theorem, together with   and the subsumption check in  , essentially states that the deterministic procedure gives all and only the most general correct answers to a given query w.r.t. a logic program. 4.4 An example of how the deterministic procedure works Given the program in  , it can be noticed that there are no disjunctions in program rules","Hence, we do not use any answer in the form of   for  ","Given a query  , the deterministic procedure works as follows (in  
                         , nodes are numbered in chronological order, and changes in answer lists are also indicated): 
                         . applying   to   creates a tree with root node (1) and two children (2) and (3);  . applying   to atom   in the node (3) creates a tree with root node (4) and two children (5) and (6);  . applying   to node (5) adds an answer   to the answer list;  . applying   to atom   in node (3) using answer   creates node (7);  . applying   to atom   in node (3) creates a tree with root node (8) and two children (9) and (10);  . applying   to node (10) adds answer   to the answer list;  . applying   to node (9) adds answer   to the answer list;  . applying   to atom   in node (7) using answer   creates node (11);  . applying   to node (11) adds answer   to the answer list;  . applying   to atom   in node (3) using answer   creates node (12);  . applying   to node (12) stops it by answer  ;  . applying   to atom   in node (3) using answer   creates node (13);  . applying   to atom   in node (6) creates a tree with root node (14) and three children (15), (16), and (17);  . applying   to node (16) adds answer   to the answer list;  . applying   to node (15) adds answer   to the answer list;  . applying   to atom   in node (17) using answer   creates node (18);  . applying   to node (18) adds answer   to the answer list;  . applying   to the answer list of node (14) removes   from the list;  . applying   to atom   in node (6) using answer   creates node (19);  . applying   to node (19) adds answer   to the answer list;  . applying   to atom   in node (17) using answer   creates node (20);  . applying   to node (20) adds nothing since   is already in the answer list;  . applying   to atom   in node (13) using answer   creates node (21);  . applying   to node (21) adds answer   to the answer list;  . applying   to atom   in node (6) using answer   creates node (22);  . applying   to node (22) adds nothing to the answer list;  . applying   to atom   in node (3) using answer   creates node (23);  . applying   to atom   in node (23) using answer   creates node (24);  . applying   to node (24) adds nothing to the answer list;  . applying   to atom   in node (2) creates a tree with root node (25) and two children (26) and (27);  . applying   to node (27) adds answer   to the answer list;  . applying   to node (26) adds answer   to the answer list;  . applying   to atom   in node (2) using answer   creates node (28);  . applying   to node (28) adds nothing to the answer list;  . applying   to atom   in node (2) using answer   creates node (29);  . applying   to node (29) adds nothing to the answer list","At this point, since no rules can make any modifications to the forest, the procedure terminates","From three answers  ,  , and  , we have three computed answers  ,  , and   for  , respectively","In comparison to the non-deterministic procedure in  , we can see that the deterministic procedure does not create a redundant tree for  , and it removes redundant answers like   in answer lists as soon as a more general answer appears","More precisely, at Step   of this example, we follow   to apply   to atom   in node (3), creating a tree for it","Otherwise, we would apply   to atom   in node (7) first, creating a tree for it, and later we have to create another tree for   as in  ","This shows that the deterministic procedure is more efficient than the non-deterministic one. 4.5 Dealing with conflicting answers In other real examples, it is possible that, for instance, one rule leads us to conclude that fuel consumption of a PT will be high, and another leads us to conclude that its fuel consumption will be low","Intuitively, these answers are (partially) conflicting (see, e.g.,  )","This kind of conflict can occur in FLLP, for example, in one of the following cases: (1) we use only one fuzzy predicate  , denoted  , but not  , denoted  , and two different branches of the tree for   give us two answers, e.g.,   and   (the latter means that high fuel consumption of the PT is (at least)  )","In this case, the rule   will remove the latter since it is less general than the former, and we have only one answer  ; or (2) we use two different fuzzy predicates   and   and obtain two answers   and   for them","Since FLLP is an LP framework without negation, we do not consider   as a negation of  , and they are independent of each other","Therefore, we still have the two answers in the answer lists of   and  . 4.6 Applicability of the deterministic procedure The deterministic procedure can have applicability to various problems of information retrieval, e.g., threshold computation and top-  query answering. 
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      4.6.1 Threshold computation Threshold computation is the problem of finding answers to a given query w.r.t. a logic program such that the computed truth value is not less than a particular threshold",This is the case when one is interested in looking for only “good enough” answers to the query,"For example, given the program in  , the problem of finding answers to query   with a threshold   (resp.,  ) has no result (resp.,  )","In order to effectively solve this problem, we need to cut branches of the tree for the query that cannot generate an answer having a truth value greater than or equal to the threshold as soon as possible","Given a program  , a query ? , and a threshold  , one of the possible methods is to add an artificial answer   into the answer list of the tree for  , where   is the greatest truth value which is less than  , if there does not exist such a computed answer in the list",It can be seen that   is the most general answer for   which has a truth value less than  ,"Thanks to this answer,   will stop the development of branches which cannot generate an answer with a truth value greater than or equal to  , and   will not add such answers","For instance, given the program in  , to find answers to the query   with a threshold  , we add an artificial answer   to the answer list of the tree for  ","Of course, this artificial answer must not be used in   and  ","It can be seen that a rule   can be replaced by two rules   and   since the truth function of Gödel disjunction   is maximum, and the deterministic procedure will give the same answer(s) for  ","However, for Łukasiewicz disjunction, the truth value of   can be greater than the maximum of that of   and that of  , so the rule   cannot be replaced by two rules   and  ","In a case where there is no disjunction in a program rule whose head is unifiable with  , the threshold can be propagated to trees created for subgoals in the rule body","Consider a child node of the root of the tree for   created using a program rule  , where the truth value   (otherwise, it is stopped by  ),   has no disjunction, and   is an mgu of   and  ","Thus, the node is  , where   is obtained from  ",Assume that   are all atoms appearing in  ,The computed truth value of   must not be less than  ,"It can be easily verified that if the implication ← is the Gödel one, then  ","If it is the Łukasiewicz one, then   with  ,   and   being values in the truth domain","Since  , we have  , and if   (i.e.,  ), we have  ","We recall that without disjunctions, rule bodies can be built from their components using the Gödel conjunction, the Łukasiewicz conjunction, or hedge connectives","Therefore, the new threshold   for the components can be worked out as follows: (i) For the case of the Gödel conjunction,   is the new threshold for each of the components; (ii) For the case of the Łukasiewicz conjunction, the new threshold for each of the components is  ","The case   occurs, for example, when truth values of all facts in   are less than 1 (thus, so is the computed truth value for any component in  ); (iii) The problem of finding an answer to a hedge-modified formula   with a threshold   can be reduced to that of   with a new threshold  ","Therefore, we can determine a threshold   for each atom  ,  , and   can be used in the tree for  ",The process can be continued for subgoals appearing during the creation of the trees for atoms   if the condition is satisfied for the rules whose heads are unifiable with them,"Also, if the condition holds, we can propagate the already obtained answers of   to all its subgoals   (and further if applicable) as thresholds to stop redundant branches in the trees for  . 4.6.2 Top-k query answering The top-  retrieval problem, i.e., determining   best answers to a given query w.r.t. a logic program, becomes important when the set of facts becomes large","Consequently, the number of answers may become large as well, and we are likely interested in top-  answers only","In fact, it may be the case where many answers are of very low truth values, so we are interested only in a number of answers with the highest truth values","Similar to threshold computation, the problem of finding top-  answers can be done by adding an artificial answer   into the answer list of the tree for the query ? , where   is the greatest truth value which is less than the truth value   of the current  th best answer, if there does not exist such a computed answer in the list","Whenever an answer is added to the answer list, this artificial answer is updated accordingly","The threshold can also be propagated to the trees for subgoals, as mentioned in threshold computation. 5 Related work 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   5.1 Vagueness versus uncertainty and their underlying theories In many cases, real world information to be represented and reasoned with in a knowledge representation system is imperfect, i.e., vague and/or uncertain","As a consequence, the management of vagueness and/or uncertainty in such systems is an important issue and has attracted considerable attention of researchers (see, e.g.,  )","In the literature, sometimes the term “uncertainty” is used in a wide sense, covering the underlying notions of the theories of fuzzy sets, fuzzy logic, possibility and probability","However, in essence, such notions as well as their underlying theories are distinct and should be used for different purposes","The following are some basic differences among them  : (i) In modeling imperfect knowledge, at the semantic level, weights attached to sentences may have two distinct meanings: they are either   or   (or  )","In the former case, logical sentences are fuzzy/vague and can have an   truth value lying between   (1) and   (0)","Thus, the algebraic setting of fuzzy sentences is not Boolean","In the latter, most of the time, truth remains binary, and weights may express the more or less strong inability, for an agent, to know whether a sentence is   or  ","In other words, the algebraic setting remains Boolean and the weights are located at the meta-level","In general, uncertainty is due to incomplete knowledge about the truth of sentences, whereas graded truth is due to their vagueness. (ii) The underlying theories for dealing with vague sentences are often many-valued logics/FLn   while those for uncertain sentences can be possibility theory and possibilistic logic   or probability theory and probabilistic logic  ",FLn is a discipline derived from fuzzy set theory,"In FLn, degrees of membership which graduate the property of being a member of a fuzzy set are interpreted as degrees of truth of a sentence expressing this property","The standard set of truth values is the real unit interval   with its natural ordering, but one can work with different domains, finite or infinite, linearly or partially ordered","Almost all systems of FLn are  , i.e., the truth value of a formula is evaluated along the complexity of the formula using truth functions of connectives","However, the degree of uncertainty of a compound formula cannot be a function of those of its constituents, i.e.,  ","The main difference between probability and possibility theories is that the probability of an event is the   of the probabilities of all worlds that satisfy this event, whereas the possibility of an event is the   of the possibilities of all such worlds","Intuitively, the probability of an event aggregates the probabilities of all worlds that satisfy the event, whereas the possibility of an event is simply the possibility of the “most optimistic” world","Hence, although both probability and possibility theories allow for quantifying degrees of uncertainty, they are conceptually quite different from each other","That is, probability and possibility theories represent different facets of uncertainty. 5.2 Managing vagueness in logic programming In order to deal with vagueness, LP is usually extended with many-valued logics/FLn","There have been numerous frameworks (see, e.g.,  ) proposed under this direction","Essentially, the frameworks differ in (i) how truth values are associated with rules and facts; (ii) how truth values are managed; and (iii) the underlying truth space","Based on (i), the frameworks can be classified into   (AB) and   (IB)  . 
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                      5.2.1 Implication-based approach In this approach, rules are associated with a truth value, which is sometimes called the   or   of the rule, and so are facts",The underlying logic is usually truth-functional,Most work deals with logic programs without negation,"More concretely, a rule is of the form:  where  ,  , are atoms,   is the degree of truth of the rule, and @ is an operator combining atoms","Computationally, given a fuzzy interpretation   of  's by truth values in a truth space, the truth value of   is computed by taking the value of the truth function   of the truth values of   and then somehow “propagating” it to the rule head",One of the most well-known frameworks in this approach is Vojtáš's fuzzy LP (FLP)  ,The truth space is the unit interval   with its usual ordering,"A fuzzy logic program is a finite set of fuzzy rules of the form   and facts, which are graded atoms of the form  ",The   @ covers all ranges of fuzzy conjunctions and disjunctions,"The truth function   of an implication connective ← and its   
                             satisfy the residuation property","A declarative semantics, a fixpoint semantics, and a procedural semantics of fuzzy logic programs are provided",FLLP is under this approach,"More precisely, FLLP is developed by extending or modifying FLP on the following aspects, among others: (a) using a linguistic truth domain instead of the unit interval  ; (b) allowing rule bodies to use hedge connectives; (c) adding an admissible rule for handling hedges in the procedural semantics; and (d) providing detailed proofs of the mgu and lifting lemmas, and the completeness theorem",Another well-known LP framework under this approach is van Emden's quantitative LP (QLP)  ,"A quantitative rule is a special case of a fuzzy rule, in which ← is the product implication (with the product t-norm as its residual conjunctor), @ is the conjunction taking Gödel t-norm as its truth function","If  , the rule is equivalent to a fact   in FLP",The semantics of QLP can be captured by the semantics of FLP in the sense that a (Herbrand) interpretation   satisfies a rule in QLP iff   is a model of the corresponding rule (or fact) in FLP,"Therefore, it can be said that QLP is a special form of FLP both syntactically and semantically","Another special case of FLP is Łukasiewicz logic based Prolog  , which is an extension of Prolog based on Łukasiewicz logic with a finite set of truth values  , where  ","An implication clause is a special case of a fuzzy rule, where ← is Łukasiewicz implication, and @ can be a combination of non-decreasing binary connectives",The proof procedure works similarly to the immediate consequence operator of FLP,"Also, the FPROLOG interpreter in   can be seen as an instance of FLP in which the truth values of rules are always 1","The fuzzy Prolog in   can be classified into the IB approach since its rules have a weight, and its facts have a truth value","A weight can take values in  , and a truth value can be in  ",The implication is interpreted as in classical logic,"Inference is based on a fuzzy resolution principle, which is an extension of the resolution principle in classical logic by introducing the concepts of confidence and confidence of resolvent",This approach is based on an heuristic technique to compute weights of rules,"Other frameworks in this approach include monotonic and residuated LP  , multi-adjoint LP  , and fuzzy semantic web rule language  ","The framework in   can also be classified into the IB approach. 5.2.2 Annotation-based approach In the AB approach, a rule is of the form:  where   is an n-ary computable function, and annotations  's are either a constant or a variable ranging over an appropriate truth/uncertainty domain","Rule   asserts that the truth value of   is at least (or is in)  , whenever the truth value of   is at least (or is in)  ,  ","That is, truth values are associated with each component of an implication rather than the implication itself, and implications are interpreted in a “classical logic” fashion","In practice, frameworks under this approach can also be used to handle uncertainty whose underlying notion is degree of belief/uncertainty",Generalized annotated LP (GALP)   is a fundamental formalism in this approach,A truth space   is an upper semilattice with an order relation ≤ and the least upper bound operator ⊔,A   (GAP) is a set of annotated clauses of the form  ,There are two alternative model-theoretic semantics for GAPs called   and  ,Two fixpoint operators are provided for two kinds of semantics,"The one for the general semantics is continuous while the one for the restricted semantics is monotone, but, in general, not continuous","An SLD-style proof theory is developed for GAPs based on the general semantics, and answering a query w.r.t. a GAP ends up with solving a constraint",The proof theory is sound and complete for   queries,"Nevertheless, there is no sufficiently general proof procedure for the restricted semantics","GALP is claimed to unify, and in some cases, to generalize results and treatments of other annotated LP frameworks such as Subrahmanian's  , Kifer and Li's  , Blair and Subrahmanian's  , Kifer and Lozinskii's   as well as some other multi-valued LP frameworks. 5.2.3 Comparison between the two approaches The following can be seen: (i) while the way implication is treated in the AB approach is closer to classical logic, the IB approach is an elegant extension of traditional LP since it allows one to explicitly represent and reason with partial truth; (ii) the AB approach is more expressive than the IB",It is known that any IB framework can be simulated within an appropriately-defined AB framework with multisets as the basis of its semantics  ,"On the other hand, AB frameworks can also be embedded into IB frameworks  . (iii) the disadvantage of the AB approach is that its query processing, especially, resolution and fixpoint evaluation, is more complicated than that of the IB approach","Moreover, the fixpoint operator in the AB approach is, in general, not continuous, so the least fixpoint may not be reached in   steps, while the computation for many IB frameworks can be done in time polynomial  ","In summary, it is believed that the IB approach is easier to use and is more amenable to efficient implementation. 5.2.4 Logic programs with negation for handling vagenuess In the literature of LP for handling vagueness, there is little work which addresses the issue of  ",Logic programs with non-monotonic negation are usually referred to as  ,The frameworks in   can be classified into the IB approach,Answer set programming (ASP) is a form of declarative programming oriented toward difficult search problems,It is based on the   ( ) semantics of normal logic programs  ,Fuzzy answer set programming (FASP)   is an extension of ASP with FLn for modeling continuous optimization problems,"FASP allows   literals to occur in rule bodies, and FASP programs can contain  , which are rules with no atom in the head",Atoms are associated with a truth value (weight),"In FASP, the weight of each rule is 1","In aggregated FASP (AFASP)  , rules can have a weight other than 1","Nevertheless, like other frameworks with negation, the query answering of (A)FASP is based on the fixpoint semantics, which is not goal-oriented and thus not very efficient. 5.3 Linguistic reasoning based on hedge algebras Linguistically-expressed human knowledge normally consists of vague sentences and their degrees of truth which are also expressed in linguistic terms","A vague sentence can be expressed by an expression  , where   is a variable or a constant,   is a fuzzy predicate, and   is a linguistic analog of the classical predicate","For instance, the sentence “Mary studies  ” can be represented by  ",An   is a pair   where   is the linguistic truth value of the sentence  ,A knowledge base is a finite set of assertions,A vague sentence   is sometimes denoted by   for short,"In the HA-based linguistic reasoning methods  , new assertions are deduced from a given knowledge base using inference rules of the form:  where  , are premises, and  ,  ,  , are conclusions","These methods can be classified into the class of approximate reasoning in Zadeh's sense  , since it also lacks the depth of precise reasoning (no soundness and completeness results are given)","The linguistic reasoning methods defined in   are based on monotonous HAs, a subclass of lin-HAs in which every hedge in   (resp.,  ) is positive w.r.t. every hedge in   (resp.,  ) and negative w.r.t. every hedge in   (resp.,  )","The inverse mapping   of a hedge   in this work, which is analogous to the hedge function   in this paper, is defined by the following conditions: 
                          does not satisfy the semantic entailment  , e.g., since  , by  ,  , and by  ,  , thus  ",Linguistic propositional and first-order logics proposed in   also have a truth domain based on lin-HAs,"The resolution methods in these works associate each clause with a degree of reliability, which is also specified by a linguistic truth value. 5.4 Tabulation technique for logic programming In the literature, the tabulation technique has been utilized for query answering in logic programming frameworks  ","It has been shown that tabulation-based query answering procedures are able to reduce the search space, avoid looping, and have better termination properties than SLD-based procedures","For example, SLG resolution is guaranteed to terminate for all logical programs with the bounded term-size property such as Datalog programs  ","There have been several attempts at utilizing the tabulation technique to improve the efficiency and termination properties of the procedural semantics for logic programming dealing with vagueness and uncertainty, resulting in so-called  ",The procedures in   and   deal with propositional residuated logic programs and propositional multi-adjoint logic programs,A termination property   states that the procedures terminate for any query w.r.t. a logic program if the least fixpoint of the immediate consequence operator of the program can be obtained in finitely-many steps,"In  , the tabulation technique is further combined with a thresholding technique to avoid the generation of redundant trees and nodes which would certainly yield lower computed truth values than the current ones",The procedure in   is for first-order residuated logic programs,"Nevertheless, it is non-deterministic and not very efficient since it still generates redundant trees and nodes","Moreover, its termination is still left open. 5.5 Further results on fuzzy linguistic logic programming 
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                      5.5.1 Fuzzy linguistic datalog Information to be stored in databases is often fuzzy, i.e., vague and/or imprecise","Two important issues in research in this field are the representation of fuzzy information in a database and the provision of flexibility in database querying, especially via including linguistic terms in human-oriented queries and returning results with matching degrees","Deductive databases attempt to combine LP and relational databases to construct systems that are powerful, fast and able to deal with large volumes of data  ","On one hand, LP can handle recursion, but usually processes one tuple at a time and is thus inefficient w.r.t. large volumes of data","On the other hand, algebra of relational databases does not support recursion, but can deal efficiently with large databases by using set-at-a-time operations",The main aim of deductive databases is to provide an extension of relational algebra which includes support for recursion in a manner that allows the efficient handling of large datasets,Datalog   is such a data model for deductive databases,"An extension of Datalog, called   (FLDL), is developed based on FLLP in  ","An FLDL program is, in fact, a fuzzy linguistic logic program with the usual restrictions that every rule is safe, and an   (EDB) predicate, which occurs only in rule bodies and facts, cannot simultaneously be an   (IDB) predicate, which is defined by rules","A fuzzy predicate can be interpreted as a  , which can be represented in the form of a (crisp) relation with an extra attribute to store a linguistic truth value for each tuple","A  , which is a collection of fuzzy linguistic relations, can be used to store relations of EDB predicates in an FLDL program.   (FLRA) is a collection of relational operations to manipulate data in such fuzzy linguistic databases",The   of an FLDL program is defined as its least Herbrand model as usual,"From the   point of view, relations for IDB predicates are computed from relations for EDB predicates","A translation algorithm is used to convert rules into FLRA expressions which produce a relation for each IDB predicate in rule heads, based on relations for predicates in rule bodies",The collection of such expressions is proved to be equivalent to the immediate consequence operator in deriving new data from the current data,The expressions also give rise to a set of equations which are formed by assigning the obtained expression for an IDB predicate to its relation,"The semantics of an FLDL program, which is the least fixpoint of the associated immediate consequence operator, is also the least fixpoint of the set of equations","Based on this, two algorithms for bottom-up evaluation of the   of an FLDL program from the relations for EDB predicates called   and   are given","These algorithms take linear time. 5.5.2 Implementation of fuzzy linguistic logic programming In  , a method to translate FLLP programs into pure Prolog programs, which can be safely executed inside any standard Prolog interpreter, is presented",This is inspired by the translation technique in FLOPER (Fuzzy LOgic Programming Environment for Research)  ,Queries are also converted into Prolog goals and then evaluated against the translated code,The returned computed answers are pairs consisting of a truth value and a substitution,"Consider the following program:  Instead of computing with truth values, we can work with their indexes in the truth domain","Hence, Program   can be coded as: 
                            
                             where 33 and 41 are, respectively, the indexes of the truth values   and   in the truth domain in  ","During the parsing process, pure Prolog code is generated as follows: (i) Each atom appearing in a fuzzy rule is translated into a Prolog atom, whose arguments are of the original atom augmented by an additional one, an anonymous truth variable of the form  , which is intended to store the truth value obtained in the subsequent evaluation of the atom. (ii) Truth functions of binary body connectives can be easily defined by standard Prolog clauses as follows: 
                            
                            
                             where   is the index of the truth value 1 (in  ,  )",Truth functions of hedges can be defined by listing all cases in the form of Prolog facts,"For example, the ones in   can be defined as: 
                            
                            
                            
                             where 25 is the index of the truth value  ",The fact  . represents the case  ,"The facts  . and  ., where   is a variable of hedges, respectively, represent the cases ∀ ,   and  . (iii) Each fuzzy rule is translated into a Prolog clause in which the calls to the atoms appearing in its body must be in an appropriate order","More precisely, the call to the atom corresponding to an operation must be after the calls to the atoms corresponding to its arguments in order for the truth variables to be correctly instantiated, and the last call must be to the atom corresponding to the t-norm evaluating the rule","For example, the rule in Program   can be translated into the following Prolog clause: 
                            
                            
                         (iv) Each fuzzy fact is translated into a Prolog fact in which the additional argument is its truth value","The facts in Program   are translated into Prolog facts  . and  . (v) A fuzzy query is translated into a Prolog goal, in which a non-anonymous truth variable, used to store a computed truth value, is added as an extra argument of the atom","For instance, the query   is translated into a Prolog goal:  ","Given Program   and the above query, a Prolog interpreter will return a computed answer  , i.e., we have  . 5.5.3 More Results on Fuzzy Linguistic Logic Programming In  , we prove a number of additional results of FLLP, which can be seen as a counterpart of those of traditional definite logic programming (TDLP)  ","Let ⊗ and ⊕ denote the   (or infimum, greatest lower bound) and   (or supremum, least upper bound) operators, respectively","For all interpretations   and   of a program   and for all ground atoms  , we have: (i)  , and (ii)  ","A counterpart of the   in TDLP is stated that if   is a non-empty set of models of a program  , then   is a model of  ","Moreover, the least model   can be characterized by the greatest lower bound of the set of all models of  , i.e.,   is a model of  }","Also, if we consider FLLP as a logical system of FLn and a program   as a fuzzy theory, then for each body formula  ,   is a model of  }, i.e.,   is the   of   over   in the sense of Pavelka  ","Furthermore, for all  ,   is a computed answer for ?  w.r.t.  }, i.e., if one considers a computation as a proof, then   is the   of   in the sense of Pavelka","Therefore, we have a   
                             for all ground atoms, which states that the truth degree and provability degree of a ground atom w.r.t. a program coincide","Nevertheless, we do not have a similar result for non-ground atoms","In addition, aggregation operators, which subsume all kinds of fuzzy conjunctions and disjunctions, can occur in rule bodies",Aggregation operators are very useful since they enable us to describe increased fulfillment of user requirements,"A conjunction is one extreme case where one desires that all the criteria be satisfied, and a disjunction is the other extreme where the satisfaction of any of the criteria is all one needs","Indeed, rule bodies of FLLP can be extended with aggregation connectives whose truth functions are non-decreasing and can directly act on linguistic truth values, for instance, the linguistic ordered weighted averaging (LOWA) operator defined in  ","Furthermore, an arbitrary body formula may occur as a query in FLLP, and we can obtain the Pavelka-style completeness for all ground ones. 6 Conclusion In this paper, in order to overcome the drawbacks of the existing query answering methods for FLLP, we develop two tabulation proof procedures",The non-deterministic procedure is close to the procedural semantics and is presented in order to ease the proofs of soundness and completeness,It is proved to terminate and to be sound and complete,"Nevertheless, the non-deterministic procedure still generates less general answers to a given query and creates redundant trees and nodes","Therefore, the deterministic procedure is proposed to solve these problems",We optimize the tabulation rules and determine the priority orders in which the tabulation rules are selected to apply and the nodes are selected for the application of each tabulation rule,The deterministic procedure is more efficient than the non-deterministic one in terms of the number and size of trees created,"Moreover, it gives all and only the most general answers to a given query w.r.t. a logic program","The deterministic procedure has applicability in several problems of information retrieval such as threshold computation and top-  query answering, which can be used, for instance, in a case when the number of answers becomes large due to a very large set of facts in the logic program","Appendix A Proofs 
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s095741741830215x, 1 Introduction Text classification is a construction problem of models which can classify new documents into pre-defined classes ( ),"Currently, it is a sophisticated process involving not only the training of models, but also numerous additional procedures, e.g. data pre-processing, transformation, and dimensionality reduction","Text classification remains a prominent research topic, utilising various techniques and their combinations in complex systems","Furthermore, researchers are either developing new classification systems or improving the existing ones, including their elements to yield better results, i.e. a higher computational efficiency ( )","Literature overviews of text classification usually reveal its crucial elements, techniques, and solutions, proposing the further development of this research area","Nevertheless, the existing reviews are still useful as they address the significant problems of text classification ( )","However, these works are slightly outdated as they do not include the latest studies","Furthermore, their explanation of text classification has some limitations, for example, they lay emphasis only on machine learning techniques or algorithms, omit some essential elements of text classification, or focus on a particular research domain ( )","We reiterate here that these are excellent works, which are still useful to the research community","However, with the increasing interest in the area of text classification, we need the most recent systematic overview to better understand what has been achieved in this field","In this study, we aim to overcome the difficulties mentioned above","Moreover, the article presents a latest and holistic summary of text classification",We direct significant effort to generate a research map for text classification to assist in recognising its main elements by examining both the most recent and former studies,"More specifically, in addition to the understandable requirement to complement the existing reviews, the objectives of this study are as follows:
 
                   According to the best of our knowledge, there are no similar recent studies in the form of an overview of the investigated field","Furthermore, we believe that this study significantly systematises and enhances the knowledge regarding the modelling of classification systems",The results of the text classification process with its elements are particularly relevant,"Moreover, we show that it is possible to identify, explore, and develop new aspects of text classification or alternatively upgrade its existing components","In addition, our study constitutes a relevant and modern complement to the current reviews",The paper is structured as follows.   presents a comprehensive description of the existing reviews,"Next,   describes the text classification process and explains the review procedure","Then,   explains the problems, objective, and components of text classification via a qualitative analysis.   introduces a quantitative analysis of the text classification journals, including conference proceedings","Finally,   concludes the research study. 2 Related works The accessible reviews mostly describe and focus on the following five elements of the text classification process: (1) document pre-processing, i.e. tokenisation, stop-word removal, and stemming or lemmatisation, (2) document modelling, i.e. representing a document in an appropriate form, to be processed by a machine learning algorithm, (3) feature selection and projection, (4) machine learning algorithm utilisation to construct a classification model or function, and (5) quality indicators and evaluation methods.  
                       lists all these elements and the review works directly related to them","In addition, the table notes dataset issues and domain-specific difficulties. 
                       have previously enumerated and described the text classification steps, namely, pre-processing, vector space model creation, dimensionality reduction (feature selection and projection), training of a classification function, and performance measurement","In their work, text classification was used to present several schemas of feature weighting, e.g","Boolean, term frequency, inverse document frequency, and entropy","Moreover, the authors explained three feature selection methods, namely, document frequency thresholding, information gain, and  
                      -statistic, and one feature projection method, namely, latent semantic indexing (LSI)","In addition, they summarised and elucidated six machine learning methods: Rocchio’s algorithm, naive Bayes, k-nearest neighbour, decision tree, support vector machine (SVM), and ensemble learning, including bagging and boosting algorithms","Furthermore, they described the performance measures for binary, multi-class, and multi-label classification tasks","They also provided a Reuters-21578 dataset, suitable for different classification experiments. 
                       described text processing similarly to  , but they provided more examples","Additionally, the authors presented a more in depth discussion on each element of the text classification process","Their paper starts with a description of the classification problems, software, and examples of the domains in which text classification is commonly used","In the article, first, the authors introduce (1) feature selection methods, including the Gini index, information gain, mutual information, and  
                      -statistic, and (2) feature projection methods, such as different types of LSI, supervised clustering for dimensionality reduction, linear discriminant analysis, and generalised singular value decomposition","Second, they describe the following different types of classification learning algorithms: decision tree, rule-based classifiers, naive Bayes (multivariate Bernoulli and multinomial models), SVM, regression-based classifiers, neural network, and proximity-based classifiers (k-nearest neighbours, Rocchio’s)","In addition, they discuss ensemble learning techniques, including simple committees, boosting, and bagging","Finally,   explain the measures of accuracy of the classification process",We also notice some implicit and explicit observations made in their paper regarding the classification techniques,"These observations are related to, for example, different types of classification tasks and their solutions and performance of the linear classifiers","Moreover, it is worth mentioning that   address the interesting problems of linked and web data classification",The last two works of   and   in   present a classification process similar to the studies mentioned above,"However, the authors focus on solutions for domain-specific problems.   widely discussed the issue of spam filtering","In their work, we notice an extended description of text processing elements, such as datasets, document representation (e.g. n-grams techniques), performance measurement, and comparative studies of spam filtering methods","Moreover, the authors state and describe the problem of a concept drift, which other researchers have not considered",The paper by   explains the approach for improving systematic medical reviews,"For this purpose, the authors briefly describe all the elements of the classification system and present a text corpus for the experiments for the evaluation of the proposed system",All the works discussed above contribute significantly to text classification,"However, the description of the text classification process needs further improvement","We assumed that we can deliver a more comprehensive, holistic, and organised schema, including a dictionary, to deliberate on the text classification problems","Moreover, in our view, there is a requirement for a broader but not necessarily a deeper perspective of the text classification process which focuses on appropriately grouped domain keywords","Based on this perspective, we may better understand the level of advancement which has been attained in the field of text classification","Moreover, the text classification process herein is more sophisticated and complex than the previously discussed examples","It includes six primary elements: (1) data acquisition, (2) data analysis and labelling, (3) feature construction and weighting, (4) feature selection and/or projection, (5) model training, and (6) solution evaluation",All these elements should be better justified,"Furthermore, the techniques related to each component should be classified appropriately","Finally, it is worth mentioning that a learning method may utilise different learning algorithms to train a classification model or function used to classify documents into defined classes","Numerous well-described and well-known machine learning algorithms are suitable for this purpose, e.g. artificial neural network, k-nearest neighbour method, decision tree, decision rules (rule-based classifiers), naive Bayes, cost-sensitive selective naive Bayes, and support vector machines, etc. ( )","Therefore, herein, a discussion on this issue is not presentence; instead, we focus on more general learning approaches which, in our view, have not been well-justified in the reviews mentioned above. 3 Overview of a review method In this section, we present a brief outline of the baseline framework for text classification along with our review method for the studies. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   3.1 Text classification framework The investigated baseline process for text classification includes the six elements mentioned in  .  
                          presents a flowchart of this process, which we discuss briefly below","As we can see in  , the classification process starts with data acquisition from various text sources, including internal datasets, the Internet, and open databases","From the data acquisition, we obtain a dataset representing a physical or business process","Next, the dataset is pre-processed to generate a representation required by the selected learning method",This non-trivial issue consists of two phases,"First, the features are constructed from the data","Then, they are accordingly weighted with the selected feature representation algorithm to yield the appropriate data representation","Then, the number of features are reduced by the feature selection method","Subsequently, the reduced features are projected onto a lower dimensionality to achieve the optimal data representation","Following this, different learning approaches are used to train a classification function to recognise a target concept","When a classification model is adequately developed, it can classify incoming data that have to be represented in a manner similar to in the training phase","Consequently, the classifier produces a decision that defines the class of each input vector","Technically, the decisions are probabilities or weights","Finally, the evaluation procedure is utilised to estimate the text classification process operation","All the above phases are the main elements of the framework for text classification. 3.2 Review method We apply a review method which included three phases, namely, (1) article acquisition, (2) analysis of the quality of the articles, i.e. assigning them to the appropriate steps of the text classification process, and (3) quantitative analysis of the articles.  
                          presents the above phases and their sub-steps",It can be seen from the figure that the phase of article acquisition is composed of three sub-steps,"First, the article sources are selected","In practice, we utilised Springer and Elsevier sources most extensively","In addition, we used ACM and IEEE repositories but much less broadly","We have to mention that we excluded arXiv which is undoubtedly a well-known repository, but mostly it includes only pre-print manuscript versions",They are merely moderated instead of being reviewed,We should be aware that some authors submit their works to arXiv and a peer-reviewed journal concurrently,"If a particular manuscript fails a review pipeline, arXiv includes only the unverified study containing questionable results","Since the results of our study are the guidelines in text classification, we based research only on repositories, which contains thoroughly reviewed and well-established journals","For these reasons, arXiv could not be included in the literature review",We made an exception from this rule only for one literature review paper,"Second, the search constraints regarding the article type and publication year are set","We focused only on regular papers published in journals, rather than conference articles","Occasionally, we included books and conference papers if there was the lack of relevant articles from journals or they were strongly related to text classification",The baseline range of the publication year was between 2013 and 2018,A lower range was used if a query did not return any relevant articles,"In the final sub-step, for each primary element of text classification, several keywords which describe them most accurately are selected","Keywords with the constraints are included in queries that are sent to search engines, and accordingly, their responses are acquired","The achieved articles are stored for manual review including analysis.  
                          presents the request examples for each investigated category and our preliminary comments regarding the returned results","In the phase of qualitative analysis of the articles, we manually reviewed the collected papers","In addition, we created a taxonomy in this step",Each article was assigned to the appropriate element of the text classification framework,"Thus, we generated a database of classified articles","This database is freely available as an open dataset repository. 
                          In some cases, an article might be related to more than one category",The phase of quantitative analysis of articles included a quantitative study of the research on text classification,It was based on the collected manuscripts and their taxonomy,"Thus, we could identify the elements of the classification framework which were explored most thoroughly. 
                          also lists the request examples for baseline keywords",We used more general terms when the requests could not find articles,"Therefore, we could highlight research topics that were at no time extensively investigated in the articles on text classification","Thus, we consider the above keywords as the basic dictionary or vocabulary of text classification. 4 Qualitative analysis of text classification studies This section extensively outlines the literature related to text classification","More specifically, we explain the problem and objectives of text classification","Furthermore, we discuss the main components of the text classification process, learning methods, and evaluation approaches","The conducted analysis establishes the methods which are used in text classification and identifies the still unexplored areas of the application. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   4.1 Problem and objectives of text classification Text document classification (text classification) is the problem of assigning predefined classes (labels) to an unlabelled text document",Numerous studies present different approaches and applications of text classification,"The various categories of text classification are domain, classification purpose, classification task, general approach, and dedicated approach.  
                          presents some details about these categories","First, we found that several works, including surveys, applied the classification task only in a particular domain, where the domain is considered as a source of the textual data for the classification.   lists several domains, such as industry, finance, and medicine","Second, the studies which were determined to have an explicit reason for text classification were placed in the classification purpose category","For example, the reasons may be recognition of commercial or non-commercial websites and academic or researcher home pages","Third, the classification task category includes articles related to the topics that consider binary, multi-class, multi-label, and hierarchical classification problems","Briefly, a binary classifiers model a two-class problem and classify objects into one class",A multi-class classification problem models a classification problem with more than one class,A binary classification problem is a multi-class classification problem with two classes,"In multi-label classification, a classifier attempts to assign multiple labels to each document, whereas a hierarchical classifier maps text onto a defined hierarchy of output categories",The hierarchical classification problem may be seen as a multi-class classification problem (each node is a class) and a multi-label classification problem (each path from the root to the leaf is a set of labels for the object of that node),"Fourth, we added the works focused on the general approaches to text classification to the class called the general approach",The group includes for example studies on tuning naive Bayesian classifiers and creation of a lightweight text classifier,"Finally, in the last category, dedicated approach, we placed some papers dealing with particular or dedicated text classification problems","In the last group, we have studies resolving the text classification problem based on well-defined text data sources such as an SMS dataset for spam filtering and Hadiths. 4.2 Qualitative analysis of studies In this subsection, we discuss the elements of text classification based on the framework depicted in  . 
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                      4.2.1 Data acquisition The data acquisition stage is executed when we do not already have the necessary data","In this phase, we acquire data required to solve a stated research objective, i.e. an assumed research hypothesis related to a classification task","There are several open data sets, e.g","Reuters, TDT2, WebKB, and Newsgroup ( ).   have enumerated as well as used in their research 47 different datasets",We have to underline that datasets related to solving particular classification purpose are publicly available,"For example, datasets from STS-Test, HCR, and IPhone6 were created to detect views in short text messages ( ). 4.2.2 Data analysis and labelling During the phase of data analysis and labelling, mainly labelled data-sets are prepared",There are two strategies for data labelling: labelling groups of texts and assigning a label or labels to each text part,"The first strategy is called multi-instance learning ( ), whereas the second one includes different supervised methods ( )","The results of this phase are employed in the succeeding stages. 4.2.3 Feature construction and weighting In this phase, the labelled data set is represented appropriately for a learning algorithm.  
                             shows the basic steps of this phase","Two well-known representations of textual data are:
 
                         Both representations are based on features and their weights","In the literature, there are numerous approaches for their generation","The most distinctive feature types are as follows:
 
                         Only after the features are selected, numerical values can be assigned to them",The feature weighting issue including its impact on text classification is widely discussed in the literature,"Currently, there are well-known and widely used schemes of feature weighting in the text processing and classification fields","Moreover, there are some recent and probably less known feature weighting methods.  
                             lists both the older and latest schemes of feature weighting. 4.2.4 Dimensionality reduction The dimensionality reduction phase is performed when numerical values are assigned to features","This step can be seen as a sort of data compression, which mainly is realised in two steps","First, the feature selection methods select the most important features","Next, the feature project techniques transform an existing feature space into another one ( ).  
                             shows the different types of dimensionality reduction. 
                            
                            
                            
                            
                            
                         
                            
                            
                            
                         Feature selection The feature selection techniques retain only the most relevant/descriptive features or dimensions and discard the remaining","The standard review approach classifies them into three main groups, namely ( ) filter, wrapper, or embedded feature selection methods",The filter methods utilise a feature ranking function to select the best features,The ranking function assigns a relevance score based on a sequence of examples,"Intuitively, a more relevant feature will be higher in rank","Subsequently, the  -top features are retained or the  -worst features are removed from the dataset",The wrapper methods are general-purpose algorithms employed for searching the space of feature subsets,"Furthermore, the wrapper methods test the performance of each subset using a learning algorithm","Finally, the feature subset that yields the best performance is selected for use",The embedded feature selection methods learn which features contribute most to the accuracy of a model while the model is being created,Some learning algorithms (e.g. decision trees) include an embedded feature selection method so that it becomes an implicit part of their learning process,"Some of the well-known state-of-the-art implementations of the above techniques are Fisher ranking, correlation coefficients, mutual information, normalised punctual mutual information,  
                               , Kolmogorov–Smirnov test, Mann–Whitney   test, LASSO, elastic net, and ridge regression ( )","Also, we can enumerate solutions that were projected in special for text classification, like:
 
                            In addition to the above approaches, we can examine the following other types of feature selection:
 
                            The research field of feature selection is extensively explored by researchers, and elaborated methods are widely applied in practice",Instance selection methods are also known apart from feature selection,"In this approach, the space of instances rather than the space of features is reduced ( )",Feature projection Feature projection methodologies project the existing features onto different dimensions,The aim here is to obtain new data axes so that the new dataset structure and its variance retain the original dataset structure as closely as possible ( ),"Herein, we list several multidimensional scaling techniques, including (1) Convex sparse PCA (CSPCA) ( ), (2) Imprecise spectrum analysis (ISA) for a linear spectral analysis of documents ( ), (3) t-Distributed stochastic neighbour embedding (t-SNE) ( ), (4) LSI ( ), which is based on singular value decomposition (SVD), (5) Kernel principal component analysis (nonlinear PCA) ( ), (6) Linear discriminant analysis (LDA) ( ), (7) Linear principal component analysis (PCA) ( ), and (8)  ","It is worth mentioning that feature projection uses cases that are extensively discussed in the literature, for example,
 
                            4.2.5 Training of classification models Several learning approaches are used to train a classification function to recognise a target concept",This stage is executed only if an appropriately prepared training set is available,"Training algorithms can be grouped into supervised, semi-supervised, ensemble, active, transfer, or multi-view learning approaches. 
                            
                            
                         
                            
                            
                         
                            
                            
                         
                            
                            
                         
                            
                            
                         
                            
                            
                         Supervised learning It refers to any machine learning process that trains a function by using data comprising of examples (labelled data) that have both input and output values ( )",Semi-supervised learning It uses both labelled and unlabelled data to perform a supervised or an unsupervised learning task,"This learning is also known as self-training, co-training, learning from the labelled and unlabelled data, or transductive learning ( )",Web page classification is widely discussed in the text classification field as a conventional example of this learning method,"It uses a co-training schema based on two views, such as text content of a web page including the anchor text of any web page linking to this web page ( )",Ensemble learning It refers to procedures employed to train multiple classifiers by combining their outputs by considering them as a “committee” of decision makers,"Various approaches accomplish this learning concept, for example, bagging, boosting, AdaBoost, stacked generalisation, mixtures of experts, and voting based methods ( )",Active learning This term is used to refer to a learning problem or system where a training algorithm has some role in determining the data it will use for training,"In this case, the algorithm is allowed to query a data provider to label an additional subset of training instances ( )",Transfer learning It refers to the ability of a learning mechanism to improve the performance for a current task after having learned a different but related concept or skill from a previous task,This type of learning is also known as inductive transfer or transfer of knowledge across domains ( ),"Multi-view learning It is also known as data fusion or data integration from multiple feature sets, multiple feature spaces, or diversified feature spaces that may have different distributions of features","This is a case where the data views are conditionally independent sets of the features, and each view has a specific statistical property ( )","Such learning aims to learn a single function by modelling a particular view, and then to jointly optimise all the functions to improve the generalisation performance ( )","We can enumerate articles, for instance  ,  ,  ,  , related to the topic of multi-view text classification","Furthermore, the most similar works in this field focus on semi-supervised learning","This learning utilises the co-training method mentioned earlier to resolve a given multi-view classification task ( ). 4.2.6 Solution evaluation Once a classification model is formed, we can choose and establish indicators to measure its performance.  
                             shows the three phases of the model evaluation","First, we measure a single indicator or group of indicators","There are several state-of-the-art statistical indicators, including precision, recall, accuracy,  -score, specificity, area under the curve (AUC), and error rate, etc. ( )",They are described at a macro or micro level ( ),"Their computation procedure is related to the problem of classification tasks, i.e. binary, multi-class, and multi-labelled ( )","In addition, we may select more performance-oriented indicators, such as CPU time training, CPU time testing, and memory allocated to the classification model ( )","Second, we establish a method to measure or estimate the indicator values","Subsequently, we determine the method for dividing the available dataset or datasets","For this purpose, procedures, such as leave-one-out and ten-fold cross-validation may be used ( )","Third, all the measured values are summarised and compared","In this phase, we may utilise several methods, such as (1) different plots to visualise and compare the results, e.g. receiver operating characteristic (ROC) ( ), (2) statistical tests to summarise and examine used processes of text classification ( ), and (3) a multi-criteria decision making approach (MCDM) to generate the ranking for used solutions ( )",These basic steps produced a well-known experiment plan to evaluate the performance of different methods from text classification,"Furthermore, an experiment plan with selected evaluation techniques is strongly related to a chosen research objective. 4.3 Summary of qualitative analysis We can enumerate the following main conclusions based on qualitative analysis of each stage of text classification process:
 
                      5 Quantitative analysis of studies In this section, we describe the quantitative analysis conducted of the studies related to text classification. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   5.1 Research questions Because journals including conferences are platforms where scientists share and discuss their results, so that the quantitative analysis, is a type of analysis of research forums",Such analysis provides the opportunity to display a quantitative map of the research topics,"More specifically, it assists us in obtaining the answers for the following research questions:
 
                      We attempt to provide answers to the above questions by qualitatively analysing our dataset that contains 242 selected articles, as presented in  ","For this purpose, we present and discuss the distribution of manuscripts by their research topics, publication time, and journals. 5.2 Research topics The articles are grouped into nine categories as presented in  
                         ",It includes a bar chart showing their percentage share in the entire dataset,"We analyse 233 articles, the omitted 9 articles are reviews or unclassified materials","Each category in   consists the following number of articles, respectively:
 
                      Based on  , it can be easily noticed that the most prominent research topic in text classification is related to features selection, construction, weighting, and projection","Over 2/5ths of the studies deal with these issues, and approximately 1/5th of the papers are dedicated to learning methods, which shows the distinctive focus of the researchers on this problem",The same can be stated for various approaches for the construction of classification systems including their application areas because 1/4th of the works deal with this topic,"The remaining issues, namely, evaluation methods, document representation, and instance selection are found in 1/5th of the studies","Among this group, only evaluation methods have a considerable contribution. 
                      In   we present the distribution of articles in classification research topics over publication years",Some trends can be distinguished if we scrutinise the number of articles in classification research topics over publication years,"In the recent years, one may notice the predominant interest in feature construction and selection; learning methods; and classification systems and application areas","Among them, two topics, i.e. (i) learning methods and (ii) classification systems and application areas seem to be worth exploring for scientists despite the course of years","Conversely, feature projection gains less attention currently than before",The remaining categories do not show observable patterns. 5.3 Publication time All the 242 articles from our dataset are grouped into fifteen categories,Each category represents a publication year,"The papers published earlier than 2005 are grouped into a category called  ≤ 2005.  
                          presents a bar chart of the distribution of articles by publication time","In  , 2/5th of the articles are published in 2016 and 2017",A continuous decrease in the number of published works can be observed from 2015 onward,"However, there are some irregularities, such as a more significant decrease in 2012 or sudden increase in 2010 and 2006","It is apparent that in recent years, researchers pay more attention to text classification",One-fifth of the manuscripts are from 2017,"Simultaneously, 2016 is the year the most prolific for the publications","Moreover, it is highly distinctive that the number of works doubles between 2016 and 2017",One-fifth of the papers appear before 2005,Since 2005 we observe almost a linear increase in the number of works each year until its profound increase in 2016,"Accordingly, we can state that text classification is a well-known issue that has a long tradition of research. 5.4 Journals In the next part of our quantitative assessment, we systematise the articles into twenty five categories",Each category represents the different journals in which the articles are published,The journals or conference series containing only one article are grouped into one category named  ,The   category includes journals such as  ,"The   category also includes conference proceedings, such as the  , and  ","The research sample contains 217 articles because the 25 articles published in book chapters were excluded from our dataset.  
                          presents a bar chart of the article distribution by journals including conference proceedings. 
                         shows that there are three most distinctive journals in text classification, i.e.  , and  ",They cover approximately 1/3rd of the publications from our dataset,"In contrast, 1/3rd of the works originate from journals or conference proceedings with only one publication in the dataset",The distribution shows a significant dispersion of the research discussion forums in the text classification,"In between these two extreme observations, there are several journals including conference proceedings, each containing between 1% and 4% of the works",Together they cover over 1/3rd of the studies. 5.5 Distribution of publications by countries and a cooperation network We were able to identify country names in 218 publications from our database,Only 24 papers without a country name we had to exclude from analysis,We have to note that a country name means an affiliation country of an author,An extraction process produced 690 different affiliations with many repetitions of a country name,"As a result, we identified 42 unique country names.  
                          and  
                          show distribution of publications by countries and a cooperation network between them, respectively",The percentage distribution of authors by their affiliation countries is shown in  ,This statistic also reflects the number of papers in text classification by countries,"However, it does it only to some extent because these works may have many co-authors affiliated to various countries","Based on  , we can conclude that nowadays the superior position in text classification takes China, as 1/4th of authors comes for this country",The second country is The United States having 1/8 of authors in this research area,The third position goes to Brazil,We must underline that these numbers say nothing about the quality of studies,They cannot show in which country appeared the breakthroughs in text classification,An interesting issue is a cooperation across countries in research on text classification,"Papers usually have more than one author, and they may originate from different countries","Thus, we can easily produce a diagram showing a cooperation network between countries by analysing affiliation countries of authors ( )","By scrutinising this network diagram, we can distinguish three types authors",The first group forms researchers who work only with co-workers for their country,"In  , these countries lay in the outer semi-circle",The second group comprises scientists who publish with at least one co-author from another country,These countries are mainly located in the outer sphere of the most prominent cooperation cluster in  ,The third group are scientists who worked on papers with co-authors originating from more than two countries,These countries are mainly laid in the centre of the cluster in  ,"Unsurprisingly, they correspond to the countries with the highest number of publications as shown in  ","The thicker a line connecting countries is, the more co-work has been noticed",The most distinguishing are links between China (including Hong Kong) and The United States; and China and Australia,"However, it impossible to find any pattern linking scientists based on their geographical location or known similarities between countries, or historical issues",It seems that researchers ignore political and historical conditions and focus only scientific issues. 5.6 Distribution of the number of authors in publications The 242 articles from our dataset are analysed in term of author numbers,"We have 743 different names of authors with repetitions.  
                          presents a bar chart of the distribution of the number of authors in publications. 
                          shows how many authors usually have papers on text classification",About 1/3rd of studies are prepared in the team of three people,"Whereas, 1/4th and 1/5th of works contain two and four authors respectively","As we can notice, the majority (3/4th) of studies are carried out in teams composed of 2–4 researchers","Only about 1/10th of papers have a sole author and 1/15th are prepared in teams of four or more people. 5.7 Summary of quantitative analysis At the beginning of this section, we posed several research question","Herein, we answer these question by summarising the quantitative analysis as follows:
 
                      
                         
                      
                            
                            
                         Additional quality remarks Despite the quantitative analysis, we express our subjective view on the quality of these sources",The option is based on our overall impression acquired during the literature review,"In our view, the crucial and relevant sources are the  , Springer computer science series (for example,  ), and different journals published by ACM or IEEE","Furthermore, we can enumerate some top conferences excluded from our bibliography but which have a significant effect on an investigated area","The main conferences are the following:  , ACM series of international conferences called   or  ","In addition, the   conference series may be useful to identify new concepts. 6 Conclusions In this study, we reviewed the works dealing with text classification","Accordingly, we wanted to achieve three objectives, namely, (1) to extract the most crucial phases of the text classification process, (2) to qualitatively analyse each phase to identify the common techniques, and (3) to quantitatively analyse studies to observe some trends","By realising the first objective, we identified the most distinctive phases in the process of text classification, which we called the text classification framework","The framework included several elements, namely, (1) data acquisition, (2) data analysis and labelling, (3) feature construction and weighting, (4) feature selection and projection, (5) training of a classification model, and (6) solution evaluation","In addition, we constructed a vocabulary allowing us to query search engines for papers corresponding to various sub-fields of text classification",The framework and vocabulary together constitute a map of this research topic,The map should assist researchers for promptly addressing their questions and realise the complexity of the topic,"On accomplishing the second objective, we examined the literature related to each phase of text classification",The studies were based on applying algorithms in particular domains,"Alternatively, they deal with the methods development in a more general way",Researchers published several well-described and highly tested datasets available to everyone for testing new approaches,Most of them were labelled for supervised learning,The studies usually utilised simple data examples,Multi-instance labelling was rather uncommon,"Textual data could be represented in a vector or graph form, with the first form being the most prominent",We have to highlight that document representation is the vital issue for classification quality,"There are some recent methods for feature construction, e.g","Word2vec, GloVe, and their modifications that outperform the classification results in some cases ( )",Some authors advocated that it is advisable to reduce the feature dimensionality,"For this purpose, they applied the PCA, SVD, and LDA algorithms the most frequently",Various algorithms can be used for classifiers training,"They can be grouped into several types, including supervised, semi-supervised, ensemble, active, transfer and multi-view learning",The most frequently explored approach was the supervised learning,"Trained models were usually assessed by using a ten-fold cross-validation procedure with quality indicators such as precision, recall and  -score","However, some studies used more sophisticated measures","The third objective involved the quantitative analysis of the studies based on the research topics, publication date, and publication location",We found that in recent years there has been an observable increase in the number of papers on text classification,"In the group of journals, including  , and  , the discussion regarding this topic was notably lively; whereas, others journals were not particularly focused on it","It was noted that researchers primarily paid significant attention to the issues of features selection, construction, weighting, and projection, as well as to the training or construction of models",It can be concluded that text classification is a well-developed research topic,"Simultaneously, it is also a prominent subject in which new approaches can be discovered and the findings can be utilised in various domains",Several issues have not been thoroughly addressed yet,"We did not find works explicitly related the problems of over-fitting of text classification models, or transfer, multi-view learning, and dynamic selection classifier, which is the most promising approach for multiple-classifier systems ( )","Moreover, the concept drift which is strong related to data stream analysis ( ) requires more research attention","In our opinion, the current hottest topics are multi-lingual or cross-lingual text classification ( ), text stream analysis ( ), opinions or sentiments analysis ( ), and ensemble learning method ( )","Furthermore, many works are related to embedding features to create better semantics vocabularies for classifiers learning ( )","Because our study has some limitations, there is scope for further research","First, we omitted a description of the machine learning algorithms because this topic has been widely discussed in other literature reviews","However, a new systematic comparison of the algorithms may be relevant to some readers","Second, it would be worth exploring additional publication sources, including ACM and IEEE",We omitted these databases owing to our licence limitation,"Also, we excluded arXiv since it mostly includes pre-prints which are only moderated but not peer-reviewed","Third, currently, text streaming is an emerging topic worth analysing","More information on it can be found in  ,  ","In addition, classification techniques other than machine learning has been considered in the following reviews","For example, some researchers, constructed classification models based on fuzzy logic which were more interpretable than those based on machine learning ( )","Finally,  ,   enumerated several tools for text classification, namely, OpenNLP and Standford solutions","More recently, tools based on Spark, Python, or R-project have also been developed",It appears to be a good idea to present all such tools in a systematic manner,"While being aware of the advantages and disadvantages of our study, we believe that it will assist other scientists and professionals to conduct and publish their studies in the area of text classification.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0925231214000265, 1 Introduction A tensor is a multidimensional array which is the higher-order generalization of vector and matrix,"It has many applications in information science, computer vision and graph analysis  ","In the real world, the size and the amount of redundancy of the data increase fast, and nearly all of the existing high-dimensional real world data either have the natural form of tensor (e.g. multi-channel images and videos) or can be grouped into the form of tensor (e.g. tensor face  )","Therefore, challenges come up in many areas when one confronts with the high-dimensional real world data","Tensor decomposition is a popular tool for high-dimensional data processing, analysis and visualization",Two particular tensor decomposition methods can be considered as higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP)   and the Tucker  ,"Tensor decomposition gives a concise representation of the underlying structure of tensor, revealing when the tensor data can be modeled as lying close to a low-dimensional subspace","Although useful, they are not as powerful","For general tensors, tensor decomposition does not deliver best low rank approximation, which will limit its applications","In this paper, we will try to recover a low-n-rank tensor from a subset of its entries",This problem is called the tensor completion problem,It is also called missing value estimation problem of tensors,The problem in computer vision and graphics is known as image and video in-painting problem  ,The key factor to solve this problem is how to build up the relationship between the known elements and the unknown ones,"Owing to this reason, the algorithms for completing tensors can be coarsely divided into local algorithms and global algorithms","Local algorithms   assume that the further apart two points are, the smaller their dependence is and the missing entries mainly depend on their neighbors","Thus, the local algorithms can only exploit the information of the adjacent entries","However, sometimes the values of the missing entries depend on the entries which are far away and the local algorithms cannot take advantage of a global property of tensors","Therefore, in order to utilize the information of tensors as much as possible, it is necessary to develop global algorithms that can directly capture the complete information of tensors to solve the tensor completion problem","In the two-dimensional case, i.e. the matrix, the rank is a powerful tool to capture the global information and can be directly determined","But for the high-dimensional case, i.e. the tensor, there is no polynomial algorithm to determine the rank of a specific given tensor","Recently, based on the extensions of trace norm for the minimization of tensor rank, some global algorithms   solving the tensor completion problem via convex optimization have been proposed",Liu et al.   first proposed the definition of the trace norm of an  -mode tensor as  ,"And similar to matrix completion, the tensor completion was formulated as a convex optimization problem","For tackling this problem, they developed a relaxation technique to separate the dependant relationships and used the block coordinate descent (BCD) method to achieve a globally optimal solution",The contribution of this paper is realized at the methodological level by considering a more general kind of the tensor completion problem,"By the extension of the concept of Shatten-q norm for matrix, Signoretto et al.   defined tensor Shatten-{ ,  } norm, which is formulated as   and consistent with that for matrix","Compared to the trace norm defined in  , Shatten-{ ,  } norm is a more general tensor norm and the trace norm of tensor can be seen as a special case of Shatten-{ ,  } norm  ","Though the general tensor Shatten-{ ,  } norm was defined in this paper, they mainly focused on the special case trace norm of tensor in their algorithm","Similar to the above two works, Gandy et al.   used the n-rank of a tensor as sparsity measurement and tried to find the tensor of lowest n-rank that satisfies some linear constraints","In their algorithm, the tensor completion was converted into a multi-linear convex optimization problem","Based on the Douglas–Rachford splitting technique   and the alternating direction method of multipliers  , trace norm was introduced as the convex envelope of the n-rank and an efficient algorithm to solve the multi-linear convex optimization problem was proposed","In these trace norm based algorithms, they consider the tensor completion problem as recovering a low-n-rank tensor from a subset of its entries, that is, where   are  -mode tensors with identical size in each mode","The elements of   in the set   are given, while the remaining elements are missing.   is the mode-  unfolding of  .   denotes the trace norm defined by the sum of all singular values of the matrix","On the other hand, Zhang et al.   exploited the recently proposed tensor-singular value decomposition (t-SVD)   that is a group theoretic framework to solve the tensor compression and recovery problem",They first constructed novel tensor-rank like measures to characterize informational and structural complexity of tensor,"The core strategy of all these algorithms to achieve the optimal solution is the same as that they estimate the variables sequentially, followed by certain refinement in each iteration","Although the details of the solution procedure in each algorithm are different, unfortunately, the refinement in each iteration of all these algorithms requires computing singular value decompositions(SVD) that a task is increasingly costly as the tensor size and n-ranks increase",It is therefore desirable to exploit an alternative algorithm more efficient in solving tensor completion problem,"In this paper, a new global algorithm for tensor completion called tensor completion via a multi-linear low-n-rank factorization model (TC-MLFM) is proposed",As the size and structure of each mode of the given tensor are not always the same (e.g,"RGB images), the new algorithm combines n-ranks of each tensor mode by weighted parameters","However, the problem is that the function is generally NP-hard and hard to approximate due to the non-convex optimization of  ","To solve this problem, we use n-rank factorization optimization problem to substitute  ",The new function is solvable and considered as our model,"With the new weighted objective model, the proposed algorithm can utilize the mode information of the tensor with choice","To solve this model, a minimization method based on the nonlinear Gauss–Seidal method   that only requires solving a linear least squares problem per iteration is applied","By adopting this method along each mode of the tensor other than minimizing the trace norm in Eq.  , the new algorithm can avoid the SVD computational strategy and reliably solve a wide range of tensor completion problems much faster than the trace norm minimization algorithm",The rest of the paper is organized as follows.   presents some notations and basic properties of tensors,"In  , we review the definition of Tucker decomposition and tensor n-rank, which suggests that a low-n-rank tensor is a low rank matrix when appropriately unfolded.   discusses the detailed process of the proposed algorithm.   reports experimental results of our algorithm on simulated data and image completion","Finally,   provides some concluding remarks. 2 Notations and basics on tensors In this paper, the nomenclatures and the notations in   on tensor are partially adopted","Scalars are denoted by lowercase letters ( ,  ,  , …), vectors by bold lowercase letters ( ,  ,  , …) and matrices by uppercase letters ( ,  ,  , …)","Tensors are written as calligraphic letters  . 
                      -Mode tensors are denoted as   Its elements are denoted as  , where  ,  ",The mode-  unfolding (also called matricization or flattening) of a tensor   is defined as  ,"The tensor element   is mapped to the matrix element  , where Therefore,  , where  ","Accordingly, its inverse operator fold can be defined as  
                   The n-rank of a  -dimensional tensor  , denoted by  , is the rank of the mode-  unfolding matrix  ","The inner product of two same-size tensors   is defined as the sum of the products of their entries, i.e",The corresponding Frobenius norm is  ,"Besides, the   norm of a tensor  , denoted by  , is the number of non-zero elements in   and the   norm is defined as  ","It is clear that  ,   and   for any  ",The  -mode (matrix) product of a tensor   with a matrix   is denoted by   and is size  ,"In terms of flattened matrix, the  -mode product can be expressed as follows: 
                   3 Tucker decomposition and low-n-rank tensor The Tucker decomposition   is a form of higher-order principal component analysis",It decomposes a tensor into a core tensor multiplied (or transformed) by a matrix along each mode,"Thus, in the three-way case where  , we have here   is called the core tensor and its entries show the level of interaction between the different components.  ,  ,   are the factor matrices (which are usually orthogonal) and can be thought of as the principal components in each mode","If   are significantly smaller than  , respectively, the core tensor   can be thought of as a compressed version of   and then we consider   as a low-n-rank tensor","Formally,   is called low-n-rank tensor if its unfoldings are low-rank matrices",Thus we can use the ranks of unfoldings of a tensor   to learn low-n-rank tensor  ,This rank should not be confused with the idea of tensor CP-rank  ,"An illustration of Tucker model for third-order tensors is given in  
                      ","For notational simplicity, we illustrate our results in this paper using third order tensors, while generalizations to high order cases are straightforward. 4 Tc-MLFM This section is separated into two parts",Part 1 extends the matrix completion problem to tensor case and converts the tensor completion problem into a weighted multi-linear low-n-rank factorization model,"Part 2 applies the nonlinear Gauss–Seidal method to solve the objective model and presents the details of solution procedure. 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   4.1 The factorization model of tensor completion problem The derivation starts with the well-known optimization problem for the low rank matrix completion  : where   denotes the rank of  , and the elements of   in the set   are given while the remaining elements are missing",Eq.   aims to use a low rank matrix   to approximate the given matrix with missing elements,The optimization problem in Eq.   is a non-convex optimization problem since the function   is non-convex,"The higher-order tensor completion problem can be generated from the matrix (i.e. 2nd-order tensor) case by utilizing the form of Eq.  , leading to the following formulation: where the rank of   denotes the CP-rank of tensor,   are  -mode tensors with identical size in each mode",The elements of   in the set   are given while the remaining elements are missing,"The definition of CP-rank, in the form of  , is the minimum number of rank-1 tensors that generate   as their sum  ","In other words, CP-rank is the minimum number of components in an exact CP decomposition","The CP-rank of a tensor is defined as an exact analogue to the definition of matrix rank, but the properties of matrix are quite different from that of tensors","For instance, the CP-rank of a real-valued tensor may actually be different from mode to mode",One major difference between matrix rank and tensor CP-rank is that there is no straightforward algorithm to determine the CP-rank of a specific given tensor,"Therefore, Eq.   is difficult to solve","In fact, the problem is NP-hard  ","On the other hand, the n-rank is defined as the dimension of the vector space spanned by the mode-  unfolding matrix","As discussed in  , when the given tensor is a low-n-rank tensor, the n-ranks instead of the CP-rank of a tensor can be used to capture its global information","Therefore, we can minimize the n-ranks of the given tensor instead of minimizing the CP-rank to solve the tensor completion problem","As a result, a function   which minimizes the n-ranks of the given tensor to replace Eq.   is obtained as the following shows: where   are the mode-  unfoldings of  ","As the size and structure of each mode of the given tensor are not always the same, the contribution of each mode to the final result may be different","Then the n-rank minimization problem of each mode can be combined by weighted parameters: thus, the tensor completion problem becomes: Eq.   aims to find a low-n-rank tensor   to approximate the given tensor with missing elements",A tensor is a multidimensional array or an element of the tensor product of   vector spaces,That is to say matrix is special instance of tensor,"By comparing Eq.   to Eq.  , we can observe that Eq.   is derived from the tensor completion problem and can be viewed as a weighted multi-linear matrix completion problem","In other words, matrix completion problem is also a special instance of tensor completion problem","Although the elements involved in are all matrices, it is a highly non-convex optimization problem since the optimism function includes n-ranks",Without converts there is no efficient solution to this optimization problem  ,"In this paper, our goal is finding a low-n-rank tensor   so that   ( 1 to   ) is minimized","In fact, any matrix   having a rank up to   can be expressed as a matrix multiplication   where  ","In order to solve the function Eq.  , additional auxiliary elenments   will be introduced, while  ","To simplify the problem, we will minimize a  -norm instead of directly minimize the rank of the mode-  unfoldings","Thus, Eq.   can be converted into the following form: 
                      Instead of directly solving Eq.  , we can solve the following problem: 
                      4.2 Minimization via the nonlinear Gauss–Seidal method The function   is differentiable and the gradient of the function   is shown as follows: Let  , obtaining the optimal  : where   is the transposed matrix of  .   denotes the Moore–Penrose pseudo-inverse matrix of   that is a generalization of the inverse matrix","In linear algebra,  ,  ,   and  ","In Eq.  ,   and  ","Thus, Eq.   can be formulated as follows: In  , a sequence of lemmas have been derived","According to these lemma,  , where   is an orthonormal basis for the range space   of  ","Therefore, Eq.   can be converted into the following form: In order to optimize the algorithm, the nonlinear Gauss–Seidal method   can be competent",The optimized process starts with initializations,The core idea of this strategy is to optimize a group of variables while fixing the other groups,"The variables in the optimization are  ,  and  , which can be divided into three groups","To achieve the optimal solution, the method estimates  ,   and   sequentially, followed by certain refinement in each iteration",The underlying optimization can be implemented using the initialization,"The final solution is deduced by utilizing the result of each mode with the weighted parameters   in Eq.   given by the following: The pseudo-code of the TC-MLFM algorithm is given in  
                          below. 5 Experiments In this section, the performance of the proposed algorithm is evaluated and compared with LRTC (Low Rank Tensor Completion)   on both simulated data and real data",LRTC solves the optimization problem: which is derived as the substitution to their original problem Eq.  ,The codes for our algorithm and LRTC both are implemented under the Matlab environment,All the experiments are conducted and timed on the same desktop with an AMD Athlon(tm)×4 640 Professor 3 GHZ CPU and 4 GB RAM,A major challenge of our algorithm is the selection of parameters and the initial values,"In experiments, we simply set the parameters and the initial values as follows: where   is a diagonal matrix with one on the diagonal.  ,  ","The stop criterions of the proposed algorithm are defined as follows: 
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   5.1 Numerical simulations The tested low-n-rank tensors are created randomly by the following procedure",The N-way Tensor Toolbox   is used to generate a third order tensor with the size of   and the relative small n-rank of [ ],The generated tensor is in Tucker model   described as  ,"To impose these rank conditions,   is a   core tensor with each entry being sampled independently from a standard Gaussian distribution  ,   are  ,  ,   factor matrices generated by randomly choosing each entry from  ",Here without loss of generality we make the factor matrices orthogonal,But one major difference is that the n-ranks are always different along each mode while the column rank and row rank of a matrix are equal to each other,"For simplicity, in this paper, we set the n-ranks with the same value",Then a subset of   entries was missing by a probability which follows a uniform distribution,The ratio   between the number of missing entries and the total number of entries in the tensor is denoted by   (missing ratio),The values of missing entries are set to 0,"In this section, the relative square error (RSE) to   is used to measure the quality of recovery, which is defined as  , where   is the recovered tensor from the tensor with missing entries","Firstly, the ability of our algorithm in recovering low-n-rank tensors with missing elements is tested","We address this recoverability issue by generating phase diagram and curve diagram in  
                         
                         ","The simulated tensors used in this test are of size   with the missing ratio   chosen in the order as it appears in {0.05:0.05:0.95} and with each n-ranks value of {[555]:[222]:[494,949]}","In each case, TC-MLFM is run on 10 random instances",The phase diagram depicts the average value out of every 10 runs by our algorithm for each test case,A run was successful when the LRSE between the true and the recovered was smaller than  ,"In  , a white box indicates a successful recovery, while a black box means a failing recovery.   plots the average LRSE corresponding to the set of missing ratio   with different n-ranks, respectively","For all cases with different n-rank, the average LRSE increase gradually with the increase of  ","That is to say if our algorithm recovers the random instance successfully for   and n-rank  , then it ought to have equal or higher recoverability for   and n-rank  ","Thus, it is concluded that TC-MLFM is particularly sensitive to the change of   for this class of problems over a considerable range","Furthermore, under the condition of fixed  , it can be seen from   that the smaller the n-rank, the smaller the LRSE minimum is","According to the above experiments, it is reasonable to infer that TC-MLFM is an acceptable algorithm to solve the low-n-rank tensor completion problem","However, an important question about the proposed algorithm is whether or not it is able to recover low-n-rank tensors similar to that of solving the trace norm minimization approach","Or simply put, does our algorithm provide a comparable recoverability to that of a good trace norm minimization algorithm? In the following part of this section, we will answer this question","Using numerical simulations, the proposed TC-MLFM algorithm is compared with LRTC algorithm  ",The numerical simulated tensors used in our experiments are both of the same size and different size along each mode,"A brief comparison of the two algorithms is presented in  
                         
                          (across 10 instances), where time denotes the CPU time measured in seconds and   denotes the relative square error between the true and the recovered tensors","From the data in  , TC-MLFM algorithm is at least several times (often a few orders of magnitude) faster than the LRTC algorithm while the results of TC-MLFM are comparable to LRTC in terms of accuracy","Of course, the reported performances of the two solvers involved are pertinent to their tested versions under the specific testing environment","Improved performances are possible for different parameter settings, on different test problems, or by different versions","However, given the magnitude of the time between TC-MLFM and LRTC, the advantage in speed of TC-MLFM should be more than evident on these test problems. 5.2 Image completion In the above experiments, we tested the proposed algorithm on numerical simulations and compared it with LRTC",The numerical simulated experiments can be considered as low-n-rank tensor completion problems because the given samples are from a true low-n-rank tensor,"In the following experiments, the proposed algorithm is applied on real data",The given samples of real data are taken from a tensor of mathematically full n-ranks,"Therefore, the problem is considered as the low-n-rank approximation",The key difference between the two classes lies in whether a given sample is from a true low-n-rank tensor (with or without noise) or not,"In fact, low-n-rank approximation is more frequently used in practical applications",One straightforward application of our algorithm is image completion,"In this section, we outline the image completion examples with three different types of data: façade image, natural image and CT/MRI image","In the following tables, time denotes the CPU time measured in seconds and   denotes the relative square error between the original data and recovered data. 
                         
                          For façade image, we select three uppercase letters as the missing parts shown in white","The input data is a three-channel image and can be seen as a third-order tensor of dimensions  .  
                          shows a recovery experiment of facade image: (a) is the original image, (b) is the input data, (c) is the recovered image by TC-MLFM, and (d) is the recovered image by LRTC. 
                         
                          tabulates the RSE and CPU time of TC-MLFM and LRTC on façade image","From the data in  , we can see that TC-MLFM algorithm is much faster than the LRTC algorithm while the results of TC-MLFM are comparable to LRTC in terms of accuracy. 
                         
                          For natural image, the missing entries are randomly removed by different percentage and the randomly removed pixels are shown in white","The input data is a three-channel image and can be seen a third order tensor of dimensions  .  
                          shows a recovery experiment of natural image","The top row of   shows the original image with 40%, 50%, 60% and 70% randomly missing entries from left to right, respectively",The second row shows the recovered image by TC-MLFM,The third row shows the recovered image by LRTC,"The bottom image is the original data. 
                         
                          tabulates the RSE and CPU time of TC-MLFM and LRTC on natural image","From the data in  , it can be seen that TC-MLFM algorithm is at least several times faster than the LRTC algorithm while the results of accuracy are better than LRTC. 
                         : In this part for the experiments we used MRI images, which contain 140 slices through a human brain, each having dimensions  ",So the input data is a third-order tensor of dimensions  ,"The missing entries were randomly removed with different percent and the randomly select pixels for removal are shown in white.  
                          shows a recovery experiment of five slices in the MRI images",The top row shows the five slices of the original data,The second row shows the original image with 50% randomly missing entries,The third row shows the recovered image by TC-MLFM,"The bottom row shows the recovered image by LRTC. 
                         
                          tabulates the RSE and CPU time of TC-MLFM and LRTC on MRI image","From the data in  , it is observed that TC-MLFM algorithm is several times faster than the LRTC algorithm while the results of TC-MLFM are better than LRTC in terms of accuracy. 5.3 Potential application In the above, the proposed algorithm is applied to image completion, such as façade image completion, natural image completion, and CT/MRI image completion","Besides image completion, may be the proposed algorithm can be used in other areas such as text mining, image classification, and video indexing","In  , Dunlavy et al. analyzed data with multiple link types and derived feature vectors for each individual node","For the multiple linkages, a third order tensor was used, where each two-dimensional frontal slice represents the adjacency matrix for a single link type",Our algorithm is also a tensor based algorithm,The data used in   can be seen as a special case of our algorithm input,"In  , Zha et al. considered image classification as both a multi-label learning and multi-instance learning problem","Based on hidden conditional random fields, they proposed an integrated multi-label multi-instance learning approach, which simultaneously captures both the connections between semantic labels and regions","Thus, they formulated correlations among the labels in a single model","As is known that tensor is a useful tool for representing multi-mode correlations, and tensor decompositions facilitate a type of correlation analysis that incorporates all mode correlation simultaneously","In  , based on the optimum experimental design criteria in statistics, Zha et al. proposed a novel video indexing approach that makes use of labeled and unlabeled data and simultaneously exploits sample’s local structure, and sample relevance, density, and diversity information","In our knowledge, tensor also can formulate these elements into one model and mining the correlations between them simultaneously","And in the former work, we have applied tensor recovery into background modeling using video data   and traffic data completion  ","In the future, we will investigate the applications of our algorithm to these areas. 6 Conclusion In this paper, we extend the low-rank matrix completion problem to a low-n-rank tensor completion problem and propose an efficient algorithm based on the multi-linear low-n-rank factorization model","For the solution of this model, the nonlinear Gauss–Seidal method that only requires solving a linear least squares problem per iteration is applied","Thus, the proposed algorithm can avoid the singular value decompositions (SVD) strategy which is much time cost",And the proposed algorithm can automatically complete a low-n-rank tensor with missing entries,The performance of the proposed algorithm is evaluated on both simulated data and real data,The experiments show that the proposed algorithm is much less computational cost than the trace norm minimization algorithm especially facing the large data,Different applications in image completion show the applicability of our proposed algorithm in the real world data,"In the future, we would like to investigate how to automatically choose the optimal weighted parameters in our algorithm and develop more efficient algorithm for tensor completion problem",Also we will explore additional applications of our method in the real world data.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s1071581918303471, 1 Introduction Determining when to interrupt a user at appropriate times as s/he performs computer-based tasks is an ongoing problem ( ),"From an algorithmic perspective, it is difficult to determine the precise time to interrupt a user",This is because there are several subproblems that need to be solved to be confident that an interruption will be beneficial to the user,"Some subproblems include: i) determining the intent (or goal) of the user as s/he is performing the task; ii) determining the task difficulty ( ); iii) determining the user's current cognitive load ( ); iv) estimating the cost of the interruption and the resumption lag time ( ); and v) incorporating personal user characteristics, such as sensitivity to being interrupted, distractibility level, etc. ( )",A solution to these problems is needed to make accurate decisions about the timing of interruptions,"Since interruption is a key human-computer interaction problem, systems must be developed to manage interruptions in terms of reasoning about ideal timings of interruptions","In designing the classifier, the following desirable characteristics were identified: 
                   
                      
                      
                      
                   1.1 Outcomes and contributions We created a machine learning classifier that performs as well as user-determined interruption points",The rationale why user-determined interruption timings are ideal is explained in the following sections,"Additional outcomes and contributions include: The structure of this paper is:   presents a literature review of interruption research and a survey of candidate machine learning algorithms.   presents the design and methodology including the classifier requirements, the Interruption Classifier, data capturing techniques, and the details of the machine learning technologies used in the implementation of the classifier.   presents the findings (analysis and evaluation),   presents a discussion on the implications of the empirical results for theories of interruption and implications for deploying classifiers for detecting interruptible moments in practice","Lastly,   presents the conclusions. 2 Literature review The goal of this research is to identify the most appropriate classifier for predicting the interruptible moment given the context and task of the user","As previously discussed, the classifier created draws information from a user model and real-time data of the users’ actions","Acknowledging that the literature in this area is very broad and diverse, the review conducted specifically concentrated on the desirable characteristics as outlined above","For example, criterions 1 and 5–8 reduce the number of machine learning algorithms suitable for review. 
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   2.1 Interruption Interruptions happen for a multitude of reasons and there are four known strategies for managing them: (a) immediate, (b) scheduled, (c) negotiated, and (d) mediated ( )",The   interruption strategy involves interrupting the person immediately regardless of what they are doing in a way that insists that the user immediately stop what they are currently working on and respond to the interruption,The   strategy involves restricting the agents’ interruptions to a prearranged schedule,The   interruption strategy would have the agent announce their need to interrupt and then support a negotiation with the person,This approach gives the user full control over how to deal with the interruption—when or even at all,"The fourth strategy, called  , involves agents indirectly interrupting and requesting interaction through a broker like a smartphone",The smartphone would then determine when and how the agents would be allowed to interrupt the user,The Interruption Classifier is designed as a broker with the intelligence to reason about when to interrupt the user,Most of the current research is focused on mediated and negotiated strategies with research in the mediated strategy area growing considerably ( ),Associated with mediated strategies are intelligent systems that observe the user as s/he is performing tasks to decide when to interrupt the user and how best to present the pertinent information,"Despite the progress that has been made in systems supporting mediated strategies, negotiated strategies (user determined) are still the best overall solution when considering factors such as cost of interruption, resumption lag, and overall performance in carrying out multiple tasks ( )",The proposed system corresponds to the ‘mediated’ strategy,"The following section presents these topics on interruptions: (a) tasks and task boundaries; (b) cognitive load, cost of interruption and resumption lag; and (c) models of interruption and an interruption taxonomy. 
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                      2.1.1 Tasks and task boundaries Task and interruption researchers are interested in acquiring contextual information surrounding the task so that the timing of the interruption and the information presented will be minimally disruptive and of the utmost benefit to the user at that time ( )",Reasoning systems must incorporate task properties because these systems must be able to decide optimal times to interrupt,This decision often hangs on the very task the user is engaged in at the time ( ),"In many situations if it is possible to defer an interruption to a task boundary, the inconvenience to the user by responding to the interruption is significantly reduced ( )",In these situations the resumption lag is much less for the user than if the interruption occurred during the task ( ),The concept of a task boundary will be integrated into the classifier as one of the features that it implicitly learns through user training data sets. 2.1.2 Automatic task boundary identification Task boundary identification techniques are used to detect and identify breakpoints during tasks to establish policies for interruption software ( ),"Researchers have focused on building statistical models that dynamically extract characteristics of the interaction to a specific type of breakpoint (i.e., coarse, medium, fine) ( ;  )",The findings indicate that these models can pick out task breakpoints with a reasonable amount of accuracy for prescribed tasks ( ),An example of a prescribed task is solving a jig-saw puzzle,Each individual subtask is the act of dragging a piece to its appropriate location in the jig-saw puzzle,"As it relates to interruption, these models could be used to augment interruption management software to effectively determine better times to interrupt the user by establishing defer-to-breakpoint policies ( )","However, these studies have primarily focused on prescribed tasks",It is significantly more difficult to detect breakpoints within tasks that are highly variable in nature,"For example, free-form tasks are by far the most common type of computer-based task and are still largely an unsolved problem for interruption researchers ( )",In the context of this research the definition of a   is one in which the tasks are highly variable and the interaction including actions and timing between the user and the task cannot be predicted,Currently there are very few algorithms that pick ideal times to interrupt a person working on free-form tasks (Please see: ( )),"This is one area where this research contributes by providing an innovative solution to this difficult problem. 2.1.3 Cognitive load, cost of interruption and resumption lag 
                             is an indicator of the degree of working memory utilized when the user is performing a task ( )",The   (COI) is a subjective measure of a user's wish to remain undisturbed while working on a computer based task ( ),"We acknowledge that other definitions of COI exist, however, in this research we use the previous definition",The COI may include various kinds of alerts disrupting a user in different contexts ( ),"The COI has been used as an assessment tool for several decades in decision analysis in various fields ( ). 
                             (RL) is defined as the time required to resume the primary task after completing the interrupting task ( )",RL can be measured as the time from closing the interrupting task to the first keyboard or mouse action in the primary task in direction of the task goal ( ),There is a strong correlation between cognitive load and the COI ( ),"Thus, it is important to assess the cognitive load on the user while s/he is performing a task to decide whether to interrupt the user","Researchers have shown that if a user is interrupted during a high cognitive load task by being forced to switch tasks, then the COI can be very high ( )","Consequently, an important design consideration for the Interruption Classifier was that it considers the user's workload or cognitive load when deciding whether to interrupt the user",A design aspect of our classifier acknowledges that the COI can be reduced by aligning the interruptible moment with subtask boundaries ( ),"Furthermore, it should be noted that the identification of ideal interruption points for free-form tasks is largely an unsolved problem ( )","Therefore, uncertainty is part of what this classifier needs to consider (criterion #1). 2.2 Ideal interruption points In this research, an   is defined as the time when a user would normally choose to serve an interruption while considering: (1) the user's cognitive load should be low ( ); (2) the user should be at a coarse task breakpoint (e.g., about to switch from a spreadsheet to email); and (3) the length of the interruption task should be short so that the user can quickly return to the primary task without the loss of continuity in performing the primary task. 
                          Criterions 1 and 2 are design characteristics of our classifier; criterion 3 is supported by the experiments we have prepared",Our motivation for designing a system that learns from user determined timings stems from the literature that indicates that users   full control over when to serve interruptions ( ),The primary purpose of this research is  ,This research does not focus on designing a system that increases the overall performance of the user,"If a real-world interruption management system is implemented using our classifier, it would suggest interruptions at times that would be most in tune with his/her interruption preferences","This would enable the system to serve as an effective  –one that would receive incoming interruption requests for the user and decide, based on that user's characteristics, when is the most appropriate time to interrupt him/her. 2.3 Models of interruption and an interruption taxonomy Models of Interruption refer to the set of models researchers have proposed to assist in representing the context from which an interruptible moment may be reasoned about",The Memory for Goals model has been used by many different researchers to understand and model interrupted task performance for nearly two decades ( ),There have been a myriad of studies mostly based on the Memory for Goals model that provide support for why an interruption alert is useful and improves performance ( ),The Memory for Goals model helped the design of our experiment during which participants choose their own times to serve interruptions after an alert,"Currently, each model is very specific to the types of tasks intended to be performed by its users",Much of the research to date has been centered on using attributes from the task domain ( ),"However, there are other aspects surrounding the problem of determining when to interrupt a user ( )",These other aspects involve the user and environment contexts,"Models have been proposed to provide a more encompassing perspective of interruption; however, currently, there is no standard or unified model that has been accepted in the research community","Currently, there are several interruption taxonomies ( )",We focused on the one proposed by Gievska & Sibert since it aligns well with the goals of this research,"This taxonomy includes three dimensions: (1) User Context captures the salient features of the user's characteristics and traits; (2) Environment Context represents attributes representing the user's current working environment; (3) Task Context aims to capture the significant properties of the computer based task ( 
                         )",The purpose of this taxonomy is to serve as a framework to identify attributes and relationships appropriate for conceptualizing factors that influence the timing of interruption,"Gievska & Sibert have designed and implemented a system to automatically detect interruption points; however, they have only designed systems that focused on Task Context variables—User and Environment context variables were not included ( ). “Subjective preferences were not considered as a factor in the current implementation of the interruption mediator","However, they should not be neglected when designing user interfaces that give equal priority to user's satisfaction and comfort as to other performance measures.” pg. 176, ( )","It follows then that the direction of research is to embrace aspects from all three dimensions. 2.4 Machine learning algorithms Before designing the Interruption Classifier, many machine learning algorithms were reviewed (i.e., Bayesian networks, neural nets, statistical classifiers, etc.)",Many of these were discarded because they failed to meet one or more of the desirable characteristics and they are limited for modeling human behaviour,Certain machine learning techniques in the area of   have been applied to a number of user modeling problems by learning from user behaviour and integrating them as part of the user model ( ),"Soft computing is a family of methods that are based on fuzzy logic, neural networks, and probabilistic reasoning tools ( )","Furthermore, soft computing algorithms have been shown to be very effective at deriving solutions to problems where other approaches have failed ( ).  
                          summarizes the characteristics of different soft computing techniques based on six criteria from  ): 
                      Human interaction is a key component of any user modelling application—like the Interruption Classifier—which implies that the data available will be most likely imprecise, incomplete and heterogeneous ( )","In this context Soft Computing, specifically Neuro-Fuzzy systems appear to be the appropriate paradigm to handle the uncertainty in this problem",Adaptive Neuro-Fuzzy Inference Systems (ANFIS) offer these benefits: The selection and use of an ANFIS for this research appears to have great promise since it satisfies all the desired characteristics; offers a significant amount of flexibility because it fits the soft computing paradigm; and has long-standing support from the user modeling community as a viable tool for user modeling problems ( ),"As a result, an ANFIS was chosen as the machine learning tool for the Interruption Classifier","In summary, the goal of this research is to design a classifier that will perform as well as user-determined interruption timings as outlined in  . 3 Methodology This section presents the details of the methodology","It includes the approach taken to identify appropriate computer-based tasks, machine learning techniques, and the experiment and analysis techniques","Additionally, the design of the Interruption Classifier is presented","The following section describes various aspects of the methodology including: experiment task design; pilot studies; Interruption Classifier; experiment procedure; participants; methodology for designing, training, testing and refining the Interruption Classifier; and analysis techniques. 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                   3.1 Experiment task design The experiment design involved a primary task (Jumping game) and an interruption task (intermittent Matching game)",The primary task is modeled after a video game by Nintendo called   and McFarlane's interruption work ( ),The interruption task is modeled after the matching tasks used in experiments of the Stroop Effect ( ),The tasks are unrelated by design to ensure that different cognitive resources are required to complete each task,"The experiment task design is largely based on  ). 
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      3.1.1 The primary task This task requires the user to move stretcher-bearers to catch other game characters as they jump from a building.  
                             depicts the game in which the user must successfully bounce each falling character three times in three different locations and into the awaiting ambulance","If a character lands on the ground, then that character is not saved","The game is simple when only one character at a time jumps out of the building; however, when many game characters are jumping at a time, the game becomes more difficult",A   is defined as the task for the participant to manage an individual jumping character and to save him/her,This is accomplished by moving the stretcher in the appropriate position to ensure the character is bounced several times and then finally into the ambulance,The time between subtasks is sufficient such that the participant is not required to provide constant attention,This design is intended to provide participants the ability to successfully serve an interruption task if needed,"The game runs continuously, so even if the participant serves an interruption task, the game continues",The composition of the game permits observation of participants’ behaviours to be directly mapped onto discrete subtasks,"The primary task offers the following beneficial task characteristics: 
                         The primary task was designed to enable analysis of ideal and undesirable interruption points","The primary task was also designed to be uncomplicated so that additional noise would be reduced as much as possible: 
                         3.1.2 The interruption task A matching task was used as the interruption task",This task uses a graphical matching task that is based on Stroop effect studies ( ),"This task requires the participant to make matching decisions (by shape or colour) based on the rule presented on the screen ( 
                            ) using these principles: 
                         The experiment was designed so that it is not possible to predict when an interruption task will appear, nor is it possible to predict how or when the user will interact with the interruption task","It follows then, from the definition of a free-form task presented in  , that this task is also a free-form task","Furthermore, viewed at a more abstract perspective, the combination of the primary and interruption tasks represents a free-form task. 
                            
                         3.2 Full experiment This section presents an overview of the experiment including treatments, objective and subjective measures, and data for the classifier","In preparing for the full experiment, a pilot study was conducted which represented all aspects of the whole experiment—from introduction to debriefing",The pilot study provided an opportunity to set the primary and interruption task complexity to the appropriate level; identify issues so that the experiment would run consistently and the results would be as reliable as possible; and to gain an understanding of the strategies and tactics relating to each participant's negotiated interruption style,Amongst the most interesting findings from the pilot study were anecdotes from the participants revealing their strategies: “The matching task was demanding because you struggle to keep focus,I worked hard to keep focused during the matching task,"It wasn't frustrating—it was fun! The majority of the time I switched when the jumpers were at the highest point, which would give me time to complete the interruption.” (Pilot Study Participant #1). “My strategy was to move the stretcher to the next bounce position, and then serve the interruption immediately","This gave me plenty of time to do the interruption during a full bounce cycle.” (Pilot Study Participant #3). 
                         
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      3.2.1 Treatments Three treatment conditions were used in this research experiment. 
                         All the participants received the three treatments","To avoid potential confounds such as tiredness, each treatment was administered by two sequential trials with a 30 s rest period in between","Furthermore, a diagram-balanced Latin squares ordering was used for counterbalancing","For example, consider the following sequence of treatments: This section describes the measures collected in the experiment and those used by the Interruption Classifier","For the experiment, the participant's performance was the dependent variable and was determined by the analysis of five objective measures and 17 subjective measures","For the classifier, the same objective measures were used with an additional set of measures for training and testing purposes. 3.2.2 Objective experiment measures 
                            
                             depicts summative objective measures and data collection properties","The objective measures are grouped into the following categories: correctness (metrics 1 and 2), efficiency (metric 3), and timeliness (metrics 4 and 5)",The purpose of collecting these measures was to be able to calculate the participant's performance with respect to the base cases and the negotiated interruption strategy,These measures were collected during trials with summative values computed at the end,"However, during the experiment formative updates of these measures were computed","For example, the state of the number of jumpers saved, the number of matches done wrong, etc. was computed every millisecond within each trial (4 ½ min [270,000 ms])","Thus, there are approximately 270,000 data points (i.e., cases) for each trial containing all the contextual information for the classifier to reason about interruption points. 3.2.3 Subjective experiment measures Two questionnaires were administered to the participants",An opening questionnaire captured the participant's characteristics (please see  ),These measures collectively represent the core of the user model from which the classifier draws user-specific contextual information: These specific personal characteristics were measured because they represent subjective aspects that directly or indirectly impact how a person perceives and responds to interruptions,One objective in this research involved exploring which of these personal characteristics were predictive of the interruption points,This is elaborated in the Discussion,"The closing questionnaire was used to assess the subjective measurements of the participant's opinions on various aspects of the experiment (please see  ). 3.2.4 Classifier measures—data for the classifier Data were collected and used by the classifier to design a solution for the mediated interruption strategy.  
                             presents a scenario to motivate the selection of data used by the classifier","The sequence diagram shows a participant undergoing the negotiated interruption strategy treatment: 
                         The time from t  to t  and from t  through t  represent the entire contextualization from initial notification of an interruption to the time when the participant is fully engaged in the interruption task","However, the most important timings are when the participant decided to switch from the primary task to the interruption task","To train the classifier, timings t  and t  are especially important since it is at these times that the participant decided to serve the interruption","These timings, in combination with user model and task details were important to train the classifier",The following data were used in designing the classifier to satisfy the research goal of designing a system that performs as well as the negotiated interruption strategy at detecting ideal interruption points,"The data are: 
                         3.3 Interruption classifier This section describes, at an abstract level, the Interruption Classifier","The classifier needs the following information to determine appropriate times to interrupt the user ( ): The initial design of the classifier is based on a Fuzzy Inference System where linguistic variables, membership functions and rules were identified that use information from the three contextual dimensions (user, task, and environment)","A high-level description of the classifier is presented in  
                         . 3.4 Initial design of the interruption classifier This section presents the initial design of the Interruption Classifier using an Adaptive Neuro-Fuzzy Inference System","Workload levels for the game task are based on the number of jumping characters currently on the screen that need to be managed by the participant and the QueueSize represents how many interruption tasks are waiting to be served.  
                          presents the surface view of initial model.  
                          shows the initial fuzzy rules in the model. 3.5 Participants Twenty-eight volunteers were involved as participants in this experiment (3 involved in the pilot study and 25 in the full experiment). 
                          In the full experiment, there were 21 males and 4 females","Participants had a median age of 21 (mean 26.5, min. 19, max. 49)",All participants were sampled from the general population of Sheridan College,Participants were recruited by a set of posters posted at various locations throughout the campus,"The recruitment message did not disclose the purpose of the experiment, but described the task as fun and like a video game",The message indicated that each volunteer would receive compensation for his/her time and that volunteering would be a contribution to the advancement of science. 3.6 Analysis Two types of analysis were performed on the collected data—quantitative and qualitative analysis,We report only on the quantitative analysis as it is most closely tied to the design and evaluation of the classifier,"Three types of quantitative analysis were performed: (1) determining the participant's performance using statistical tools; (2) evaluating and refining the classifier using standard machine learning evaluation tools; and (3) determining the accuracy of the classifier using statistical tools: 
                         
                         
                      4 Findings (analysis and evaluation) This section presents the findings (analysis and evaluation) of the research conducted","The topics include full experiment findings and the evaluation of the Interruption Classifier. 
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   4.1 Summary of main findings This section presents a summary of the main findings from design to refinement of the classifier",The following model variations were created and performed quite well,"The main findings were: 
                      4.2 Full experiment findings Data from the participants’ trials were used in the analysis to determine the participant's performance",Observations recorded during this experiment were used to explore the timings of user-initiated interruption points,"Furthermore, during this experiment, observations were recorded on the details of the context in which the people chose to serve interruptions. 
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      4.2.1 Number of jumpers saved on the game task 
                            
                             presents the descriptive statistics","A one-way ANOVA was performed to determine if there was difference in performance in the treatment conditions for the number of jumpers saved by treatment condition (Baseline–Primary Task only and Negotiated).  
                             presents the results","There was a statistically significant difference between the groups Baseline—Primary Only and Negotiated performance at the 0.05 level,  (1, 48) = 5.585,  = 0.022","This means the additional activity to serve interruptions reduced the participants’ performance in the primary task (i.e., being able to save jumping characters). 4.2.2 Number of matches done right of those attempted 
                            
                             presents the descriptive statistics (mean, standard deviation, min., max., median, skewness and kurtosis)","A one-way ANOVA was performed to determine if there was difference in performance in the treatment conditions for the matches done correctly of those attempted by treatment condition (Baseline – Interruption task only and Negotiated).  
                             presents the results","There was no statistically significant difference at the 0.05 level,  = 0.271,  = 0.605",This means the participants’ performance level in the interruption task is independent of whether it is performed as the sole task or whether it is as an interruption task within performing the primary task. 4.3 Interruption classifier evaluation This section presents the evaluation of the classifier,"The criteria that was used for classifying a model as acceptable based on its performance is presented in  
                         ","These criteria have been used in similar studies in Human Computer Interaction (Please see: ( )). 
                         
                         
                         
                         
                      
                         
                         
                         
                      4.3.1 Statistical analysis to test the classifier This section presents the statistical analysis that was performed to test the effectiveness of the classifier",Numerous confusion matrix computations were performed including supporting statistical measures and summative standard descriptive statistics,"The confusion matrices show the range of values (min, max, mean, median and standard deviation) across all 25 participants","Three of these computational results are shown in  
                            
                            – 
                             based on models created from training data sets from Participant #2, #21, and #22 respectively (the others are omitted because they are quite similar)","After reviewing all the confusion matrix computations and supporting statistical results, it was discovered that the following models exceeded the performance criteria (all of the criterions shown in  ): 6 models based on Participant #2, #4, #6, #17, #21, and #22 training data sets; 3 models based on incorporating user characteristics: Initial model augmented with  ; Initial model augmented with  ; Initial model augmented with  ; and several models based on the initial model with historic event knowledge. 4.3.2 Determining the classifier's performance using the bin analysis framework This section presents the analysis that was performed to assess the classifier's performance based on the bin analysis framework.  
                             presents the summary of computed accuracy ( )) of the classifier involving all 25 participants using descriptive statistics","The classifier performed at 96% accuracy (mean), with a variation from 88% in worst case to 100% in the best case. 4.4 Summary of most successful and least successful models 
                         
                         
                         
                      
                         
                         
                         
                         
                      4.4.1 Most successful models Beyond the successful models presented earlier, models that incorporated historic information performed the best across the entire population.  
                             presents the results of a model based on Participant #2 training data set with PreviousTimeStep","Twelve other models that used historic information performed similarly with very high accuracy. 4.4.2 Least successful models During this multi-year research project, hundreds of models were created using different training data sets and combinations of properties from the user, task and environment dimensions, and classifier parameter settings",Most of these models performed unsatisfactorily,One of the models that performed poorly was based on Participant #8′s training data,"This participant saved the fewest jumpers: 28/48 (population mean = 35.2), completed fewer interruption tasks: 24.5/50 (population mean = 45.5), and took longer than other participants to perform the interruption tasks: mean = 13.3 s (population mean = 3.6 s)","Furthermore, it was also discovered that this participant did not execute a consistent strategy with respect to choosing appropriate times to serve interruptions.  
                             shows the confusion matrix results of a model based on this participant",Another poorly performing model was based on Participant 25's training data,"This person reported “very low tolerance to interruptions,” “very high level of frustration,” “very high level of distractibility,” and “very difficult to regain focus [after an interruption].”  
                             shows the results of a model based on this participant. 5 Discussion This section presents a summary of the significant factors in designing good models; predictive factors of interruptible moments; implications of the empirical results for theories of interruption; and implications of the empirical results for deploying classifiers for detecting interruptible moments in practice. 
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   5.1 Significant factors in the design of good models 
                         
                      5.2 Predictive factors of interruptible moments In this research, both quantitative and qualitative data were collected and analyzed","Since the paper primarily focused on the quantitative aspects, this section presents some of the insights from the qualitative data as to why users preferred certain points for interruption","One of the goals in the qualitative area of research was to shed some light onto the motivation and context to answer the question “Why now?” —that is, why did participants choose those specific points to serve interruptions and what was the context surrounding that decision",The following selected anecdotes are from the closing questionnaires from Participant #2 and Participant #17 whose data sets produced good models (  presents all of the participant's strategies with respect to interruption timings): ``I tried to position the paramedics in a logical place and tried to switch when people were bouncing up,I picked people that were closer to the ambulance when deciding who would be saved/not saved,"For the matching task, I just focused on the word, and let my peripheral eyesight decide [colour vs. shape].” (Participant #2). ``The matching [interruption] task by itself and the game [primary] task by itself were very simple",Together though it was more difficult,I would position my stretcher so the jumpers would be secure while I switched tasks.” (Participant #17),"Researcher observation and analysis showed that both participants followed the same advanced strategies when compared to other participants: 
                      These strategies enabled the participants to perform well on both the primary and interruption tasks: Participant #2 saved 90% of the jumpers, and completed 93% of the interruption tasks correctly; Participant #17 saved 85% of the jumpers, and 99% interruption tasks were completed successfully. (Their performance was statistically significantly higher than other participants – across the entire study the mean number of jumpers saved was: 73%, and the mean number of interruption tasks completed successfully was: 91%)","In regards to predictive factors of the interruptible moment, these strategies were consistently and accurately performed and were implicitly recorded in the participants’ data sets which served well in training the classifier","Regarding the participants’ user models, both Participant #2 and #17 reported on their opening questionnaires a strong capacity to perform well on the upcoming experiment tasks (please see  : Opening Questionnaire).  
                          presents these participant's personal characteristics and the means across all participants","On the closing questionnaires, both Participant #2 and #17 reported:   (100%) and   (100%) (please see Appendices C: Closing Questionnaire)","Across the entire study the means for   was: 78%, and   was: 84%","In developing the models for the classifier, numerous combinations of characteristics were used ( , and  )","Through experimentation many of these models were discarded, however, the models that performed well were the Initial model augmented with  ; Initial model augmented with  ; and Initial model augmented with  ",These personal characteristics were found to be significant in designing models that were predictive of good interruption points,This section presented some of the insights from the qualitative data as to why users preferred certain points for interruption,"It also showed that the predictive factors of interruptible moments for free-form tasks is very dependent on well-defined data sets that accurately represent the specific task details, strategies and nuances, coupled with good user models",These data sets are essential for training in the machine learning process to create good classifiers. 5.3 Implications of the empirical results for theories of interruption Based on the empirical results in this research there are several implications to theories of interruption,Gievska's theory focused exclusively on task-based contextual information ( ),These researchers stated that additional work needs to be done in this area by including information from the other dimensions in their taxonomy: “Subjective preferences were not considered as a factor in the current implementation of the interruption mediator,"However, they should not be neglected when designing user interfaces that give equal priority to user's satisfaction and comfort as to other performance measures” ( , pg 176)","One specific implication of the empirical results is that Gievska's theory of interruptions could be updated to emphasize the characteristics that were found to be significant in this research, namely:  ; and  
                      The empirical results from this study also highlight the importance of well-defined rules that capture the salient features of the tasks",The models that performed the best in this study incorporated good task management strategies in their design,"From this perspective, it reinforces the theory that task properties and details are fundamentally important in designing systems that determine good interruptible moments",This perspective is well represented in the literature on theories of interruption,"Beyond these implications, there are many opportunities for addition research to be conducted in the spirit of refining these theories. 5.4 Implications for practice This research showed that there is significant potential to create classifiers using machine learning to determine appropriate times to interrupt the user","In regards to deploying machine learning classifiers in practice, there are a number of opportunities through which we may witness a significant uptake on classifiers that improve our quality of experiences with computers and computing devices (mobile devices, wearable devices, etc.) that determine good interruption points. 
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                      5.4.1 Considerations for a robust multi-user classifier The next step in developing this classifier for use in practice would be to create a system of classifiers in which each classifier would be responsible for learning and personalizing the interruption strategy for a specific user for a specific set of tasks the user routinely performs","In this way, the classifier would be able to tune its behaviour to the specific user it is assisting","The current method for creating satisfactory models requires substantial manual effort on the part of the researcher to train a classifier, analyze the results and then select the best performing models that meet the requirements","Further work is needed to refine this process by designing an automated solution ( ) that would receive the interactive datasets, generate and analyze a variety of models, and then rank and select the best model from this list","In terms of implementation, a cloud-based system would be a suitable architecture to receive interruption and associated contextual information to support each user","This research focused on a primary and interruption task, however, in a practical implementation, rather than collecting datasets from a variety of people performing the same tasks, it would be more effective for many unique classifiers to be created primarily based on datasets from individuals","In this way, a variety of classifiers would learn based on personalized models and thus be better attuned to each user",Each classifier would learn its user's personal interruption characterization profile of when s/he prefers to be interrupted and when to be left undisturbed,"Once the cloud service has a sufficient amount of data from a user, the machine learning algorithm would be trained and models created",The overarching module ( ) would evaluate and tune the machine learning parameters and attempt to improve the models for that specific user,The best performing model that meets the requirements (see  ) would be used for determining the timing of interruptions for that user,"If none are acceptable, the cloud-based system continues to collect additional interaction data",This cyclic process continues over time to refine the models generated and to persistently select the best model representative for that specific user,"In this way, this system would provide multiple dynamically adjusting classifiers for each user for tasks that s/he performs on a regular basis. 5.4.2 Users learning to interact with the classifier One of the key benefits of the classifier created in this work is that the user is not required to be involved in the classifier's learning process whatsoever","The interruption points are discovered by the classifier from the interaction datasets (e.g., the task switches from the primary task to the interruption task among other contextual information)",This desirable characteristic is one of the main contributions of this work and essential for future versions of the classifier,"We believe this approach is an appropriate direction of research due to two main powerful trends: 1) Big Data, and advances in machine learning, and 2) Computer Vision and facial feature detection",With the advances in Big Data and machine learning there are many algorithms that can extract features from data that could be used to drive or enhance the operation of the classifier,"It is foreseeable the associations that are not obvious to us may in fact be readily uncovered by an ML algorithm that can identify if a user is becoming frustrated, confused, or is concentrating intensely, etc",This input in combination with task understanding could aid the Interruption Classifier with an increased ability to interrupt the user more accurately and in more widespread scenarios,"Additionally, with advances in computer vision and facial feature analysis, a future version of the Interruption Classifier could accept input such as eye-tracking information (e.g., saccades, 
                             perclos, fixations, gazepoints, percentage of time the user focused on tasks and subtasks, pupil diameter, and blink rates), and facial features (e.g., head positions and rotations; eyelid aperture; lip and eyebrow movements with appropriate inferences (e.g., smiling, frowning, concentrating, etc.)",These feature rich details provide significant insight into the cognitive state of a person and may be key indicators to assist in picking appropriate times to interrupt the user ( ),"These inputs, in combination with the other data collected in this study, would provide a robust version of our classifier a more holistic view of the user and the tasks that s/he is performing, ultimately enabling the classifier to better learn and reason about ideal interruption points. 5.4.3 Content and relevance of the tasks The content and nature of the tasks the user is performing needs to be precisely represented (as in this study with the various task rules) for the Interruption Classifier to make good decisions about ideal interruption points","In this research, the primary and interruption tasks and user contexts were well represented in the machine learning algorithm",The primary task had a substantial number of rules that collectively captured ideal situations for when an interruption could occur,"However, by comparison, there were significantly fewer rules embodying the task characterization for the interruption task","In practice where users are regularly multi-tasking, it may be difficult to determine what the primary / interrupting task is","In these scenarios, when the distinction between the primary and interruption tasks may not be obvious, the same degree of rigor is required to represent the tasks the user routinely performs",In this way the classifier would have models of all relevant tasks in play and can more effectively reason about making interruption decisions,"Essentially, the rule base for both the primary and interruption tasks would be equally well- defined so that interruptions could occur from within either task","This approach could be extended to multiple tasks the user may perform. 5.4.4 User productivity and contentment A natural question to ask regarding this work is: “Would users become more productive and/or more content over time?” Theoretically, the classifier is generating interruptions at points when the user would have selected them","As mentioned earlier, this classifier is not concerned about optimizing the user's productivity, but rather choosing times that most appropriately coincide with the user's preference while considering contextual factors such as task and user characteristics","While our classifier may not explicitly try to optimize the user's productivity, it is a plausible outcome because the classifier is in tune with that person's personal interruption profile","As a result, the user's overall satisfaction with respect to interruptions could be said to be optimized","If the user is more content, then the user's productivity may well increase too ( ). 5.5 Limitations and future work This research showed that there is significant potential to create classifiers using machine learning to determine appropriate times to interrupt the user",This section presents some of the limitations and goals for future work,Limitations As with any research there are limitations,"Some of the main limitations of this work are: 
                      
                         
                      
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                      5.5.1 Mobile device interruption management services The field of mobile computing is growing at a staggering rate and the problem of interruptions is even more poignant in this context ( )",Not until quite recently did smartphones provide support for developers to customize user notifications and interruptions ( ),"The classifier could be extended to draw from the user model and rich set of sensory components on these devices (accelerometer, GPS location, microphone, calendar, etc.)",These additional input sources may enhance the performance of the classifier to increase user satisfaction and productivity,"Building on the findings of this research, future studies should explore how the classifier could be adapted and improved to serve in the context of mobile devices. 5.5.2 Intelligent personal assistant In alignment with the current trends of cognitive computing and personal assistants (e.g., Siri, Cortana, etc.), the classifier could be extended to observe what the user is doing in a broader more comprehensive perspective","For example, suppose the user is working on a paper that involves many activities and the classifier observes that the user has attempted to perform the same activity many times within a ½ h period","The classifier reasons about the ideal interruption point and decides to issue an interrupt message, such as, “Excuse me, Sue, it appears that you have you been trying 〈 〉 repeatedly now for 30 min – the following 〈 〉 will accomplish the same goal more quickly.” Since the machine learning tool used in this research (i.e., ANFIS) contains details about its reasoning process, an appropriate detailed message can be provided","The classifier is well-designed to support future work regarding presenting specific interruption messages that are personally relevant and of substantial utility to the user's current context. 5.5.3 Enhanced personal modeling This research showed that some user characteristics (e.g.,  ) are significant in the creation of good classifiers","Future research that aims to deploy machine learning classifiers for detecting interruptible moments in practice should focus on collecting additional user characteristics such as, biometrics from wearable devices (e.g., SP0  pulsioximeter, spirometer, blood pressure, EMG, ECG, temperature, etc.), and enriched personality trait attributes (e.g., Myers–Briggs personality test)","These additional inputs in combination with task and environment details may result in classifiers that offer improved accuracy, performance and relevance to users’ interruption experiences. 6 Conclusion Determining when to interrupt a user at appropriate times as s/he performs computer-based tasks is an ongoing problem to which we have provided a solution","We created a classifier that contributes to the field in the following ways: 
                   This research also assessed the participant's performance at the tasks and evaluated the classifier's performance",It was demonstrated that many models performed extremely well,The classifier was designed with a framework so that it could be generalized to other tasks and problem domains,"In the spirit of furthering science and this work, the MATLAB source code for the classifier, models, and data sets will be openly available on the author's and/or journal's website",We hope this will encourage other researchers to extend and explore our work and to test and compare our classifier with other interruption systems,Appendices This section presents supporting documents used in conducting this research,"The appendices are: 
                   
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                   
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                   Appendix A: experiment protocol This appendix presents the experiment protocol that was used by the researcher to ensure the experiment was being conducted in a consistent way for all participants. 
                         
                         
                      Experiment protocol 
                            
                         Appendix B: opening questionnaire – participant This appendix presents the opening questionnaire that was used to collect information from participants in this research. 
                         
                      Appendix C: closing questionnaire – participant This appendix presents the closing questionnaire that was used to collect information from participants after completing the experiment. 
                         
                      
                         
                      Appendix D: closing questionnaire – results: strategies and general comments This appendix presents the strategies and general comments from selected participants (data extracted from closing questionnaires). 
                      Appendix E: ANFIS fuzzy rules This appendix presents the ANFIS Fuzzy Rules used during the development and refinement of the interruption classifier. 
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      Appendix F: functional description of the interruption classifier The Interruption Classifier is based on an Adaptive Neuro-Fuzzy Inference Systems machine learning tool",The ANFIS is an advanced hybrid soft computing tool that use fuzzy logic and artificial neural networks ( ),Fuzzy systems and neural networks are two tools that nicely complement one another,"While fuzzy logic allows a problem to be viewed on higher and more human-intuitive level, neural networks have been shown to work very effectively in learning, adapting and dealing with raw data ( )","However, fuzzy systems lack the ability to learn and make self-adjustments","Thus, the amalgamation of a fuzzy system with a neural network into one system (i.e., ANFIS) offered a number of benefits for building the Interruption Classifier.  
                          shows a 2-input ANFIS",Layer 1 is the input layer,Layer 2 is the fuzzification layer where neurons determine the degree of membership based on the input and quantifier (this is the   of the fuzzy rules),Layer 3 is the rule layer: each neuron in this layer corresponds to a fuzzy rule,Layer 4 is the normalization layer,Layer 5 performs defuzzification during which each neuron calculates the weighted consequent value of a given rule (this is the   of the fuzzy rules),Layer 6 produces the final output,For a comprehensive description of ANFIS please see  ),ANFIS's offer efficient and effective learning and adaptiveness capabilities ( ),This is accomplished through forward and backward propagation of error signals through its network,"In the forward pass, a training set is presented to the ANFIS, neuron outputs are computed layer-by-layer in the network and the rule-consequent parameters are determined by the least-squares estimator ( )",These results are then used in the next pass in the learning process—the backward pass,In the backward pass the back-propagation algorithm is used,Error signals are sent backwards through the network and the antecedent parameters are tuned appropriately using the chain rule ( ),"In this way, an ANFIS can learn and adapt quickly based on training datasets","In this work, the successful models created by the Classifier were based on the following principles: 
                         : The success of a model is dependent on well-defined rules that represent the task and user contexts as fully and precisely as possible","A set of rules were created based on user modeling information such as personality traits, frustration level, tolerance to interruptions, etc",Additional set of rules characterized task details in combination with the user's real-time activities,"For example, in the main experiment, the fuzzy variable,  , is dynamically adjusted based on the number of jumpers in the primary (game) task",This variable was characterised by 4 membership functions:  ,"Similarly,   representing the number waiting of interruption tasks, was characterized by 4 membership functions:   and  ","Each membership function was a trapezoidal wave form carefully constructed from ANFIS research literature, researcher observation, user's strategies and empirical evidence","The Classifier then tuned these functions based on the training datasets. 
                         : Using different training datasets had a profound impact on the models created",Several participant datasets produced models that met and exceeded the performance criteria,These models performed well and were generalizable across the entire participant group,"Analysis showed that these participants were very consistent in their strategies and interruption timings when performing the tasks. 
                         : Checking datasets were used for model validation",The model error for the checking data set tends to decrease as the training takes place up to the point that overfitting begins ( ),A substantial amount of research was conducted to reduce overfitting,"This was accomplished by experimenting with different checking datasets and the number of epochs (iterations of forward-backward passes) for the ANFIS to learn effectively. 
                         : The impact of historic event knowledge is significant","Knowing the past, especially when it is 100 ms or less in the past has a profound impact in reasoning and deciding whether to interrupt the user at a given point in time",All models created that used historic knowledge performed extremely well,The Classifier is generalizable to a degree,We discovered that user modeling information such as personality traits are generalizable across all models in all the experiments conducted,"However, other aspects of the modeling process (e.g., task details), need to be explicitly represented in a model for it to be successful and are therefore not directly generalizable","Appendix G: raw numbers for the cases of ``interruptions"" relative to the total number of cases 
                         
                      Supplementary materials Supplementary material associated with this article can be found, in the online version, at  ","Appendix H Supplementary materials 
                      
                  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0957417415000238," 1 Introduction One of the main objectives of research and learning processes is achieving maximal effectiveness from the creation, transfer and dissemination of new knowledge",This effectiveness can be measured by the quality and speed of memorization of the principal concepts of a particular domain and of the relationship between these concepts,"Wide evidence exists that the visual thinking used to address the subject of study is positively connected with the quality and speed of memorization, and thus with the effectiveness of knowledge dissemination",Visualization is working as a cognitive tool that facilitates communication both in teacher/learner interaction and within research communities,"Special interest in such graphical forms of knowledge codification can be observed in education science, especially within learning where the students are engaged in group knowledge sharing and co-creation processes with continuous feedback",Mutual understanding and mentalization in research is of special interest in collective study or discovery,One of the most productive methods of research and learning collaboration promises to be group ontology design,An ontology is a set of definitions we make in understanding and viewing the world ( ),The specific problem being addressed in this work deals with the problem of improving the quality of group or collective ontologies,"We are also interested in filling the gaps in understanding the group ontology design process specifics, such as the causes of differentiation between the form and the content of individual ontologies","During the last decade, visual knowledge representation has become one of the key considerations in knowledge engineering methodology, and it is strongly associated with ontology design and development","These ontologies, which form a conceptual skeleton of the modeled domain, might serve various purposes such as better understanding, knowledge creation, knowledge sharing and reusing, collaborative learning, problem solving, seeking advice, or developing competences by learning from peers ( )","Recently, the ontological engineering perspective has gained interest in many research domains, such as medicine, business and computer science ( )",These studies rely heavily on theory and tools from knowledge engineering analysis that already has a long-standing tradition in the knowledge-based systems domain ( ),The largest number of knowledge engineering research articles has been generated around the theme of descriptive logics and formal foundations of ontology design ( ),"Our work, however, emphasizes the informal approach based on human-centred ontology design processes, an aspect neglected by most of the existing approaches","Several attempts have been made to bridge this gap and ease the overall ontology development process, such as HCOME – Human-Centered Ontology Engineering MEthodology, by  ; and human-centred ontology design, by  ",The tools and techniques developed in the domain of ontology engineering can be applied fruitfully in the field of knowledge structuring and design ( ) and semantic web applications ( ),The idea of using ontologies and visual structuring in research description and introduction has been discussed in many works ( ) and is now being implemented in several research projects and software tools ( ),"This paper presents the main results of the KOMET (Knowledge and cOntent structuring via METhods of collaborative ontology design) project which was devoted to developing methods that use group visual ontology design in educational purposes, with regard to the respondents’ individual cognitive styles",The group ontology design was tested in the medical domain by a smaller group ( ) and computer science (informatics) domain by a larger group of participants ( ),"In the larger group of 79 respondents, all the participants were graduate students of the School of Computer Science of Saint Petersburg Polytechnic University","Almost all had 1–2 years’ experience of research in computer science, and were in their fifth year of study, on the Masters programme",The domain “computer science” was chosen as all the students are young professionals in this area,We use the term synonymously with “informatics”,The paper is organized as follows,"First, it describes the concept of ontology, with an emphasis on the visual approach to ontology design","Section   concentrates on the theoretical background, with sub-Section   describing ergonomic metrics and their purpose and sub-Section   providing an overview of cognitive styles and the tests used to assess them","Section   presents our human-centred research paradigm and framework, and Section   the results obtained in the study of the relationship between cognitive styles and the peculiarities of individual development of ontologies","Section   introduces the main results of collective ontology development, taking into account the cognitive styles of participants","Finally, some conclusions are drawn and future work is outlined. 2 Theoretical background of ontology engineering: visual bias The idea of using visual structuring of information to improve the quality of understanding and mentalization among research colleagues is not new ( )","For more than twenty years, concept mapping ( ) has been used to compile maps and mental models that support the process of knowledge sharing","Many scholars, especially those who teach science courses, operate as knowledge analysts or knowledge engineers by making visible the skeleton of the studied discipline and showing the domain’s conceptual structure ( )",This structure is frequently represented by a so-called “ontology”,"From a philosophical viewpoint, “ontology” is the branch of philosophy which deals with the nature and organization of reality","Ontologies aim at capturing domain knowledge in a generic way and providing a commonly agreed understanding of a domain, which may be reused and shared across applications and groups ( )","Neches and colleagues ( ) gave the classical definition as follows, “An ontology defines the basic terms and relations comprising the vocabulary of a topic area as well as the rules for combining terms and relations to define extensions to the vocabulary”",There are numerous other definitions of this milestone term ( ),"Together, these definitions clarify the ontological approach to knowledge structuring while giving sufficient freedom for open-ended, creative thinking",Many researchers and practitioners have argued about the differences between ontology and a conceptual model,"We propose that ontology corresponds to the analyst’s view of the conceptual model, but is not   the formal model itself",The visual approach to presenting ontologies is not only compact but also comprehensive,It makes ontology a powerful mind tool ( ),"By definition, ontology is a declarative representation of a certain precise domain specification, including the glossary of the domain terms and the logical expressions describing the meanings and the relationships of these terms, thus allowing structured sharing of knowledge related to the domain ( )","The relationships between the concepts in ontologies can be of different types, e.g. “is”, “has part”, “has a property of”, etc",The concepts and relationships are universal for a certain class of objects in a subject area,"Conceptual model visualization methods such as ontologies are also widely and effectively used in education, and many learning ontologies have been developed for a number of disciplines ( )","However, the ontology-based approach to conceptual knowledge representation in research and pedagogy is a relatively new development",Ontologies are now considered as the most universal and shareable forms of such representation and modeling,There are more than a hundred techniques and notations that help to define and visualize conceptual models,"Ontologies are useful structuring tools, in that they provide an organizing axis along which every researcher (or student) can mentally mark his/her vision in the information hyper-space of domain knowledge","Frequently, it is impossible to express all the information as a single ontology","Accordingly, subject knowledge storage consists of a set of related ontologies","Some problems may occur when moving from one ontological space to another, but constructing group meta-ontologies may help to resolve these problems","For both formative and summative assessment purposes, creation of ontologies and explanation of the processes involved can clearly indicate the extent and nature of the knowledge and understanding",Knowledge entities that represent the static knowledge of the domain are stored in hierarchical order in the knowledge repository and can be reused by others,"At the same time, those knowledge entities can be reused in descriptions of the properties or a methodological approach as applied in the context of another related knowledge entity","Of course, the ontologies are inevitably subjective to a certain extent, as knowledge by definition includes a component of personal subjective perception; however, using the ontologies developed by others is a convenient and compact means of acquiring new knowledge","At the same time, collective ontology development experience allows the participants in the process to gain the fullest possible understanding of the subject area","Meta-ontology provides a more general description dealing with higher-level abstractions (mind maps ( ) and concept maps ( )).  
                       ( ) illustrates different ontology classifications in the form of the mind map",This representation may be called the knowledge map,Such maps are graphical tools for organizing and representing knowledge,"Knowledge maps are now widely used for visualizing ontologies at the design stage, while ontology editors (like Protégé) facilitate the development stage",Research on knowledge mapping in the last 12 years has produced a number of consistent findings ( ),"People recall more central ideas when they learn from a concept map than when they learn from text, and those with low verbal ability or low prior knowledge often benefit the most",It seems that knowledge maps reduce cognitive load,The use of knowledge maps also appears to amplify the benefits associated with scripted cooperation ( ),Learning from maps and communication via maps are enhanced by active processing strategies such as summarization or annotation and by designing maps according to gestalt principles of organization ( ),"Bearing in mind that ontologies are to be used not only as a knowledge component of knowledge management systems but also as a mind tool for comprehensiveness and better understanding, we have tried to follow the principle of good shape (or beauty) that is not new in basic scientific abstraction, and modeling (e.g. physics, chemistry, etc.)",It is difficult to give a formal definition of this concept but it features the imprecise sense of harmonious or aesthetically pleasing proportionality and balance,The most substantial impulse to it was given by the German psychologist Max Wertheimer ( ),In a previous paper ( ) we described how to apply this beauty-centred approach to business ontology design,"In the KOMET project we consistently develop this method by combining expert assessment with formal assessment by ergonomic metrics, as presented in the next section. 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   2.1 Cognitive ergonomic metrics Many aspects affect the quality of an ontology from the cognitive ergonomic point of view, including the basic principle of evaluating visual perceptibility and understandability",Metrics of this kind were first proposed by the research group of Aldo Gangemi ( ),The ontology evaluation based on these metrics is formal but it helps to assess the quality of the ontology,The expanded list of metrics and the software tool for its assessment COAT (Cognitive Ontology AssessmenT) were presented in detail in two works ( ),The COAT software environment provides the calculations for more than 30 metrics,COAT is implemented as an application in Java,"Ontologies in OWL format are supported by the Jena Semantic Web Framework, a Java library class for working with RDF and OWL ontologies",The main metrics are calculated automatically,"In evaluating the quality of the designed ontologies, the following two aspects are most important: (1) correctness and depth of reflection of the subject domain, and (2) ergonomic aspect of the ontology representation from the point of view of quality and human speed of perception","In the KOMET project the quality of the developed ontologies was assessed by two methods: 
                      The formalized method is preferable as it is free from experts’ and analysts’ subjective interpretations and had the potential to be automated","In our research the developed ontologies were assessed by an augmented set of metrics (such as minimal depth, absolute width, etc.) suggested in  ","The notation used to describe the metrics is as follows: 
                      
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      a A minimal depth 
                            where   and   are the path lengths   and   from the set of paths   of the graph  . b An absolute width 
                            where   is the number of nodes of degree   from the set of nodes   of the graph  . c An average width 
                            where   is the number of nodes of degree   from the set of degrees   of the graph  ,  , the number of all degrees of the graph (a maximal graph depth augmented by 1, if considering only a chosen dominant relationship). d 90% line depth 
                            where   is a 90% percentile of the graph depth (possible value of the graph path length, not exceeding the length of 90% of the graph paths). e Root-mean-square deviation of neighboring levels/degrees width ratio 
                            
                         f Complexity metric A number of nodes with multiple inheritance to the set of all the graph nodes: where   is a set of all the graph nodes with more than one “is-a” relationship arc,   is the number of all the elements of this set,   is the number of graph nodes. g Average number of the parent nodes of a graph node 
                            where   is a set of all the parents of the node  ,   is the number of all the parent nodes of the node  ,   is the number of the graph nodes",These metrics can help in understanding what should be corrected in the description of the subject domain in order to improve it from the point of view of cognitive ergonomics or better perception,"Thus, it is supposed that each next version of the ontology will be better and can be perceived faster by users",The metrics can also be used in evaluating ontologies of the same subject domain produced by different people/teams,"The calculated metrics help to estimate which of them is better from the point of view of cognitive ergonomics, and to choose the best of them if the evaluations of other important criteria differ insignificantly.  
                            
                             show different perspectives of ontology structure. 2.2 About cognitive styles The cognitive-styles view has acquired great influence within the education field, and is frequently encountered at levels ranging from kindergarten to graduate school","There is a thriving industry devoted to publishing cognitive-style tests and guidebooks for teachers, and many organizations offer professional development workshops for teachers and educators built around the concept of learning styles ( )","However, we will use the concept of cognitive style only for the aim already stated","As the aim of the KOMET project was to develop a paradigm of structuring data and knowledge with regard to individual cognitive styles, we had to choose the appropriate parameters or features of cognitive style",The cognitive styles explain and describe how an individual acquires knowledge and how an individual processes information,"The cognitive styles are related to problem solving, and generally to how information is acquired, structured and used","Among the main features of cognitive style ( ) we can name: 
                      Three characteristics have been chosen from the plethora of cognitive style characteristics described in the literature ( ): field dependence/field independence (FD/FID), impulsivity/reflection, and narrowness/width of the category","According to the definition by Witkin ( ), FD/FID is “a structuring ability of perception”",The field-independent style is defined by a tendency to separate details from the surrounding context,"It can be compared to the field-dependent style, which is defined as a relative inability to distinguish detail from other information around it",The FD/FID characteristic can be interpreted as a proxy of the structuring capability of an individual mind,"The characteristic of this style does influence the structuring process as a whole (e.g. ontology development “from scratch”), and even more it affects the restructuring process (the merging of individual ontologies)",FD/FID exerts considerable influence on the collective problem-solving process,"In dyads where members have cognitive styles differentiated by the FD/FID characteristic, the final solution is usually closer to the variant suggested by the FID participant","The FID dyads experience difficulty in developing common decisions on arguable points, while the FD dyads are more successful in coming to agreement in collective problem solving","Psychologists in our research group were used to working with on-line test, based on the popular modification of Witkin’s method of “embedded figures” which aims to find a simple figure hidden within a complicated one ( )","The impulsivity/reflection characteristic considers the amount of information collected prior to making a decision: impulsive individuals are able to make decisions on a considerably bounded information basis, while reflective individuals are more inclined to make decisions considering full information concerning the respective situation","For assessing the respondents’ impulsivity/reflection features, the “Matching Familiar Figures Test” ( ) was used","As for the narrowness/width of the category, the main difference between the extreme poles of this characteristic is that narrowly categorizing individuals are inclined to restrict the area of application of a certain category, while the broad categorizers are, conversely, inclined to include a plethora of more-or-less related examples in a single category",The psychologists consulted on the experimental part of our work advised us to use a modification of the   by  : the so-called “range width test”,"The procedure is based on respondents’ opinion on the minimum, average and maximum evaluations of a concept or category. 3 Research design: from individual to collective 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   3.1 Research paradigm: human-centred approach The KOMET project was targeted at developing a paradigm of data and knowledge structuring with regard to individual cognitive styles, using recent advances in knowledge engineering and conceptual structuring, aimed at creating structurally holistic knowledge bases for various areas of science and technology","We supposed that individual cognitive peculiarities may dramatically affect the shape and the content of domain ontology, as each ontology designer (knowledge engineer) has specific personal cognitive features","These features may affect such syntactic and semantic parameters of the domain ontology as: 
                      We studied the medical case of a dermatology ontology design process, which took months to complete because of misunderstandings and contradictions within the development team, as a result of cognitive dissonance issues","The difficulties, problems and obstacles within another project on an optics ontology were similar, even in such a well structured domain","The actors– both knowledge analysts and experts – often disagreed over concept vocabulary, relations and even the principles of the designed structure","Most of the methods and methodologies for building ontologies are focused on the technological development activities, especially on ontology formalization and implementation, and they do not pay too much attention to other important aspects related to management, learning, merging, integration, evolution and evaluation of ontologies ( )",There are several methodologies for current ontology development,"On the basis of the knowledge acquisition method, the methodologies can be classified into automated, semi-automated and manual (from “scratch”)","Even though automatic ontology learning methods, such as text mining and knowledge extraction ( ), support ontology engineers by speeding up their task, significant amount of manual work is still required in the completion, consolidation, and validation of the automatically generated ontology",The manual method is based on the interaction between the knowledge analyst and the expert (as it was in classical expert systems:  ) and is an example of the collective ontology design process,"The literature review identified over a dozen such approaches; for example, Iqbal and colleagues (2013) compared 15 methodologies, concluding that collaborative construction methods are not standardized and little attention is paid to this aspect","Other methods include UPON (Unified Process for ONtology building), based on a well established and widely used software engineering process, the Unified Process (UP) ( ); and the classical and popular METHONTOLOGY, which has been tested on several knowledge domains","Of course, such methods of design and development are not without drawbacks, although collective construction mitigates the extreme differences between individual biases","However, none of these methods takes into consideration the individual peculiarities of the experts and analysts","These peculiarities comprise education, background, experience, temperament and personality, including the specific cognitive style parameters","The novelty of the KOMET project is that we propose to expand the existing palette of ontology, merging approaches from the human-centred viewpoint by preliminary psychological testing to identify the characteristics of individual team members",This makes it possible to propose a better way of collective working in ontology merging and alignment,The results allow different roles to be assigned to the team members,"If several experts/analysts are available they can be paired or grouped to optimize the knowledge structuring work. 3.2 Research framework The KOMET project’s objectives include: 
                      The research was divided into two phases: A (individual) and B (collective)","The phase-A research was composed of four consecutive steps: 
                      A2 step performed using the same test sample of students as A1",All the students were given the task of developing a light-weight ontology for the computer science domain,They did this either by using a visual mapping approach with a pen-and-paper technique or by using the mind mapping software,Later they developed the same ontology with the Protégé tool and we assessed their ontologies in OWL-format,"Phase B consisted of the following steps: 
                      The KOMET-DILIGENT collective ontology development methodology was designed for step B2 ( )",This methodology enriches the findings of the Neon Project ( ),All the individual and collective ontologies (both in pairs and in groups of 3–5) were analyzed,"In the KOMET project, specificity of the collective ontology development was researched","The experiments also aimed to establish how the collective categorization style is developed. 4 Analysis of the individual construction affected by cognitive style As mentioned in the introduction, the research sample consisted of 79 students were enrolled in the Intelligent Systems Development course",They were given the task of developing an ontology for the computer science research domain,"Due to the professional specificity of the sample, a bias toward narrow, reflective and field-independent test persons was found in the sample","However, a statistically significant Spearman’s negative correlation between the FID score and the time of the first answer in the Kagan was calculated, showing that the sample was dominated by the fast FID and slow FD respondents","On the basis of the literature review and the practical ontology development experience, the following hypotheses were suggested: 
                      
                      
                      
                   
                      
                       presents part of the two series of test results","It describes the correlation coefficients for several metrics and the main parameters of cognitive style: 
                   The correlation between the cognitive style and ontology metrics values was assessed by Spearman’s coefficient (rank correlation)","The significant correlation between the metrics and such features as field dependence/field independence was not found, so it is not presented in the table",Empty cells in the table mean that no significant correlation was found,"Hypothesis 1 was not confirmed, as no significant correlation between the FD/FID metric and the quality of the ontologies was found; this result gave rise to optimistic feelings about the whole project, as it shows that it is possible to teach any individual to develop ontologies of a high quality","Hypothesis 2 was partially confirmed: the “90% line depth” metric demonstrated significant positive correlations with the time of the first answer in the Kagan test, thus showing that reflective test persons tend to develop deeper ontologies; however, no significant negative correlation between the time of the first answer and the ontology width was found","Hypothesis 3 was confirmed, as the number of mistakes in the Kagan test demonstrated a significant positive correlation with the values of the “average number of parents of a graph node” metric that characterizes the ontology complexity","Furthermore, the number of mistakes in the Kagan test demonstrated significant positive correlations with the metrics of the “minimal depth of the ontology” and the “families branching coefficient” and significant negative correlation with the weighted leaves branching coefficient","Hypothesis 4 was fully supported: the broad categorizers developed larger ontologies in terms of the number of concepts, achieved mainly by the greater number of “children” of each parent concept","Respectively, the results of the “range width test” demonstrated significant correlations with such metrics as the “average ontology width”, “number of leaves”, “absolute cardinality of families”, etc",These results also demonstrated significant correlation with the root-mean-square deviation of the average ontology width,"This result shows that the number of concepts belonging to the neighboring levels and to different branches is significantly different, indicating imbalance in the ontologies developed by the wide categorizers","Despite the objectivity of the quantitative metrics-based method of ontology assessment, this method has the significant drawback of being too formalized and lacking semantic analysis elements","Having augmented the quantitative metrics-based analysis by a semantic analysis performed manually, we found that the ontologies developed by the field-independent test persons tended to have simpler and clearer structures","However, this simplicity and clarity tended to be achieved by truncating the concepts that did not fit into the developed ontology, thus sacrificing the ontology’s completeness and integrity for formal logical consistency","As for the collective ontology development, including wide categorizers in a single group with a FID individual can be useful, with the wide categorizers generating a plethora of sub-classes and the FID participant restructuring them",This hypothesis was tested on the stage of research dedicated to collective ontology development,"Thus, the following relationships between the respondents’ individual cognitive styles and the peculiarities of their subject domain ontology development have been identified as follows: 
                   5 Collective ontology design Specificity of the collective ontology development was also studies, both in dyads and in groups of 3–5",One objective of this study was to establish how the collective categorization style was developed,"The KOMET-DILIGENT collective ontology face-to-face design methodology proposed within the KOMET project uses the following algorithm ( ): 
                   Students first developed individual ontologies and were then asked to develop a collective common ontology on the same topic, “computer science”",The time allowed was one hour,The experiments aimed to establish how a collective categorization style was developed,"Two strategies were identified: 
                   These strategies were affected by the peculiarities of analyzing and merging individual ontologies in the collective ontology development methodology suggested and explained above","The second strategy, S2, is of greater practical interest","In this case the respondents effectively applied all three basic ontology engineering operations (matching, merging and alignment)","The results showed that merging usually follows either of two scenarios: 
                   We researched the mechanistic scenario in more detail, as this scenario was used more often than the others","The implications led to the design of two models: 
                   Comparison of these scenarios with the cognitive styles of the test participants revealed the following relationships: 
                   
                      
                      
                      
                       illustrate the experiment and demonstrate the variation from the synthesizing scenarios","This variation is the best alternative, with collective effort showing the synergetic effect of “jumping” to a higher level of abstraction",The higher level always demonstrates deeper knowledge and better understanding of the domain specifics,"Although these ontologies can be criticized as they were developed by young researchers, our experiment was targeted at the study of collaborative ontology design, not the production of a serious domain ontology",Mind-mapping representation is used only to illustrate the changes which occurred during the merging process. 6 Recommendations and future research directions This paper addresses the conceptual limitations of traditional research and learning communication and proposes using a visual metaphor for illustration and presentation of essential pieces of knowledge,The first most obvious possible application of the results can be found in presenting the research findings to the participating groups of researchers; the collaboratively developed ontologies may be used for conference presentations and journal publications,"Secondly, this approach may facilitate supervising Masters and PhD research and thesis writing","Indeed, team ontology development can lead the participating students to a deeper and more holistic understanding of their respective subject domains, especially when the instructor takes the students’ cognitive styles into consideration when organizing groups for team ontology development",The results may also help in organizing brain storming and design thinking sessions ( ),"Finally, the impact of individual cognitive styles on team ontology development can be taken into consideration in all the areas where intellectual team work on ontology development might be used, e.g. fundamental science, R&D or management consulting",Using visual inspection of the ontology it is possible to detect gaps and misunderstandings in state-of-the-art knowledge and cognitive models of the domain knowledge,"However, there is as yet little consensus on the useful design and orchestration of such structures","Furthermore, in many cases it is not known what the structure of socially legitimate knowledge patterns looks like, or how a particular instance of a knowledge model deviates from that “ideal” state (e.g. the guru’s view) ( )","However, researchers are individuals, and they may disagree among themselves",The main focus of future research is concentrated around the deeper understanding of the cognitive issues of collaborative work,"This discourse on the concept of cognitive style encompasses a number of interconnected topics, ranging from the impact of psychological theories to current perceptions about relationships between learning, understanding and teaching","As this journal focuses on expert systems development research, it may be interesting to aim further investigation at collaborative interdisciplinary ontology design for mapping the emerging research areas of artificial intelligence and cognitive computing",It may be fruitful to discover new possibilities for such design and to investigate how ontologies can be arranged to support the process of knowledge-base development,Future research relates such issues to specific features of interdisciplinary collaboration. 7 Conclusion The chief result of the KOMET project may be formulated as its “human-centred approach to ontology design and development”,"It proposes not a tool but a methodology that can be easily and cheaply implemented for group knowledge engineering, in which the cognitive peculiarities of experts and knowledge analysts affect the form and content of the designed ontology",The study described here is only a first step in interdisciplinary research dedicated to enquiring into the effect of the expert/user/learner’s individual cognitive style on group structuring design,Our results are therefore of a preliminary nature,"Two stages of research have been completed: first, research into correlations between the expert’s individual cognitive style and the peculiarities of his/her development of the subject domain ontology; and secondly, research into correlations between the expert’s individual cognitive style and group ontology design (including design performed in groups consisting of experts either of similar or of different cognitive styles)","The results of these research stages can be applied to organizing collaborative ontology design (especially for research and learning purposes), data structuring and other group analytical work",The implications for practice are briefly delineated,The results of the interplay of different cognitive styles can be observed as cognitive dissonance and discussions at different stages of ontology development,This is why these results may be used in the team-building phase of the ontology design lifecycle,"The “recipe” for a good expert/analyst team may be defined as: 
                   Our empirical study was organized as individual and group visual ontology development","Group ontology design, both in dyads and larger groups, was performed either “from scratch” or on the basis of the drafts prepared by the members of the group individually",Merging of the individual drafts into a single group ontology follows either an absorption (uptake) or compromise (synthesis) scenario,The mechanistic scenario of the merging of ontologies can be implied from either a conjunctive or a disjunctive model,"From the different cognitive styles, the field-dependent participants tended to prefer the disjunctive model of merging ontologies, with the higher-power ontology absorbing the lower-power one, and further merging of the same-level nodes in the resulting ontology","The field-independent participants tended to prefer the conjunctive model, with node reduction by conjunction or interception only of the same-level nodes","Despite the preliminary character of the research results, all the findings can be used in organizing collective ontology development, data structuring and other group analytical work","By taking into consideration their students’ cognitive styles when organizing them into groups, the instructor enhances her or his ability in guiding the ontology development process to reach deeper levels of students’ understanding of the subject domain",Our work presents a new human-centred viewpoint on collaborative ontology development,Knowledge of the personal cognitive peculiarities helps to leverage the subjectivity of the individually designed ontologies,"This approach may lead to better results, through avoiding the pitfalls of personal incompatibility and conflict within the team of ontology developers",Eventually this will shorten the development period and lead to an increase in ontology quality,"Finally, our approach does not require the construction of an entirely new technique; it may be embedded in the context of existing methods by separating the roles within the ontology development team.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s1071581918304312," 1 Introduction In the past few decades Human-Computer Interaction research has moved beyond concerns of usability to study experience related topics such as beauty, enjoyment, fun, emotion, and engagement ( )","Indeed, engagement has been identified as one of the most desirable and essential experiential qualities of HCI activities ( )","Within studies of engagement, creative engagement has been identified as a sustained and intrinsically rewarding engagement experience ( )","This is where a user is engaged in an active, reflective and constructive cognitive process in pursuing a creative outcome with an interactive system ( )","In this way creative engagement emphasizes users’ creative experience over their creative output, and helps to make an interactive experience a ‘memorable one, rather than a ‘pretty one ( )","As a relatively new and elusive concept in HCI, the challenge for studying creative engagement include the lack of an agreed definition and positioning within the broader context of HCI","This is partly because most previous discussion took place within the context of interactive arts ( ) and education ( ), resulting in a lack of design suggestions for supporting creative engagement in other domains",There is also a lack of evaluation criteria as creative experiences should not be evaluated solely on the quality of the contributions or the output as the creative output is valued on a personal level ( ),"Of particular interest to this paper is the challenge of how to design support for novices’ creative engagement and to inform future design of such systems. 
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   1.1 Music making Music making is an ideal activity to study in terms of creative engagement as it is regarded as a fundamental form of creative human activity which has played a major role in human intellectual evolution ( )","Moreover, it combines self expression and creativity with entertainment","It also provides an excellent ground for studying and comparing the interactions of a range of users, for example individuals and groups, amateurs and experts, children and adults, etc. ( )","The experience of creating and enjoying music through playing is often rewarding, offering “an affirmation of life” because of these exploratory, engaging, intuitive and enjoyable qualities ( )","Studying support for music making activities can contribute to HCI research in a wide variety of topics, theories, methodologies and technologies, especially as music making and HCI share concerns of simplifying complex tasks ( )","For example, recent research in the fields of New Interfaces for Musical Expression (NIME) ( ) and Human-Computer Interaction (HCI) have a range of overlapping research themes that could illuminate both fields in terms of theory and methodology ( )","These overlapping themes include: methodology for evaluation, i.e. ethnographic inquiry, situated approach ( ); cognitive topics such as spatial cognition, embodied cognition( ); interaction topics such as parameter mapping ( ), control, i.e. haptic or gestural control ( ), multimodal interaction, i.e. audiovisual interfaces ( ), tangible interaction ( ); experience topics such as intimacy ( ), playfulness, creativity ( ), participation ( ), engagement ( ); and social topics related to Computer Supported Collaborative Work (CSCW) such as collaborative music making ( ). 1.2 Musicking In the NIME field the term ‘musicking’ ( ) has emerged to describe a more accessible music making activity that is not exclusive to trained musicians ( )",This trend has produced designs which have enabled non-musicians to actively   music rather than only passively listening to music ( ),This paper considers non-musicians as amateurs of musicking who are interested in musicking activities but with no intention to be professional musicians,"Non-musicians should be distinguished from people who are musical beginners who have the intention to become professionals later on, as non-musicians will have less access to formal music training ( ) and less motivation to engage in formal music training","Musical creativity, which has often been considered the exclusive domain of professionals, is reported to be hard for non-musicians to achieve ( )",Studies reveal that it is difficult for non-musicians to develop their musical ideas from scratch due to their lack of conceptual and technical music making knowledge and skill (ibid),"Studies also indicate that not only are novices restricted by their abilities but also by their lack of confidence in producing creative outcome ( ). 1.3 Creativity support tools The domain of Creativity Support Tools (CST) has been exploring the design and evaluation of systems to facilitate creative processes for more than a decade, particularly for professional purposes ( ), making it promising as a domain to inform the design of novel musical interfaces for creative experience and engagement","However, most research into supporting novice’s creative acts focus on how to scaffold novices’ creative output rather than to support their creative experience per se, for example, in the domain of design ( ), video making ( ) and painting ( )",There is still substantial work to be done to understand novice’s creative process from an experiential perspective and the factors that might affect their sustained creative engagement,"Studies have highlighted that a user’s  , that is, whether a user is given an exploratory goal that aims for an hedonic experience or given a utilitarian goal that aims for a concrete output, will strongly affect their choices, experience, and engagement with products ( ), as well as their creative performance ( )",One challenge for studying designs which aim to support creative engagement is therefore whether to give novices an explicit utilitarian motivation to create an output or to simply ask them to explore the interface as a form of experiential motivation,"Most new musical interfaces designed for non-musicians follow the dynamic real-time conventions of conventional instrument design ( ) such as a guitar or flute, inherently offering an   musicking mode ( ) of interaction (as discussed in  )","In this case music is produced in real time in direct response to user input, much as it might be by a conventional instrument",Improvisation can be a barrier for non-musicians to create music because their working memory is insufficient ( ),"Moreover, according to studies of CSTs outlined above, ‘rich history keeping’ is a fundamental mechanism for supporting creativity because having a record of which alternatives have been tried makes creative modification and improvement easier to achieve ( )","However, current musical interfaces that provide history keeping and allow for modifications, e.g","GarageBand 
                          and Logic Pro, 
                          are mostly designed with for a   musicking mode ( ) (as discussed in  )",The two musicking modes of compose and improvise outlined here employ different user interface features (e.g. editing and replay versus real-time sound manipulation) and require different sets of user skills and activities in order to produce a creative output,"The rest of this paper is organized as follows:   introduces Interactive Musical Systems and literature on Creativity Support Tools, as well as features of musicking mode and motivation, which leads to the research question","This is followed by   which provides an overview of the design and implementation of Interactive Musical System used in the study reported in this paper.   introduces the experiment design including the hypothesis, independent and dependent variables, and study procedure.   presents the results of the study along with statistical analysis and thematic analysis.   provides a detailed discussion of the hypothesis in relation to the results, followed by a descriptive model of creative engagement and design implications",Limitations and future work are also discussed in  ,"Finally,   concludes this paper with a summary. 2 Related work Unlike traditional musical instruments that generate sound through physical acoustic mechanisms, new interfaces for musical expression generate sound digitally by mapping users’ input to sound output ( )","Generally, there are two paradigms of design in this domain: (i) a Digital Musical Instrument (DMI) which is designed for expert users involved in professional level music production and performance; (ii) an Interactive Musical System (IMS) designed for non-expert users for exploratory and experiential purposes ( )","The interfaces discussed in this paper are within the scope of IMS because we are interested in supporting novices’ creative engagement. 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   2.1 Interactive music systems There are three typical design features of IMSs designed for individual participants ( ):   Unlike the design of DMIs that emphasise the system’s expressiveness, responsiveness and the final sound output, IMSs are designed to foster and facilitate engaging experiences that are rewarding to participants ( )","As the priority in these systems is the process and experience, the design is typically not driven by the musical outcome but is more likely to be driven by experience related themes such as social dynamics, communication, awareness, the rules of interaction, and so on.   As non-musicians usually have little or no skills and domain knowledge of music, the interfaces are designed with low entry fee to enable users to easily understand, learn, and intuitively interact with them ( )","Simplified mapping strategies between the input and sound, limited sound parameters, pre-recorded samples or pre-composed materials ( ) and generative algorithms to control all or part of the sound generation ( ) are often utilized to reduce the complexity of the interaction","Intuitive control mechanisms such as tangible interactions ( ), mobile interactions ( ), wearable interactions, spatial or gestural interactions ( ) and laptop-based interaction are widely adopted to provide intuitive interaction with low or little barriers to use ( ).   As discussed in ( )’s framework for the design of expressive musical interfaces, the faster the real-time sound processing and generation is produced in response to a player’s interaction, the higher level of control will the player will experience",The majority IMSs employ a dynamic real-time design paradigm offering immediate sound output in response to a player’s interaction ( ),"Only a small number of ISMs have embedded history keeping mechanisms to enable players to revisit, reuse or revise previous creations, usually following a step sequencer design ( )",A number of design practices have emerged through the design and evaluation of ISMs which contribute to facilitating non-musicians’ creative experience,"Firstly, using physical objects to directly control or represent musical parameters, referred to as  , is one promising way engage non-musicians intuitively and creatively ( ), as the haptic feedback provided is easy to learn and utilize people’s ‘sophisticated skills for sensing and manipulating physical environment’( )","These are realized in a number of ways including using portable devices to detect continuous motion or gestural data ( ), using tabletop systems for players to arrange and to manipulate a set of musical objects ( ), or using an instrument metaphor for players to control musical parameters directly through the interface ( )","Secondly, there are practices which aim at providing intuitive feedback","ISM design often integrates graphics and audio in real-time to use graphics to reinforce physical interactions by offering supplementary information and feedback on players’ interactions, the system state and the audio output ( )","Levin provided a summary of four metaphors for the relationships between computer graphics and electronic music in the field of visually-orchestrated computer music ( ), including:   that use visual representations to display musical information, such as musical score display;   that mimic the physical controllers in analog synthesizers;   that use virtual objects to manipulate or to modify sound parameters; and   that use drawings and free-form images from gestural interactions to generate or control sound","Thirdly, there are practices which aim to creatively engage non-musicians through fostering a collective collaboration ( ), suggesting the use of collective knowledge and creativity to support sustained musical creative engagement","Despite the research in these three design practice directions, it remains unclear how to engage individual non-musicians in a creative experience. 2.2 Creativity Support Tools The domain of Creativity Support Tools has been exploring the design and evaluation of systems to technologically mediate creative processes ( ), based on the view that creativity can be enhanced and fostered ( ), and that there are shared features across different domains of creative activities ( )","The main approach to support creativity is through facilitating the activities involved in creative processes, including   and   from previous works;   by consulting with peers and mentors at early, middle, and late stages;  , and   possible solutions;   and   the results and contribute to libraries ( )","Some approaches seek to support creativity through influencing individual abilities, interests, attitudes, motivation, intelligence, knowledge, skills, beliefs, values and cognitive styles ( )","A set of practical design guidelines derived from the research and studies into supporting activities involved in creative processes and improving the potential of creative output are summarised below: 
                      The above guidelines could be used to inform the design of IMSs as a form of Creativity Support","However, there is inherently a conflict between the iterative creative process that calls for the rich-history keeping with accessible records, and the current designs of IMSs that offer real-time music making with no history keeping","Furthermore, these two conflicting features correspond to two different modes of creation in the domain of music (composition and improvisation) which have different characteristics, processes and skills ( ). 2.3 A note on: composition, improvisation and comprovisation Composition and improvisation are the two most commonly discussed creative modes in traditional Western music theories ( ), having distinct features, and requiring different creative strategies, mental and physical skills","Composition is regarded as an iterative process of putting together musical elements, revising and storing them, whereas as improvisation is defined as a real-time performance process ( )","Compared to composition, the real-time pressure of improvisation requires more reliance on automated cognitive activities without conscious attention, highly constrained music structures, and pre-existing familiar patterns in order to reduce decision-making tasks due to the limitations of conscious attention (ibid)",Another distinction is whether the creative process involves rational reflection and revision (composition) or instantaneous innovation (improvisation),There is no tolerance of mistakes in the output of composition therefore revision of mistakes is indispensable for composition but not necessary for improvisation ( ),"Consider the representative activities of improvising with an instrument in a performance, and composing with audio composition software such as Logic Pro",When improvising with an instrument it is not possible to replay or to edit the previous creation,"However, with software such as Logic, users can replay and edit previous creations","With the emergence of electronic and experimental musical techniques since 1950s, the boundary between composition and improvisation began to blend ( )","In the context of electronic music, a more common form of performance is now regarded as  , a creative process in which improvisation is used as a precursor to composition to generate musical ideas and extend existing structures, and in which composed structures and instruments are then widely used in an improvisational setting ( )","These emerging musicking activities tend to incorporate composed material within an improvisational setting (ibid), allowing compositional structure as well as the expressiveness of improvisation","One example would be live coding performances, a form of musical performance via real-time composition of music by means of writing code ( ), which encourage improvisational creation using pre-composed sound materials and structures mixed with real-time programming of audio systems",Live coding also involves activities such as reuse and revision of the previous records as a live production,"Another example would be live performance using hardware such as Launchpad 
                          or Ableton Push 
                         , with which a player can play and record musical ideas such as rhythms, patterns of notes and combinations of these to one button, and replay, store, and restore them when necessary","However, in this setting there is no chance to edit the previous ideas","The above literature discussed typical features of composition, improvisation, and comprovisation, for example whether the process is in real-time or not, and whether the process allows revisiting or revising records","Although most of current IMSs are designed with the real-time features for the mode of improvisation and comprovisation, it is not clear how features of the composition mode may affect non-musicians’ approach to creative endeavo urs, especially as CST research suggests providing a mechanism of rich history keeping in keeping with a composition mode. 2.4 Effects of motivations Motivation is regarded as an essential factor for creativity, without which creative innovations are unlikely to occur ( )","Indeed, task motivation is regarded as one of the key components of creativity ( )","Given the goal to behave more creatively, people tend to produce more creative responses, compared to what they would normally do without an assigned goal ( )","When setting a difficult productivity goal, high levels of creativity and productivity were attained by employees, while low levels of creativity were obtained with no creativity goal ( )",This might be caused by the different cognitive styles triggered by different motivations,"Studies have suggested that a risky, exploratory cognitive style would facilitate creative thought, relative to the risk-averse, conservative cognitive style ( )",Motivation has a profound impact on HCI product evaluation and user experience ( ),"Research suggests that a user’s motivational orientation, whether an experiential goal or a utilitarian goal, will strongly affect their choice and preference of a product ( ), emotional experiences of an e-commerce website ( ), experience of control and engagement in voice mail browsing ( ), and also subsequent retrospective judgment of an interactive product ( )",An experiential motivation usually aims for hedonic experience whereas a utilitarian motivation usually aims at a concrete result or output ( ),"Furthermore, experiential and utilitarian motivations might have different effects on a user’s flow, engagement, and experience","For example, online flow experience was more likely to be observed when users were engaged in task-oriented rather than experiential activities ( )","Furthermore, among the three necessary preconditions of a   state (clear goals, optimal challenges, and clear immediate feedback), a set of clear goals are suggested to be helpful to add direction and purpose to behaviors, thus serving to structure the experience ( )","In contrast, Rozendaal et al.’s study indicated that there might be a positive link between the increased engagement and experiential motivation ( )","They reported that when assigned with an experiential goal users’ experience of engagement gradually increased with increased levels of richness in product appearance, which is not the case when assigned goal-directed tasks.   suggested that having an active instrumental goal negatively impacted on the experience of an interactive product, and also subsequent retrospective judgment, as a result of barriers made by increasing mental effort",A more neutral view on the effects of motivations has also been proposed,"By examining the relationships between motivations and factors of user engagement in the context of an e-commerce environment,   provided predictive connections between hedonic and utilitarian motivations and aspects of engagement",She suggested an interconnection between utilitarian and hedonic motivations as they both have central effects on some aspects of engagement,"The above literature suggest that a clearly defined utilitarian motivation can contribute to more optimal creative performance, compared to an experiential goal which may be more uncertain or vague","The effects of motivation on engagement, however, is not so obvious","Whether a positive influence or not, the above related works reveal that there is a relationship between the different motivations and creative performance and engagement experience","In the context of NIME, musical interfaces for non-musicians are usually designed for an experiential purpose in the form of sound toys ( ), music games ( ), and social tools ( )",It is not clear whether non-musician’s creative engagement will be affected when they are given different motivations,"Therefore a key concern of this paper is to examine whether different motivations will affect non-musicians’ creative engagement with musical interfaces. 2.5 Research questions As discussed above, factors that might affect non-musicians’ creative engagement with musical interfaces can be summarized as: (i) the motivation of players, i.e. whether with an experiential or a utilitarian goal; (ii) the features of musicking modes, i.e. whether the musical interface allows players to replay records or revise records","Based on this our research are: 
                      3 MTBox In order to investigate these research questions an intuitive musical interface, MTBox was designed","With MTBox, a player can compose or improvise music with pre-recorded musical samples by pressing the buttons","The following sections introduce the MTBox design, rationale of design choices, and its implementation in detail. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   3.1 Tangible interaction MTBox was designed as a tangible musical interface, following the TUI paradigm ( ) of music applications for users to manipulate and control sound directly and intuitively through buttons and rotary knobs","To remove preconceptions of instruments and to reduce non-musicians typical nervousness about playing with conventional instruments, MTBox was purposefully designed to   look or function like a conventional instrument such as a keyboard or a guitar ( )","Therefore, MTBox was designed as a cube because the form of a cube which does not look like a conventional musical instrument, is easy to pick up, and offers six separate surfaces that could be used for different functions, see  
                         ",Offering different sounds on different surfaces responds to the results of a previous study which suggested utilizing separate spaces to help non-musicians to manage different sound objects ( ),Each vertical of the side of MTBox holds four buttons,Each button corresponds to one pre-recorded sample that belongs to one sound genre,As each side has buttons MTBox can be used by left handed and right handed people,Participants press a button to choose an audio sample,"In terms of the sound design, there are melodic samples and beat samples",Each of group contains long samples (more than 3 notes/beats) and short samples (less than 3 notes/beats),"Therefore there are four types of samples (melodic/long, melodic/short, beat/long, beat/short) and they are distributed on four sides of the MTBox","An iPod screen, a rotary knob and operational buttons (On and Off buttons, Play/Pause button, Back button) are embedded on the top surface",The iPod screen is for displaying the timeline interface,The rotary knob is for controlling the movement of the timeline interface,Both will be discussed in detail in  ,"When the ON button is pressed, the chosen sample will be triggered and loop until the OFF button is pressed",The Pause/Play button is to pause the box or start play again,The back button is to reset the timeline interface to the current playback position after being scrolled,There is a LED embedded at the back of each button,"If the corresponding sample is playing, the LED will illuminate",The choice of buttons instead of touch screen controls was made to reduce the need for visual attention to the controls with the help of physical feedback and affordances from buttons and knobs,"For a similar reason, the choice of semi-transparent material is to allow the LED light to be seen from different angles giving additional visual feedback on the button state, and to hide the complex electronic components to avoid distraction","The MTBox is 15cm wide, 15cm heigh, and 15 cm deep","The size of screen is 9cm width and 5cm height. 3.2 Timeline interface The timeline interface was displayed on an iPod screen embedded on top of MTBox, see  
                         ","The timeline provides a visual   of the sound events created by participants, see  ",This was designed to respond to the CST design guidelines of providing history keeping ( ) and the call for providing support for compositional structures and events organization and modification ( ),The timeline moves from right to left as time progresses,There are sixteen tracks on the timeline to record the activity of each sample individually,When a sample is started it loops and can be stopped,This is represented as a line recorded from its starting point to its stopping point on its corresponding track on the timeline,A real-time animation is simultaneously drawn in the middle of the track while the sound is active,"As a previous study suggested that non-musicians require readiness time in the creative process ( ), MTBox was designed to allow players to plan musical events in the future by using the timeline","In the middle of the timeline, a red vertical line divides the timeline into two parts","The left side of the timeline records the previous musical events and the right of the timeline records the future musical events, whilst the middle indicates the current playing point",Using the rotary knob the timeline can be scrolled into the future (clockwise turn of the rotary knob),"In this situation a player can start or stop samples ahead of current playing point, which will be recorded on the future timeline","The future records won’t take effect until it reaches the vertical line in the middle. 3.3 UI features of musicking modes As discussed above, the primary design features of systems to support different musicking modes are whether the system allows the activities of (i) replay and (ii) revision of previous and future records","In order to examine the effect of these features, the timeline was designed with two key user interface features beyond sound production: 
                      
                         
                          shows an example when the timeline interface in   is scrolled to the previous time zone","When the Play button is pressed, the line indicating the current playing point will jump to that point, and the system will play the sound according to the records on the right","To allow for comparison between these two user interface features, four user interface modes were designed for MTBox","Each mode was designed with or without the two functions so as to trigger different modes of musicking.  
                          lists all MTBox modes and their functions","M  is designed with non-changeable playing point and non-editable records, aimed at triggering the musicking mode that is similar to improvising with an instrument","M  is designed with non-changeable playing point and editable records, aimed at triggering the music mode of comprovising that allows editing on previous records, such as live coding","M  is designed with changeable playing point and non-editable records, aimed at triggering the music mode of comprovising that allows replaying previous creation, such as playing with a Launchpad","M  is designed with changeable playing point and editable records, aimed at triggering the music mode that is similar to composing with Logic. 3.4 Implementation MTBox has three main components","First, the hardware interface such as buttons, rotary knob and LEDs were integrated with a microcontroller board,   ( )","Second, the timeline interface was programmed in   
                         ","Third, the sound interface was built in   ( )",A working setup of MTBox included a MacBook Pro,The Processing and Pure Data were running on the MacBook Pro,"The iPod embedded in MTBox was connected with it via USB and was used as a screen extension to display the timeline interface via   software  , which was set in full-screen mode with no other user interface objects visible or accessible",Arduino Mega was also connected with the MacBook Pro for power supply and data transfer,The user interaction data was transfered from Arduino Mega to Processing,"After processing, the data was then transfered to Pure Data to control the state of the samples, and also back to Arduino Mega to control the state of LED lights","A technical set up of MTBox please see  
                         . 4 Experiment design 
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                      
                      
                   4.1 Independent variables To examine the effects of the four modes of MTBox that addressed different musicking features, we conducted a cross comparison between two groups of participants","In addition to this, we built on the two tasks used by   and   to examine the effect of task motivation on online users’ flow and engagement: (i) experiential motivation versus (ii) utilitarian motivation",We use these two task motivations to examine the effects of the task motivations on non-musician’s creative engagement,The first motivation is an exploratory task to encourage participants to explore the MTBox in their own way,This is to give participants an experiential goal that aims for a hedonic experience,The second is a creative task to encourage participants to create a piece of music with MTBox,This is to give participants an explicit utilitarian goal that aims for a concrete creative result,"In total, three independent variables were manipulated in the experiment","For how they are related to two groups of participants please see  . 
                      4.2 Hypothesis According to Sawyer, expert musicians are usually motivated by a utilitarian goal for creative output, and most of the great music was created after engaged in long periods of preparation and frequent revision ( )","We hypotheses that creative engagement when using MTBox will be greater when non-musicians are involved in the composition mode with the ability to replay (with changeable playing point) and revise records (with editable records), or when participants are given an explicit utilitarian goal to create a piece of music","Therefore we developed three hypotheses relating to the independent variables: 
                      4.3 Dependent variables Candy and Bilda proposed two indicators for assessing creative engagement in the context of interactive art: (i) the conceptual change, when there is a shift in participant’s intentionality and expectation with the system; and (ii) the behavioral change, which is often observed before and after an unexpected change in the system ( )","According to them, the observed behavioral change needs to be confirmed participants’ retrospective reports","This involves observation of participants’ behaviour and analysis of participants’ feedback, demanding a huge amount of work on data interpretation, and also bringing with it a risk of missing points due to superficial interviews, especially when the interaction process is long","However, in contrast to the context of interactive art, where the audience’s behaviour change is usually caused by the unexpected change in the system, the behaviour change in the scope of this study is usually initiated by the player themselves",Therefore it is difficult to determine participants’ behaviour change via video recordings in the context of our study,"Instead we propose using survey methods as the main method to assess the conceptual change based on a set of creative engagement factors, and collecting interaction logs as a complementary source for analyzing behaviour change during the interaction process","We developed two categories of dependent measures to assess participants’ creative engagement: i) participant feedback (agreement on interval scale statements) and ii) activity assessment (what participants did). 
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                      4.3.1 Participant feedback Attributes of user engagement ( ) and the factors that are used to evaluate CSTs ( ) informed the design of survey questions used in this study","Attributes of user engagement include: focused attention, perceived usability, endurability, novelty, aesthetics, and felt involvement ( )","The factors that are used to evaluate CST include results worth effort, expressiveness, exploration, immersion, enjoyment, and collaboration ( )",We also drew on a previous study on non-musicians’ creative process with musical interfaces ( ) to inform the design of the survey questions,This study indicated that the factors such as the learnability of system and whether or not the system helps to structure the composition and support planning ahead are also crucial for non-musicians’ creative engagement,Therefore we combined and merged the above factors into a single set of factors to evaluate the level of creative engagement of our participants,"These factors include Interest, Aesthetics, Learnability, Feedback, Structure Composition, Plan Ahead, Enjoyment, Exploration, Expressiveness, Challenge, Control, Focused Attention, Results Worth Effort","As this paper is focused on the individual creative process rather than a collaborative process, we exclude the factor that addresses collaboration.  
                             illustrated the origins of the factor, and how they are integrated into the questionnaire statements",The questionnaire used in this study to access participants’ Creative Engagement (referred to as the CEQ) is based on the factors discussed above,"It is necessary to note that this questionnaire is not proposed as a validated instrument for measurement of creative engagement, instead it is used to explore the feedback on factors that relate to creative engagement",The CEQ has two parts: The first part was a list of statements addressing factors on creative engagement,Participants were asked to rate their agreement for each statement on a seven-point Likert scale from 1 (Strongly disagree) to 7 (Strongly agree),"There were three separate lists of statements: initial self-assessment on music creativity, statements for explore session (ES) and statements for create session(CS), see  
                            ","There were eight paired statements in ES and CS addressing the same factors: interest(ES1, CS1), feedback(ES4, CS5), exploration(ES5, CS6), expressiveness(ES6, CS10), challenge (ES7, CS4), control(ES8, CS7), focused attention(ES9, CS9), results worth effort (ES10, CS11)",The paired statements addressing the same factors were aimed at offering comparisons between the task sessions,The statements marked with the symbol * were coded in a negative way,"In the second part of CEQ, participants were asked to choose one option that is most appropriate for a set of statements from the two given MTBox modes","With the comparisons between MTBox modes, we were able to capture participants’ opinions on the six factors of creative engagement: (1) Enjoyment: I enjoyed my self most; (2) Exploration: I explored more music ideas; (3) Expressiveness: I felt I was more expressive; (4) Challenge: The interface was frustrating; (5) Creativity: I felt more creative with; (6) Results worth effort: I felt more satisfied with the result. 4.3.2 Activity assessment Each interaction with the buttons and timeline controls on MTBox, was logged with a coded interaction type and time, see  
                            ",Numerical measures of the interaction with MTBox can be derived from analysis of these logs of participants’ activity with the user interface,"We focused on activity with the timeline and compute the ratio of time each participant spent on the timeline, both in the future timeline (f-duration) and in the previous timeline (p-duration). 4.4 Qualitative interview assessment In addition to the quantitative data collection of logs and questionnaires, a semi-structured interview was conducted to collect supplementary qualitative feedback in order to understand the participants’ subjective experience with MTBox",Interview questions were designed based on the task sessions,"Questions include:  
                      4.5 Procedure Twenty-four participants (12 male, 12 female) who considered themselves to be non-musicians were recruited to take part",The average age of the participants was 25 (  = 5.247),"The participants were a mixture of undergraduate students, graduate students, and non-students",They signed a consent form and were informed that they could leave at any time,Each participant received £10 (GBP) as compensation,"Before starting to play with the MTBox, the participants were asked to complete a pre-questionnaire to self-assess their musical creativity",Participants were divided into two groups: Group 1 and group 2,In the study they interacted with two UI modes separately,"Group 1 interacted with M  and M , and group 2 interacted with M  and M , see  ","To eliminate the influence of the sequence of exposure to UI mode, the order of the UI modes was randomly assigned for participants","With each prototype there were 4 sessions: 
                      5 Results 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   5.1 Questionnaire feedback Three analyses were carried out on the questionnaire data: (i) comparison of the paired factors of creative engagement was conducted to examine the effects of task motivations; (ii) comparison by MTBox modes; and (iii) comparison by dependent variables were conducted to examine the effects of prototype modes. 
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      5.1.1 Comparison on paired factors of creative engagement A three-way mixed ANOVA was conducted to investigate the impact of three independent variables (playing point, record, and task) on the agreement on the paired factors of creative engagement in the questionnaire","There is a significant three-way interaction between the three variables for the factor of feedback ( (1,22) = 6.480,   = .018)","There is also a significant two-way interaction ( (1,22) = 8.000,   = .010) between the playing point and task","There is a significant main effect of task on the agreement on the paired factor of expressiveness ( (1,22) = 8.469,   = .008), with a higher agreement (  =  4.979) on the expressiveness of the prototypes when assigned an exploratory task, compared with the creative task (  = 4.438)","There is also a significant main effect of task on the agreement on the paired factor of results worth effort ( (1,22) = 55.640,  <.001), with a higher agreement (  = 6.250) on the result worth effort of the prototype when assigned with an exploratory task, compared with the creative task (  = 4.250)","A summary is presented in Part 2 of  
                            . 5.1.2 Comparison by MTBox modes A paired samples t-test was conducted to compare the difference between the agreement on ES0 and CS8 with all prototypes","There was no statistically significant difference between the initial self-assessment of music creativity and creativity with MTBox modes including M , M  and M ","However, participants’ ratings on their creativity with M (  = 4.50) is statistically significantly higher (  = -3.095,   = .010) than their initial self-assessment of music creativity(  = 3.0), see Part 1 in  ","For each statement in the questionnaire, a  -test was conducted to compare with MTBox modes",A summary of significant differences is presented in Part 3 of  ,"A paired samples  -test indicates that the agreement on ES2 (“This prototype was aesthetically appealing.”) with M  (  = 5.50,   = .905) in exploration session is statistically significantly lower (  =  −2.419,   = .039) than that of M  (  = 5.83,   = .718)","A paired samples  -test indicates that the agreement on CS8(“I was very creative with the music.”) with M  (  = 4.50,   = 1.087) in creation session is statistically significantly higher (  = 2.345,  = .034) than that of M  ( = 3.67,   = 1.231)","An independent samples  -test finds that the agreement on CS9(“When I was improvising with the music box, I lost track of the world around me.”) with M  (  = 5.92,  = .996) in creation session is statistically significantly higher ( = −2.328,  = .030) than that of M  ( = 4.83,  = 1.267). 
                            
                             details the results of the prototype comparison questionnaire (second part of CEQ) with significantly different results highlighted in bold using a Chi test","Between the M  and M  comparison, there is no significant difference between the enjoyment, creativity and results worth effort, but significant differences are found in the factor exploration ( 
                             = 10.667,  = 0.001), expressiveness ( 
                            = 6.000,  = 0.014), and challenge ( 
                            = 6.000,  = 0.014)","Between the M  and M  comparison, there is no significant difference between the enjoyment, expressiveness, challenge, and results worth effort","However, significant differences are found in the factors of exploration( 
                             = 16.667,  <0.001) and creativity ( 
                             = 10.667,  = 0.001). 5.1.3 Comparison by independent variables The data of M  and M  was combined to compare these results with the data of M  and M , in order to examine the effects of editable records",An independent sample  -test was conducted on the agreement of questionnaire statements for two different task sessions,There was no statistical difference in any of the data between these two groups,"Similarly, the data of M  and M  was combined to compare it with the data of M  and M , in order to examine the effects of changeable playing point",A Paired sample T-test was conducted on the agreement of questionnaire statements for two different task sessions,"In the creation session, the agreement on CS5 (“The timeline offers support to implement different music ideas and possibilities”) with prototype M  and M  (  = 4.67,   = 1.373) is statistically significantly lower ( =−2.228,   = .036) than that of M  and M  (  = 5.25,   = 1.260)","The agreement on CS9 (“When I was improvising with the music box, I lost track of the world around me”) with prototype M  and M  (  = 5.17,   = 1.239) is statistically significantly lower (  = -2.632,   = .015) than that of M  and M  (  = 5.58,   = 1.248)","A summary of significant difference is presented in the Part 4 of  . 5.2 Timeline activity The percentage of time a participant spent on the previous records of the timeline (p-duration) and on the future records of the timeline (f-duration) was calculated, illustrated in  
                          based on MTBox modes","A summary of significant differences is presented in  
                         ",A paired samples t-test indicates that participants spent significantly more time ( <.001) on the future records of the timeline than on the previous records of the timeline,"There is also a significant strong positive correlation (r = .599, n = 96,  <.001) between the p-duration and f-duration according to Pearson correlation","A three-way mixed ANOVA was conducted to investigate the impact of changeable playing point (within subjects), editable record (between subjects) and task (within subjects) on p-duration and on f-duration","There is a significant main effect ( (1,22) = 19.370,  <.001) of playing point on p-duration, with higher percentage time spent on p-duration with changeable playing point prototypes (  = .167,   = .093), compared to that with non-changeable playing point prototypes (  = .110,   = .076)",A Pearson correlation was conducted to determine the relationship between f-duration and p-duration and agreement on statements in two sessions,There is no correlation between p-duration and agreement on statements in the exploration session,"However, in the creation session, there are statistically significant positive correlations between f-duration and CS2 (The timeline helps me to organize my composition)(r = .322, n = 48,   = .026), and between p-duration and CS5 (The timeline offers support to implement different music ideas and possibilities) (r = .297, n = 48,   = .040). 
                         
                         
                         
                         
                         
                      5.2.1 Summary To summarize, with   we found significantly higher agreement on prototype   and   when assigned with the exploratory task, as compared to the creative task","With the   we found: 
                         In terms of   we found: 
                         5.3 Interview feedback A bottom-up deductive thematic analysis ( ) was conducted to extract participants’ ideas about the playing mode and task motivation","The themes are reported below with a representative quote from participants (Participant ID is included in brackets). 
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      5.3.1 Skill set 
                            
                         In terms of the expertise of musical performance, two sets of skills, namely   and  , are required for expert musicians to articulate the music in their mind and express it through the instrument ( )",Our data suggests a similar categorization for non-musicians’ creative engagement with digital musical interfaces,"Based on the feedback from all the participants, we identify   which are concerned with various cognitive facts related to the conceptual understanding and creation of music","For example the strategies of idea exploration and generation, and the ability to shape sound structures( ), or the   that help to plan and reason the actions, and to monitor the performance ( ).   are concerned with the ability to execute the music ideas, similar to the concept of   proposed by Webster ( )",It was more difficult for participants to play in the improvisational mode as several of them reported they can not ‘think’ or to ‘concentrate’ when music was playing,"According to our participants, the most demanding mental skill was to memorize all the sounds, and to make decisions at the presence of the ongoing music","Some features of the timeline were reported to be conceptually helpful during the process, which will be discussed in later session","In terms of the physical skill, our participants reported that they found it hard to press the right button at the ‘right time’","Some participants suggested offering visual feedback when they achieved a synchronized action, or to have auto-synchronization embedded in system. 5.3.2 Structured records and plan 
                            
                         Participants spoke highly of the timeline as it helped to organize their records and to plan future music events in a structured way, which allowed them to store musical ideas for the future and helped to reduce the mental workload required for music making, e.g. “freed up to think about other things” (Participant 19)",The records on the timeline also helped to remind user of the previous interactions and sound combinations they had made,"Apart from offering an overview of the current events, e.g. “you can see which sound is on and off at each time” (Participant 16), the visual representations of the timeline enables non-musicians to approach music visually, e.g. “the reference of the timeline, which is a lot like a graph, and then the sounds” (Participant 23)",The timeline was reported by participant to offer three parts of information: i) the previous records reminded participants of what was done; ii) the current status indicated what was going on; and iii) the future timeline helped participants to anticipate what was going to happen,"The structured records and future ideas offer an easy trace back to previous success and mistakes, and free participants “to use their imagination’. 5.3.3 Improvise 
                            
                         
                            
                         Participants’ concept of improvisation was associated with the activity of playing live",The term   refers to play directly with the sound in real time with MTBox,This might or might not involve some planning ahead,"According to participants’ feedback, there were two levels of playing live: One is   with possible interactions, sound combinations and patterns in real-time","When playing in this mode, participants usually focus more on the music ideas and process rather than the results",Thus they reported less pressure as they worry less about the mistakes,"Moreover, participants report playing experimentally is intuitive, engaging and responsive for beginners to learn and explore, because of the direct sound feedback from interactions","The other level of playing live is  , using the interface as an instrument, performing music in real-time with the musical structures or ideas in mind","Contrary to the   mode,   mode is result oriented",Participants viewed the interaction process and its results as a whole output when playing in this mode,"With the emphasis on the result and its quality, participants put more mental effort on musical aspects such as timing, structure planning, etc","Participants reported more pressure, felt less confident and encountered more barriers such as skill, readiness time, etc. in this level of playing live",They also reported great pleasure and fun when playing with this mode successfully as ‘I enjoy at the moment right now (Participant 5)’,"Also, the function of planning ahead plays an important role in supporting participants’ live performing by providing enough readiness time to release the real-time pressure as the participants ‘didnt have to worry about playing the button at the right point (Participant 19)’. 5.3.4 Compose 
                            
                         Participants viewed composing as an iterative process of building up a piece, creating, reflecting on and revising the previous records, from which they can learn and get inspirations from the success and mistakes","For example, one participant reported when he looked back on the mistakes he made, he thought to himself ‘I’m not gonna do that again’ (Participant 7)",This is in keeping with the concept of composition in the traditional music context,"Participants who enjoyed this mode reported its advantages, including offering more ‘freedom’, allowing them to modify mistakes, e.g. ‘I can correct it, so that will be much better.’ (Participant 5), requiring less physical skills and offering enough readiness time as they do not ‘have to be quicker’, producing less pressure for users as they felt ‘its more secure’, and ensuring good quality of results which ‘the start would be good as well as the end’","In terms of the two features of MTBox, replay and revise records, participants reported being able to replay records plays a more important for supporting composition, compared to being able to revise records",This is consistent with the results from the quantitative analysis,"In terms of the process of composition, most participants started with exploration on music ideas by randomly putting sounds together, and once they have accumulated enough music ideas, they would start building up a general structure for the whole piece, e.g. ‘with practice you could really layer up things’ (Participant 19)",This process could be thought of as a bottom-up strategy ( ),"Contrary to the bottom-up strategy, one participant began with a general structure of music in mind, followed by exploring and creating sound ideas and then filling them into a structure","This could be thought of as a top-down strategy (ibid). 5.3.5 Motivational orientations 
                            
                         
                            
                         Given an explicit utilitarian goal for music output, participants preferred the composing mode as they reported ‘for actually creating a nice song, it would be really good to have the timeline and to be able to go back and forth’","Whilst with an exploratory task, participants were more likely to be engaged in the playing live as they enjoyed the responsive feedback of playing live, e.g. ‘its really easy to do at the current time, cause you can actually hear it.’(Participant 16) and reported being excited about the serendipity they encountered, e.g. ‘the experiment of possibly creating something is good.’ (Participant 24)","Also because they did not have a goal for output, they reported being more relaxed, being less worried about the mistakes, and were more encouraged to explore more music ideas in this condition. 5.3.6 Inspiration source 
                            
                         
                            
                         From the feedback there were primarily three sources for inspirations in musicking","The primary source was participants’ previous interactions and the music events recorded on the timeline, including the general music structure, and the sound ideas, combinations or patterns","These allowed participants to evaluate and to ‘learn from’ the previous success and failures, e.g. learn ‘how they work together’ (Participant 16), decide ‘what needs to be changed’ (Participant 11), and thus build on the previous creations",Another source was the visual clues,"Graphic information such as the shape, color, length of the graphic representations inspired participants on sound combinations and patterns ‘cause you can see which one is playing with which, with the other one’ (Participant 16) so you ‘know which one to cut and extend’ (Participant 24)","Finally, constraints were another source for inspiration","Although participants reported they felt frustrated when interacted with prototypes that had non-changeable playing points or non-editable records, it turned out that these constraints triggered the exploratory behaviors, and lead to more creative music ideas, e.g. ‘a task that you have to, I guess it helps to get different ideas",Cause you know you have this limit.’ (Participant 8). 6 Discussion The hypothesis H1 ( ) was not supported by our results,"Given an exploratory task, participants’ rating of expressiveness of the prototype (ES6 and CS10) and satisfaction with the results (ES10 and CS11) were significantly higher than when they were given a utilitarian goal",This suggests that an experiential goal has more potential than a utilitarian goal to increase the positive experience in terms of perception of expressiveness of the prototype and satisfaction with results,"This may be because when participants were given an experiential goal they were more willing to explore more musical expressions and were encouraged to employ divergent thinking ( ), while the pressure of a utilitarian goal may have limited divergent thinking and the exploration of musical ideas","Interestingly, participants’ rating of the aesthetic appeal of M  was significantly higher than M  in the exploration session","In another word, participants found the prototype without changeable playing point to be more appealing than the prototype with changeable playing point when playing with an exploratory task","This may be because M  has fewer functions than M , and it’s simpler to learn and to play when given an exploratory task",In this condition players were not obliged to create anything in particular and so they may not have needed the functionality of a changeable playing point resulting in it becoming a cognitive burden that affects the perceived aesthetic of MTBox,"This is contrary to the results that changeable playing point mode received higher agreement on creativity (M  >M ), focus attention (M  >M ) and feedback (M  and M  >M  and M ) when playing with a creative task","From the above discussions, we infer that the task motivations largely affect the need for the changeable playing point on MTBox",The hypothesis H2 ( ) was supported by our findings,"Firstly, participants’ rating for feedback (CS5) and focus attention (CS9) were higher with prototype M  and M  (which both had changeable playing point) than M  and M ",These higher ratings for feedback suggest that the interface with changeable playing point better supports creative engagement in keeping with findings by O’Brien and Toms who propose feedback as a key element of engagement ( ),"Secondly, participants rated their attention as significantly more focused with M  (has changeable playing point only) than with M  (has editable records and no changeable playing point)",Higher ratings for focused attention suggest deeper level of creative engagement - focused attention is proposed as a key element of engagement ( ) and factor contributing to creativity ( ),"Thirdly, in   significantly more people reported that M  was more challenging than M  but no difference between M  and M , and significantly more people reported that M  was less creative than M  but no difference between M  and M ","Also, both M  and M  were rated to be more exploratory than M  and M ",Both of these results indicate that a changeable playing point contributes to increased reporting of factors of creative engagement,"Moreover, the ratings of creativity with M  were significantly higher than with M , indicating that the changeable playing point increased perceived creativity","Finally, the findings that when playing with a changeable playing point there was significantly more time spent on the previous timeline, and that the more time participants spent on the previous timeline the better feedback they gained from the timeline, suggest that the changeable playing point increased participants positive experience of the prototype",Hypothesis H3 ( ) is partially supported by our findings,There is no significant difference between the participants’ responses between non-editable prototypes (M  and M ) and editable prototypes (M  and M ),This suggests that the edit-ability of content does not have a direct effect on people’s perception of their creativity,"Or, more generally the findings suggest that there was no perceived difference in support for creativity from a prototype which was designed more for improvisation (non-editable) and one which aimed to support composition (editable)","This may be due to the musicking tasks given to participants which were purposefully vague (e.g. “explore” or “create”), or possibly because the participants were non-musicians who had a (relatively) short time to learn to use the system, or it could be because the comparison between editable and non-editable prototypes was between group as subjective Likert scales are compromised because of different reference groups ( )","However, participants’ ratings of focus attention with M  are significantly higher than with M , and the ratings of the creativity with M  are significantly higher than with M ","This indicates that when both features (editable records and changeable playing point) are available, creative engagement is higher as elements of creativity are rated higher","Interestingly, the results also seem to indicate that the feature of changeable playing point may be more crucial to non-musicians’ creative engagement with musical interfaces than the feature of editable records","The ratings of expressiveness and challenge are significantly different between M  and M , but there is no significant difference between M  and M ","Whilst ratings of creativity are significantly different between M  and M , but no significant difference between M  and M ","This indicates that whilst support for editing has some effect on ratings of expressiveness, challenge, and creativity, the primary effect is due to whether there is a changeable playing point or not","These results suggest that the effect of the feature of changeable playing point is enhanced by the addition of the feature of editable records. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   6.1 Timeline activity We found that when playing with a changeable playing point, there was a higher percentage of time percentage spent on the previous timeline",We also found a strong positive correlation between f-duration and p-duration,These two findings allow us to claim that the usage of both previous and future timeline is higher with the prototype that has a changeable playing point than with a non-changeable playing point,"Players did use the previous and future zone of the timeline, and these activities were correlated with positive feedback on factors such as feedback and support for composition. 
                          showed that non-musicians reported more creative engagement when they had more time to prepare and to implement their musical ideas",Our finding that the more time spent on the previous and future timeline the better feedback and support on composition were gained from timeline also supports this claim that non-musicians’ creative engagement increases when the musical interface provides functions for planning ahead,"Together with the qualitative results discussed in the theme  , we propose that replay and revision of the previous records helped non-musicians to learn, explore and implement music ideas","As presented in the theme  , the timeline serves as a distributed cognitive tool for non-musicians as it allows them to store knowledge and ideas temporally in the system rather than in the memory ( ), and offload tasks and cognitive process on to environment or tools ( )",We emphasize the importance of providing a   records and plan,"It’s with a structured resource, the records could possibly serve a better use","As novices need to learn from mistakes ( ), a structured records allows them to trace back efficiently and to recall previous mistakes and success easily","Moreover, a structured records/plan could also contribute to a clear representation of the overall music structure, supporting users to create a structured piece of music. 6.2 A Descriptive model for creative engagement We propose a descriptive model of non-musician’s creative engagement with musical interfaces from the qualitative analysis of participants’ feedback, see Fig.  
                         ",The rationale for developing a descriptive model is to offer a structured and generalized description on creative engagement with IMSs and interactive systems that involves real-time activities,"In this model we describe creative engagement based on six factors: (i) the motivation of playing; (ii) the playing modes; (iii) the output; (iv) the status; (v) the skills required, and (vi) the activities involved",There are three modes of playing progression from   to   and on to  ,"Each mode is driven by a different motivations, and demands a different set of skills",There are different activities involved in these modes,We propose that there is an increasing level of difficulty between the three modes outlined below from the easiest mode to the more advanced mode,"And the output of each mode is with a progressive quality. 
                          is when a player is focusing on experimenting in real-time with possible musical ideas such as rhythmic patterns, typically using a trial and error approach",This playing mode requires no skill and the output is non-structured musical fragments,"It is usually the first mode of play adopted by novices, of which the main purpose is to learn and incubate ideas for later creation ( )","As has no conceptual and technical requirements, it encourages players to play in the initial stages We propose when playing with this mode the players are in the very first level of creative engagement","It is oriented to exploration and involves behaviors such as learn, explore, and adapt to the system ( ). 
                          is an iterative process of building up a structured piece and involves behaviors such as exploring, creating, listening, evaluating, improving, and recreating","It requires cognitive skills and the output is a structured piece of music, which is similar to the musicking mode of composition discussed in  ","It is usually adopted at the second stage of the interaction process after players reach a deeper understanding of the system ( ), and when the player has an explicit utilitarian goal for producing good results",In this proposed framework it acts as a   ( ) to keep the player engaged after the initial encounter,"We propose when playing with this mode player is in the second level of creative engagement. 
                          is implementing musical ideas in a structured way in real-time and involves create and perform behaviors","It requires both mental and physical skills and the resultant output is a structured piece of music, which is similar to the mode of comprovisation and improvisation discussed in  ","It is usually adopted at the final stage of the interaction process when the player is pursuing the enjoyment of playing as well as a good result, and when the player is getting more confident with their mental and physical skill, and starting to play fluently ( ) with the interface","This mode encourages the relationship between the system and the player continues to grow We propose this mode is a more advanced level of creative engagement, and also the desired phase of creative engagement","With MTBox, the most common trajectory of modes starts with   followed by  , similar to a bottoms-up strategy of composing proposed in  ","In contrast to this, one participant reported starting with a general musical structure in mind and experimenting live with musical ideas to fill it in, which is similar to a top-down strategy of composing proposed in ( )","The trajectory towards   such as C and D, illustrated by a dotted line, was reported as being more difficult to handle, however it was more enjoyable","Therefore, the trajectory of modes progressing towards performing live is the optimal trajectory of creative engagement we would like to propose, as it offers challenges as well as joy cf. ( ). 
                         
                         
                         
                         
                      6.2.1 Barriers and catalysts The barriers inhibiting non-musicians’ creative engagement with IMSs include limits of cognitive skills, i.e. working memory, multi-tasking, and physical skills, i.e. synchronized or real-time action, and their lack of confidence and experience, i.e. pressure to produce a good quality result, and ease of becoming fixated without knowing what to do next",User interfaces could be designed to provide scaffolding to overcome these aspects,"In our case, the timeline supported planning ahead and allowed players to save working memory and reduce the amount of multitasking required","The ability to change the playing point supported real-time activities by allowing access to records in real-time, which is an important feature of comprovisation discussed in  ","In terms of participants becoming fixated without knowing what to do next, the visual representations on the timeline helped to inspire participants to create more musical expressions","From our data, we propose several potential external and internal catalysts that could trigger further levels of creative engagement",External catalysts include constraints and social pressure,"For example, as presented in the theme  , when the prototype has limited control, the constraint may trigger participants to explore more possibilities","Alternatively, some participants reported that they were thinking about audiences when playing, which led them to explore and create",Internal catalysts include motivation and serendipity,"When the motivation shifted from an experiential goal to a utilitarian goal, we found that players typically changed to different playing modes","Or when participant found unexpected or surprising ideas, they were encouraged to explore more possibilities, as presented in the theme  ","These catalysts are different to those reported in studies of interactive art which suggest that participants start of engage in creative pursuits when their intentionality and expectation are not achieved ( ), or when the system initiates an unexpected change ( ). 6.3 Design implications To break the barriers to creative engagement for non-musicians, and to support their activities in the process, a list of design implications are discussed in detail below based on motivation, mental workload, insights and real-time activities","These design implications will have direct implications for the design of similar musical systems for non-musicians in NIME ( ), or systems that aim to engage novices creatively in HCI. 
                         ",Designing motivations in different stages of interaction is a good way to catalyze novices in an optimal trajectory of creative engagement,"According to the descriptive model of creative engagement, applying different motivations could catalyze users towards different levels of creative engagement","It could be achieved by promoting experiential exploratory goals by designing stepwise functions to be discovered stage by stage, or by promoting utilitarian creative goals by encouraging participants to share the musical outcome to their social networks",This is in line with the proposal to foster and enhance motivation by setting stages and context for creative works ( ),"It suggests an integration of different motivations into a single system, and differs from the previous practices that focused on design only for experiential motivations ( ) or utilitarian motivations ( ). 
                         ","As discussed earlier, non-musicians are not skilled at music making which puts greater demands on their working memory and multi-tasking than experts","There are two practical implications to reduce novices’ cognitive workload in the creative process. 
                      
                          Novices can easily get fixated on previous ideas ( )","It is necessary to provide mechanisms to support in gaining insights. 
                      
                         ","For real-time interactions that require both cognitive and physical skills, it is difficult for novices to achieve good performance in a short time as it takes time to become fluent","Supporting real-time activities can be achieved through the following two practices. 
                      6.4 Limitation and future work There are some limitations to our work that might affect our results",MTBox was designed with a limited number of buttons and therefore offers limited sound choices,The samples were restricted to electronic sound genre,"Moreover, the sound of MTBox was generated from the computer instead of MTBox itself or headphones","The monotony of expressiveness and disconnected sound might restrict players from becoming creatively engaged in the interaction, and thus affect their feedback",Future improvements need to be carried out to integrate the sound generation mechanism into MTBox,One limitation of the study design is that the study was conducted in a controlled scenario within limited time,Even though a session was designed to provide guided learning and allow time for practicing it might still be that it is difficult for some participants to become confident with the prototype,"Moreover, the study did not evaluate non-musicians’ long-term creative engagement with the prototype, nor did it examine natural scenarios of use, or with multiple players, which could all be interesting to look at in future research, e.g. by conducting long-term studies with participants in real scenario, or design multiple MTBoxs to allow collaborative music making with multiple participants",In this study we did not include people who had musical experience,Even though MTBox was designed for non-musicians it would be interesting to see how experienced musicians’ creative engagement might be influenced by the different modes of MTBox,The effects of user interface mode and design implications might be different as experts have better musical skills and knowledge compared to non-musicians,The questionnaire which was designed based on a set of factors extracted from engagement attributes and evaluation factors for creativity support tools provided evidence about our hypothesis,"The questionnaire could be useful as a set of criteria for evaluating creative engagement with interactive systems more generally, however this would need to be verified with further studies, and could be an exciting contribution to this field",From the very brief analysis of the interaction log data we find the potential to examine states of creative engagement with evidence extracted from interaction log data,More in-depth analysis methods such as data mining could be applied to detect patterns of activity or to quantify activity levels. 7 Conclusion In this paper we explored the effects of task motivation and user interface features on non-musicians’ creative engagement with interactive musical systems,"Based on the results of an empirical study of twenty-four participants, we highlighted that an experiential motivation is better than a utilitarian motivation for creatively engaging non-musicians",We found that a replay feature is less important when a player has an experiential motivation compared to a utilitarian motivation,"However, we also showed that supporting participants to replay previous musical ideas increased some aspects of their creative engagement",And when participants were able to edit their creations the increase in reported creative engagement was more pronounced,We also found that creative engagement increased when the musical interface provided features for planning ahead,A descriptive model of non-musician’s three levels of creative engagement was proposed with three playing modes,We highlighted the mode of performing live as the desired mode of playing and identified barriers and catalysts for non-musicians to achieve it,"Design implications were proposed to inform future design for supporting novices creative engagement taking into consideration motivation, cognitive skills, insights and real-time activities.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0888613x13002910, 1 Introduction Bayesian networks   provide a popular framework for modeling and decision-making under uncertainty,"Since most of our decisions are based on information that is (at least partially) uncertain, Bayesian networks were applied in many diverse domains","Their fundamental advantage over different frameworks is their ability to divide the modeling problem into two basic stages: first, the structure of the modeled domain is described using a graph and secondly, the numerical values describing the quantitative relationship between model variables are provided",The model is either built by domain experts or automatically learned from collected data; and possibly created using a process that combines both ways,The key property of Bayesian networks that allows them to be applied in domains with up to hundreds of variables is the decomposability of the joint probability distribution they represent,"The structure of a Bayesian network is defined by an acyclic directed graph  , where   is the set of directed edges, i.e.,  ","The joint probability of a Bayesian network is defined for all configurations   of discrete variables   as  where   denotes the set of parents of node   in the graph  , i.e.,  ","Using a common shorthand for a formula valid for all configurations of variables we can write formula   as 
                   Formula   allows efficient computations of probabilistic queries  for all values   of all  ,  ","The computational complexity of this task when the popular junction tree method   is used is exponential with respect to the size   of a largest clique   of the triangulated moralized graph   of  , see   for details",The value   is called the treewidth of  ,"If the treewidth is not large, then computationally efficient probabilistic inference is possible","This allows the application of Bayesian networks in domains with hundreds of variables, where a naive computation with the full joint probability table would not be tractable","Unfortunately, the treewidth is large in some applications, for example, some variables  ,   may have a very large parent set ( )",In such cases the exact inference with the standard junction tree method is not tractable,"One solution is to resort to approximate inference methods, e.g., to Monte-Carlo methods  , the Pearl polytree algorithm   applied to Bayesian networks with loops, variational methods  , inference using probability trees  , or binary probability trees  ","Based on our experience with different applications of Bayesian networks we believe that the conditional probability tables (CPTs), which are the basic building blocks of Bayesian networks, may often have a simple structure, for example, they correspond to a noisy functional dependence",This property should be exploited not only when building a Bayesian network model but also during the probabilistic inference,Various methods that can exploit the local structure of CPTs were proposed,An early example is the Quickscore algorithm   exploiting noisy-or relations in the Quick Medical Reference model,Olesen et al.   proposed the so-called parent-divorcing method,Heckerman and Breese   use a temporal model transformation,Zhang and Poole   introduced deputy variables that are used to create heterogeneous factorizations in which the factors are combined either by multiplication or by a combination operator,"Takikawa and D'Ambrosio   used auxiliary variables, which allowed them to transform an additive factorization into a multiplicative factorization",The additions are achieved by the marginalization of the intermediate variables,Díez and Galán   pointed out that the transformation of noisy-max can be done using a single variable,"CP tensor decomposition, called tensor rank-one decomposition in  , is a generalization of Díez and Galán's decomposition",It is based on the CP decomposition of tensors  ,"Savicky and Vomlel   described CP tensor decompositions of several canonical models – noisy-max, noisy-min, noisy-add, noisy-xor",Previous research reveals that for certain models the computational savings achieved by this transformation are very large – instead of a representation that is exponential with respect to the number of variables in a CPT we get a representation that is only quadratic  ,The application of the CP tensor decomposition allows application of Bayesian networks in domains where exact probabilistic inference would not otherwise be possible,"We believe this may have a substantial impact on the quality of decision-making in complex domains such as: medical decision support systems and health monitoring  , troubleshooting complex devices  , etc",This paper is organized as follows,Conditional probability tables with a local structure are discussed in Section  ,"In Section   we introduce the necessary tensor notation, define tensors of the exactly  -out-of-  and threshold functions, and present their basic properties",Sections   and   represent the main original contribution of this paper,We propose methods for the decomposition of tensors of the threshold and exactly  -out-of-  functions in the real and complex domains and prove results about the symmetric rank of these tensors,In Section   we analytically compare the CP decomposition and the parent-divorcing method using an exemplary class of models,In Section   we present experimental comparisons of the CP decomposition method with the standard junction tree method and the parent-divorcing method,The experiments are performed on a generalized version of the QMR-DT network,In Section   we briefly review other methods exploiting local structure of CPTs,We outline how the CP decomposition can be combined with weighted model counting,"Major proofs are moved to  . 2 Conditional probability tables with a local structure Canonical models   represent a class of CPTs with the local structure being defined either by: 
                   For the graph of a deterministic model see  
                      ","The graph of an ICI model contains auxiliary variables  , one for each parent, see  
                      ","The graph of a simple canonical model contains one auxiliary variable  , see  
                      ",The joint probability distribution of the Bayesian network in   is  where the first term   corresponds to a deterministic function and terms   to the probabilistic part (often called noise),We can replace the Bayesian network of   with a model without auxiliary variables   (see  ) by marginalizing them out from the Bayesian network,The values of   can be computed from the original model by  This model is equivalent to the original one in the sense that it can be used to compute correct marginal and conditional probabilities for any subset of its variables present in both models,"As was suggested in   we can rewrite each CPT as a product of two-dimensional potentials 
                       
                      ,  , and  :  where   is an auxiliary variable","This transformation can be visualized by the undirected graph given in  
                      ","If the state   of variable   is observed, we can omit the variable   and the corresponding potential and decompose the CPT   as:  This transformation can be visualized by the undirected graph given in  
                      ","To guarantee either of the above equalities, variable   has to have a certain number of states","Trivially, the equality can always be satisfied if the number of states of   is the product of the number of states of variables  ","However, the transformation becomes computationally advantageous if the number of states is substantially lower. 
                       Again, this model is equivalent to the original one in the sense that it can be used to compute correct marginal and conditional probabilities for any subset of its variables present in both models",The above decomposition specified by formula   or   can be integrated into any inference engine that allows us to work with real-valued tables (potentials),It can be beneficial to perform inference with complex numbers,"Although the complexity of addition and multiplication of two complex numbers is higher than of those operations on real numbers, the decompositions based on complex numbers have better numerical stability","In a junction tree method, the decomposition can be applied as a preprocessing step replacing the moralization step","Instead of connecting all parents of a node (as is done in moralization), one auxiliary node is added","This node is connected to all parents by an undirected edge and, if the child node was not observed, also to the child node",This undirected graph is then triangulated and a junction tree with cliques as its nodes is created,Each table is attached to a clique containing all variables of that table,The computations than proceed in the same way as in the standard junction tree method,"Note that in   results were reported of numerical experiments based on an experimental R implementation 
                       of the lazy propagation   exploiting approximate decomposition based on formula   in the real domain. 3 Tensors of the exactly  -out-of-  and threshold functions Each probability table can be understood as a tensor  ","A tensor is simply a mapping 
                       
                       or  , where  ,   is a natural number called the order of tensor  , and  ,  , are index sets","Typically,   are sets of integers of cardinality  ",Then we can say that tensor   has dimensions  ,"All index sets considered in this paper will be  ,  . 
                      
                   Probability table   defines tensor   as  for all combinations of states   of variables  .   
                       
                       
                      
                   It was observed in   that the minimum number of states of   in the decomposition defined by formula   equals the rank of tensor  ",The decomposition of tensors into the form corresponding to the right hand side of formula   has been studied for more than forty years   and it is now known as Canonical Polyadic (CP) or CANDECOMP-PARAFAC (CP) decomposition,In   it is called an outer-product decomposition,In this paper we deal with conditional probability tables representing two specific canonical models – deterministic threshold and exact functions and their noisy counterparts,"An  -threshold function is a function of   binary arguments that yields the value 1 if at least   out of its   arguments take value 1 – otherwise the function value is zero, the exactly  -out-of-  function takes value 1 if exactly   out of its   arguments take value 1",The noisy version allows noise at the inputs of the function,In the model of   the noise is represented by conditional probability tables  ,The noisy-threshold models represent a generalization of two popular models – noisy-or and noisy-and,They constitute an alternative to noisy-or and noisy-and in case they are too rough,"The conditional probability tables of the noisy-threshold models appear, for example, in medical applications of Bayesian networks  ","The noise on the parent variables cannot increase the rank, see  ",Therefore all results about rank of deterministic tensors represent an upper bound on rank of their noisy counterparts,"Also, it is easy to combine the noise with the CP decomposition of a deterministic tensor to get a CP decomposition of its noisy counterpart","For details, see  ","In this paper we present results about tensors of deterministic canonical models since they are easily extendable to ICI models and simple canonical models. 
                       
                       
                      
                   
                       
                      
                   
                      
                   It is straightforward to see that the exactly  -out-of-  tensors and the  -threshold tensors are related as 
                   The above tensors correspond to the CPTs where the value of   was observed to be 1","Next, we shall consider extended versions of the above tensors that correspond to CPTs with   being unobserved","Such CPTs are part of the model if   has a successor with evidence that is not its child. 
                       This applies, for example, to the case where observations of the state of   are noisy, which can be modeled by an auxiliary variable   being deterministically related to   with a child   of   probabilistically related to   by a CPT  ","These models are called simple canonical models in  , see  . 
                       
                      
                   
                       
                      
                   
                      
                   Tensors  ,   are symmetric, and  ,   are partially symmetric with respect to the last   coordinates",We concentrate on CP decomposition of the former symmetric tensors first,We will present results for tensors   and   that correspond to evidence  ,The decompositions of tensors corresponding to evidence   can be derived similarly,CP decomposition of the extended tensors will be studied in Section  ,"For symmetric tensors it is possible to define a symmetric rank as follows: 
                      
                   
                      
                   We will use the following property of symmetric tensors.  
                   
                       
                       
                      
                   An immediate consequence of   is  .  
                   Each symmetric tensor   of rank-one can be written as  where   or  ","In the following lemmas we treat the border cases with symmetric rank equal to one.   
                      
                   
                       
                      
                   In the next lemma we present a case with a low symmetric rank equal to two.   
                      
                   
                      
                   4 CP decompositions of tensors of the exactly  -out-of-  and threshold functions In this section we establish the rank of tensors   and   and present explicit formulas for the CP decomposition of these tensors in both the real and complex domains",We conclude the section by explaining the relationship between the symmetric tensor decomposition and the decomposition of homogeneous polynomials,"Let   be a symmetric tensor of order  , in particular one of  ,   for some  ",We shall seek for a decomposition of a tensor   in the form of  where   is a vector of nonlinear parameters and   is referred to as a vector of amplitudes,"We will refer to this decomposition as the  . 
                      
                   
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   4.1 CP tensor decomposition in the real domain In this section we will discuss CP tensor decompositions of   and   in the real domain.  
                      The proof of the proposition is constructive, see  , respectively","In the formulas we will use polynomials  , determined by their roots  ,  ,  ,  where   denotes the  -th coefficient of polynomial  ","In the case of decomposition of tensor   the first   elements of vector   are taken at random, but distinct and with the restrictions 
                          that  ,  ","Then   is computed as a rational function of these  ,  :  The vector   of amplitudes has its elements defined for   as  Here   is the value of the polynomial   at point  ",The derivation of   and   is given in  ,"Note that the right-hand side of   is a function of   and is distinct from these  's provided that they are mutually distinct – otherwise a decomposition with   terms would exist, which is not possible. 
                         
                      For tensor  , a similar decomposition as in the case (A) was proposed in  , with the difference that   and   are replaced by 
                         
                      In   we present a novel decomposition of the tensor   to   factors for  , to prove part (B) of  ","Here, the first   elements   are taken at random, but distinct, and   and   are computed so that the decomposition holds","These   and   are given as solutions of a quadratic equation whose coefficients are functions of  ,  ",It appears that the roots of the equation are not always real-valued,"For now, for all tested values of   and   ( ) we were able to find (by maximizing a discriminant of the equation) a choice of  ,  , such that the discriminant was positive and all  s were real-valued","However, we were not able to prove that such a choice always exists – therefore we state it only as a conjecture.  
                      The decomposition is highly ambiguous, in general",For the numerical stability it is better to have the elements   as separate as possible but at the same time their absolute values should not be too large,"In other words, Vandermonde   matrix   defined as  should not be badly conditioned. 
                          Therefore we propose optimizing the choice of the initial values of   by jointly minimizing the condition number of  ","The minimization can be done by the Nelder–Mead simplex algorithm with multiple random initializations, to avoid local minima. 
                          
                         
                      4.2 CP tensor decomposition in the complex domain For large  's, such as   and higher, the condition number of the Vandermonde matrix becomes high for any choice of initial values of  ,  ","Therefore, we suggest considering a complex-valued decomposition of the tensors, which can be expressed in closed forms, and might be numerically more suitable","In particular,   are uniformly distributed on the unit circle and depend only on the tensor order  , but not on parameter  ","The following proposition gives a method for how to construct a decomposition of tensors   into   terms for   with the same set of the nonlinear parameters  , and consequently decompose any symmetric tensor (and   in particular) into   terms","For example, a “soft threshold” tensor (see  ) can also be decomposed in this way.   
                         
                      
                          
                          
                         
                      
                         
                      The decompositions in   are not guaranteed to be minimal",We note from   that   has rank   in the real domain; hence its rank cannot be higher in the complex domain,The importance of   lies in the decomposition method of an arbitrary symmetric tensor of the order   and dimensions 2,"The next proposition determines the true symmetric rank of the tensors   and   in the complex domain, and presents an example of the decomposition.   For the proof see  ","The following two examples illustrate that by using   we get CP decompositions that require fewer terms than those based on  .   
                         
                      4.3 Relationship to the decomposition of homogeneous polynomials Finding a symmetric CP decomposition of a symmetric tensor of order   and dimension   is equivalent with (can be formulated as) a decomposition of a homogeneous polynomial in   variables of total degree   as a sum of  th powers of linear forms – see  ",All symmetric tensors studied in this paper have a dimension of  ,"Therefore, the corresponding polynomials are homogeneous polynomials in two variables, which are also called binary forms",As long ago as in 1886 J.J,"Sylvester   proved a theorem whose proof is constructive and yields an algorithm that, for a given binary form, constructs its minimal decomposition as a sum of powers of linear forms",Brachat et al.   generalized Sylvester's algorithm for dimensions  ,"However, Sylvester's algorithm offers solutions to our problem in the complex domain only",The rank of the decomposed tensors is not known until the algorithm is completed,"On the other hand, the CP decompositions described in the previous subsections explicitly present the rank of tensors of our interest and their decompositions into factors","Also, note that, generally, tensors   and   are not generic tensors in the sense of Definition 1 in  , i.e., the sets of tensors having ranks the same as those of individual tensors   and   are not open dense subsets of the set of all tensors that have the given order and given dimensions","Therefore, Sylvester's theorem  , which implies that the rank of generic symmetric tensors of order   and dimensions 2 is at most  , does not apply to them. 5 A CP decomposition of extended tensors Let   be one of   and   and let it be decomposed as   in (4) with a certain  ,  Then, the corresponding extended tensor   in   and in  , respectively, can be written as  It follows that the extended tensor has a decomposition to   rank-one terms, and hence its rank is at most  ","If, however, one of  's can be set to 1, say  , then the decomposition can be written in only   terms,  Note that the condition   can be satisfied in constructions treated in   and in  , but not in those considered in  ,  (B), and in  ,  (B). 6 Comparison with parent-divorcing In this section we compare complexity of statistical inference using the proposed CP decomposition method versus the parent divorcing","Consider a Bayesian network like the one in  
                      , which has two layers","The lower layer has two nodes, and the upper layer has   nodes: the first   nodes are connected only to the first node in the lower layer, the middle   nodes are connected to both nodes in the lower layer, and the remaining   nodes are connected to the second node in the lower layer","In  ,  ,  , and  ","In both techniques, auxiliary nodes are added to the network, together with some additional edges due to moralization (only in case of parent-divorcing) and triangularization (in both methods)","The resultant graphs are shown in  
                       and  
                      , together with the number of states at each node","The optimal triangulation was obtained by the procedure implemented in the software package Hugin. 
                       Complexity of each method, measured by the total table size, is 68 and 104, respectively","In the case of general  ,   and   and larger  , a different triangulation is optimal, namely the one where the nodes   and   are connected by an edge",The number of the states of the auxiliary variables   and   depends on parameter  ,"For  , for example, the number of the states (ranks of the tensors) is maximal and it is equal to the number of parents, i.e.,   and  , respectively",We will use this worst case for our comparisons,"The corresponding graph will contain   cliques consisting of three nodes having  ,  , and 2 states plus   cliques consisting of two nodes with   and 2 states and   cliques with   states","Together, the total table size for the CP decomposition equals 
                   In the parent-divorcing method, the graph structure is more complex, see  ","Assuming   the optimal triangulation contains, at the part common to both observations, two cliques of the table sizes  ,  , and   and for each   one clique with the table size   and one with the table size  ",At the part exclusive for the first observation there are cliques with   table size for   and one clique of the table size  ,At the part exclusive for the second observation there are cliques with   table size for   and one clique of the table size  ,"The total table size is  We can see that the total table size of the PD method grows as a biquadratic function of  , while that of the former, CP method grows as a cubic function of  ","The former method is clearly superior. 
                      
                   7 Experiments We performed experiments with the Quick Medical Reference – Decision Theoretic version (QMR-DT) derived from the original QMR   as it is described in  ",The Bayesian network of QMR-DT contains 570 diseases (variables  ) and 4075 observations (variables  ),The conditional probability tables for observations given related diseases are noisy-or models,We generalized the QMR-DT by replacing noisy-or with noisy-threshold models,These experiments were performed with subnetworks of QMR-DT,"In the first test, we randomly selected 14 observations",We included all their parents in the generated subnetwork,In   we give an example of a subnetwork of the QMR-DT network generated by two observations and their parents,We generated 100 different networks this way,"For each network we compared computational complexity of the junction tree method   applied to models after two different transformations:  In the second test, we repeated the same process with 28 observations instead of 14",In both tests we utilized 200 bipartite graphs with their sizes ranging from 38 to 582 nodes,We measured the computational complexity of probabilistic inference by the total table size of models computed by the optimal triangulation procedure implemented in the software package Hugin,If the total table size is larger than 2  the models are intractable in Hugin,"Also, for some models Hugin was not able to find an optimal triangulation within reasonable time. 
                       In our experiments we limited the time allowed for the triangulation algorithm to a maximum of 300 seconds","The results of our experiments are summarized in  
                      ",In plots of the top row in this figure we present results of experiments with graphs for which Hugin found optimal triangulation for both methods under comparison,"It is important to note that using the standard method we were not able to get optimal triangulations for 89 out of 200 graphs (i.e., for 44.5% graphs) and using the parent-divorcing for 109 out of 200 graphs (i.e., for 54.5% graphs)",For graphs that Hugin was not able to triangulate optimally we used a triangulation heuristics implemented in Hugin,This heuristics is based on restricting the maximum number of minimal separators (we used the value 10 ),To make fair comparisons we used the same triangulation heuristics also for the graphs obtained after tensor CP decomposition (despite the fact that we were able to find optimal triangulations of these graphs),Numerical experiments reveal that by using tensor CP decomposition on the QMR subnetworks we can get a gain in the order of several magnitudes over the standard method and a gain of one up to four magnitudes over the parent-divorcing methods,"More importantly, many intractable models become tractable when the tensor CP decomposition is used. 8 Relationship to arithmetic circuits A different approach that exploits local structures of CPTs makes use of arithmetic circuits  ","An arithmetic circuit is a rooted, acyclic directed graph",The leaf nodes are labeled with numeric constants or variables and all other nodes correspond to summation or multiplication,"Arithmetic circuits are usually constructed by a conversion to multilinear function, which is then converted to an algebraic circuit through a logical formula of propositional logic",Details can be found in  ,In   the CP tensor decomposition was used to preprocess Bayesian networks containing noisy-or models,"The ACs of the preprocessed networks were compared with ACs created by Ace 
                       from networks after parent-divorcing",The CP tensor decomposition decreased the size of ACs for a majority of tested networks (about 88%),We conjecture we would get similar results for experiments reported in this section,"However, we did not perform these experiments – since Ace does not have any direct support for noisy-threshold models","Performing experiments with Ace without this support (i.e., treating them as general CPTs) would most probably lead to results similar to the standard junction tree method","For the construction of arithmetic circuits, methods used for logical reasoning can be utilized","It was shown that when Bayesian networks exhibit a lot of determinism or context-specific independence, the weighted model counting (WMC) can be an efficient method for probabilistic inference  ","The basic idea is to encode the Bayesian network in a conjunctive normal form (CNF), associate weights to literals according to the CPTs of the Bayesian network, and then compute the probability of given evidence as a sum of weights of all logical models consistent with that evidence",The weight of a logical model is the product of weights of all literals,A naive computation of WMC by listing all models and summing their weights is intractable for large problems,"Fortunately, efficient WMC solvers exist, that compute WMC using several advanced techniques such as clause learning, component caching, etc","An example of a successful WMC solver is Cachet. 
                      
                   The CP tensor decomposition can be combined with weighted model counting (WMC)",This was done by Wei Li et al.   for noisy-or and noisy-max models,"They have shown that for BNs with CPTs representing noisy-or and noisy-max models the Díez and Galán's transformation  , which is a special case of the CP decomposition, often improves the efficiency of inference by several orders of magnitude",The BNs with CPTs representing noisy-threshold models (or exactly  -out-of-  models) can also be transformed by the CP tensor decomposition and then encoded as a CNF,"Either Darwiche's encoding   or Sang, Beame, and Kautz's encoding   can be used",Then WMC solvers as Cachet can be utilized to compute the probability of evidence as the WMC of logical formula encoded by the CNF,"As it was noted in  , it should be possible to use a WMC solver for the construction of an arithmetic circuit by modifying it to keep a trace of its operations","As a criteria for fair comparisons of different inference methods we suggest using the number of multiplication and addition operations in the computations, e.g., in the constructed arithmetic circuit",This would allow us to make fair comparisons of different inference methods with inference by arithmetic circuits or by WMC methods based on different CNF encodings and using different WMC solvers such as Cachet and Ace,The mentioned experiments represent an interesting topic for future research. 9 Conclusions We proposed a CP decomposition of tensors corresponding to conditional probability tables of the threshold and exactly  -out-of-  functions and their noisy counterparts,We applied this decomposition to probabilistic inference in Bayesian networks containing conditional probability tables representing noisy-threshold functions,We performed computational experiments with a generalized version of QMR-DT where the noisy-or models were replaced by noisy-threshold models,The CP tensor decomposition when compared to the standard junction tree method led to a computational gain in the order of several magnitudes and made many intractable models manageable,Theoretical analysis and numerical experiments reveal that tensor CP decomposition is clearly superior to the parent-divorcing method,Our CP decomposition approach can be used as a preprocessing step for weighted model counting,"Appendix A In the proof of   we will use the following lemma.   
                      
                   Appendix B CP decomposition of   in   terms In this appendix we present the proof of part (A) of  . 
                      
                   Appendix C CP decomposition of   in   terms In this appendix we present a constructive proof of part (B) of  . 
                      
                   Appendix D Proof of  
                   
                      
                  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0888613x14000796, 1 Introduction Probability theory is the most well-known approach to model uncertainty,"However, even when the existence of a single probability measure is assumed, it often happens that its distribution is only partially known","This is particularly the case in the presence of severe uncertainty (few samples, imprecise or unreliable data, etc.) or when subjective beliefs are elicited (e.g., from experts)","Some authors use a selection principle that brings us back to a precise distribution (e.g., maximum entropy  ), but other ones   have argued that in some situations involving imprecision or incompleteness, uncertainty cannot be modelled faithfully by a single probability measure","The same authors have advocated the need for frameworks accommodating imprecision, their efforts resulting in different frameworks such as possibility theory  , belief functions  , imprecise probabilities  , info-gap theory  , etc. that are formally connected  ","Regardless of interpretive issues, the formal setting of belief functions offers a good compromise between expressiveness and calculability, as it is more general than probability theory, yet in many cases remains more tractable than imprecise probability approaches",Nevertheless using belief functions is often more computationally demanding than using probabilities,"Indeed, its higher level of generality prevents the use of some properties, valid in probability theory, that help simplify calculations","This is the case, for instance, for the well-known and useful inclusion–exclusion principle (also known as Sylvester–Poincaré equality)","Given a space  , a probability measure   over this space and any collection   of measurable subsets of  , the inclusion–exclusion principle states that  where   is the cardinality of  ","This equality allows us to easily compute the probability of  , when the events   are stochastically independent, or when their intersections are disjoint","This principle has been applied to numerous problems, including the evaluation of the reliability of complex systems","It does not hold for belief functions, and only an inequality remains","However, it is useful to investigate whether or not an equality can be restored for specific families   of events, in particular the ones encountered in applications to diagnosis and reliability",The main contribution of this paper is to give a positive answer to this question and to provide conditions characterising the families of events for which the inclusion–exclusion principle still holds in the belief function setting,This paper is organised as follows,"First, Section   provides sufficient and necessary conditions under which the inclusion–exclusion principle holds for belief functions in general spaces; it is explained why the question may be more difficult for the conjugate plausibility functions",Section   then studies how the results apply to the practically interesting case where events   and focal elements are Cartesian products in a multidimensional space,"Section   investigates the particular case of binary spaces, and considers the calculation of the degree of belief and plausibility of a Boolean formula expressed in Disjunctive Normal Form (DNF)",Section   then shows that specific events described by means of monotone functions over a Cartesian product of totally ordered discrete spaces meet the conditions for the inclusion–exclusion principle to hold,"Section   is devoted to illustrative applications of the preceding results to the field of reliability analysis (both for the binary and non-binary cases), in which the use of belief functions is natural and the need for efficient computation schemes is an important issue","Finally, Section   compares our results with those obtained when assuming stochastic independence between ill-known probabilities, displaying those cases for which these results coincide and those for which they disagree",This work extends the results concerning the computation of uncertainty bounds within the belief function framework previously presented in  ,"In particular, we provide full proofs as well as additional examples","We also discuss the application of the inclusion/exclusion principle to plausibilities, as well as a comparison of our approach with other types of independence notions proposed for imprecise probabilities (two issues not tackled in  ). 2 General additivity conditions for belief functions After introducing some notations and the basics of belief functions (Section  ), we explore in Section   general conditions for families of subsets for which the inclusion–exclusion principle holds for belief functions",We then look more closely at the specific case where the focal elements of belief functions are Cartesian products of subsets,"Readers not interested in technical details and familiar with belief functions may directly move to Section  . 
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   2.1 Setting A mass distribution   defined on a (finite) space   is a mapping   from the power set of   to the unit interval such that   and  ","A set   that receives a strictly positive mass is called a  , and the set of focal elements of   is denoted by  ","The mass function   can be seen as a probability distribution over sets, in this sense it captures both probabilities and sets: any probability   can be modelled by a mass   such that   and any set   can be modelled by the mass  ","In the setting of belief functions, a focal element is understood as a piece of incomplete information of the form   for some parameter   of interest","Then   can be understood as the probability that all that is known about   is that  ; in other words,   is a probability mass that should be divided over elements of   but is not, due to a lack of information","From the mapping   are usually defined two set-functions, the belief and the plausibility functions, respectively defined for any   as 
                          with   the complement of  ","The belief function, which sums all masses of subsets that   
                         , measures how much event   is certain, while the plausibility function, which sums all masses of subsets   with  , measures how much the event   is possible","Within the so-called theory of evidence  , belief and plausibility functions are interpreted as confidence degrees about the event  , and are not necessarily related to probabilities","However, the mass distribution   can also be interpreted as the random set corresponding to an imprecisely observed random variable  , and the measures   and   can be interpreted as describing a set of probabilities, that is, we can associate to them a set   such that  is the set of all probabilities bounded by   and  ",The belief function can then be computed as a lower probability   and the plausibility function likewise as an upper probability,"Note that, since   and   are conjugate ( ), we can restrict our attention to one of them",Consider now a collection of events   of subsets of   and a mass distribution   from which a belief function   can be computed,"For any collection   the inequality  
                          is valid","This property is called  , and belief functions are super-modular for any  ","While the inclusion–exclusion property   of probabilities is a mere consequence of the additivity axiom (for  ), supermodularity of order   does not imply supermodularity of order  ; and the supermodularity property valid at any order is characteristic of belief functions","If Eq.   is an equality for some family  , we say that the belief function is   for this collection, or  -additive, for short",Eq.   is to be compared to Eq.  ,"Note that in the following we can assume without loss of generality that for any  ,  , i.e., there is no pairwise inclusion relation between the sets of   (otherwise   can be suppressed from Eq.  )","Then the family   is said to be  . 2.2 General necessary and sufficient conditions In the case of two events   and  , neither of which is included in the other, the basic condition for the inclusion–exclusion law to hold is that focal elements in   should only lie (be included) in   or  ","Indeed, otherwise, if there exists an event   with  ,   and  , then  This means that, in order to ensure  -additivity, one must check that  where   denotes the set of subsets of  ","So, one must check that for all events   such that  , either   or  , or equivalently 
                         
                      
                         
                      Note that if   is not proper, the belief function is trivially additive for it.  
                          provides an illustration of a focal element that makes a belief function non-additive for events   and  ","This result can be extended to the case where   in a quite straightforward way: 
                         
                      
                         
                      So, based on  , we have:  
                      
                          shows that going from  -additivity to  -additivity is straightforward, as ensuring  -additivity comes down to checking the conditions of  -additivity   in  ",This feature makes the verification of the property rather inexpensive,"Finally, note that if the family   is not proper, it means   for some   and it is then impossible that   and  ","So, we can dispense with checking the condition for those pairs of sets. 2.3 Inclusion–exclusion for plausibilities Note that by duality one also can write a form of inclusion–exclusion property for plausibility functions:  for a family of sets   where   satisfies the condition of  ","Although Eq.   provides us with a kind of inclusion–exclusion property for plausibilities, it does not provide insight about the conditions under which the equality  holds","In this section, we will investigate this issue, concluding that the case of plausibility functions is harder to deal with, and less practically interesting than the case of belief functions",Let us first deal with two events   and  ,"In this case, any focal element   overlapping   should not overlap   and   without overlapping  , otherwise let   be the non-empty set of focal elements that overlap   and   without overlapping  ","It is then clear that   is strictly submodular, i.e.:  and additivity fails","This leads us to the following condition for a plausibility function to be  -additive. 
                         
                      It should be noted that this condition is similar to, but quite different from the one in  , as any set overlapping   but not included in   can receive a positive mass without leading to a violation of  -additivity for the associated plausibility function","This is not the case for belief functions: for instance, the focal element of   is not in contradiction with   (plausibility could still be  -additive).  
                          pictures a focal element that would make the plausibility not  -additive","Nevertheless, the condition for  -additivity in   can be equivalently expressed as follows  which can be deduced from Eq.  , using the fact that for two subsets  , the  -additivity of a plausibility function is equivalent to the  -additivity of the dual belief function for   (clearly,   is the same equation as  )","However, the situation for plausibility functions is more involved than the one for belief functions, and   cannot be straightforwardly extended to the case of   events, as the  -additivity of a plausibility function for  :  is no longer equivalent to  -additivity for the conjugate belief function for  ","For one, the two equations are no longer the same","Moreover, the condition  no longer ensures the  -additivity of the plausibility function","For instance if  , a subset of   pictured in  
                         .a, is focal, the mass of this element is counted once in the left-hand side of Eq.  , twice in term 1 and zero times in terms 2 and 3 of the right-hand side, so  -additivity fails for this plausibility function on these sets","Also, in the case of plausibility functions, we cannot expect  -additivity to follow from  -additivity between all pairs of events","Consider indeed the following possible focal elements:  This suggests that obtaining easy-to-check conditions for  -additivity to hold for plausibility in a general setting will be difficult, if not impossible","Of course, one can check for a given collection   that every focal element   overlapping   is counted once in the right- and left-hand side of  , yet such a tedious verification would defeat the purpose of using the inclusion–exclusion principle as a practical means to achieve efficient computations","For this reason, we shall not deal with general conditions for the inclusion–exclusion principle to hold for plausibility functions",Yet we will mention those cases (which turn out to be often met in practice) when the conjugacy Eq.   with respect to belief functions can be exploited to compute  . 3 When focal elements are Cartesian products The previous section has formulated general conditions for  -additivity to hold for a given family of sets,"In this section, we investigate a practically important particular case where focal elements and events   are Cartesian products","That is, we assume that   is the product of   spaces  ,  ","We will call the spaces   
                      ","We will denote by   the value of a variable (e.g., the state of a component, the value of a propositional variable) on  ","Given  , we will denote by   the projection of   on  ","Let us call   a subset   that can be expressed as the Cartesian product   of its projections (in general, only   holds for all subsets  )",A rectangular subset   is completely characterised by its projections,"In the following, we derive conditions for the  -additivity property over families   containing rectangular sets only, when the focal elements of mass functions defined on   are also rectangular (to simplify the proofs, we will also assume that all rectangular sets are focal elements)","Assuming focal elements to be rectangular is a restrictive assumption, as they cannot be freely manipulated and decomposed in different ways, but we discuss in Section   those (many) practical situations where such focal elements will appear","However, assuming that the collection   over which the belief value   must be evaluated contains only rectangular sets is not very restrictive, at least in the finite case","Indeed, any such set   can then be decomposed into a (non-unique) finite union of (not necessarily disjoint) rectangular subsets","To see this, note that there exists an elementary way to always achieve such a decomposition: one can always decompose   as the union of its singletons, each of them being a degenerate rectangular subset.  
                       illustrates two possible decompositions of the same subset  ","However, we shall see that such a decomposition is not always very interesting for applying the inclusion–exclusion principle","The results of this section also provide conditions under which such a decomposition will allow one to apply the inclusion–exclusion principle. 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   3.1 Two sets, two dimensions Let us first explore the case   and  , that is   with   for  ","The main idea in this case is that if   and   are rectangular with disjoint projections, then 2-additivity holds for belief functions, and this property is characteristic. 
                          
                         
                      We can now characterise under which conditions 2-additivity holds for belief functions.  
                      
                         
                      
                         
                         
                          show various situations where conditions of   are satisfied and not satisfied, respectively. 3.2 The multidimensional case We can now proceed to extend   to the case of any number   of dimensions","However, this extension will not be as straightforward as going from   to  , and we need first to characterise when the union of two singletons is rectangular",We will call such rectangular unions  ,"A singleton is a degenerate example of a minimal rectangle. 
                         
                      
                         
                      As mentioned before, any set can be decomposed into rectangular sets, and in particular any rectangular set can be decomposed into minimal rectangles","Also, any rectangular set that is not a singleton will at least contain one minimal rectangle, implying that there always exist at least two singletons of a rectangular set forming a minimal rectangle (we will use this in subsequent proofs)","Let us now show how   can be extended to   dimensions. 
                         
                      
                         
                      Using  , the extension to  -additivity in   dimensions is straightforward: 
                         
                      Note that the second condition is insensitive to set-complements, hence the following result: 
                         
                      3.3 On the practical importance of rectangular focal elements While limiting ourselves to rectangular subsets in   is not especially restrictive, the assumption that focal elements have to be restricted to rectangular sets may seem restrictive (as we are not free to cut any focal element into rectangular subsets)","However, such mass assignments actually appear in many practical situations","They can result for example from the combination of marginal masses   defined on each dimension  ,   under an assumption of (random set) independence  ","In this case, the joint mass assigned to each rectangular set   is  Additionally, the random set independence assumption makes the computation of the belief and plausibility functions of any rectangular set   easier, as they factorise in the following way: 
                          where   are the belief/plausibility measures induced by  ","An interesting fact is that since the proofs of Section   only require focal elements and events to be Cartesian products, they also apply to the cases of unknown or partially known dependence, as long as these latter cases can be expressed by linear constraints imposed on the joint mass  ","Considering more generic models than belief functions is also possible, e.g. lower probabilities","Then the positivity of the mass functions   no longer holds, but the approach can be carried out without modifying our results since the product of (possibly negative) masses in such approaches preserves the approximation properties of random set independence  ","The following sections explore and discuss specific cases of interest where the inclusion–exclusion property applies. 4 The case of Boolean formulas In this section, we explore the case where spaces   are binary","In particular, conditions are laid bare for applying the inclusion–exclusion property to Boolean formulas expressed in Disjunctive Normal Form (DNF)",We also discuss the problem of estimating plausibilities of Boolean formulas using the inclusion–exclusion property,"In propositional logic, each dimension   is of the form  ","It can be associated to a Boolean variable also denoted by  , and   is also called the set of interpretations of the propositional language generated by the set of variables  ","In this case,   is understood as an atomic proposition, while   denotes its negation",An element of   is called a literal (  is a positive one and   a negative one),"Any rectangular set   can then be interpreted as a conjunction of literals (it is often called a partial model), and given a collection of   such partial models  , the event   is a Boolean formula expressed in Disjunctive Normal Form (DNF – a disjunction of conjunctions)",All Boolean formulas can be written in such a form,"A convenient representation of a partial model   is in the form of an orthopair   
                       of disjoint subsets of indices of variables   such that  ","Then an element in   is of the form  , i.e. corresponds to an orthopair  ",We consider belief functions generated by focal elements having the form of partial models,"To this end, we consider that the uncertainty over each Boolean variable   is described by a belief function  ","As   is binary, its mass function   only needs two numbers to be defined","Indeed, it is enough to know   and   (for instance a probability interval  ) to characterise the marginal mass function   since: 
                   Given   independent marginal masses   on  ,  , the joint mass   on   can be computed as follows for any partial model  , applying Eq.  :  We can then give explicit expressions for the belief and plausibility of conjunctions or disjunctions of literals in terms of marginal mass functions: 
                      
                   
                      
                   Using the fact that  , we can deduce 
                   We can particularise   to the case of Boolean formulas, and identify conditions under which the belief or the plausibility of a DNF can be easily estimated using the inclusion–exclusion Equality  ",Let us see how the conditions exhibited in this theorem can be expressed in the Boolean case,"Consider the first condition of  
                       Note that when spaces are binary,   (if  ), or   (if  ), or yet   (if  ).   therefore means that for some index  ,   (there are two opposite literals in the conjunction)","The condition can thus be rewritten as follows, using orthopairs   and  :  
                      
                   The second condition of   reads  and the condition   can be expressed in the Boolean case as:  The condition can thus be rewritten as follows, using orthopairs   and  : 
                   
                      
                   We can summarise the above results as 
                      
                   This condition tells us that for any pair of partial models: 
                   As a consequence we can compute the belief of any logical formula that obeys the conditions of   in terms of the belief and plausibilities of atoms  . 
                      
                   The conditions of   allow us to check, once a formula has been put in DNF, whether or not the inclusion–exclusion principle applies","Important particular cases where it applies are disjunctions of partial models   having only positive (resp. negative) literals, of the form  , where   (resp.  )","This is the typical Boolean formula obtained in fault tree analysis, where elementary failures are modelled by positive literals, and the general failure event is due to the simultaneous occurrence of some subsets of elementary failures (see Section  )","Namely, we have  where the terms on the right-hand side can be computed from belief values of atoms as   as per  ","More generally, the inclusion–exclusion principle applies to disjunctions of partial models which can, via a renaming, be rewritten as a disjunction of conjunctions of positive literals: namely, whenever a single variable never appears in a positive and negative form in two of the conjunctions",This is equivalent to the second condition of  ,"Then, of course, values   must be used in place of   for negative literals","For such Boolean formulas, the inclusion–exclusion principle can also be used to also estimate the plausibility of  ","Indeed, consider the formula   possibly obtained after a renaming, then  using distributivity, where   ranges on  -tuples of indices (one component per conjunction  )","Namely, starting with a DNF involving conjunctions of positive literals,   is turned into a DNF with only negative literals, to which the second condition of   applies, and  On the other hand, it is not always possible to put the complement of every formula satisfying the second condition of   in a DNF form that also satisfies  . 
                      
                   
                      
                   The last remark suggests that normal forms that are very useful to compute the probability of a Boolean formula efficiently, such as BDD   may be useless to speed up the computation of its belief and plausibility degrees","For instance,   is a binary decision diagram (BDD) for the disjunction, and this form prevents   from being properly computed by standard methods as the inclusion–exclusion principle fails in this case","The question whether any Boolean formula can be re-expressed in a form satisfying   is answered to the negative by Formula  , which provides a counterexample to this claim. 5 The case of events defined by monotone functions In this section, we show that the inclusion–exclusion principle can be applied to evaluate some events of interest defined by means of monotone functions on Cartesian products of discrete linearly ordered spaces","Such functions are commonly used in problems such as multi-criteria decision making  , reliability assessments   or optimisation problems  ","We assume that we have some function   where variables  ,   take their values on a finite linearly ordered space   of   elements",We denote by   the order relation on   and assume (without loss of generality) that elements are indexed such that   iff  ,"We also assume that the output space   is ordered and we denote by   the order on  , assuming an indexing such that   iff  ","Given two elements  , we simply write   if   for  , and   if moreover   for at least one  ","We assume that the function is non-decreasing in each of its variables  , that is  iff  ","Note that a function monotone in each variable   can always be transformed into a non-decreasing one, since if   is decreasing in  , it becomes non-decreasing in   when considering the reverse ordering of   (i.e.,   iff  )","We now consider the problem where we want to estimate the uncertainty of some event   (or  , that can be obtained by duality)","Evaluating the uncertainty over such events is instrumental in a number of applications, from checking whether a threshold can be trespassed in risk analysis   to computing level sets when solving the Choquet integral, e.g., in multi-criteria decision making  ","Given a value  , let us define the concept of minimal path and minimal cut vectors.   
                      
                   Let   be the set of all minimal path vectors of some function   for a given threshold demand  ",We denote by   the event corresponding to the set of configurations dominating the minimal path vector   and by   the set of events induced by minimal path vectors,"Note that each set  is rectangular, hence we can use results from Section  . 
                      
                   
                      
                   It can be checked that  ","We can therefore write the inclusion–exclusion formula for belief functions: 
                   Under the assumption of random set independence, computing each term simplifies into 
                      
                   The computation of   can be carried out similarly by using minimal cut vectors",Let   be the set of all minimal cut vectors of   for threshold  ,"Then   is rectangular and we have the following result, whose proof is similar to the one of  . 
                      
                   Denoting by   the set of events induced by minimal cut vectors, we have that  , hence applying the inclusion–exclusion formula for belief functions gives  This also shows that the inclusion–exclusion formula can be used to estimate both belief and plausibilities of events of the type   and   when   is monotone. 6 Application in reliability analysis In this section, we illustrate how our results can be used in the particular field of reliability analysis, as this field is typically concerned with monotone and potentially large systems for which marginal uncertainty models are specified on components","We first deal with binary systems before addressing the case of Multi-State Systems (MSS). 
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   6.1 Reliability of binary systems In classical system reliability,   are the states of some component, and the system state depends on the joint state of elements","We will consider the most common case in which   is binary,   being a Boolean variable meaning that component   works,   that it failed",The structure function   specifies when the system works ( ) and when it does not,"The problem is then to evaluate, from the joint mass  , the values   and  ",This is a special case of the one addressed in Section  ,"In the binary case, a minimal path   can be expressed as a subset   (the counterpart of a minimal vector path in Section  )","It indicates a minimal set of elements that must be in working state, in the sense that if only those components are working, then the system is guaranteed to work but will fail if one of them fails","For example,   states that   whatever the values of the other components","Note that two minimal paths   and   are such that   and  , otherwise one of the two is not minimal",A minimal path   specifies a partial model (a conjunction of literals)   with no negative literal (an orthopair   in the notations of Section  ),"If   are the minimal paths of a system, then   is the disjunction  ","This means that computing our belief in the fact that a system will work is given by  That   satisfies   is immediate, as only positive literals appear in the formulas",The same reasoning can be carried out for minimal cuts   to obtain  ,"In this case we can specify a set of   minimal cuts, a minimal cut   being encoded as a subset   indicating a minimal set of components such that, if all of them fail, then the system is guaranteed to fail","For example,   states that   whatever the values of the other components","As for minimal paths, a minimal cut   specifies partial models   such that   and  ","Given the minimal paths   and cuts   of a system, the whole reliability of the system can be computed as 
                          These equations are particularly easy to evaluate using Eq.  . 
                         
                      6.2 Multi-state systems (MSS) reliability In the previous subsection, we made the usual assumption in system reliability analysis that components can assume 2 states: failed or working",Multi-State Systems (MSS) reliability goes beyond this assumption,It allows each component to be in one of multiple (exclusive) states,"For example, a power station may have four different states corresponding to generating electricity at 0, 25, 50, 75 and 100 percent of its full capacity","In recent years, multi-state system reliability analysis has received considerable attention, yet less than binary systems",The complexity in MSS analysis is due to the non-binary nature of the system and its components,"There are many solutions to reduce this complexity, such as Markov methods  , discrete event simulation, among others",We refer to Lisnianski and Levitin   for a detailed review of the problem,"MSS analysed in this section are such that  Let us now show that for such systems, we can define minimal path sets and minimal cut sets that satisfy the inclusion–exclusion principle. 
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      6.2.1 Minimal path sets and minimal cut sets of MSS In reliability analysis, variables  ,   correspond to the   components of the system and the value   is the  th state of component  ","Typically, states are ordered according to their performance rates, hence we can assume the spaces   to be ordered.   corresponds to the system states and   is the ordered set of global performance rates of the system",The structure function   maps the system states to the global system performance,"Our assumption that the system is coherent means that the function   is non-decreasing, in the sense of Eq.  ","Note that, in this work and for simplicity, the term “multi-state system” is used to designate systems where both components and system performance take several possible states (such systems being usually called multi-state systems with multi-state components)","As the structure function of an MSS is monotone, we can directly apply the results from Section   to estimate uncertainty bounds about the event  , where  ","Estimating such an uncertainty is a typical task in multi-state reliability analysis, as it amounts to estimating the degree of certainty that a system will guarantee a level   of performance","This is illustrated in the next section. 6.2.2 Example Let us now illustrate our approach on a complete example, inspired from Ding and Lisnianski  ","The results of the belief function approach will be compared to the ones obtained using the UGF 
                             probability interval approach proposed by Li et al.  . 
                            
                         In this example, we aim to evaluate the availability of a flow transmission system design presented in  
                             and made of three pipes",The flow is transmitted from left to right and the performance of a pipe is measured by its transmission capacity (tons per minute),"It is supposed that elements 1 and 2 have three states: a state of total failure corresponding to a capacity of 0, a state of full capacity and a state of partial failure",Element 3 only has two states: a state of total failure and a state of full capacity,All performance levels are precise (see  ),The state performance levels and the state probabilities of the flow transmitter system are given in  ,"In Li et al.  , these probabilities are obtained with the imprecise Dirichlet model  ",We aim to estimate the availability of the system when  ,"The minimal paths (see  
                            
                            ) are  The sets   and   of vectors   such that  ,   are  and their intersection   of vectors   such that   (with  ) is  Applying the inclusion–exclusion formula for a demand level  , we obtain  For example, we have  and  ,   can be computed similarly","Finally we get  and by duality with  , we get  The availability   of the flow transmission system for a demand level   is given by  ",The use of the interval UGF method proposed by Li et al.   leads to  ,"Note that we always have  , as the Li et al. approach uses an interval arithmetic approach, which is known to provide quite conservative approximations in the presence of repeated variables (as is often the case when using the inclusion–exclusion principle). 7 Comparison with strong independence It should be noticed, as shown by Jacob et al.  , that using random set independence should not be confused with an assumption of stochastic independence between ill-known probabilities","In this section, we will compare the previously used notion of random set independence to the one of strong independence, which can be interpreted as a robust version (i.e., applied to sets of probabilities) of the notion of stochastic independence","We will then discuss the effects of using one independence notion in place of the other in the previously treated problems. 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   7.1 Two distinct independence notions So far, we have mainly considered that the joint mass over   was obtained by combining marginal masses   using Eq.  ",It corresponds to the notion of random set independence,"Yet, within the imprecise probabilistic literature, there are many other notions of independence available  , and it is out of the scope of this paper to discuss all of them","In this section, we will compare our results with those that would be obtained using a robust version of stochastic independence, usually called  ","If   denotes the set of probabilities compatible with   on dimension  , then the joint model   obtained by applying stochastic independence to elements of   is  Assume that strong independence consists in representing our knowledge by means of the convex hull of  , that we will denote by  ","Particularly interesting elements of   are its  , that are obtained by computing the product of extreme points of   (hence extreme points of   are also in  )","In the particular case where a probability set   is induced by a mass function   on  , its extreme points can be obtained in the following way: specify (select), for each focal element  , an element   and take the probability measure   such that  ","This comes down to taking a convex mixture of Dirac measures located at  , weighted by masses  ",Let us denote by   the set of probabilities induced by considering the joint mass   obtained by Eq.  ,This corresponds to the random set independence assumption,"To build extreme points  , a  -tuple   has to be specified (selected) in each set   with  , while to build extreme points of  , one has to specify elements   within each marginal model  , then take the product of corresponding probabilities","One can check   that the latter construction is more constrained than the former, hence   (already in Couso et al.  )","Among other things, this implies that the lower probabilities  are such that   for any  , meaning that random set independence can be used to outer-approximate strong independence","Also recall that such lower probabilities are obtained for extreme points of the set, hence for such inferences working with   or its convex closure   makes no difference","The two notions also have different interpretations: random set independence can be associated to an independence of sources providing the uncertainty, making no claim about the possible interaction between variables of different marginal spaces, while strong independence can be interpreted as an extension of stochastic independence between random variables when probabilities are partially known","The next example illustrates the inequality   as well as how the selection process to obtain a probability reaching the lower bound is different in the two cases. 
                         
                      
                          clearly shows the difference of meaning between the two notions: in the strong independence case, fixing the element of   does not influence the selection on  , while in the random set independence case, obtaining the lower bound implies considering a very strong relation between the selections (e.g., fixing   implies selecting   whenever possible). 7.2 Consequences for Boolean formulas Assume we have a formula   in DNF   with  , i.e. the sets of models of   are disjoint","Such formulas, in the form of a disjunction of exclusive conjunctions of literals (it can be at worst, just the disjunction of models of  ) can be obtained by using the Shannon decomposition of   (the basic notion from which BDDs are derived)","In this case, assuming stochastic independence between variables/atoms, the probability of   reads  When the probabilities   of atoms are incompletely known, bounds   for   can be obtained by interval analysis of   
                         ","As Eq.   is a multilinear function, 
                          it is locally monotone in each of its variables (it is either increasing or decreasing in   once the probabilities of other atoms are fixed)",This means that each bound   is attained for some vertex of the hypercube  ,"We have that  , since selecting the right vertices comes down to making the right selection for each marginal, selection   corresponding to   and   to  ","This also means that, in practice, we will have  ","However, a noticeable exception is when each variable will always appear either in a positive or negative way in the expression of a Boolean formula.   
                         
                      Such a situation occurs for connectives like the conjunction, disjunction or implication, and more generally for all expressions that obey condition 2 of  ",In such cases the choice of the dependency assumption (between variables or sources) has no influence on the output interval,"The fact that in such cases the same results are obtained by both approaches does not make the belief function analysis redundant: it shows that the results induced by the stochastic independence assumption are valid even when this assumption is relaxed (the independence assumption of mass functions is indeed weaker), for some kinds of Boolean formulas","On the contrary, expressions satisfying condition 1 of   correspond to non-monotonic functions, like for the   and the   
                         ","In this case, an exhaustive computation for all combinations of interval boundaries must be carried out in the case of interval analysis and strong independence, while the computation of the belief and plausibility are still very simple, but provably less precise than the result of interval analysis.  
                      
                          is also of this kind. 7.3 Extension to the multivariate case Let us now deal with the non-binary case and with conditions of  ","Using the fact that   and  , it is clear that bounds computed using   and   will not coincide if events in   satisfy condition 2 of  , as they already fail to coincide in the binary case","In the case of the first condition, however, we can give a result similar to  . 
                         
                      
                         
                      Although such a situation will not always appear, this result directly applies to the case of monotone functions   treated in Section  , showing that in this particular case, choosing between assumptions of random set independence or of strong independence will not change our inferences about events of the kind   and  ","In practice, this means that in those cases we can either use tools originating from evidence theory, imprecise probability theory or interval analysis to carry out computations (whichever is the most suited to the situation). 8 Conclusion In this work, we have studied families of events for which the principle of exclusion/inclusion applies to belief functions","Although the framework we have retained may look restrictive at first glance, it can be applied to a number of practical situations, and we have shown that one particular application is the evaluation of system reliability (both in the binary and multi-state cases)",Such results facilitate computations and are particularly useful when probabilistic data are imprecise,"An interesting perspective to this study is to look for conditions under which other uncertainty theories (e.g., general lower probabilities) satisfy the exclusion/inclusion principle","Further potential applications of our results include the study of other problems of reliability analysis, such as importance measures used to detect critical components.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0888613x15000912," 1 Introduction Rule-based models are composed of collections of rules, which serve as local models describing complex relationships existing in system modeling",Takagi–Sugeno rules   assume the well-known form  where   is a fuzzy set of condition of the  -th rule and formed in the input space and   is a certain local model standing in the conclusion part are commonly encountered in fuzzy modeling,There is a significant number of design procedures of rule-based models  ,"In general, the design of the fuzzy model is carried out in two phases: (i) formation of condition parts of the rules – fuzzy sets built by fuzzy clustering, and (ii) determination of the conclusion parts of the rules; here we are concerned with a parametric optimization and resort the analytical solutions of the ensuing estimation problem","Quite commonly we encounter the local models assuming a simple form being e.g., linear functions",The design process is well documented in the literature  ,Each of these two design phases comes with various augmentations,"In  
                      , we offer several highlights of the visible representatives of the fuzzy models, specify their development strategies and look at the optimization tools supporting the construction of the fuzzy model","Irrespectively of the diversity of approaches, fuzzy rule-based models share a visible commonality: complex phenomena are modeled   through a series of local models (which are less complex than a single global model)","The differences lie in a way in which the rules are formed, how local (typically linear) models are constructed and how aggregation of the rules is completed","While rule-based models are about local modeling of the overall system, incremental fuzzy models follow a radically different line of thought","We consider a global model coming in a form commonly encountered in the literature (say, a linear regression model) while the rule-based model is constructed to enhance – compensate for some discrepancies of the global model",The individual rules of the incremental model are developed in a way so that the deficiencies of the global model are eliminated or substantially reduced,"In this way, the incremental model improves the quality of the global model",Some initial studies in this area were reported in  ,"This incremental rule-based approach to system modeling exhibits some tangible advantages: (i) we rely on the well-known and commonly acceptable model, such as linear regression and offer its improvement, (ii) the methodology of fuzzy rule-based modeling fully applies here so we take advantage of the existing design strategies of fuzzy models, (iii) the two-phase design process is well-motivated","The objective of the study is to develop a detailed concept of incremental fuzzy models, propose algorithms, and demonstrate the performance of the model with the use of synthetic and publicly available data sets",The study is structured as follows,"In Section  , we present a structure of the incremental fuzzy model showing how the rules are formed to account for the existing discrepancies of the global model","In Section  , we discussed augmented fuzzy clustering (extended version of the Fuzzy C-Means), which allows to articulate a structure of data given a directional character of the data used in system modeling",Experimental studies are reported in Section   in which both synthetic and publicly available data sets are considered,"In the paper, we adhere to the standard notation and symbols used commonly in rule-based models and fuzzy sets","In particular, capital letters are used to denote fuzzy sets, while vectors are shown in boldface. 2 A structure of the incremental fuzzy model We are concerned with the incremental fuzzy model (in essence being a certain rule-based architecture) whose topology is built directly by discovering and exploiting the structure of the data being determined with the use of an augmented model-oriented fuzzy clustering","In the construction of the model we use a collection of data coming in the form of input–output pairs  ,   where   is in the  -dimensional input space,  ,  ",The generic global model built for these data comes in the form of a certain input–output relationship  ,"Typically, this function is very simple and describes the relationships in a global fashion","For instance   could be sought as constant, linear or quadratic (polynomial of the second order), say  ,   etc","Owing to form of the model, its parameters could be easily estimated by using a standard least square error method for which analytical solutions are obtained","In light of the simple form of the global model (constant or linear), its approximation abilities are limited","To alleviate this shortcoming, we look at the error coming as a result of using the model, where the error   is defined as the following difference  , determine its structure especially look at the distribution of error in the input space (error clusters) and on the basis of these clusters construct a collection of rules, which compensate for the observed error","In this way, we form a family of fuzzy clusters   in   where each of them is associated with its corresponding typical error for this cluster, say  ",The determination of this typical error will be discussed in detail in the subsequent sections of the study,These clusters localize a position of error of the model and describe its structure,"A schematic view at the model and the resulting error is displayed in  
                      ",The augmentation of the generic model   is done by compensating for the error in the following way  Here   is a representative value of the error associated with the  -th error cluster.   can be regarded as a degree of “activation” of the local error-compensation rule for a given input  ,"It is apparent that from the processing point of view,   delivers a compensation mechanism","Being more specific, if we consider   coinciding with the prototype of the  -th cluster,   and  , then   so the compensation is realized by adding the error value typical for the local region of the input space specified by  ","The condition parts of the rules are expressed as fuzzy sets.   is defined in the  -dimensional input space and its membership function is determined by running fuzzy clustering (Fuzzy C-Means, being more specific)","In this case, the membership functions   read as follows  
                       where   is a fuzzification coefficient and   stands for the weighted Euclidean distance between the prototype   and input datum  ","Recall that for any two vectors   and  , this distance reads as follows  where   is a standard deviation of the  -th variable and   are the prototypes of error compensating the lowered performance of the global model","In essence, we can see that the introduced fuzzy model comes as a realization of a multi-linearization   where the points of linearization are determined by the prototypes of the clusters in the input space, the values of the mapping at these linearization points are",The receptive fields are described by the membership functions  ,One can also view the model as an augmentation of the simple generic model modified by the local changes (viz. error compensation terms) developed around the prototypes   and captured by the following rules  In some sense the discussed model relates to the incremental fuzzy models   where the rules are allocated to the regions where an initial linear model fails visibly and exhibits a concentration of error,"The error compensation rules can assume a more general character where instead of the constant error typical for the corresponding cluster, one considers the modification  where   is an explicit function of the constant error and  ",It is worth stressing that the incremental model comes with some appealing properties,The incremental model is built on a basis of any existing model (including a fuzzy rule-based model) with intent to compensate some errors produced by the original model,"In this sense, the resulting architecture original model-incremental model gives rise to the better performance that the one offered by the initial model only","It is easy to construct, as there is a well-established and experimentally sound methodology of building rule-based models (and this type of the incremental model has been proposed here)",It is easy-to-use because of the hierarchy of the original (base) model that leads to the compensation (in an additive format) of the results produced by this particular model. 3 Augmented fuzzy clustering Fuzzy clustering is   – free procedure meaning that when searching for a structure in data no distinction is being made among the input and output variables,The data are treated in the same way,This treatment is not desirables when dealing with fuzzy models where a distinction between input (independent) and output (dependent) variables plays an important role,"In contrast to Fuzzy C-Means clustering method where the distance is computed in the same way for all the variables, we amplify the impact of the output variable by introducing a certain non-negative control parameter   in the calculations of the distance",For the purpose of clustering we concatenate the vector of inputs   with the output variable (target) to form an ( )-dimensional vector   in the form  ,In the calculations we use the following weighted Euclidean distance  where   is a standard deviation of the  -th input variable and   is the standard deviation of the output variable,It is clear that the parameter   plays a role of the weighting factor whose adjustment helps achieving a sound balance between an impact coming from the input and output variables when computing the distance and building a structure in the data space,The parameter   plays a pivotal role in the formation of the structure of information granules forming the antecedent part of the rules of the incremental model,It could be noted that the value of   close to 1 comes with an amplification effect given the form of the original distance function   so that the relevance of the output variable is in fact quantified by the coefficient  ,"As usual, the FCM clustering completed for these ( ) dimensional data   produces a collection of prototypes whose entries are structured into two blocks as follows  ",The fuzzy sets   defined in the input space are determined on a basis of the prototypes   as given by  ,The choice of the control parameter   deserves more attention,"As expected, low values of   imply that the structure becomes implied by the data positioned in the input space",Higher values of   favor the structure present in the output space,"In case the data are impacted by noise, such values of the weight parameter state that the clusters start exhibiting a tendency of becoming more sensitive to the noisy data present in the output space rather than ignoring them",Low values of   may lead to a significant averaging (damping) affect resulting in a collection of quite “crowded” clusters (with their corresponding prototypes positioned in the output space),As a consequence the clusters are not capable of covering the entire range of the output space making the model mentioned above not being able to cope with the entire range of possible outputs,"The performance index used to assess the quality of the model is a standard RMSE expressed in the following way  where   is the number of data (either training, validation or testing)","To find the optimal value of  , we use a validation data set using which this optimal value is determined","In other words, we minimize the sum of squared errors obtained for the data coming from the validation set that is  where   is the performance index obtained for the validation data set","In what follows, we outline the main flow of the design process. (i) A base model   has been constructed",We are not concerned what the form of the model is and how it has been constructed,"For instance, one could envision   to be realized as a neural network whose design was completed using one of the standard learning methods such as e.g., back-propagation","The model could be a fuzzy rule-based model again developed with the use of one of the standard optimization techniques reported in the literature. (ii) The evaluation of the base model is carried out by using the training data and for each input data  , one determines an error   thus producing a family of pairs of data relating input with the error (residual of the base model). (iii) The design of the incremental model is based on the structure revealed in the input–error space in which the pairs of the data ( ,  ),   are positioned",The structure of the error produced by the base model is identified by the augmented version of the FCM algorithm endowed by the augmented parameterized version of the Euclidean distance function in which we establish a suitable balance between the structure existing in the input space and the output (error) space,"The incremental model is a fuzzy rule-based one and here its construction comprises two key phases, namely (a) choosing a number of rules ( ), and fixing the value of the fuzzification coefficient   (typically set up as 2.0)","The choice of the number of rules is data-driven and a sound design practice is to experiment with several different values of   and monitor a behavior of the incremental model, (b) optimizing the parameters of the local models (either constants, linear or polynomials) by using standard regression techniques (noticeably the local models are linear with regard to their parameters)","The development is of iterative nature in a way that for a predefined value of  , one selects the best value of  ","The determination of the optimal value of   is realized experimentally by choosing such value of   for which   attains its minimum. 4 Experimental studies In this section, we report on a series of experiments completed both on synthetic datasets and a number of real-world publicly available datasets ( )","The experiments are essential in demonstrating the main design phases of the model, assess its performance and compare it with the performance produced by some reference models","The datasets used in the experiments are split into three parts, namely a training (60%), validation (10%), and testing set (30%)",Each experiment is repeated ten times to obtain reasonable results with high stability and to facilitate further statistical analysis,The role of the validation set is to determine an optimal value of  ,"For comparative purposes, several reference models were used in the experiments, namely linear regression  , polynomial regression  , and a fuzzy model built with the use of the generic version of the FCM",We also report on experimental results reported for other models available in the literature,The RMSE performance index   is used to quantify the performance of the model,"In the optimization of   we use a validation set using which an optimal value of this parameter is determined, refer to  . 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   4.1 Synthetic datasets Here we consider one two-dimensional nonlinear function of the form 
                      We consider 729 pairs of input data ( ,  ) that are equally distributed in the over the Cartesian product of intervals [ ]","The plot of the function is presented in  
                         ","In the series of experiments, the fuzzification coefficient was set to several values, namely 1.1, 2.0, and 2.7","The number of clusters was set to 3, 4, 5, 6, 7, 10, and 15",The optimal value of   was found by experimenting with their successive values coming from the interval [ ],"For reference, we use linear regression, polynomial regression of the second order, and the FCM-based model","More formally, linear regression and polynomial regression can be stated by the expression   and  , respectively. 
                          Here   and   are vectors of parameter.   is the number of input variables",The FCM-based model serves also as a reference structure using which we can quantify an improvement delivered by the augmented clustering,"The results for the augmented FCM fuzzy model, incremental forms of the models (constant, linear, polynomial) are summarized in  
                         ",As expected the polynomial incremental model exhibits the best results (viz. the lowest values of the RMSE),"Across the experiments, one can acknowledge an essential role of the control parameter   as it helps strike a sound balance between the need to capture the output variable (cover its range) and reflect a dominant structure of data present in the input space","An example dependency of the performance index on the values of synthetic dataset is shown in  
                         ",There is a clearly visible optimal value of   (  and   are equal to 1.1 and 27.3566 respectively),"In other words, amplifying the impact of the output variable in clustering process help increase its accuracy",The RMSE value obtained on the validation set decreases until it has reached the minimum and then starts increasing,Low values of   indicate that the structure becomes predominantly implied by the data located in the input space while high values of   favor the structure present in the output space,Lower values of   may lead to a significant averaging affect resulting in a collection of quite “crowded” clusters when observed in the output space,As a consequence the clusters are not capable of covering the entire range of the output space subsequently making the model not being able to cope with the entire range of possible outputs,"When it comes to the membership functions constructed in the input space, they exhibit a visible diversity implied by the number of rules and the values of  ","A snapshot of this diversity is illustrated in  
                         ",There are several observations of general character that can be made with regard to the behavior of the constructed models,It is apparent that the increase of the number of rules has a beneficial impact on the performance of the model (and this has been anticipated),The fuzzification coefficient has an impact on the performance of the model meaning that the geometry of the receptive fields (fuzzy sets) is essential to the quality of the model,There is a notable improvement observed when considering incremental models,"On average, such improvements range from 11.3777 ( ,  ) to 3.6053 ( ,  ) when comparing the best (polynomial) model with the linear one",The improvement of the polynomial incremental model vis-à-vis FCM based model is in the range 42.2–72.1%,"When contrasting augmented FCM model with its generic FCM-based model, we see the improvement of 0–7.5%","To quantify the obtained optimal values of  , their optimal values are shown in  
                         . 4.2 Real world dataset In the sequel, we also construct the proposed models for a number of publicly available data sets, refer to  
                         .  
                          visualizes the values of the performance index describing the proposed models and the reference models in the same way as presented for the synthetic data",Several general observations can be drawn here,"It is apparent, as shown in these figures, that the increase of the number of clusters (rules) in most cases implies the improvement of the performance of the models",This is related with the fact that more structural information can be captured through a collection of more detailed rules,It is also visible that the fuzzification coefficient impacts the quality of the results meaning that not only the number of local rules (models) is essential but a way in which the models are combined (aggregated) together,"As anticipated, the polynomial incremental model produces the lowest performance index value. 
                         
                          summarizes the improvement of the polynomial incremental vis-à-vis other models when the optimal parameters have been used","The better improvement of approximation and prediction abilities of the polynomial incremental models can be attributed to their nonlinear characteristics, which might offer a good fit to the nonlinearity of the dataset itself. 5 Conclusions The fuzzy rule-based model serves as a conceptual and algorithmic setting for the construction of incremental fuzzy models",The augmented clustering method delivers an efficient tradeoff between structural relationships in the input and output spaces when building information granules in the input space,The determination of the structure in the space of input and error becomes essential to the construction of local models compensating for the deficiencies of the global model,The series of experiments completed for selected classes of global models demonstrate the improvements realized by the incremental models,The two-level model (global one and its incremental enhancements) can be generalized by introducing additional levels of hierarchy so that the compensation can be realized on a basis of errors produced by the incremental fuzzy model,Another promising augmentation of the model is to consider more advanced structures of local models with conclusions formed as linear or polynomial functions of error.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s1071581917300320," 1 Introduction In the New York Journal of 2nd June 1897, Mark Twain wrote “The report of my death was an exaggeration”","The same might be said about printed newspapers themselves, despite falling circulation, as some readers switch to on-line news content consumed on computers, tablets and phones (c.f.  )",This situation is one of the latest manifestations of an old debate about the future of paper and which has variously raged and rumbled on since the earliest presentation of textual information on computer screens,"Comparisons of paper versus screen reading took place in the context of a debate about the paperless office, and the prospects of dedicated reading devices that might eventually replace the printed book ( )","Predictions of new forms of reading in hypertext and hypermedia followed, and eventually came true in the realization of the world-wide-web ( )","New technologies for the presentation and distribution of electronic books, magazines and newspapers were eventually commercialized by companies like Amazon and Apple","And the rise of user-generated content on the web, shifted the balance of power from publishers to individual authors, allowing a wider range of people to create and share information in either screen-based or paper form","Despite these cataclysmic changes in the authorship and form of media content, paper has proved stubbornly resilient in the face of potential obsolescence",The paperless office never materialized because people simply moved backwards and forwards between paper and screen-based materials to make use of the different affordances of each ( ),Reading the web simply became a new form of literacy alongside conventional reading ( ),"Hence printed materials of all kinds continue to be used with new generation laptops, e-books and tablets for different reading and writing activities, specialized by content","In fact, they are often used simultaneously or serially in paper-and-screen environments such as architect's offices, transport control spaces and news rooms, where information is compared from different sources, and passed between people ( )","The co-existence and use of both paper and digital content has led several authors to call for the creation of closer links between the two, through technologies of   ( )","Interactive paper is the opposite of electronic paper because instead of making existing devices simulate the properties of paper, it   ( )","A number of different technologies exist for linking paper and digital information together, including 1D and 2D barcodes, RFID and NFC tags, optical page recognition and finger tracking, digitizing tablets, ultrasonic, optical and inductive pens, and capacitive touch regions ( )","Perhaps because of this variety of core technologies, the applications of interactive paper are many and varied (see related work below) and principles for the successful design of interactive paper are lacking",In fact   conclude their review of the area by listing this as one of the major challenges for the field:  ,"Taking this observation as our starting point, we focus here on a single technology and application domain for interactive paper that we think has great promise, and test out multiple designs in order to extract design principles and recommendations for that domain",The technology comprises capacitive touch points that can sense the position of fingers and thumbs on a printed page through changes in electrical conductivity at the point of contact,We use this interaction technique in the   domain to invoke associated speech or other sounds from printed newspapers and to register subjective responses in the form of ‘votes’,"In the rest of the paper we briefly explain our reasons for focussing on newspapers, and summarise related work on augmented paper before introducing a new framework for understanding interactive paper experiences at multiple levels","We then describe the interactive newsprint platform and documents, and the way they were tested in a study to elicit reader responses, before summarizing those responses and their design implications. 2 Why newspapers? There are two main reasons for considering newsprint as a promising domain for interactive paper, aside from the fact that it had not been fully explored before","The first has to do with the   of printed news layouts as a particular kind of paper, while the second has to do with the parallel development of the online news   alongside printed news","In the first case, newsprint benefits from several interactive properties or affordances of paper, pointed out several years ago by authors such as   and  ","Newspapers can be flicked through to assess their contents quickly, jointly viewed, annotated, and passed around physically without the need for compatibility between devices or formats (see Table 6.2 in  )","They are therefore easier to share and collaborate around than their e-book counterparts, being not only mobile, but ‘micro-mobile’ allowing elements to be spread out, re-orientated, juxtaposed and memorized spatially ( )","The information and graphic design of newspapers in particular, lends itself to this behavior because different physical sections (for News, Sport, Business, etc) are indexed on the front page and often segmented physically into different supplements","Article headings and opening paragraphs summarise the content, which is expanded later as you read on",And many pictures are usually embedded in the layout to illustrate the accompanying text which has a certain shape and style,All these elements make it easy to browse and read a printed newspaper using the affordances above,"This structure also lends itself to augmentation with digital content, because sections, headings and pictures can be used to locate interactive regions or buttons from which further multimedia content is available","Digital augmentation could also overcome limitations of space on paper, which leads to very large newspapers in some countries such as the US","In the second case, there is a business imperative for re-considering the design of newspapers linked to falling newspaper circulations, which have been recorded since the 1970s","This trend has been accelerated by the advent of digital news consumption as readers switch to on-line news content consumed on computers, tablets and phones (c.f.  )","The drivers behind these changing consumption habits, and seeming mass-exodus from print to digital landscape, are complex","Shifting audience demands and instant interactions via digital have seen news and information, or perhaps more accurately the function of news and information to inform a community, migrate not from offline to online newspapers, but from offline to social media, search engines and listings services such as eBay or Craigslist",The result of this audience migration has undermined advertising revenues from printed newspapers ( ),"Advertising revenue is the central revenue stream of many local and national publications across the globe ( ), and its fall has marked the end of a number of newspapers","According to the American Society of Newspaper Editors, ( ), the number of journalists employed in the US fell by 17,500 in the decade between 2003 and 2013","In the UK, government Labour Force data suggests that, between 2013 and 2015, 6000 fewer people described themselves as journalists, newspaper and periodical editors ( )",These falling figures represent fewer journalists ‘doing journalism’,"As such, there are question marks associated with journalism's fourth estate function, defined as its’ ability to hold those in power to account and inform the public","However, the dynamic between print and digital journalism is more complex than a migration from one platform to another, falling circulations, ad revenue and job losses","Many printed newspapers have online editions, providing choice to readers about which platform they can chose for particular articles or situations","Printed articles can point to further information on the web, and web articles can point to books or other printed materials for longer treatment","Studies of the daily news repertoires of individuals show complex media diets involving combinations of print, TV, radio, websites and social media, with some people continuing to favour printed newspapers as their primary source (e.g.  ;  )","And new business cases are emerging for free newspapers such as the Metro and the London Evening Standard in the UK, which increase circulation by removing the cost to the readers, thereby increasing advertising revenue again","Local journalism is in particular flux as citizen journalists publish local news online, and community groups publish free newsletters and magazines for targeted audiences","This is leading to a ‘hyperlocal’ news ecosystem of professional and citizen news, both online and in printed form for the same area ( )","Given these developments in the design of parallel online and printed news content, and the growing practice of reading between them for particular purposes, we believe that the time is right to try to connect paper to web in this domain","Current attempts involving QR codes have met with modest success, and require a kind of double reading action to scan and then read associated visual content from a mobile phone hovering over the paper","In this study we explore a different paradigm in which web associations are invoked by direct touch of points on the paper, and limited to audio output or voting input","A mobile phone can still make the connection to the web, but remain in the pocket of the reader who handles the newspaper in the usual way",Such interactive newsprint could offer a third way for newspapers to address a number of challenges facing the industry whilst continuing the trend towards greater democratization of news content (e.g.  ),"For example, the multimedia nature of modern news could be partially recreated   print and digital platforms by linking between them, audio can be overlayed on print content and even updated dynamically, and the ability for readers to respond to articles can be supported by voting on, or even speaking to, paper","Touch sensitive paper might also protect print advertising revenues by providing advertising or purchasing web links, or supplying advertisers with ‘paper-analytic’ feedback on when a page or item has been read or listened to. 3 Related work Experiments on the design of interactive paper are usually said to have started 24 years ago with Wellner's   ( )",This explored the use of an overhead camera and projector for enhancing reading activities with paper documents (see also  ),"However, earlier experiments include Edison's use of phonographic paper in 1878 to transport   in the physical mail, and Hoshino's 1959 experiments with paper   coated in magnetic paint for recording and playing back sound (cited in  , pp. 16–19)","Today, most experiments of this kind would be classed as   systems, in which paper artifacts rather than other physical objects or places are enhanced with digital resources","Examples are too numerous to mention exhaustively, but can be summarized in terms of the type of paper documents to which they apply and the technologies they use","Hence, a number of augmented paper   have been explored, linking handwritten notes to meeting recordings, photographs and video clips","Early systems use PDAs and tablet digitizers with overlaid paper, while later systems use the Anoto pen and paper system (e.g.  ;  ;  )",The   smart pen and paper system is a commercial audio-notebook product in this area based on Anoto technology,"Augmented   have been created using embedded audio chips, overhead cameras, digitizer/PDA combinations, and Anoto pen and paper (e.g.  ;  ;  ;  ;  )","The primary digital data in these cases has been associated verbal storytelling or non-speech audio.   are an example of a commercial application in this domain, linking printed photographs to associated video clips via a smart phone app","Augmented   of various kinds have been created using electric field sensors, RFID tags, barcodes, PDAs, cameras and head mounted displays","Most examples make extensive use of audio associations to enhance the reading experience (e.g.  ,  ;  )","Others link animations and video clips to enhance learning or bring characters to life in a story (e.g.  ,  )","A commercial example in this space was the  
                       which used a double-sided digitizer with booklet overlay to present audio annotations on childrens’ books","There has been very little work on augmented  , aside from promotional weblinks through printed QR codes",The only research explorations we can find is through the   system developed recently for Indian and South African contexts ( ) and the   system published recently ( ).   allowed users to access audio content on low-end cameraphones via interactive voice services,"Users took snapshots of printed pages from maps, leaflets, packaging or newspapers that contained QR reference points and identifiers",Thereafter they interacted with the snapshots to hear associated audio such as spoken texts and explanations,Preliminary evaluations suggested benefits for those with low levels of literacy or difficulty interpreting articles or following instructions,RocReadaR was a similar system designed to trigger digital augmentations to the printed Research Newsletter of Rochester Institute of Technology,Holding a smartphone over pages of this magazine caused hotlinks to be presented on the smartphone screen,These could then be touched to bring up related content on the smartphone itself,"The associated information was found to add interest, information and fun to the reading experience and make it easier to remember","However, new design issues were thrown up such as the potential distraction of digital from printed content, the division of labour between digital and print, and how best to cue and control the associations","These preliminary findings suggest a rich design space for interactive newsprint and some potential benefits which we will now explore in our own study. 4 Aims and design framework Building on previous work, we set out to understand the potential role of interactive paper in local print journalism, and some of the design constraints and principles governing this",Our starting concept was an interactive newspaper from which readers could seamlessly access associated   information through handling gestures,"Unlike previous attempts to support interactivity through optical page recognition on a smartphone, we set out to explore an alternative paradigm of instrumenting the paper itself, and presenting associations in audio-only form over wireless headphones without the involvement of a second screen device","This avoided visual distraction from a second screen, and was technically challenging; leading to both innovation and design insights on the use of printed electronics in this domain",Some of the challenges of designing for interactive newsprint are highlighted further in a study of pen-and-paper user interfaces,"When designing for these types of interfaces,   proposes a model which separates   from   level activities of interaction between paper and digital resources","For example, conceptual activities of annotating and linking resources are separated from core interactions used to achieve this, such as inking and clicking - both for single and multiple sheet documents","Unfortunately, activities like inking do not apply to touch-based interfaces lacking pen input, and are at a level that overlooks broader factors such as the type of content being presented in different media formats","As  , even in instances where input technologies focus only on one single modality – in our case a touch-paper interface – we need to understand the broad limitations of that modality, which may include the specific attributes as ‘display’ size and form factor, target market, user experience and application design","Hence our first prototypes represent an exploration of a new hybrid form of interactive content that faces similar design challenges to those faced by content creators in newspaper publishing at the outset of desktop publishing, the internet and more recently mobile and tablets","Question marks around what forms of advertising are readers/users prepared to view for access to ‘free’ content, the length of on-screen articles compared to those of their print siblings and fundamental questions around the user experience and understanding of what content is ‘clickable’ or ‘interactive’ have all been the concerns of editorial teams since the emergence of online newspapers in the 1990's","Furthermore, we were intrigued by the idea of maintaining paper-based news publications as a form factor and as a hybrid technology triggering digital content consumption","For these reasons, we developed our own simple framework for understanding the dynamics of what might be called   interfaces, based on the different levels of design contributed by intersecting design disciplines (see  
                      )",This is also useful for understanding design constraints and user responses to different elements of interactive paper more generally,"For example, at a top level of  , an interactive document should have a coherent product concept and fall into a recognizable document genre such as book, leaflet, magazine or newspaper","Below this, the document should have a graphical   that identifies which elements of the paper are interactive and interprets manual actions on these elements as requests for certain kinds of content or responses to content.   specifies the way in which content and responses are controlled through this input over time.   is specified separately through a combination of aspects such as information design, editorial design, sound design and journalistic and editorial voice (sub-divisions not shown)",This depends on the multimedia capabilities of the adjunct technology being used to present the associated content,Using this framework we designed and tested a number of ‘interactive newsprint’ documents involving only capacitive touch sensitive paper,"This is an alternative implementation to   and   that allows the paper, rather than a snapshot of the paper, to be used as an interface to associated digital content provided by an adjunct mobile device","Although this adds cost and complexity to the paper itself, it provides a more direct form of interaction with printed news content","It also opens up the prospect of making the experience more self-contained in the future, with the printing of further electronics like speakers, storage and CPU. 5 Interactive newsprint platform Stepping through the levels of our framework helped us to decide what to implement within this design space. 
                   The resulting   was implemented by our company partner Novalia who printed capacitive touch regions at selected points on a printed page, together with conductive ink tracks to an embedded Bluetooth chip",This minimized the amount of physical electronics on any page by moving computation to an adjunct device,"In fact, the electronics were printed on the back of the interactive front pages of several newspaper versions, as described in   below.  
                       shows the electronics for the reverse of the Lancashire Evening Post front page (Webpage version in  
                      b)",Page two was stuck to the reverse of page one to hide these electronics from the reader,"The electronics worked as follows, allowing the reader to access associated audio over wireless headphones from certain touch points, or to ‘vote’ on printed articles by pressing on others","The system in   consists of four components, a sheet of paper with screen printed conductive ink, a capacitive touch module, battery pack and a Bluetooth module","In the first component, conductive ink is screen printed onto the sheet of paper to form capacitive touch sensors and interconnects to the second component",The screen printed conductive ink has sufficient conductivity to carry electrical charge between the sense area (larger areas of conductive ink) and the capacitive touch control module,The second component (capacitive touch control module) runs software that charges the conductive ink lines connected between the control module and the sense areas sequentially,The module is connected to the conductive ink using a silver conductive epoxy,The capacitive sensing used in this set up is achieved by charging and discharging lines from the IC (integrated circuit) which are connected to the printed conductive ink tracks that route to the printed sense pads on the paper,The time to discharge a line is measured and stored by the IC,When a line is charged with a voltage the amount of charge stored on the sense pad is greater if a persons’ finger is present than when it is not being touched,The finger and the sense pad form two plates of a capacitor,"This greater amount of charge is seen as a longer discharge time on the IC pin, ranging from 10 µs to 100 µs, each pin/pad is charged sequentially and the process is complete in less than one microsecond","This captive touch module is connected via a few wires (SPI connection) to communicate to the Bluetooth module, the Bluetooth module is connected wirelessly to an internet connected device, in this case a laptop",Audio can be played from the internet connected device and data sent to the cloud,"This whole system has since been replaced with one control module (1.6 mm thick) with one IC and thin 2016 coin cell, and connects directly to a users' smart phone","The experimental platform was used here to create four different interactive newsprint documents, developed in collaboration with a community of professional and community reporters",These are described in the next section,"Despite the technical complexity of the system, the user experience was surprisingly simple","Users could browse what looked like a regular newspaper laid out on a table, and touch graphical buttons to playback associated audio over wireless headphones or rate printed content. 6 Document design The final stage of our design process was to consider possible sources of local news content that would be of interest to residents of a local community","For the study we recruited 16 suburban residents of Preston in Lancashire, UK who were regular readers of either printed or on-line news","The biggest local newspaper was the Lancashire Evening Post (LEP) written by professional journalists, while a popular on-line source was Blog Preston (BP) written by locals themselves",It struck us that interactive newsprint versions of both news sources would lead to two quite different hybrid papers in varying formats,An interactive printed Lancashire Evening Post would result in a conventional newspaper having some of the properties of a web page with voting cues and hyperlinks out to audio clips,"Conversely, a printed interactive Blog Preston would have the properties of a printed webpage with voting and audio annotation",This webpage might be at broadsheet newspaper-size or - given the increasing popularity of tablets for reading on-line news - tablet size in format,"Graphically then, the content design of these documents would look very different if they simply inherited the conventions of newspapers or the web; different kinds of updateable audio content might be used by professional or community journalists to extend these publications","Our conceptual deliberations went one step further: we wondered what would happen if we switched design conventions for the same content, by printing the Lancashire Evening Post in the style of a webpage and Blog Preston in the style of a traditional printed newspaper","Hence, varying the origin and format of local news across web and print dimensions led to four possible interactive newspapers as follows: 
                   We decided to compare and contrast reactions to all four of these hypothetical documents, as realized in  ","This is because preferences between them might reveal important design principles to carry over from print and web design, or places where these break down for interactive newsprint","They might also indicate the relative strength of audio in augmenting professional or community journalism, and the value of paper in each domain","Hence a range of audio annotations were designed to supplement a selected issue of the Lancashire Evening Post on Saturday April 21st, 2012 and a selected snapshot of Blog Preston on Thursday 8th November 2012. ‘Newspaper’ versions of both documents were printed at broadsheet scale measuring 29×38 cm, with only the first page being interactive ( a and  c). ‘Webpage’ versions of both documents were printed at different scales, with LEP measuring 29×38 cm as above (2b) but BP formatted as a double-sided paper tablet-size measuring 29×19 cm (2d)",Audio and voting touch points were supported only on the front pages of newspapers 3a-3c which thereafter contained a number of non-interactive pages taken from the selected LEP and BP issues,"This is because the Bluetooth chip and cell batteries on the back of each front page only supported up to 8 capacitive touch points, could not currently be printed, and added about 2 mm thickness locally","However the electronics on page 2 could be effectively hidden by sandwiching it to page 3, resulting in a document with localized interactivity which felt like a conventional newspaper","In contrast, both sides of newspaper 3d were interactive because it was created out of a printed webpage folded in half, with the electronics hidden in the middle",This meant that it had no other non-interactive content like the other newspapers,"These documents therefore served as user experience prototypes for the purpose of this study, to give users a taste of interactive newsprint in a variety of formats for subjective feedback, in anticipation of being able to print multiple interactive pages in the future","Different audio annotations and voting options for the LEP and BP documents are summarized in  
                      ",Audio annotations were drawn from professional and community-created content,"For example, on the interactive LEP a full recorded interview with David Cameron was associated with a print article about his visit to Preston; the latest Olly Murs single ‘Oh my goodness’ was linked to a headline picture; and a BT Infinity radio advert was linked to its printed version","In contrast, items from community radio station Preston FM were linked to articles in the BP documents","These included spoken news items about a miniature model of Preston and a local family event, as well as an interview with the winner of the Preston half marathon","Voting on both documents included opportunities to ‘rate’ or ‘like’ a number of articles, and to participate in a straw poll on whether or not a Preston marathon should be an annual event. 7 Methods Sixteen participants, were recruited through a snowball sampling method",All of the participants lived within a 14-mile radius of Preston city centre and therefore qualified as the target audience for both the LEP and BP,Participants ranged from 20 to 72 years of age and included ten women and six men,"They varied in their experience of reading printed or online news, with approximately one third reading only print, one third reading only online news, and one third reading both",The user trials were all conducted on campus at the University of Central Lancashire in an area with sofa-style seating and coffee tables,"Before they were introduced to the prototypes, each participant took part in a short (10 min) audio-recorded interview designed to gather further contextual information about their use of digital/interactive technologies, their consumption of printed and broadcast sources of news, and their use of websites, apps, social media or other services to access news content ‘on-screen’","Participants had no prior knowledge of the interactive nature of the prototypes and, following the initial interview, were presented simply with four ‘different news publications’, one at a time on a coffee table in front of them",The prototypes were presented to participants in varying permutations across the sample in order to prevent a ‘learning effect’ bias (a latin squares repeated measures design),Participants were asked to explore the content of each interactive document wearing a pair of wireless headphones,"They were encouraged to think aloud as they did so, although most people listened or read silently unless they encountered a problem","When the participants were satisfied that they had fully explored the first document, they were invited to rate the prototypes’ ‘ease of use’, ‘informativity, ‘enjoyability’ and ‘usefulness’ on a five-point Likert scale (ranging from ‘strongly agree’ to strongly disagree’)",This was followed by a short discussion where participants were asked to explain their responses to the four statements,This procedure was repeated for each of the four prototypes,Once the participants had explored all four prototypes they were asked to rank the newspapers in order of preference,"To assist in this process, the prototypes were laid out for the participants to view again, and the researcher encouraged participants to explain their decision-making. 8 Findings The following results are based on observations of 16 user testing sessions recorded on video, a qualitative analysis of transcripts generated from interviews in the testing sessions, and a quantitative analysis of 64 user evaluation forms on which participants were asked to rate the four interactive news platforms on the basis of how useful, informative, easy to use and enjoyable they found them to be",The analysis of transcripts was done by identification of recurrent themes corresponding to each level of design in our framework,"The use of these levels ensures that feedback is represented on a variety of aspects of the interaction, from the overall metaphor and value (at the product design level), to the details of interface and content, and the dynamics of the interaction that connects them","Although our main technical intervention involves adding audio and voting interactivity to paper, we do not confine our analysis to these aspects alone","This is because they involve design decisions and responses affecting print layout, content partitioning between print and audio, interface controls, challenges to the relationship between newspapers and radio, and so on; all of which are mixed up in participant responses",Collecting themes by levels in our framework allows us to organize these responses systematically and holistically,"Hence, quotes on each theme were assembled into collections and are summarized below","We refer to each version of the newspaper shown in   and   with the following acronyms: LEPN=Lancashire Evening Post Newspaper ( a), LEPW=Lancashire Evening Post printed-webpage ( b), BPN=Blog Preston newspaper ( c), BPW=Blog Preston printed-webpage","Occasionally the interview analysis led us to review parts of the original video corpus to see the context in which comments were made, especially in relation to the handling of the documents",This led specifically to some findings on grasping and handling the newspapers,"The quantitative analysis was done by summarizing the rating scores and rankings, and conducting statistical tests of significance on group differences","We organize these findings now, according to the levels of our framework. 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   8.1 Product design 
                         
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      8.1.1 User experience scores and newspaper preferences Participants scored all four interactive newspapers highly on the different dimensions of user experience (see  
                            )",This is an interesting finding in its own right for the interactive paper prototypes and approach,"It suggests that despite their shortcomings in implementation, they were still seen as easy, informative, enjoyable and useful to use","There were no significant differences between these scores for individual newspaper types on any dimension, according to four non-parametric Kruskal-Wallis tests (p ranging from 0.953 for Useful to 0.414 for Enjoyable)","There were also no significant differences in the rank ordering of newspaper types at the end of the testing session, according to a chi-squared test, with ranks varying only between an average of 2.4 and 2.65","These quantitative findings suggest that there is no clear   preference for the format in which an interactive newspaper should be presented, or for the origin of the news content it relates to","Some stronger preferences were expressed by   in the interview data, complicating the picture somewhat","But on the whole, the approach can be seen to lend itself to a variety of news content and format types. 8.1.2 Value When asked whether they would consider buying any of the interactive newspapers introduced during the trial sessions, responses from participants were mixed",Judgments were often made in terms of perceived value for money,These assessments were primarily based on quantity of content and this meant that the LEP was favoured over BP,"Although the BP platforms offered an equal amount of interactivity to LEP, the LEP platforms included more printed text, and some participants felt that BPW in particular was ‘too short to be a proper newspaper’ and was ‘likely to be free’","Interestingly, few participants thought that the interactive content of any of the newspapers justified an increase in cost to the consumer","In addition to value for money, participants made judgements about the usefulness of interactive newspapers - and whether they would buy them – based on their use of Internet and digital technologies and, in particular, portable online devices such as iPhones, tablets and laptops","For example, Darren, 20, said ‘I’d never buy this","I can’t ever see a time when I would do that for news instead of an iPhone’, and Lisa, 41, agreed that ‘for me personally it isn’t particularly useful because I do use online (news websites) a lot’","However, participants with a keen interest in local and community news were more optimistic about the potential of interactive newspaper technology","Andrea, 20, spoke at length about how the availability of interactive newspapers would encourage her to read more local news: ‘It's just great… If you have newspapers like this I would be more willing to buy newspapers… I would (buy them) because I’m involved with Preston FM and… it's very important that I know about local news and I am engaged with community issues’","Like George, Andrea felt that interactive newspapers would be useful for reporting and consuming community-related, hyper-local information. 8.1.3 Potential uses Whilst many participants felt that they would not buy interactive newspapers for their own personal use, several people felt that the technology would be useful in a variety of public contexts","Participants felt that BPW in particular could be used in health centres, museums, tourist information centres and art galleries","George, 58, envisaged BPW operating as an ‘interactive information centre’ where users of public services could access information regarding museum exhibits, local ‘what's on’ information, and details about health clinics and vaccinations, for example",Other participants thought that interactive newspapers would be beneficial for people with learning difficulties and the partially sighted,The audio news stories were felt to be particularly useful in this respect,"Jocelyn, 70, explained, ‘with my dyslexia it would be great because I could do the reading whilst listening to it as well’, and similarly, Ann, 58, suggested that ‘it would be good if you can’t read small writing","It… could help people with limited vision possibly, or (people) who are not very good at reading, or people who understand the spoken word rather than the written word’","Whilst these are important insights, it is implied in these statements that the audio content would be identical to the printed content but read aloud, therefore performing the function of a ‘talking newspaper’","However, as will be discussed in the following section on content design, most other participants felt that it is important for the audio content of the interactive newspaper to bring ‘something extra’ or an ‘added dimension’ to the everyday newspaper experience by providing supplementary or additional information rather than simply reading printed news articles verbatim",Another important way in which participants felt that interactive newspapers could be used was as a platform for sharing news about local arts and culture activities,"Kate, 50, who is a member of a local drama group thought that the use of audio and voting combined with traditional newsprint stories would be an excellent way of raising the profile of local events and engaging with the local population","Similarly, Darren, 20, felt that visual artists and musicians in particular could benefit from the technology","After listening to the Olly Murs song on LEPN he explained, ‘that is a really good idea because it's a new platform to promote music’","In addition, the landscape photographs on BPW really captured his imagination",He said ‘that I actually find quite interesting because if there's a photographic contest then those (voting) details will go straight onto the site… which will be very interactive,I’d like to participate in something like that because it sounds… really engaging’,"Kate and Darren both felt that the combination of audio, print and interactive voting features would be particularly beneficial for the local arts and culture scene. 8.2 Content design 
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      8.2.1 Audio content The audio content provided with the newspapers was the main channel by which additional value was created through interactivity, certainly in the short period of testing experienced by participants",Longer term values were also demonstrated by voting mechanisms but never really resulted in benefit to the user within the experimental context,Comments about the overall value and potential of the interactive newspapers in   therefore already relate to audio content to some extent,Here we unpack these perceptions with reference to specific comments about the properties of sound and particular audio associations as shown in  ,"Regarding the properties of sound while reading, many participants commented on its element of surprise","Sound is initially hidden from view behind the touch point buttons, creating an element of curiosity and suspense which invites touching","The fact that this is supported from a traditional printed newspaper heightened this feeling, and delayed some people discovering the audio links when presented with the first prototype",As Rebecca said succinctly ‘How often can you pick up a newspaper and listen to a song through the newspaper? That's brilliant’,"In other cases, people reported pressing a button by accident and being pleasantly surprised by the resulting audio",The downside of hidden content was also mentioned by participants,It was not possible to predict how long an audio clip is or to vary its speed of playback,"Participants asked for a printed indication of the duration of sound clips, and controls for both speeding up and slowing down playback in tandem with their reading behaviours",The multitasking aspects of reading and listening simultaneously were also found to be a benefit of audio,"As Patch, 59, put it, ‘I like the facility of being able to listen to the story rather than reading it because it gives time to look at other things that are happening on the page’","People reported multitasking with respect to the same article, as with reading the jilted lover article whilst listening to reader comments on it, and across different articles as a way of skimming more content in a given time","This raised comments about the similarity with listening to the radio whilst reading, but in a fashion that can potentially synchronise the content across both platforms","This conjured up a Sunday morning feeling for one participant, Christine, 28, who stressed the fit to relaxing and pottering at home: ‘It seems to me to be more like a Sunday paper, in the morning, not going anywhere",Being at home rather than on the go,It might be while you are watching TV or whatever as well,To be sat down especially to listen to some of the articles like the interviews’,"Several participants likened this to browsing social media, such as Rebecca, 22, ‘It's not like reading a newspaper","Its becoming fully immersed because you can like things as on Facebook, you can read things, you can listen to songs, and you even listen to the stories whilst you’re looking through it",To me that's really appealing’,"Although from one perspective the audio associations were seen to provide extra volume of content and therefore better overall value for money from the newspaper (see  ), from another perspective they offered economy of print","Some participants realized that more of the printed story could be shifted from text to speech, as in the Championsheep LIVE article that was longer in audio than print",This was attractive to people who disliked reading dense textual articles and preferred to read headings or pictures with associated spoken stories,"This was expressed nicely by Christine, 28, who said ‘To be honest if there's something where you can listen to it, I’m not necessarily going to read the text",It's nice to have a balance and seems a bit more modern,Nice pictures and it's a bit more colourful,More like a magazine than a newspaper’,"The webpage versions of our newpapers began to give this economic feel to the printed articles, but this could be taken much further in future designs","In general, most participants couldn’t see the point of audio associations which simply read aloud the printed article, as with the Superhero article, unless it was an aid for partially sighted or dyslexic readers",But reading aloud an   article from a headline or picture was thought to be a good idea,The most popular individual audio associations revealed a final property of sound that goes to the heart of the newspaper as journalism,"These appeared to be the full speech by David Cameron on the article about his visit to the Far East, the interview with runners in the Preston marathon, and reader comments on the jilted lover article","All these clips introduced an authenticity to the articles given by independent   voices, which allowed participants to check the truth of the story or understand other perspectives on it","For example, Andrea 20 observed that ‘usually in a news article you just get a couple of quotes, whereas if we had interactive newspapers like this we’d be able to have more analysis… and listen to the whole speech ourselves, judging it for ourselves’","Participants paid particular attention to who was speaking, their tone of voice and regional accent",Comments were made about the interesting mix of BBC voices in the ‘additional information’ clips and the accents of local people represented by the marathon runners and commenting readers,The latter served to give a regional slant on articles which participants liked,"Added to this, human voices were said to   the printed content, as with the marathon runner comments on the race from an insider's point of view. 8.2.2 Advertisements Few participants responded positively to the advertisements (both interactive and non-interactive) on the four prototypes","There were two general responses to the adverts: some participants demonstrated a strong negative reaction to them and found them irritating and distracting, and other participants demonstrated a general indifference and a disinclination to listen to them","Among those who demonstrated a strong negative response was Jocelyn, 70, who said ‘I can’t understand why they have an advert on the top right hand side","I’d prefer to see the headline, to see what it's about rather than advertising… I prefer it to concentrate on its message more than divert your eyes to irrelevant rubbish’","Similarly, when Ann, 58, was asked by the researcher why she had not listened to any of the audio advertisements she responded ‘I can hear those on tele(vision) can’t I? I haven’t pressed them you’re absolutely right","Why would I want to press on it?’ These participants clearly feel that they are exposed to enough advertising elsewhere, and therefore would not choose to actively engage with advertisements on an interactive newspaper","Other participants responded with more indifference, however","For example, Rebecca, 22, explained: ‘I like the fact that I can choose to listen (to the advert) or completely skim over it if I want to’","Some suggested that although they had not listened to the advertisements, they would be open to listening if they were interested in buying the featured product or were shopping for a special offer or discount","A small number of participants did not notice that the BT (British Telecoms) advert on LEPN and BPN was interactive, perhaps due to the fact that they ‘skimmed over’ the adverts if they were not interested in the product","When the researcher asked Andrea, 20, why she had not pressed the ‘listen’ button on the advert she explained ‘maybe because it's an advert… I didn’t realize it was there’",Participants’ reluctance to listen to the advertisements raises some important questions regarding the usefulness of such advertising and the benefits for placing interactive adverts for the advertisers themselves,"Our findings suggest that interactive newspaper readers are unlikely to choose to listen to audio adverts except on the relatively rare occasion when they are shopping for a particular deal or product. 8.3 User interface design 
                         
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                         
                      8.3.1 Format When it came to discussions about the advantages and disadvantages of the conventional newspaper format and the new tablet format, opinion was divided among the participants","There was a division between those who preferred a conventional newspaper format because ‘it looks like a newspaper’ and ‘that's what I’m used to’, and participants who felt that an interactive newspaper should be published in a new format in order to highlight its interactivity and because ‘it is different’","Having analyzed the quantitative data, there is no clear or conclusive evidence that these opinions correlate with age","Younger participants were just as likely to express a preference for the conventional newspaper format as their older counterparts, and equally, older participants were as likely to favour the tablet format as younger participants","When asked which of the prototypes would be her first choice Rebecca, 22, explained ‘I’d go with this one (LEPN) first because it looks like a traditional newspaper but it's got the added interaction, which to me is really good’","Equally, however, there were participants who expressed a preference for the more original BPW format","For example, when evaluating the four prototypes, George, 58, said ‘I like this idea (BPW) personally… not looking like an Evening Post, but looking like an interactive newspaper… (LEPN) looks like an Evening Post and I think that's my problem: I want it to look different and be different’. 8.3.2 Size Connected to the question of format is that of size","Portability was an issue raised by many participants, and in particular, a contrast was made between the size of the more conventional newspapers and BPW","Many participants expressed a preference for the size and simplicity of BPW, but only if this did not mean a compromise in content","When discussing the merits of BPW, Emma, 29, explained that it ‘would fit in my handbag, which is a bonus, but there didn’t seem to be much content there’","So whilst size was important to her, this was secondary to assessments about quantity of content","Similarly, for others, the tablet size appeared to represent a loss of quality","John, 35, thought that BPW ‘feels like a small newspaper that you would get at a company or college or university’, and others said that BPW felt more like a leaflet or a flyer",This suggests that the smaller size of BPW meant that some participants did not recognize it as a ‘proper newspaper’,"However, as discussed above, others such as Rebecca, 22, ranked BPW highly ‘simply because it's just literally one sheet of paper and you can get all the information you need from here’. 8.3.3 Layout and iconography When participants were asked to rank the four interactive newspapers in order of preference, the layout of the content on each of the prototypes was often mentioned in relation to their decision-making","LEPW was most frequently cited as participants’ least favourite layout, which suggests that the ‘web’ layout did not work well when transplanted onto a conventional newspaper format","Participants either preferred the conventional newspaper format or the tablet format, but were less likely to favour the hybrid layouts of LEPW or BPN","Darren, 20, for example, suggested that LEPW looked as though it had unsuccessfully been printed off a website",When ranking the prototypes in order of preference he said ‘(LEPW) is going to go last because I’m just not a fan of this layout,It just looks like it's cut out from an online magazine – just a printed out version’,"Several other participants also felt that the layout did not work because there was no clear headline or large image to focus the reader's attention, and this made it difficult for them to decide where to start reading and to locate the main news stories","A smaller number of participants also made more specific comments about graphic design – most frequently in relation to BPW, perhaps because this was the most unique and innovative prototype","Christine, 28, felt that BPW seemed ‘more modern’, ‘more colourful’ and ‘nicer to read’, and Gill, 68, said that BPW had a more ‘minimal(ist)’ feel, without ‘columns everywhere’","The graphic icons used to signify the audio controls were important to the participants, and those used on LEPN were particularly popular","Participants expressed a preference for the use of symbols or icons over underlined text (as was used on LEPW) because they recognized such symbols from other audiovisual media such as DVD players, radios and online media players","For example, Lisa, 41, explained ‘I prefer the ‘listen to the story’ icons that you have on here (LEPN), which are obviously more like you would get on a (computer) screen’","Similarly, Christine, 28, said that she preferred these icons because the underlined text was ‘not that obvious’","Consistency in the size and colour of the icons was also found to be important as this made the audio controls easier to locate and recognize. 8.4 Interaction design 
                         
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                      8.4.1 Recognising interactivity Most participants identified and understood the interactive nature of the newspapers straight away, with the exception of two older participants (aged 70 and 72) who took several minutes to identify the interactive features on the first prototypes they were given","Despite the fact that most people recognized the newspapers’ interactivity, many participants commented that the red square on the top right-hand corner of LEPW highlighting the ‘interactive edition’ was especially useful",When observing the video-recorded user trials it emerged that participants could not always clearly distinguish the interactive from the non-interactive areas of the newspapers,"In relation to LEPW, George, 58, suggested ‘I think that the interactive news articles want to be very obvious and different from the ordinary journal articles",At the moment what you’ve done is made the non-interactive look very similar to the interactive’,"In addition, participants felt that on BPW, the distinction between interactive and non-interactive needed to be clarified, especially where icons and symbols are used","For example, the ‘More’ symbol on BPW caused significant confusion for participants","The arrow-shaped symbol was intended to signify to the reader that there was more content on the back of the tablet, but many participants thought that it was a button that they could press for additional audio content","A clear design principle, therefore, is to ensure that readers are able to clearly distinguish the interactive areas from the non-interactive areas, and that any symbols or icons used should look different to the audio buttons. 8.4.2 Audio controls Participants felt that it was important for them to be able to pause the audio and be able to restart play from that point","On the prototypes tested in these sessions, participants were able to play and stop the audio stories by pressing each button once to play and again to stop","If participants stopped the audio and then wished to resume listening, the story would restart play from the beginning, which participants found frustrating","Secondly – and related to this first point – participants felt that it would be useful to have separate play, pause and stop buttons rather than one button which serves as both play and stop","When exploring LEPN, Darren, 20, observed ‘It doesn’t have a pause button, it has a stop button, which plays back from the top",I think that's going to hinder some interest’,"Others thought that an indication of the length of the audio news stories would be helpful. 8.4.3 Feedback (rating, voting and ‘liking’) Participants’ opinions were mixed regarding the usefulness of the option to ‘like’ news articles and to vote on local opinion polls","As mentioned earlier in our findings, Darren, 20, felt that the voting facility could be useful to engage readers in events such as local art or photography competitions, but others were more skeptical","Lisa, 41, explained that ‘things like rating stories, personally it's not really – unless it's something I’m really interested in – I don’t really bother to vote’",A recurring comment from participants who attempted to vote or ‘like’ whilst exploring the prototypes was that it would be helpful to receive confirmation that their vote had been counted or their ‘like’ acknowledged,"Christine, 28, said ‘If I’m going to ‘like’ it, I’d like an audio thing to say (that it has worked)’, and similarly, after voting Lisa, 41, explained ‘I was waiting to get a signal; something like an indicator that I’d actually done something’","Therefore, participants would have welcomed an audio signal that their vote or ‘like’ had been registered","In addition, the ‘thumbs up’ icon used for the ‘like’ button caused some confusion",Almost all participants associated the ‘like’ symbol with the Facebook ‘like’ voting system,It was not clear to many participants whether the ‘like’ facility was linked to Facebook or not,"Christine, 28, commented that ‘the Facebook ‘like’ (icon) is somehow suggesting that it is linked to Facebook… you kind of assume it is’. 8.4.4 Grasping and handling the newspapers Observations from the video data revealed how participants handled the broadsheet formats","In each user trial, the researcher placed the newspapers on a table in front of the participant","When laid flat on the table, users generally used their index finger (or sometimes their middle finger) to press the interactive buttons","This method of handling changed, however, when participants picked the newspapers up and held them above their lap at waist/chest height","Importantly, we observed that when the newspapers are held up, users need to be able to ‘pinch’ the interactive buttons due to the lack of a hard surface to press against (see  
                            )",This pinching action means that the interactive buttons are pressed with the thumb while the fingers provide resistance and support,"In fact, this is a two-handed action with the other hand steadying the entire newspaper on the opposite page, mid-way up its margin",This observation has some direct design implications regarding the location of the interactive buttons on these kinds of newspapers,"Crucially, the ‘pinch region’ must not exceed the length of the thumb around the bottom and side edges of the newspaper",The top edge is out of bounds as it is not easy to reach over the top to pinch,"Designers should also avoid placing touch/pinch regions in the centre of each side margin, since this is the area users hold to support the newspaper or turn its’ pages","Further data on handling emerged from observations of participants using BPW, perhaps because this two-sided (single-sheet) tablet-sized interactive news publication was the most unique product design and participants had not handled anything similar before","When Benjamin, 72, was given BPW to explore, he initially tried to open it, perhaps because he could feel that there was (in our prototype) a single-sheet of paper folded and glued giving the impression of two pages and also because this is how he is used to handling a conventional newspaper – ‘opening’ a paper to reveal more contents inside","Once he realized that the tablet was just two-sided, he turned the tablet horizontally (from left to right) which meant that the content on the back of the tablet was upside down",He then performed a series of quick rotations and turns (both horizontally and vertically) until the content on the back of the tablet was the correct way up,"When he decided to go back to the front page, he turned the tablet horizontally again, and went through a similar process of rotations until the content on the front page was correctly oriented",Several other participants performed a similar series of actions,"For example, Hayley, 29, picked up BPW with one hand and turned it horizontally so that the text on the back was upside down",She then flipped the tablet around in different directions until the content was the right way up,"She suggested that it would be a good idea to ‘have the content the other way round’, and Ann, 58, agreed that it was easier to turn the tablet horizontally rather than vertically when holding the tablet with one hand",The BPW prototype had been printed so that the user had to flip the tablet vertically (from top to bottom) for the content on the reverse side to be oriented correctly,"However, most participants held BPW with one hand, which meant that by turning the wrist, it was easier to turn the tablet horizontally (from left to right)",By turning the tablet this way the text on the reverse side was upside down,"A smaller number of participants who held the tablet with two hands (usually on the bottom corners) were more likely to flip it vertically, but as the quotes from Ann and Hayley suggest, orienting the text in the same direction on the front and the back of the tablet makes it easier to use. 9 Discussion We began this article by arguing that paper still has valuable properties as a reading medium and might be designed to inter-work more intimately with various digital resources around it, especially audio","Our implementation of a talking newspaper embodied this assertion, and raised issues of how to design hybrid interactions and content using capacitive touch technology in the paper itself","Limiting interactivity to simple voting actions and audio associations, we designed four different documents borrowing design conventions from paper and the web for both professional and citizen news","Responses to these documents were complex, but reveal a number of insights into the design of interactive newsprint and its prospects in local journalism",We organize the lessons of the study in four parts in the following subsections,"The first set of lessons is derived from the design research involved in creating the prototypes, and have to do with the technology and design space involved","The second set is derived from the study findings, and concern the user experience with interactive newsprint","The third set of lessons is recommendations for design, while the fourth set comes from limitations of the study and implications for future research. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   
                      
                      
                   
                      
                      
                   
                      
                      
                   
                      
                      
                   
                      
                      
                      
                      
                   9.1 Lessons from the prototyping At the outset of the project we expected to be able to create multipage interactive documents for field trial, partly based on predictions of the printed electronics industry regarding the printing of electronic circuits, batteries and interactive components","While these predictions do indeed relate to cutting edge research in the field, we found that in practice it was difficult to assemble robust working prototypes with off the shelf components, even in the kind of short run volumes needed for field trials",For this reason we were forced to create four experience prototypes with only localized interactivity for lab testing,"This is a limitation of the study discussed in   below, which indicates that this technology is not yet ready for the big time in the journalism domain just yet","One of the challenges is that print newspapers are made out of the thinnest and cheapest forms of paper on the market, making it difficult to print on and providing little scope for attaching additional electronic components","High quality magazines with thicker, stronger paper would be better, or books with spines that could be used for hiding electronic components","As we explored the form factors for our prototypes and considered the ‘tablet’ size and construction, we realized that innovation is needed at a format as well as a content level, to overcome the limitations of current technology until better printed solutions arrive","The ability to create a thick interactive leaflet in the BPW prototype, where the electronics are hidden inside a folded sheet of paper, is a step in the right direction and a contribution of its own to the design of such documents","However, much more radical form factors for printed news might be explored, to challenge what a newspaper is and might be. 9.2 Lessons from the user study A variety of lessons for the design of interactive newsprint and augmented paper documents more generally, emerged from findings on the user experience with our prototypes",Five in particular are worthy of discussion here,"First, we found that participants came to the documents with   about their provenance, meaning and affordances, inherited from prior experiences with printed and on-line news","This was overwhelmingly demonstrated by discussions of the documents’ publication title, but also by myriad features of their particular design and behavior",Brand values associated with the Lancashire Evening Post immediately indicated that the two LEP documents were more professional and ‘serious’ than the Blog Preston documents,"Interestingly this was reinforced by the ‘proper’ newspaper format (LEPN) but not the webpage format (LEPW), which alienated conventional newspaper readers","The very same impressions attracted some participants favouring citizen news and the on-line medium, to the BPN and BPW documents",Subtler expectations could be seen in use of the ‘Like’ icons and the printed phrase ‘More’ at the end of some articles,"Participants were frustrated at the lack of response to pressing the Like icons, borrowed from Facebook, and confused about the link to Facebook itself","They also tended to press the ‘More’ labels which were non-interactive, instead of turning over to continue reading the rest of the article","These expectations govern cultural forms of reading and communication, and cannot be ignored in the design of any new medium of this kind","They show how new media design itself involves a kind of   discussed by  , in which the imagination of technologists needs to be combined with the cultural production of content creators – as on the Xerox PARC RED project",A similar point is illustrated in the following quote by   who argues that a medium is both material and cultural: “ ”,"In our experiment to test out pairwise combinations of print and web news, we seem to have designed some marks which were indecipherable or read incorrectly","In general, most confusion was associated with the LEPW and BPN combinations ( b and c), which went against the grain of the news origin and its usual presentation","In contrast, the addition of interactive functionality to a traditional-looking newspaper ( a), delighted many readers, as did the notion of printing an on-line newssheet in tablet form ( d)","Second, we found that audio annotations were most powerful when they acted to extend and personalize the printed content of the newspapers with  ",These were most dramatically illustrated by access to spoken interviews with David Cameron or the Preston marathon runners whose comments either validated the journalist's report or extended it with an insider's perspective,The tone and accent of voices appeared to be particularly valuable in conveying emotion or establishing a common regional identity with the readers,"Similar effects were reported for readers’ comments on the jilted lover article, in a way that is not usually exploited in textual responses to online news","In fact, one implication of the findings of this study is that audio annotation appears underused on the web itself, and might be explored more systematically in relation to online documents of all kinds","Third, new kinds of   activities were encountered by participants when accessing audio annotations","They found that they could multi-task not only in listening to audio associated with an article they were reading, but also while reading ahead to other articles",This raised the metaphor of radio listening and the possibility of designing a newspaper to be read with the radio,"This is an intriguing idea that deserves further attention, since it reverses the primacy of text and audio explored in this study","Instead of annotating printed text and graphics with audio through interactive newsprint, perhaps we should be annotating radio programmes with the printed word","Furthermore the very notion of annotation is challenged by these findings, which suggest that a looser association of the two media streams might be more attractive and creative","Rather than elaborating a textual or spoken story in the other medium, perhaps it is better to allow different stories to be browsed simultaneously in each medium",This may result in serendipitous connections and associations not built into the stories themselves but emerging out of a more fluid interaction with them both,"In this age of media convergence and overload, people appear to be developing new capacities to watch, listen and read to multiple things at the same time",Development of interactive newsprint should attend to this trend and play into it in an appealing way,"Fourth, the   through the newspaper was immediately understood by participants familiar with similar behaviours on social media sites","However, unlike those sites, our newspapers provided no feedback to the user on voting tallys or identities","This reduced the motivation to vote in our study and might eventually lead to its abandonment in a real life deployment, unless audio feedback could be given or further motivation provided","Given the democratic importance of voting mechanisms for engaging the public in politics and local decision making, greater attention should be paid to feeding back the results and consequences of votes to readers, perhaps through cycles of reporting in newspapers themselves","The possibility of using the newspaper as a web-connected questionnaire holds great potential for local democracy, but more work is required to understand how this should be designed as an information system involving the two-way exchange of information over a cycle of publications and encounters (c.f.  )",The fifth lesson is related to the fourth,Our findings begin to outline broader prospects for the   and information sector in which our research was situated,"Although we contrasted responses to separate professional and citizen news, the bigger opportunity appears to be in their combination","This could be seen in the attraction of personalizing professional newsprint with the human voice, voting to raise the profile of local issues, and creating an accessible printed version of an on-line blog and radio show","The fact that hyper-local news is springing up on the web, is often seen as a threat by local publishers","However, interactive newsprint provides a vehicle by which publishers might partner with local community groups to connect professional and grassroots content in new ways, and generate hybrid publications together to the benefit of both (c.f.  )","Community radio content is a key ingredient in this mix, as is the ability to record verbal responses to the newspaper and refresh published audio content over time","In short, this work suggests more nuanced possibilities for transmedia publishing across print and online platforms, that combine visual and auditory information in new ways, rather than a simple replacement of printed newspapers by the web. 9.3 Design recommendations We believe our design framework has been useful in separating out some of the varying levels of response to interactive documents, and helping us to identify   at each level","Further research is needed on these before more general design principles can be derived, as discussed in the next subsection","Hence the current recommendations for Product Design, UI design and Interaction Design apply most directly to the kind of physically augmented paper we tested here","Recommendations on Content design, by definition, apply to the design of audio-augmented content and voting on both printed and screen-based news content. 9.4 Future research Some limitations of the current study were mentioned in   above when discussing the design of the prototypes","These were suitable only for testing in laboratory conditions, albeit a comfortable laboratory with sofas and coffee tables","The results are therefore based only on short encounters with interactive newsprint, and comparisons within the medium between different versions of the same idea",Two obvious extensions of this are suggested for future research,"First, there should be attempts to test the approach in more naturalistic settings over longer periods of time",This implies the need for a field trial in which participants can experience more than one delivery of an augmented newspaper into a home setting where multiple people can read and discuss it over time,This would also provide the opportunity for participants to engage with voting in a more realistic way with feedback on the effect of voting provided in the trial period,"Second, there should be comparison across media, to compare this approach with interactive online news sites with similar properties","This would determine whether the positive reactions of participants in this study, were to new functionalities that could be provided just as well online, or to behaviours that they value intrinsically as extensions of printed newspapers they would like to keep reading",Additional future research suggested by the findings of the research itself has already been mentioned briefly above,"It includes attempts to explore novel formats for interactive paper that overcome limitations in the technology itself, innovation in the synchronized combination of radio and newspaper content, and a re-thinking of the roles of professional and community reporters in the assembly of community news",Many of these enterprises stray outside the fields of human computer interaction and media design,"However, this only goes to show the relevance of those fields to other walks of life and the need to seriously engage with application domains as we have done in this paper. 10 Conclusion This work supports the use of printed electronics to enable a range of audio-paper products within and beyond the news domain","We have shown the technical feasibility of relaying touch information off paper to a nearby mobile device, and the attraction of some kinds of audio to enhance the printed word","This extends a large body of previous work cited above, to a new domain and technology","The additional opportunity to respond to print articles through voting actions met with less enthusiasm, but this may have been an artifact of the experimental setting and the lack of any incentive to vote","The cost and feasibility of printing such documents at scale is clearly a mitigating factor for the audio-paper market, but the same challenges face other forms of augmentation in the internet of things","As advances in nanotechnology continue to enable smaller conductive elements to be laid down on a variety of substrates, we may soon see the emergence of a   with a whole new lease of digital life.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0888613x13002880, 1 Introduction Probabilistic graphical models (PGMs) have been shown to be convenient and intuitive formalisms to capture the probabilistic independence information in many application fields,"In a PGM, random variables are modelled as vertices connected by edges in a graph",These connections reflect the probabilistic dependences and independences between variables and one can associate a probability distribution to the graph that is faithful in some way to the dependences and independences,"Popular PGMs include models based on undirected graphs (UGs), i.e.,  , and based on directed acyclic graphs (DAGs), i.e.,   
                      ","However, both undirected and directed graphs have certain undesirable limitations when representing independence information for an actual problem domain","Hybrid graphs, containing both directed and undirected edges, such as  , offer an elegant generalisation of both Markov and Bayesian networks  ","A chain graph (CG) uses potentials rather than straight probabilities to represent the probability distribution of variables and is, therefore, often seen as a blackbox model","Nevertheless, chain graphs have been shown to model equilibrium systems  , which occur in many areas including biology, physics, chemistry, and economics","In fact, it was shown that particular sets of conditional independence statements, which cannot be modelled by a Bayesian network, can indeed be modelled with a chain graph; the ideal gas law and the price and demand model in economics are examples  ","On the other hand, Bayesian networks have the advantage that both structure and parameters can be assessed from either expert knowledge, data, or both, which renders Bayesian networks whitebox rather than blackbox models","For the more expressive chain graphs, it is much more difficult to exploit human knowledge in assessing their parameters, and, as a consequence, these models do not share all the advantages of Bayesian networks as whitebox models","One of the aims of the research described in this paper is to come up with ways to move chain graphs closer to whitebox models, in particular by the use of qualitative probabilistic abstractions","Probabilistic information is available in different forms, ranging from numerical, quantitative probabilistic values (possibly with a confidence interval) to qualitative information","Qualitative abstractions of Bayesian networks, called   (QPNs)  , offer a useful method for exploiting qualitative constraints in assessing probabilistic information","Qualitative information in QPNs may consist of qualitative influences and synergies, and independence information",While it is well known that QPN theory has its limitation when it comes to qualitative reasoning – the main reason why QPN theory is not used in actual systems – qualitative knowledge may be quite useful when looked at as offering constraints that should be taken into account when estimating a probability distribution,"Some algorithms have been proposed in the past to derive bounds   and qualitative influences   in the presence of both quantitative and qualitative knowledge, with applications in e.g. computer vision  ","Furthermore, it has been proposed to derive marginal probability distributions in the presence of such hybrid knowledge  ","If exact probabilistic information is not required, then such distributions, also called second-order distributions, 
                       provide insight into the domain and could, e.g., be used to make decisions","In the next section, we will first argue why chain graphs provide a good starting point for modelling feedback mechanisms",Here we explore three realistic examples drawn from the medical field,The needed theoretical basis underlying the work presented in this paper is provided in Section  ,"In Section  , we extend the known QPN theory towards chain graphs, which we call   (QCGs)","In particular, we will formally discuss qualitative relationships, compare these to the relationships in QPNs, and prove their most important properties","In Section  , we show that sign propagation, a qualitative variant of belief propagation, can be amended to qualitative chain graphs","In Section  , we also demonstrate their usefulness in semi-qualitative reasoning and present experimental results supporting this claim","Although examples were drawn from the field of medicine, which offers a rich source of qualitative modelling, the results will be of value to many other domains","The work is rounded off by conclusions and plans for future research in Section  . 2 Motivation from the medical field Many regulatory mechanisms within the human body, described by its physiology, can be seen as causal feedback systems, in which some kind of equilibrium setpoint – called   – is maintained",Diseases can be conceived as a derangement of one or more regulatory mechanisms and treatments typically interact with these systems in non-trivial ways,"In non-healthy people the equilibrium setpoint typically differs from the healthy people, but therapeutic interventions can reset the equilibrium setpoint to a state that is closer to that of the healthy people. 
                       concerns a simplified model of the blood glucose level regulation, showing how different agents, natural and pharmacological, have their role in maintaining the blood sugar homoeostasis",The representation here is often found in medical textbooks,"A plus-sign typically represents stimulation of a process, and a minus-sign typically represents inhibition. 
                      
                   The disturbance of the equilibrium of one physiological process might also alter the equilibrium setpoints of other regulatory systems, which might in turn induce new pathophysiology that deteriorates the patient's prognosis even further","In the interest of the physician, it is important to know the qualitative dynamics of such interactions, e.g., whether it is likely that a therapy for a specific disease gives rise to symptoms related to another (patho)physiological process.   shows the possible interactions when two different therapies are administered: a blood glucose lowering therapy and an antihypertensive therapy. 
                      
                   Another medical problem domain where associations without natural direction appear is the co-occurrence of two or more chronic diseases at the same time; this problem, one of the most challenging of modern medicine, is referred to as the problem of multimorbidity",Epidemiological research indicates that more than two thirds of the elderly have two or more chronic diseases at the same time,"Although various indices   show us the size, impact, and growth of the multimorbidity burden, they do not give much insight in underlying relationships between different diseases that occur simultaneously within patients",Several attempts give a probabilistic classification of the different associations between diseases that occur when multiple diseases are present within one patient,"An overview can be found in  , which discusses multimorbidity in terms of association and direct causation","Often, the associations between diseases occur because the diseases share the same pathophysiology, or the physiology is somehow related","When modelling such processes, Bayesian networks cannot adequately model the feedback mechanisms, and more expressive models are required","It is natural to model these associations by mixtures of undirected edges (lines) and directed edges (arcs), which is for example done in  , even though the models presented in that paper are not given a (formal) probabilistic meaning.   shows a simplified feedback mechanism between diabetes mellitus and a lipid disorder",The variables of this model can be easily measured by a physician in general practice,"Since we have patient data available coming from several general practices, this last example will be used as a running example throughout this paper","This patient data is then used as input for the experimental results in Section  . 
                      
                   3 Preliminaries In this section, we introduce the necessary technical preliminaries used in the remainder of this paper","In particular, we introduce chain graphs and their properties, in particular factorisation criteria and probabilistic independence","After this, we briefly introduce QPNs and their properties for qualitative reasoning. 
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   3.1 Notation In the following, we will denote random variables by, sometimes indexed, capital letters  ,   and   and we write  ,  ,  ,   and   for sets of random variables","For clarity of exposition, we will assume that each random variable   is a binary variable, which can take the values   ( ) and   ( )","Further, for notational convenience, we will sometimes write the singleton set   as  , and, if   and   are sets of random variables, then we will write   instead of  ","Also, we write   for  ",For example   is an abbreviation of  ,"In probabilistic expressions, we adhere to the standard convention that   should be understood as the conjunction  , which we also sometimes use","A conditional probability distribution   is defined as  , for positive  , and   is marginally independent of   if  ","Finally, if we write an unbounded set of random variables   in a probability expression, then the expression is implicitly universally quantified over all configurations of  , for example,   expresses that for all   (either   or   if it is a single variable) the probability of   given   is the probability of   alone. 3.2 Chain graphs 
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      
                         
                         
                         
                      
                         
                         
                         
                         
                      
                         
                         
                         
                      3.2.1 Introduction A chain graph (CG) is a probabilistic graphical model that consists of labelled vertices, representing random variables, connected by directed and undirected edges",These models were originally introduced by Lauritzen and Wermuth  ,"Frydenberg   proposed a Markov property for these models, which is now known in the literature as the Lauritzen–Wermuth–Frydenberg (LWF) interpretation of chain graphs","Alternative Markov properties have been proposed for chain graphs, in particular by Andersson, Madigan and Perlman   (AMP chain graphs)","The LWF interpretation has an intuitive equilibrium interpretation and it factorises according to the graph, whereas AMP chain graphs only partially factorise","The complete factorisation is useful in this context as this allows the qualitative representation of some of its factors, so we concern ourselves with LWF chain graphs only",The results of this paper do not generalise to the other chain graph interpretations,The concepts introduced here are in accordance with existing literature on probabilistic graphical models   and chain graphs  ,"They are illustrated by an example chain graph   as shown in  
                            . 3.2.2 Graphical concepts Let   be a  , where   denotes the set of   and   the set of  , where an edge is either directed, also called an  , or undirected, also called a  ","Let indexed letters, e.g.,   and  , indicate vertices of a chain graph",We denote an arc connecting two vertices by ‘→’ and a line by ‘−’,Suppose two vertices   and   are in  ,If   then   is a   of  ,If   then   is a   of  ,"The set of parents and neighbours of a vertex   are denoted by   and  , respectively","The set   is the   of  , denoted by  ","For example, in  , it holds that  ",We will denote   as the   of   defined by  ,"These concepts are also assumed to be defined over sets of variables, e.g., if  , then  ","One of the key concepts in chain graphs is a  , which is a sequence of vertices  , such that  ,  , or  ","This concept is distinct from a  , which is a   where no vertex appears more than once","For example, in  , the sequence   is a route, but not a path","By a   of a route  , we mean a maximal undirected subroute  :   with  ","Note that a section can consist of a single vertex, e.g., in the route  , the sections are  ",A section   is called a   section on a route   if   and   are on the route  ,"For example, in a route  , the section   is called a head-to-head section",A   is a route (with  ) where the first and last vertex are the same,"A   is a route, where there are no  ",A vertex   is an ancestor of   if there exists a descending route from   to  ,The set   denotes the set of   of  ,"A   is a route which includes at least one arc, and where all arcs have the same direction",We will call a vertex   a   of   if there exists a directed route from   to  ,"Notice the difference between ancestors and predecessor: if a vertex is a predecessor of another vertex, it is also an ancestor of that vertex, but not vice versa","For example,   has five ancestors in   ( ,  ,  ,  , and  ), but only two predecessors (  and  ).   is an ancestor of itself because of, for example, the route  ","A   is a hybrid graph with the restriction that no directed cycles, i.e., a directed route which is a cycle, exist","Removing all the arcs from the graph leaves us with vertices connected by lines, called  ; the set of all chain components is denoted here by  ","For example,   contains five chain components:  ,  ,  ,  , and  ",A chain component with its parents plays an important role in the factorisation as shown below,"For this reason, we introduce a final graphical concept which is the   of a vertex  , denoted by  , as the set   where   and  . 3.2.3 Global Markov property There exists a simple separation criterion for reading off independence statements from a chain graph, which was introduced by Studený and Bouckaert  ","Having a route   and a set of vertices  , we say that   is   by   if a vertex of   belongs to  ","If   is not hit by  , it will be called   with respect to  ","A route is called   if for every section   of  , the section   is hit by   iff   is a head-to-head section w.r.t.  ","A set of vertices   (in the graph  ) is   from a set of vertices   by the set of vertices  , denoted by  , if there are no superactive routes between   and  ","Equivalently, we can say that   and   are c-separated given   if for every route   in   between   and  , there exists a section   of   such that:  If   is   c-separated from   given  , then this is denoted by  ",Consider again the graph  ,"If  , then the route   is not superactive as the head-to-head section   is free w.r.t.  ","However, the route   is superactive as the only head-to-head section ( ) is hit by  , and all the other sections are free w.r.t.  ","This implies that  . 3.2.4 Factorisation Associated to a chain graph   is a joint probability distribution over the set of vertices   that is faithful to the chain graph  , i.e., it contains all the independences implied by the global Markov property","Such distributions can be factorised by an  :  with  , and where each   is defined by a clique-wise factorisation:  given that   are the complete (fully connected) subsets in the   graph of  , i.e., the subgraph   where each arc is replaced by a line and each pair of vertices of   is also connected by a line, also referred as to moralisation","The functions   are non-negative real functions, called  ; they generalise joint probability distributions in the sense that they do not need to be normalised","Finally, the normalising factor   is defined as:  Conversely, (discrete) distributions that factorise in this way are almost always (in a measure–theoretic sense) faithful to the graph   
                            ","As a Bayesian network is a special case of a chain graph model where each chain component consists of a single vertex, the   simplifies in that case to  which is the well-known factorisation theorem of Bayesian networks  ","In this case, the chain components are formed by single random variables","Therefore, for each of those random variables the distribution is defined as the conditional probability function of this variable, given the value of its parents. 3.2.5 Interpretation Undirected edges in chain graphs can be interpreted as an equilibrium (steady-state) in a feedback model  ","For example, consider again the graph in  , where there is a feedback relationship between lipid disorder and diabetes mellitus","In practice, this feedback system is in a steady state, although the setpoint of the feedback system may be changed, for example by the amount of insulin given in the therapy","Therefore, only the relationships between variables within a steady-state are relevant, rather than the underlying dynamic process that leads to the equilibrium","Moreover, the underlying dynamics are very difficult to measure  , hence, the parameters of such dynamical models, e.g., ordinary differential equations, are difficult to elicit","Therefore, we argue that chain graphs offer an attractive abstraction of the underlying dynamic mechanism for feedback systems in disease models, as there is plenty of data derived from patients in a state of near-equilibrium, i.e., homoeostasis","The corresponding chain graph and factorisation of   in its steady state is shown in  
                            . 3.3 QPNs 
                          (QPNs) were introduced by  , as a qualitative abstraction of Bayesian networks","Conditional probability distributions are replaced by qualitative knowledge in the form of signs, which describe the relationships among variables by the concepts of probabilistic influences and synergies",Here we briefly recall the theory in accordance with the definitions in  ,A   expresses how the value of one variable influences the probability of observing values of another variable,Let   denote the set of variables  ,"We say that   has a  , written as  , if  A   influence, written as  , and a   influence, written as  , are defined analogously, by replacing ⩾ with ⩽ and = respectively","Finally, it always holds that an influence is  , written as  , in particular if none of the other cases hold","Note that all types are influences are mutually consistent, e.g., if both   and   holds, then this implies  ",Influences adhere to a set of convenient properties  ,"The property of   guarantees that if an influence, say  , exists, the influence   also exists","Qualitative influences are  , i.e., the qualitative influences along a directed path (as defined in Section  ) between two variables, specifying at most one incoming arc for each variable, combine into a single influence using the ⊗ operator from  
                         ",The property of   further asserts that multiple qualitative influences between two variables along parallel paths combine into a single influence between these variables using the ⊕ operator from  ,"In addition to influences, a qualitative probabilistic network includes synergies modelling interactions between influences",An   expresses how the interaction between two variables influences the probability of observing the values of a third variable,"Now, let   denote the set consisting of the variables  ","We say there is a   of   and   on  , written as  , if 
                      A   is used to provide  , i.e., it expresses how upon observation of a common child of two vertices, observing the value of one parent vertex influences the probability of observing a value of the other parent","We say there is a   of   and   with regard to the value   of variable  , written as  , if  
                         ,  , and   additive and product synergies are defined analogously","It has been shown that the sign of the product synergy implies the sign of the influence between causes given the observation of the child  , for a given  ","Therefore, the product synergy expresses   to some extent  . 4 Qualitative chain graphs In this section, we will analyse influences and synergies in the context of chain graph models","The resulting representation will be referred to as   (QCGs). 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   4.1 Influences in chain graphs The properties of signs in qualitative probabilistic networks rely on the fact that signs hold in any context, i.e., intuitively, a variable   positively influences another variable   if in any possible context the probability of   is higher given   compared to  ","While such a context is relatively clear in case of directed arcs, it is more subtle for probabilistic chain graphs, in which influences can also exist through lines","From a technical point of view, multiple definitions of influences are possible, e.g., the context of the influence may be defined based on e.g., the parents, the neighbours, or possibly the parents of a chain component","Besides technical considerations, a proper definition is also particularly relevant for knowledge elicitation, as the domain expert has to be able to understand influences without reference to the technical details","We believe that such a natural semantics can be defined by means of defining influences in terms of   
                          on particular variables in the chain graph","For most domain experts, the effect of one variable after the intervention on another variable is an easy to understand concept and is captured in the following formal definition.  
                      In other words, the influence of   on  , in a particular context, can be defined as the difference in probability of   from a situation where   is manipulated to   to a situation where   is manipulated to  ","For example, obesity increases the chance of a lipid disorder (see  ); conversely, it is not thought that lipid disorder influences the weight of the patient, even though lipid disorder has a predictive value for ‘Obesity’","It is well known that without any information about causality, the distribution after an intervention cannot be established","However, similarly to directed graphs  , a causal interpretation can be given to chain graphs","Given this causal chain graph interpretation, where chain components are interpreted as equilibria, influences between random variables can be restated in terms of conditional probabilities",This forms the basis of the qualitative chain graph theory that will be developed in the remainder of this paper,"In the following lemma, we will first relate the marginal probability of a random variable after an intervention to a conditional probability","This causal interpretation of chain graphs will be assumed throughout the rest of the paper. 
                         
                      From this property it is straightforward to show that one can generalise the definition of qualitative influences","In order to do so, a convenient tool is the local Markov property of chain graphs, which states that a vertex is independent of its ancestors that are not in its closure given its boundary","This implies the following lemma.  
                      Then, using these two lemmas, we obtain an expression of influences in chain graphs in terms of conditional probabilities.  
                      Note that it follows that the influence of   on   is a zero influence if   and  ","Also note that the qualitative influences generalise the QPN definitions, since   for any   if every chain component consists of a single vertex. 4.2 Qualitative influences Given the properties of influences in chain graphs, we are now in the position to define the usual notions of qualitative probabilistic networks for chain graphs","First, we will define qualitative influences, starting with positive qualitative influences, which is defined as a positive influence in all possible contexts.  
                      Generally, if   then   denotes the qualitative influence between   and  ","The negative (−), zero (0), and ambiguous (?) influences are defined in line with QPNs, i.e., for the negative and zero influence we replace ⩾ by ⩽ and =, respectively","Finally,   always holds if  ","Qualitative influences in QPNs adhere to the properties of symmetry, transitivity, and composition  , which form the basis for qualitative inference in QPNs","Symmetric means that if there is some influence from a vertex   to a vertex  , then there is an influence from   to   with the same sign if the arc is reversed. 
                          Therefore, only a single sign is needed for every arc in a QPN","In the following, we will prove that this symmetry is preserved for qualitative chain graphs, i.e., also for neighbouring vertices the signs are symmetric",First we prove a lemma that rephrases qualitative influences in terms of relationships between potential functions,"We will focus in this lemma and theorem on positive influences, however, the same reasoning holds for negative and zero influences.  
                      
                         
                      In other words, determining the nature of a qualitative influence between two vertices implies that one only has to consider those potentials for cliques containing the two variables that describe the influence","In general, we have the following result that we were aiming for, proving the symmetry property of qualitative influences. 
                         
                      Note that this does not prove   iff   in a strict sense, as an influence of   on   is only defined if   is in the boundary of  ","Instead it should be seen as the influence after the reversal of the arc where the   obtains the boundary of  , which is similar to arc reversal in Bayesian networks and QPNs  ","As mentioned, besides symmetry, there are two other important properties","First, for a single route from vertices   to   in a chain graph  , the influences along a path are  , and the influence of   on   is the   ( a) of all influences along the path","For multiple   between two vertices   and  , the influences of these routes are  , i.e., the influence of   on   is the   ( b) of all influences of each paths","These properties are combined in the following theorem.  
                      
                         
                      4.3 Additive synergies As mentioned in the preliminaries, an   expresses information about the joint influence of two vertices   and   on a neighbour of these vertices",The intuitive idea is that there is a positive synergy if the joint influence is larger than their separate influence on this neighbour,"Following  , we define a synergy within a certain context as follows.  
                      From  , where we show that the influence of   on   in context   is equal to   where   is a configuration of  , it follows that the synergy of   and   on   in context   is positive if  ","Therefore, again, a probabilistic definition for additive synergies in QCGs naturally follows, which is defined as a synergy in any possible context.  
                      Again, the negative and zero synergies are defined similarly by replacing ⩾ with ⩽ or = respectively","Ambiguous synergies always hold. 
                         
                      It can be verified that additive synergies in chain graphs are similar to additive synergies in QPNs","For example, it is obvious that additive synergies are symmetric, i.e., if   then  ","Other properties of synergies have been well studied for QPNs, in particular in context of variable elimination  ",We do not further study additive synergies in this context as in this paper we mainly focus on the use of these kind of synergies for imposing constraints on the probability distribution,"It seems reasonable to conjecture that similar results to the QPN case will apply. 4.4 Intercausal reasoning and product synergies In QPNs, a   expresses how the value of one cause influences the probability of the value of another cause when observing the common child","A negative product synergy of   and   given   expresses that if   is the case, then this renders   less likely","While in a QPN a cause is simply the parent of a vertex, it is less clear what it means in the undirected case","First, we will define product synergies in general and then study its properties.  
                      Again, the other signs are defined analogously","As said, the purpose of product synergies is to define intercausal relationships between vertices","First, we will relate intercausal reasoning to product synergies in chain graphs using the following theorem.  
                      It is clear that in case we have a QPN, product synergies characterise intercausal reasoning if   and   and   are unconditionally independent",This particular case was discussed in   and in subsequent papers,"In contrast, in chain graphs, the following holds, which states that in case the main condition of   holds – i.e., that   and   are independent given the boundary of the conditioning vertex and the boundary of   – then these vertices must be predecessors of the conditioning vertex.  
                      In case we condition on a vertex in a chain component, it follows from the definition of c-separation, that we create a superactive route between parents of a chain component",This could be seen as intercausal reasoning between these parents,"For example, if we have a graph  , there will be an influence of   on   if we condition on  ",A natural question is whether there are any relationships between these intercausal influences,"In the following theorem we show that they can be composed by the sign-sum operator.  
                      In practice, this means that if we specify the product synergies   and   for every   in a chain component, where   and   are parents, then for each subset of variables in this chain component, there is a product synergy with a default sign determined by the sign-sum operator","To strengthen this sign, e.g., if the result is an ambiguous sign, intercausal reasoning using subsets in the chain component can be specified",It would therefore be natural to define all product synergies for pairs of parents of a chain component,"However, for QCGs, such intercausal reasoning does not completely coincide with product synergies as we will show in the next proposition.  
                      In other words,   can only be applied if there are intercausal relationships between direct parents of a vertex",One might think that this is only because the condition   in this theorem is too strong,"While this may be partially true, consider the graph  ","Therefore the product synergy of   and   given   is necessarily zero, even though  ","Therefore, it is impossible that product synergies can completely characterise intercausal reasoning in chain graphs","Previously, it was already shown that product synergies do not characterise intercausal reasoning in case there are uninstantiated ancestors  ","This paper generalises this negative result by observing that product synergies also do not characterise intercausal reasoning within chain components in general. 
                         
                      5 Sign propagation Several papers   have studied qualitative inference in QPNs using a qualitative version of belief propagation, which is called  ","Similar to belief propagation, the idea is that vertices in the graph maintain a (qualitative) belief and send their beliefs to their neighbours","Upon receiving beliefs from neighbours, beliefs are updated and further distributed","It can be shown that the complexity of sign propagation is linear in the number of vertices in the graph, making it an attractive method for purely qualitative inference","To generalise sign propagation in chain graphs, we need to introduce some additional concepts, in particular the concept of a   in a CG","However, trails may be confusing as the QPN literature contains at least two notions of trails (cf.  
                      
                       and  
                      
                      ), which are both distinct from trails in CG  ","Here we refer to the CG trails, which is a route such that no arc appears twice in this route and each section in the route consists of distinct vertices","A trail can be  , which means that none of its sections are blocked by the evidence","This is relevant in this context, because of the following property.  
                   This shows that it is sufficient to only consider the active trails rather than all routes in the graph","Active trails are also interesting because they directly relate to evidential trails, which are used to propagate signs in QPNs","This shows that the remainder of this section generalises the definitions for QPNs.  
                   Note that the QPN evidential trail is not a correct definition in the context of chain graphs, because some vertices may appear multiple times in a CG trail","This cannot be ignored as they may make   and   dependent.  
                   In case there is a single evidential trail between two vertices, we can use the transitivity and symmetry property to compute the qualitative signs.  
                   This provides a way to perform sign propagation in singly-connected chain graphs, i.e., chain graphs where the underlying undirected graph is a tree","This is because of the following property of chain graphs.  
                   For two vertices with multiple evidential trails, sign propagation is difficult",The original algorithm by Druzdzel and Henrion   propagates the signs over all trails and then combines signs by a sign-sum operator,"For a long time, it was assumed that this algorithm was correct","More recently, van Kouwen et al.   showed that this may not always give the correct result in multiply-connected networks",One issue is that direct influences dominate influences through intercausal mechanisms  ,Another issue is that the direction of arcs in a trail matter when propagating signs,"Finally, if there are multiple observations, the result may depend on the order of observations or may be unnecessarily ambiguous  ","To overcome these problems, solutions have been suggested that solve these issues, resulting in a complex inference algorithm  ","It should be noted that while these solutions overcome the problems that have been recognised in the original algorithm, as far as we are aware, a general correctness and completeness proof of recent algorithms for sign propagation in QPN is an open problem. 6 Experimental results Since chain graphs have been shown to model equilibrium systems, as mentioned in the introduction, the theoretical foundation of the previous sections can be used to explore the qualitative dynamics in such systems","Here we continue with the running example that assumes that there is a kind of equilibrium between diabetes mellitus and lipid disorders, and that its state can be influenced by other (patho)physiological conditions","These influences can now be expressed in terms of qualitative influences and synergies, and the resulting QCG can be used to perform probabilistic inference","While probabilistic inference with a QCG can be done using sign propagation, based on message-passing between neighbouring vertices as presented in the previous section, such inference has serious limitations in case of trade-offs, i.e., when there are two opposite influences",Consider again   and suppose we need to make a decision for an obese patient,"In case the patient loses weight and therapy is started, it is clear that this will reduce the blood sugar (by   and  )","Often, however, weight reduction is unsuccessful","In that case, the effect of therapy is unclear; obesity will make diabetes more likely and the therapy makes it less likely; so by the sign-sum operator the whole effect is ambiguous","An alternative approach is to look upon the qualitative signs as constraints on the joint probability distribution, as proposed in  , where a canonical representation consisting of (in)equalities expressing constraints on the hyperspace of possible joint probability distributions is used","In this approach, some of the conditional probabilities or (clique) potentials may be elicited from experts or learned from data, where for others, only qualitative information is available","In this section, we take a similar approach, where we sample the unknown potentials from the factorisation of a given chain graph (cf",Eqs.   and  ),"Without any further constraints on the hyperspace, the possibility that a positive influence exists between two variables is just as high that a negative influence exists","In fact, when sampling uniformly using a sufficient sample size, the second-order distribution of the value   resembles a normal distribution with zero mean",The same holds for synergies,"Instead of sampling the full joint probability distributions and then establishing whether the distribution is consistent with the qualitative influences, the potentials can be sampled more efficiently using  , as this shows that influences impose   constraints on the potentials",An influence can thus be introduced into the hyperspace efficiently by omitting those samples that violate the constraints,"Likewise, synergies can be stated in terms of constraints on the local potentials using the following proposition. 
                      
                   Given these properties, distributions can be sampled (cf.  
                      ) that satisfy the qualitative constraints (cf.  
                      )","Then, using these samples, second-order distributions of arbitrary marginal distributions can be derived in a straightforward manner","While typically the marginals range over the whole   interval, the qualitative constraints alter the shape (e.g., the mean and variance) of the distribution, which can then be used to draw conclusions from the model",Using patient data electronic health records of general practices in the Netherlands we were able to produce realistic qualitative and quantitative information for our running example,The patient data was routinely collected by the Netherlands Information Network of General Practice,"Information about contacts and diagnoses, prescriptions, referrals, and laboratory and physiological measurements were extracted from the information systems of each general practice linked to the network","Currently, longitudinal data of 182,396 patients from 82 Dutch general practices have been collected over a period of more than a decade. 
                      
                       shows the contingency table for familial hypercholesterolaemia, obesity, antidiabetic therapy, high total cholesterol, and high blood glucose levels measured by HbA1c","The prevalences are 0.28%, 1.4%, 7.8%, 4.1%, and 5.0% respectively","This is in line with numbers known for the Dutch population, except for obesity",Research on chronic disease prevalences in the Netherlands   showed that the prevalence of obesity is a tenfold of the one found here,The difference might be explained by the fact that in most cases obesity is only diagnosed when the patient specifically asks for a treatment,"In our experiment we will use the prevalence known from the literature.   also contains the evaluation of a possible influence between high total cholesterol and high blood glucose levels, showing that   for any context (instantiation) of  ,   and  ","This implies a positive influence between total cholesterol and blood glucose levels, i.e.,  ",It should be noted that the zero-influences of the last row are probably a coincidence due to the small numbers observed there,"Now, consider the quantitative and qualitative information available in  
                      a, representing the information derived from both the patient data and the literature","We assume that high total cholesterol strongly correlates with lipid disorder (in fact, this is one of the most important measures used to diagnose this disorder)","The second-order distribution of high cholesterol ( ) within the general population, based on   samples, is shown in  b",Interventions on   and   yield the second-order distribution in  ,"In case   the second-order distributions shift to the right, and in case of   the second-order distributions shift to the left","The opposite holds for  , suggesting with high confidence that diabetic therapy is also beneficial to reduce cholesterol levels","Note that this has been derived without any quantitative information about the chain component containing   and  . 
                      
                       compares the second-order distribution of high total cholesterol levels for the different interventions on obesity and antidiabetic therapy",One can see that anti-diabetic therapy has a small benefit to reduce cholesterol levels for both obese and non-obese patients,"However, the differences between obese and non-obese people are much larger, suggesting that an additional reduction of weight in combination with diabetic therapy is even more beneficial to reduce cholesterol levels",These conclusions cannot be derived from the data directly,"Since the data used here is derived from general practices, untreated diabetics are sparse, which makes the negative influence from   to   impossible to detect","In fact, in the data, the presence of antidiabetic therapy makes higher blood glucose levels more likely","However, from the literature it is known that diabetic therapy   glucose levels","We argue that it is better to incorporate this as qualitative information into the model rather than learning it from the data, thereby avoiding the biased relation between   and  . 7 Conclusions The work described in this paper started off with the wish to exploit qualitative information in modelling dynamic systems in a state of equilibrium",This brought us to the development of an extension of qualitative probabilistic networks (QPNs) towards qualitative chain graphs (QCGs),We were able to obtain generalised definitions of qualitative influences and additive synergies,"Product synergies still express intercausal reasoning in QCGs, although some of the intercausal reasoning in chain graphs cannot be captured by product synergies","Furthermore, we studied sign propagation for singly-connected chain graphs; similar ideas can be explored for multiply-connected networks","From the point of view of sign propagation, the key difference between QPNs and QCGs are the trails over which signs are propagated",This makes it clear that it matters whether qualitative relationships between variables in a probabilistic network are defined with respect to a line or an arc as this changes the dependences between variables,"Elicitation of qualitative relationship should, therefore, go hand in hand with graph structure elicitation",The value of QPNs for modelling biomedical problems has been recognised before  ,"Although qualitative reasoning with QCGs has similar limitations as with QPNs, we showed that by exploiting qualitative constraints on the chain graph potentials we are able to estimate arbitrary influences and synergies in the chain graph, i.e., by means of the second-order distribution of the marginal probabilities","For example, we can provide the second-order distribution of  , and decide up to a certain significance level how likely it is that an influence   exists","Therefore, we believe that this result may have a real practical bearing on areas such medicine: without knowing the exact joint probability distribution, we are still able to draw qualitative conclusions on the dynamics that exist within a model",One apparent limitation of this work is that we restrict the theory to dealing with binary variables,"The qualitative properties of QPNs are based on the concept of first-order dominance  , which does capture ordinal relationships between probability distributions, in particular that a positive influence makes the higher values of that variable more likely, i.e.,   for all  ,  , and context  ","It is not difficult to see that all our results generalise to arbitrary discrete distributions, because we can always encode a discrete distribution using binary variables","In a naive way, a random variable   with   values may be represented by binary variables for each  , which is true if   holds and false if   holds","Subsequently, factors can be added to the network to make sure that exactly one   is true","For continuous random variables, the situation is more difficult","Most of the proofs are similar by just taking   as shorthand for   and   as shorthand for  , such that  ","However, for proofs that depend on the factorisation, such as  , it is required that the distribution is faithful to the graph","While there exists research for the discrete and Gaussian case  , other distributions have not been studied thoroughly",From a theoretical point of view there are some clear directions for further research,"For QCGs, sign propagation can be investigated for general chain graphs","Another interesting line of research is to investigate qualitative abstractions of other probabilistic graphical models, such as acyclic directed mixed graphs, which have directed and bi-directed edges",The latter can be used to represent hidden common causes (see   for its Markov properties),"To improve inference in the sampling algorithm, the sampling of potentials may be improved by exploiting Monte Carlo methods which take into account bounds on the hyperspace (e.g. based on  )","With respect to the medical problems we have used for illustrative purposes, we aim to apply this formalism in a study on diabetes and cardiovascular comorbidities involving multiple feedback systems","Since most physiological systems cannot be measured directly, and relevant parameters are, therefore, mostly absent in large epidemiological datasets that are available, we believe that the methods presented in this paper form a good foundation for developing multiple disease models","Appendix A Proofs of properties 
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                   
                      
                  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0888613x14001212," 1 Introduction The theory of   
                       extends the Bayesian theory of subjective probability to cope with sets of distributions",This potentially provides more robust and realistic models of uncertainty,These ideas have been applied to classification and a number of classifiers based on imprecise probabilities are already available,"Most of these approaches are based on graphical models, whose parameters are imprecisely quantified with a set of priors by means of the   
                      ","The first attempt in this direction is the   
                      , which generalizes the naive Bayes classifier to imprecisely specified probabilities",Each prior in the imprecise Dirichlet model defines a precise classifier,"When two precise classifiers of this kind assign a different class label to the same instance, the imprecise classifier returns both labels, and the instance is said to be  ","Conversely, when a single label is returned, this is independent of the prior","Classifiers of this kind, possibly returning multiple class labels in the output, are called  . 
                       The separation between prior-dependent and other instances induced by a credal classifier typically corresponds to a distinction between hard and easy-to-classify instances, with the accuracy of a precise classifier significantly lower on the prior-dependent instances rather than on the prior-independent ones","In this sense, credal classifiers are suitable as preprocessing tools, assigning the right class label to prior-independent instances and partially suspending the judgement otherwise","Despite the relatively large number of credal classifiers proposed in the literature, no credal models specifically intended to classify temporal data have been developed so far. 
                       This is cumbersome since, on the other side, dynamical models such as Markov chains and   (HMMs) have been already extended to imprecise probabilities to model non-stationary dynamic processes  ","As a matter of fact, HMMs in their precise formulation have been often applied to classification of time series (e.g.,  ), while no similar attempts have been made in the imprecise case","This can be partially explained by the lack of algorithms to learn imprecise-probabilistic models from incomplete data (e.g., because referred to hidden variables) and, more marginally, by the lack of suitable inference algorithms",It therefore seems natural to merge these two lines of research and develop credal classifiers for time series based on imprecise HMMs,"To achieve that, we first show how to learn an imprecise HMM from a discrete-time sequence","The technique, already tested in previous works   combines the imprecise Dirichlet model with the popular EM algorithm, generally used to learn precise HMMs","After this step, each sequence is associated with an imprecise HMM","In the limit of infinitely long models, HMMs might converge to a condition of  , even in the imprecise case  ","A major claim of this paper is that, in this limit, the model becomes considerably simpler without losing valuable information for classification purposes","In the stationarity limit, the imprecise HMM becomes an   (i.e., with multiple specification of the weights) of Gaussian densities over the observable variable (i.e., the joint observation of the features)",Two novel algorithms are proposed to perform classification with these models,"The first, called IHMM-E, evaluates the mixture expected value, which becomes a static attribute for a standard classification setup","The second, called IHMM-B, uses the Bhattacharyya distance between two mixtures as a descriptor of the dissimilarity level between sequences","Being associated with imprecise-probabilistic models, those descriptors cannot be precisely evaluated and only their lower and upper bounds with respect to the constraints on the parameters can be evaluated",This is done efficiently by solving a linear (for IHMM-E) and a quadratic (for IHMM-B) optimization task,"After this step, IHMM-E summarizes the sequence as an interval-valued observation in the feature space","To classify this kind of information, a generalization of the   algorithm to support multivariate interval data is developed",The same approach can be used to process the interval-valued (univariate) distances between sequences returned by IHMM-B,"Both algorithms are credal classifiers for time series, possibly assigning more than a single class label to a sequence",Performances are tested on some of the most important computer vision benchmarks,The methods we propose achieve the required robustness in the evaluation of the class labels to be assigned to a sequence and outperform alternative imprecise methods with respect to state-of-the-art metrics   to compare performances of credal and traditional classifiers,"The performance is also good when compared with  , the state-of-the-art approach to the classification of time series","The reason is the high dimensionality of the computer vision data: dynamic time warping is less effective when coping with multivariate data  , while the methods in this paper are almost unaffected by the dimensionality of the features",The paper is organized as follows,"In Section  , we introduce the basic ideas in the special case of precise HMMs obtained from univariate data","Then, in Section  , we define imprecise HMMs and discuss the learning of these models from multivariate data",The new algorithms IHMM-E and IHMM-B are detailed in Sections   and  ,A summary of the two methods together with a discussion about their computational complexity and the performance evaluation are in Section  ,Experiments and conclusive remarks are in Sections   and  . 2 Time series classification Let us introduce the key features of our approach and the necessary formalism in the precise univariate case,Variables   denote the observations of a particular phenomenon at   different (discrete) times,"These are assumed to be  , i.e., their actual (real) values are available and denoted by  ","If the observations are all sampled from the same distribution, say  , the empirical mean converges to its theoretical value (strong law of large numbers):  Under the stationarity assumption, the empirical mean is therefore a sensible descriptor of the sequence","More generally, observations at different times can be sampled from different distributions (i.e., the process can be non-stationary)","Such a situation can be modeled by pairing   with an auxiliary discrete variable  , for each  ","The values of variables   are indexing the generating distributions: all these variables should therefore take values from the same set, say  , whose   elements are in one-to-one correspondence with the different distributions","In other words, for each  ,   is sampled from  , and   if and only if  ","Variables   are assumed to be  , i.e., their actual values are not directly observable",The modeling of the generative process requires therefore the assessment of the joint mass function  ,"This becomes considerably simpler under the  : given  , all previous values of   are irrelevant to  , i.e.,  ","Together with the chain rule, this implies the factorization:  for each  ","If the transition probabilities among the hidden variables are independent of  , the specification of the joint model reduces to the assessment of   and  , i.e.,   parameters",A model of this kind is called a time-homogeneous  ,"If all the transition probabilities are strictly positive, the model assumes a stationary behaviour on long sequences, i.e., the following limit exists 
                      :  for each  , where the probability on the right-hand side is obtained by marginalizing out all the variables, apart from  , in the joint mass function in Eq.  ",The marginal   over   is called the   of the Markov chain and it can be efficiently computed by standard techniques,"In this limit, even the probability distribution over the observation becomes stationary:  Again, as in Eq.  , the empirical mean converges to the theoretical value:  The descriptor on the right-hand side of Eq.   can be used as a static feature to be processed by standard classification algorithms","Yet, instead of considering only its mean, the whole distribution   might provide a more informative static feature to be used for classification. 
                       This can be achieved by evaluating pairwise dissimilarity levels among the distributions associated to the different sequences","In particular, given two distributions   and  , we characterize their mutual dissimilarity in terms of the popular  :  which evaluates the overlap between statistical samples generated by the two distributions","In the remainder of this paper we develop two algorithms based, respectively, on the weighted mean in Eq.   and on the distance in Eq.  ","These ideas are extended to the imprecise-probabilistic framework, taking into account the case of multivariate observations","Overall, this leads to the specification of two credal classifier for time series described, respectively, in Sections   and  ","Before doing that, let us first formalize the notion of HMM in both the precise and the imprecise cases. 3 Imprecise hidden Markov models 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   3.1 Definition In this section we present imprecise HMMs as a generalization of standard HMMs","This is based on the fundamental notion of credal set, which is reviewed first","Following  , a   over a categorical variable   is a closed and convex set   made of probability mass functions over  ","We focus on finitely generated credal sets, this means that the set of the extreme points of  , denoted as  , has finite cardinality",A Markov chain defined as in the previous section can be easily extended to the imprecise framework by replacing probability mass functions with credal sets:   is replaced by   and   by   for each  ,"While a Markov chain defines a joint mass function as in Eq.  , an   defines a joint credal set   made of (the convexification of) all the joint mass functions   obtained as in Eq.   with   and  , for each  . 
                          Given the joint credal set  , a stationary credal set  , analogous to the stationary mass function in Eq.  , can be obtained by marginalizing out all the variables apart from   and taking the limit  ",This is shown to exist if all the original credal sets assign strictly positive probability to any event,"The limit behaviour of imprecise Markov chains has been studied in  , where a formula to compute   has been derived. 
                          This formula is reported in  ","As in the previous section, for each  , the categorical variable   is in correspondence with the continuous variable  ","Given  , any other variable is assumed to be irrelevant to  ; the conditional distributions   can be therefore used to augment the Markov chain and define a (precise) HMM:  Like the   probabilities  , the   terms   are also assumed time homogeneous (i.e., independent of  )","Imprecise HMMs are similarly defined as the augmentation of an imprecise Markov chain. 
                          Both precise and imprecise HMMs describe the generative process behind a temporal sequence of observations, corresponding to the variables  ","The discrete variables   are assumed to be hidden and, as outlined in the previous section, have the role of modeling the non-stationarity of the emission process. 3.2 Learning: expectation maximization (EM) + imprecise Dirichlet model (IDM) The variables   of a HMM, no matter whether precise or imprecise, are by definition assumed to be directly unobservable, i.e.,  ",An algorithm to learn the HMM parameters from incomplete data is therefore needed,"In the precise case, the   (EM) algorithm by   is a typical choice","Given an initialization of the HMM parameters, the EM computes the probabilities of the different outcomes of the hidden variable",This is a probabilistic explanation of the values of the hidden variables in the dataset.   for the occurrences of these variables can be therefore estimated,"These, generally speaking non-integer, values are used to re-estimate the model parameters","The procedure is iterated until convergence, which is known to take place when a local maximum of the likelihood is reached",A similar approach can be considered in the imprecise case,"However, the imprecise Dirichlet model, commonly used to learn credal sets, needs the data to be complete, and no alternatives for incomplete data are available. 
                         
                      To bypass this problem it is sufficient to regard the expected counts returned by the EM as complete(d) data, and process them with the imprecise Dirichlet model","For the transition probabilities, this induces the following linear constraints:  where   are the expected counts for consecutive pairs of hidden variables with values   and   as computed by the EM after convergence","This corresponds to computing, for each  , the probability that   and multiplying it for the probability that  ; the sum of these values over the whole sequence gives the expected count","Consequently, the sums in the denominators are marginal counts, i.e.,  ","The nonnegative parameter   can be regarded as the   of the set of priors used by the imprecise Dirichlet model, thus affecting the level of imprecision of the model","Considering the linear constraints in Eq.   for each  , it is possible to compute the conditional credal set  , which is intended as the credal set of probability mass functions over   consistent with those constraints","An expression analogous to Eq.  , with the marginal expected counts in the numerator and the total counts in the denominator is used to learn  ","Overall, this procedure defines an imprecise Markov chain over the hidden variables","Regarding the number of states of the hidden variables  , as already noticed in footnote  , these variables are lacking a direct interpretation","The value of   should be regarded as a parameter of the model, for which we typically adopt small values (e.g.,   in our experiments in Section  )","The reason is that, with many categories, it is more likely to have at least a small marginal count","This makes large the difference between the upper and the lower bound in Eq.  , thus making the model quite imprecise","Regarding the   part of the model (i.e., the relation between hidden and observable variables), first note that the discussion was introduced in the case of a scalar observable   just for sake of simplicity","In many real-world problems, we often need to cope with sequences of arrays of   features, say  , with   for each  ",To define a joint model over the features we assume their conditional independence given the corresponding hidden variable,"A Gaussian distribution is indeed used, for each feature, to model the relation between hidden and observable variables:  where   is the  -th component of the array  ,   is a Gaussian density with mean   and standard deviation  , and   and   are the EM estimates for the mean and standard deviation of the Gaussian over   given that  . 
                         
                      The clustering-based technique proposed in   defines a possible initialization of the emission terms in the EM, while uniform choices are adopted for the transition and the prior","This guarantees the strict positivity of the expected counts after convergence, which implies the strict positivity of the lower bound in Eq.  ",The existence of the stationary credal set follows from this (see  ),"After this learning step, the sequence of observations in the  -dimensional space is associated with a time-homogeneous imprecise HMM, with transition and prior probabilities required to belong to credal sets and a precise (Gaussian) specification of the emission terms","The overall procedure based on a generalization to imprecise probabilities of the classical EM algorithm for HMM should be regarded as an attempt to achieve more reliable, but imprecise, estimates in the HMM parameters","The choice of confining the imprecision to the hidden variables follows from the fact that the EM estimates for these variables are based on missing information, they appear therefore less reliable than those about the observable variables. 4 Using (imprecise) expected values for classification: IHMM-E 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   4.1 An interval-valued descriptor for imprecise HMMs In this section we show how the descriptor proposed in Eq.   for precise HMMs can be generalized to the case of the imprecise HMM which we learn from a sequence of feature vectors by means of the procedure described in Section  ","In the imprecise case the stationary mass function of a Markov chain is replaced by a  , say  ","As shown in  , its computation, which is briefly summarized in  , can be efficiently achieved by Choquet integration. 
                         
                      Thus, in this generalized setup, distribution   in Eq.   is only required to belong to  ",The procedure described in   is used to obtain an outer approximation of   based on a finite number of linear constraints,"Regarding the emission terms, nothing changes as they are assumed to be precise","Thus, for each feature  , with  , we evaluate the bounds of the expectation as 
                          Both   and   are solutions of linear programs with   optimization variables and an equal number of linear constraints (see  )",The interval   represents therefore the range of the descriptor in Eq.   associated to   in the case of imprecise HMMs,The lower and upper vectors   are indeed obtained by applying the optimization in Eqs.   and   to each feature,"They define a hyperbox in the feature space, which can be regarded as the range of the  -dimensional version of the descriptor in Eq.   when imprecise probabilities are introduced in the model","Overall, a static interval-valued summary of the information contained in the temporal sequence has been obtained: the sequence, which is a trajectory in the feature space is described by a hyperbox in the same space ( 
                         )","In the next section, a standard approach to the classification of static data is extended to the case of interval data like the ones produced by this method. 4.2 Distances between hyperboxes Consider the  -dimensional real space  ","Let us make it a metric space by considering, for instance, the   distance which, given  , defines the distance between them   as  Given two points   such that, for each  ,  , the   associated with these two points is denoted by   and defined as  The problem of extending a distance defined over points to hyperboxes can be solved by considering the ideas proposed in  ","Given two hyperboxes, their distance can be characterized by means of a real interval whose bounds are, respectively, the minimum and the maximum distance (according to the distance defined for points) between every possible pair of elements in the two hyperboxes","Accordingly, the lower distance between two hyperboxes is:  and similarly, with the maximum instead of the minimum for the upper distance  . 
                          With the Manhattan distance in Eq.  , the evaluation of the lower (and similarly for the upper) distance as in Eq.   takes a particularly simple form:  The optimization in the  -dimensional space is in fact reduced to  , independent, optimizations on the one-dimensional real space","Each task can be reduced to linear program whose optimum is in a combination of the extremes, unless intervals overlap","In other words:  unless   or  , a case where the lower distance is clearly zero",A dual relation holds for the upper distance case with no special discussion in case of overlapping,"Replacing the Manhattan with the Euclidean distance makes little difference if we consider only the sum of the squared differences of the coordinates without the square root. 
                          In this case the lower distance is the sum, for   of the following terms: 
                      This is the minimum of a convex function, which is attained on the border of its (rectangular) domain",It is straightforward to check that the minimum should lie on one of the four extreme points of the domain,"Thus, the minimum in Eq.   is the minimum of the squares of the four quantities in Eq.  ","Again, the only exception is when the two intervals overlap (the global minimum is in  ), and the lower distance becomes zero","Similar considerations hold for the upper distance. 4.3 
                         -Nearest neighbors classification of interval data The above defined interval-valued distance for hyperboxes is the key to extending the  -  ( -NN) algorithm to the case of interval-valued data","First, let us review the algorithm for pointwise data",Let   denote a   variable taking its values in a finite set  ,Given a collection of supervised data   classification is intended as the problem of assigning a class label   to a new instance   on the basis of the data,"The  -NN algorithm for   assigns to   the label associated with the nearest instance, i.e., the assigned label is   with  For  , the   nearest instances need to be considered instead: a voting procedure among the relative classes decides the label of the test instance",To extend this approach to interval data just replace the sharp distance among points used in Eq.   with the interval-valued distance for hyperboxes proposed in Section  ,"However, to compare intervals instead of points a decision criterion is required","To see that, consider for instance three hyperboxes and the two intervals describing the distance between the first hyperbox and, respectively, the second and the third","If the two intervals do not overlap, we can trivially identify which is the hyperbox nearer to the first one","Yet, in case of overlapping, this decision might be debatable","The most cautious approach is  , which simply suspends any decision in this case","When applied to classification, interval dominance produces therefore a   classifier, which might return more than a single class in the output","If the set of optimal classes according to this criterion is defined as  , we have that   if and only if there exists an instance   with   such that there is no  , with  , satisfying the following dominance test: 
                      Classes in the above defined set are said to be   because they correspond to instances in the dataset whose interval-valued distance from the test instance is not clearly bigger that the interval distance associated with any other instance","A demonstrative example is in  
                         ",This approach can be extended to the case  ,"To do that, consider the instances   such that no more than   instances   satisfy the dominance test in Eq.  ",Instances of this kind are among the   nearest ones to  ,A set of   instances can be extracted from this set and the voting procedure applied,The set of undominated classes   is the union of the labels obtained by iterating this procedure over all the possible extractions,"Yet, for increasing values of  , the procedure becomes more demanding from a computational point of view (being roughly exponential in  ) and the output very imprecise","We therefore suggest to keep the value  , for which the procedure only takes linear time and the number of classes in output is minimal","This is also consistent with the typical choice of   in the classification of time series  . 4.4 The IHMM-E credal classifier By merging the results in Sections  ,  ,   and   we have a classifier for time series, to be called IHMM-E, based on imprecise HMMs","In summary, for each sequence we: (i) learn an imprecise HMM (Section  ); (ii) compute its stationary credal set  ; (iii) solve the LP tasks required to compute the hyperbox associated with the sequence (Section  )","These supervised hyperboxes are finally used to perform  -NN credal classification (Section  ) based on the interval-valued distance between hyperboxes (Section  ). 5 Directly coping with the distributions: IHMM-B The key idea of our approach so far is that considering the HMM in the limit of stationarity makes the model considerably simpler (and this is crucial for the extension to imprecise probabilities), and that classification does not suffer from this simplification",Moving to the stationarity limit basically transforms a dynamic model into a static model described by the limit distribution (or credal set) associated with the model,"In this perspective, the choice of summarizing the static model by means of the expected value of the limit distribution as in Eq.  , which in the imprecise multivariate case generalizes to the expressions in Eqs.   and  , is just one of the possible options","Another approach might consist in coping directly with the limit distribution, which in the precise multivariate case is a mixture of Gaussians   such that:  for each  ","As already noted in Section  , classification can be based on the dissimilarity level between limit distributions associated to different models","This can be identified with the Bhattacharyya distance, defined as in Eq.   (in the multivariate case the integral should be over  )","In general, such a distance cannot be computed analytically, but a good approximation has been suggested in  ","To show how the approximation works, consider a second HMM, with hidden variables taking values in   and emission terms with mean   and variance   for each   and  ",Let also   denote the stationary mass function for this HMM,"We call Bhattacharyya error, the following integral:  whose relation with the Bhattacharyya distance is  ","The approximation for mixtures of Gaussians consists in taking the convexity bound:  where the Bhattacharyya errors and distances between two Gaussians can be computed analytically by means of the following formula  :  In summary, in the precise case, classification can be performed by evaluating the distances between the limit distribution of the test instance and those of the training instances","As in Eq.  , the class label assigned to the test instance is the one belonging to the training instance at minimum distance",This method can be easily extended to the imprecise-probabilistic case,"In this case, instead of a limit distribution defined as a mixture of Gaussians, we have a set of mixtures, also called a  , one for each  ","Following an approach similar to that in Section  , we can simply characterize the level of dissimilarity between two credal mixtures by means of an interval, whose bounds are respectively the minimum and the maximum Bhattacharyya distance between mixtures of Gaussians with weights consistent with the stationary credal set of the corresponding models","This corresponds to optimizing the Bhattacharyya error in Eq.  , with respect to the optimization variables   and  ","Considering that the square root is a monotone function and that the credal sets are defined by linear constraints, this is equivalent to a linearly constrained quadratic optimization task, with the objective function having form:  with linear constraints   and  , and the nonnegative coefficient   being the product of the Bhattacharyya errors in Eq.  ",In   we prove that both the minimization and the maximization of this problem can be efficiently solved,"Note that the maximization corresponds to the upper bound of the Bhattacharyya error, which, because of monotonicity, defines the lower bound of the Bhattacharyya distance (and similarly for the minimization)","Exactly as in Section  , these interval-valued distances can be partially ranked by means of the interval dominance criterion in Eq.   and hence used to perform credal classification. 
                       We call this credal classifier IHMM-B. 6 Related works, complexity, and performance evaluation Another credal classifier for time series based on imprecise HMMs, and called here IHMM-L, has been proposed in  ","Each imprecise HMM learned from a supervised sequence is used to “explain” the test instance, i.e., the lower and upper bounds of the probability of the sequence are evaluated",These (probability) intervals are compared and the optimal classes according to interval dominance are eventually returned,"Regarding traditional (i.e., not based on imprecise probabilities) classifiers,   (DTW) is a popular state-of-the-art approach","Yet, its performance degrades in the multivariate case, i.e.,   
                      ",Our new classifiers are tested against IHMM-L and DTW in the next section,Other approaches to the specific problem of classifying interval data have been also proposed,"E.g., remaining in the imprecise-probabilistic area, the approach proposed in   can be used to define a support vector machine (SVM) for interval data","Yet, time complexity increases exponentially with the number of features, thus preventing an application of the method to data with high feature dimensionality","This is not the case for IHMM-E and IHMM-B, whose complexity is analyzed below. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   6.1 Complexity analysis Let us first evaluate IHMM-E","Our approach to the learning of imprecise HMMs has the same time complexity as the precise case, namely  ","The computation of the stationary credal set is  , while to evaluate the hyperboxes a LP task should be solved for each feature, i.e., roughly,  ","Also the distance between two hyperboxes can be computed efficiently: the number of operations required is roughly four times the number of operations required to compute the distance between two points, both for Manhattan and Euclidean metrics","To classify a single instance as in Eq.  , lower and upper distances should be evaluated for all the sequences, i.e.,  ","Overall, the complexity is linear in the number of features and in the length of the sequence and polynomial in the number of hidden states",Similar results can be found also for space,"Regarding IHMM-B, everything is just the same, apart from the evaluation of the hyperboxes, which is replaced by the evaluation of the distances",This is a (single) quadratic optimization task,"To specify the objective function   time is required, while the solution of the problem is roughly cubic in the number of constraints, thus  ",The same conclusions for the previous algorithm are therefore valid also in this case. 6.2 Metrics for credal classifiers Credal classifiers might return multiple classes in the output,"Evaluating their performance requires therefore specific metrics, which are reviewed here","First, a characterization of the level of indeterminacy is achieved by: the  , i.e., percentage of instances classified with a single label; the  , i.e., the average number of classes on instances for which multiple labels are returned",We normalize this number by dividing it by the total number of classes,"For accuracy we distinguish between:  , i.e., accuracy over instances classified with a single label; and  , i.e., the accuracy over the instances classified with multiple labels","In the latter case, classification is considered correct if the set of labels includes the true class",A utility-based measure has been recently proposed in   to compare credal and precise classifiers with a single indicator,"In our view, this is the most principled approach to compare the 0–1 loss of a traditional classifier with a utility score defined for credal classifiers","The starting point is the  , which rewards a prediction containing   classes with   if it contains the true class, and with 0 otherwise",This indicator can be already compared to the accuracy achieved by a determinate classifier,"Yet, risk aversion demands higher utilities for indeterminate-but-correct outputs when compared with wrong-but-determinate ones  ",Discounted accuracy is therefore modified by a (monotone) transformation   with  ,A conservative approach consists in evaluating the whole interval   for each credal classifier and compare it with the (single-valued) accuracy of traditional classifiers,Interval dominance can be used indeed to partially rank performances,We call   of a credal classifier a classifier always returning a single class included in the output of the credal classifier,"As an example, both IHMM-E and IHMM-B admit a precise counterpart based on a precise HMM, which corresponds to set   in the imprecise Dirichlet model","If a precise counterpart is defined, it is also possible to evaluate: the  , i.e., the accuracy of the precise classifier when the credal returns a single label; and the  , i.e., the accuracy of the precise classifier when the credal returns multiple labels. 7 Experiments In this section we present the results of an extensive experimental validation of the proposed algorithm together with details about the considered benchmark. 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   7.1 Benchmark datasets To validate the performance of the IHMM-E and IHMM-B algorithms, we use two of the most important computer vision benchmarks: the Weizmann   and KTH   datasets for action recognition","For this problem, the class is the action depicted in the sequence (see for instance  
                         )",These data are footage material which requires a   procedure at the frame level,"As shown in  , each frame is identified with a time step and the extracted features are the observable multivariate data","Our approach is based on histograms of oriented optical flows  , a simple technique which describes the flows distribution in the whole frame as a histogram with 32 bins representing directions ( 
                         )",Other benchmarks are also considered,"CAD-60   and SKIG   are, respectively, an activity and a gesture recognition dataset",The feature extraction algorithm proposed in   is used for both these datasets,"The AUSLAN dataset   is based on gestures in the Australian sign language, while the JAPVOW dataset   contains speech data about Japanese vowels.  
                          reports relevant information about these benchmark datasets",More specific information can be found on the cited references,"To avoid features with small ranges being penalized by the  -NN classification with respect to others spanning larger domains, a feature normalization step is performed",This is a just a linear transformation in the feature space which makes the empirical mean of the sample equal to zero and the variance equal to one. 7.2 Results The new IHMM-E and IHMM-B credal classifiers are empirically tested against the credal IHMM-L and the precise DTW classifiers on the ten datasets presented in the previous section,"A Matlab implementation of both IHMM-E and IHMM-B is used. 
                          Regarding the multivariate version of DTW, the Matlab implementation described in   is used","The real parameter of the imprecise Dirichlet model is fixed to  , the number of hidden states to  , and   is the value assumed for the  -NN algorithm",The choice of   is consistent with the suggestions in  ,"As already discussed in Section  , the small value of   reflects the need of avoiding overly imprecise results",The choice of   has been already motivated at the end of Section  ,"Moreover, as expected, tests with increasing values of   show a systematic degradation of the performance on our benchmark","Regarding the separation between test and training set, we adopt the same configuration of the original publication of each dataset","Leave-one-out cross validation is considered for KTH, Weizmann and CORNELL, three-fold cross validation with ten runs for SKIG","A single run with fixed test and training set is considered instead for AUSLAN and JAPVOW. 
                         
                          reports the determinacies and the normalized average output sizes of both the new algorithms and IHMM-L",These values are not measuring the performance but only the ability of these credal classifiers to return determinate results,"As a comment, we see that the proposed methods (as well as IHMM-L) provide sufficiently informative outputs","An exception is IHMM-E on AUSLAN, JAPVOW and SKIG: the classifier is always indeterminate and the output includes (almost, in the case of JAPVOW and SKIG) all the classes","In these cases IHMM-E is therefore unable to discriminate over the different classes, and the resulting classification is not informative",Results for these special cases are therefore not significant and they are not reported in the following,"More generally, the higher determinacy of IHMM-B and IHMM-L when compared with IHMM-E should be related to the multivariate nature of the benchmark dataset: the higher is the dimensionality of the feature space the more likely are overlaps between the interval-valued distances between hyperboxes considered by IHMM-E, while the two other classifiers cope with one-dimensional descriptors, less prone to overlaps","Information about the actual performance of the algorithms is in  
                         
                         ",First consider the results in   about single and set accuracy,"These refer to the accuracy of the classifiers when a single label is returned in the output and, if the classifier returns multiple labels, whether or not the right class belongs to this set","A fair comparison can be done only between the IHMM-B and IHMM-L, because of the similar values of determinacy and indeterminate output size",A clear outperformance by IHMM-B against IHMM-L is observed,"Consider for instance the KTH dataset, the algorithms have almost the same determinacy, but the single accuracy is .780 for IHMM-B and only .299 for IHMM-L","Similarly, on the indeterminate instances, the output size is almost the same, but the set of classes includes the right class label in   of the cases for IHMM-B and   for IHMM-L",Similar comparisons cannot be done with IHMM-E,"As an example, it is not obvious that a single accuracy equal to one for IHMM-E is better than the value .785 for IHMM-B, because of the higher determinacy of the second classifier",The interval-valued metrics discussed in Section   have been developed specifically for this kind of problems,Results of the performance according to these descriptors are reported in  ,"As noted in Section  , the interval   provides a better summary of the performance of a credal classifier by also allowing for a fair comparison with a traditional (i.e., precise) classifier like DTW",The results in   show that the new methods clearly outperform IHMM-L,"Moreover, IHMM-B is always better or equal to IHMM-E","Impressively, IHMM-B also competes with DTW: the average rank is 1.3 for IHMM-B and 2.5 for DTW",This shows the quality of our approach and the (known) degradation of the DTW performance in the multiple-features case,"On the basis of the above experiments, we regard IHMM-B as the algorithm of choice for credal classification of multivariate time series","IHMM-B has a precise counterpart obtained by setting   in the constraints of the imprecise Dirichlet model. 
                          We evaluate the accuracy of this precise classifier on the whole test set",Then we compute the accuracy only on the instances for which IHMM-B is indeterminate,We also consider the accuracy of the precise classifier on the instances for which IHMM-B is determinate,This is equal to the single accuracy of IHMM-B as in  ,"In fact, by definition, when a credal classifier returns a single class, this coincides with the one returned by its precise counterpart.  
                          depicts a comparison, for all the benchmark datasets, of the accuracy of this precise counterpart on the whole dataset (gray histograms), on the indeterminate instances (black histograms), and on the determinate ones (white histograms)","As expected (see the discussion in Section  ), the accuracy on the whole set of instances is always higher than the corresponding value for the indeterminate instances only, and smaller for the determinate ones",IHMM-B is therefore effective in discriminating hard-to-classify from “easy” instances,"In other words, if this credal classifier returns a single class label, we can reasonably expect that this is the correct one, while if multiple outputs are reported we should definitely prefer this indeterminacy to the single output returned by a precise classifier, which is more likely to be wrong. 8 Conclusions and outlooks Two novel credal classifiers for multivariate temporal data have been presented",Imprecise HMMs are learned from each sequence,The first classifier summarizes the model with a hyperbox in the feature space,This datum is classified by a generalization of the  -NN approach,The second classifier uses an interval-valued dissimilarity measure,The second approach has the better performance: it outperforms a credal classifier previously proposed for this task and compete with the state-of-the-art methods,"In future work, we want to investigate novel, more reliable, learning techniques such as the likelihood-based approach already considered for complete data in  ","Also alternative approaches to the evaluation of the dissimilarity level between imprecise HMMs (e.g., the KL divergence) should be considered",Appendix A Linearly constrained quadratic optimization Let us consider the linearly constrained quadratic optimization tasks to be solved by the IHMM-B algorithm,These task can be solved in polynomial (roughly cubic) time because the solution lies on an extreme point of the feasible region  ,"The minimization of the objective function in Eq.   rewrites as:  We can easily prove that the solution of this problem, denoted as  , corresponds to an extreme point of the feasible region, i.e.,   and  ","To do that, let us add the additional constraint  ",This makes the problem a linear program as the objective function becomes:  with the linear constraints  ,The solution of this linear program coincides with the optimal solution  ,"On the other side, the solution should also be an extreme point of the feasible region",We can similarly prove that  ,A similar result holds even if we consider the maximum instead of the minimum,"Appendix B Computation of the stationary credal set Given an imprecise Markov chain as in Section  , for each  , define  , such that,  :  Given this function,  , define  , such that:  for each  , with   and  ",Proceed similarly for the unconditional probability of the first hidden variable,"In this way the following numbers (instead of functions) are defined: 
                       A “lower” version of these functions and numbers can be obtained by simply replacing the lower probabilities with the uppers, maxima with the minima, and vice versa",For each   let  ,"To characterize the stationary credal set  , consider  ","Given the recursion:  with initialization  , 
                       we obtain:  and similarly for the upper.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s2589721719300029, 1 Introduction The pig industry has long been the backbone of China's livestock and poultry farming,"In recent years, China's pork production ranks first in the world, accounting for about 50% of the world's total production",About 700 million pigs are slaughtered each year,Pork production accounts for about 64% of all meat production ( ;  ),"According to statistics, the annual output of pigs, cattle, sheep and poultry in 2017 was 84.31 million tons, of which pork production was 53.4 million tons","Therefore, pork industry is the absolute pillar of China's meat industry ( )","Although the proportion of pigs in the livestock and poultry industry is in an absolute position, the China's pig industry is still lagging behind","The pig industry is dominated by decentralized farmers, and the industrialization and modernization of the aquaculture industry is relatively low","In 2016, the contribution rate of pigs with a size of <500 to the market pork accounted for about 55%","More than half of them have problems caused by poor farming environment, poor farming technology, suffering from various diseases, feed self-matching and management disorder ( )","The informal pig breeding model not only produces a great waste of resources, but also causes frequent occurrence of various pig diseases and increases the risk of pig breeding process",The industrial upgrading of the pig breeding industry from the decentralized farming model to the intensive modernization is very urgent,"In recent years, the concept of welfare farming has been increasingly valued by the industry and the health of pigs has received increasing attention","However, due to various problems in resource management of pig breeding enterprises, such as poor farming pig houses, poor breeding environment and non-standard farming techniques, they are common in free-range farmers or small-scale pig farms, which have led to insufficient disease prevention and disease monitoring in pigs, the diagnosis and treatment of diseases are not timely ( )","At this stage, effective prevention and diagnosis of pig disease is still a major issue related to the economic benefits of pig breeding enterprises","Timely access to body temperature data can help alert and diagnose pig disease, real-time and fast access to pig body temperature is of great significance to the stable and healthy development of pig breeding enterprises, which also could promote the development of welfare farming concept in the pig breeding industry","With the progress of China's urbanization construction and the need for environmental protection, the pig breeding model began to accelerate the evolution from individual decentralized farming to large-scale farming, and pig breeding industry is beginning to enter a new trend ( )",The development of the agricultural Internet of Things (IoT) has made it possible to perceive the body temperature information of pigs in real time,"With the development of industrialization and intensive farming mode, the scale of breeding has been expanded continuously, and the problems of environmental pollution in pig houses have begun to become prominent","Severe environmental pollution has a series of effects on the growth, development and reproduction of pigs, and the deteriorating environment of pig breeding has made the health of pigs increasingly serious","In the high-density breeding environment with frequent diseases and frequent epidemics, monitoring the body temperature of pigs in real time is conducive to grasping the health status of pigs at any time","It is important to the prevention and diagnosis of pig diseases, which becomes a key task for intensive farming. 2 Progress in measuring body temperature of pig 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   2.1 The significance of measuring body temperature of pig Temperature regulation is one of the most important mechanisms to maintain the homeostasis of the whole organism","Body temperature is the temperature of the deep part of the animal body ( ), which is an objective reflection of the activity in the animal body, it is a physiological signals that is very pivotal to reflect the health status of pig",The body temperature change can reflect the health status change of the pig in time,"A huge fluctuation in the body temperature of pigs may be caused by the physiology of pigs being disturbed, abnormal body temperature indicates the occurrence of certain diseases, especially infectious diseases in the incubation period ( )","Body temperature information is very helpful for the diagnosis and treatment of animal diseases, which helps to detect sick animals early, explore the extent of the disease, determine the severity of the disease, etc","Therefore, if the physiological indicators of pig body temperature change are correctly recognized and rationally used, some pig diseases can be detected early, diagnosed early and treated early ( )","On the other hand, the change in sow body temperature is an important condition for judging the estrus of sows",The identification of sow estrus is essential for pig production ( ),"Accurate identification of estrus sows by body temperature measurement, which can achieve timely breeding to improve the conception rate of sows","Real-time control of temperature changes in pregnant sows is also conducive to testing the health of sows, according to the body temperature situation, the relevant breeding work can be arranged to increase the sow's birth rate ( )","With the development of agricultural IoT technology, timely and real-time access to pig body temperature data has become a reality","In addition, the acquisition of real-time pig body temperature information can help monitor the health of pigs","For group pigs, if early detection of abnormal pigs with infectious diseases can control the epidemic early and reduce economic losses",Real-time access to the body temperature information of pigs has become an important part of the work of pig breeding,"In the actual pig raising process, the rectal temperature is generally used to represent the body temperature of the pig, the range for healthy pigs is 38–40 °C and there are some gaps between different types of pigs ( )",There are many reasons for the abnormal high fever of pigs in the actual breeding process ( ),"Among them, “high fever” is a general term for a series of diseases that can cause hyperthermia in pigs","Clinically, 80% of cases are mainly fever, mainly viral high fever, and some bacterial hyperthermia","Under normal circumstances, the sudden temperature rise of the pig's body to above 40 °C is an early warning of certain diseases",Pigs are often accompanied by some other complications when they are sick and hot,"According to the complications and the body temperature of the pigs, it can effectively diagnose the diseases of pigs","In the process of breeding, we should pay attention to the abnormal changes of body temperature in pigs in real time, and refer to the high temperature complications to strictly diagnose the disease and prescribe the right medicine","In addition, the body temperature of sow will also change significantly in the estrus state, which can be identified according to the accompanying abnormal behavior of the sow during estrus, so as to rationally arrange the breeding production activities ( ). 2.2 Traditional pig body temperature measurement method Traditional pig body temperature measurement is manual","Lubricating the mercury column before measurement, hold the end of the thermometer with a string of about 15 cm, attach a wire clip to the other end of the string","Insert the mercury column into the pig anus when measuring, the iron clip clamps the hair above the pig's tail for fixing","Remove the thermometer after 5 min, wipe the mercury column and read the data","It is necessary to appease the pigs of different temperaments before measuring the rectal temperature of the pigs, which can reduce the strong stress of pigs, so as not to affect the healthy growth of pigs ( )","The method of manually measuring the rectal temperature of pigs has the advantage that the rectal temperature obtained in the absence of stress of the pig can accurately reflect the body temperature information of the pig, but in general, contact-type manual measurements cause intense stress in pigs, resulting in rapid rise in rectal temperature, which will make measurement data inaccurate ( )","Moreover, measuring the rectal temperature of a pig usually requires 2–3 workers to take about 6 min to complete ( )",This method consumes a large amount of labor in large-scale farming,"Moreover, in the process of contact measurement, there is a risk that the disease crosses and touches between humans and animals","All show that the pig breeding industry needs more scientific and efficient ways to obtain the body temperature of pigs and technologies based on remote measurements are the most needed for this field. 2.3 Emerging temperature measurement method of livestock and poultry Cai et al. ( ) developed a system for automatically obtaining the temperature of the cow's body surface using radio frequency technology and contact temperature sensing technology, realizing automatic monitoring of the body surface temperature of the cows for 24 h","Through the statistical analysis and equalization of the data, the correction formula of the body surface temperature of the cow was fitted, and the diurnal variation of the temperature of the cow body surface was obtained, which had accumulated experience in scientifically measuring the body temperature of fur animals","After the pigs were anesthetized the measurement picture is shown in  a .   inserted a temperature measuring trachea with 3 probes into the respiratory tract of the pig to measure the tracheal temperature, the measured temperature was almost the same as the lung tracheal temperature, the measuring device is shown in  b","Zhang et al. ( ) implanted the temperature measuring electronic chip into the left side of the sow neck during the estrus to obtain the subcutaneous temperature, and the rectal temperature was obtained by using a mercury column thermometer as a reference","The relationship between body temperature changes and estrus behavior was found: the body temperature of sows before and after estrus was first increased then decreased, and then returned to normal","Therefore, it was recommended to match the gestation status of sows according to body temperature changes","Hentzen et al. ( ) invented a capsule-type wireless sensor that was implanted into the neck of a piglet, which was then able to achieve a measurement that was consistent with the manual measurement of the rectal temperature","The measurement accuracy was high, but the surgical injection of the capsule was required for the piglet, the measuring device is shown in  c","Jose M. ( ) used a button temperature collector (DS1923) to collect the skin temperature of the pig's ear, obtained the correlation between ear temperature and feed efficiency of the pig by phase space diagram, further discussed the body temperature regulation of the pig and energy conversion issues.   used ear tag temperature sensor (QSS2000) to measure the ear temperature of pigs and studied the relationship between ear temperature and intensity of behavioral activity","A diurnal rhythm in the ear skin temperature (EST) was found, with the EST being highest at night and lowest in the afternoon","About the implantable temperature measuring device, after the device is implanted into the livestock and poultry body at one time, does reduce the stress response of the livestock and poultry during the measurement, and is convenient for future experimental measurements","However, when the device is implanted into the target to be tested for the first time, the animal need to be anesthetized and then implanted","Hentzen et al. ( ) noted the disadvantages of such a method, principally associated with the necessity of immobilising the pigs",It is easy to cause damage to animals and reduce animal welfare,"Moreover, electronic implantable and fixed temperature measuring devices often need to be recalibrated after a period of use, and the operation process is complicated",Godyn et al. ( ) compared the invasive measuring techniques and the noninvasive measuring techniques,Various continuous temperature measurements in the studies on pigs were analyzed,The advantages and the limitations of each method also have been showed,"Invasive measuring techniques is contact, like using rectal thermometer to get the temperature, apart from the stress caused by holding, the result of the measurement can be disturbed by measurement process","In 1974, Bligh et al. ( ) first proposed the application of wireless remote sensing technology to the surface temperature monitoring of animals, and discussed the feasibility of using wireless telemetry to obtain animal temperature and the hindrances faced by practical applications",Bai et al. ( ) realized temperature measurement for sows in real time by using wireless radio frequency communication technology and infrared temperature sensor technology,The MLX90614 infrared temperature sensor was used to obtain the temperature of the sow buttocks,"The measured temperature was sent to the server through the gateway node, and according to the relationship between the hip temperature and the rectal temperature, the measured temperature of the node was compensated to improve the temperature measurement accuracy",Chen et al. ( ) designed a breeding monitoring system that monitors the physiological signs and environmental parameters of pigs using wireless sensor network technology,"The resources such as single chip microcomputer, wireless sensor network, WIFI communication module and body temperature acquisition module are connected and integrated through the server","The Android mobile phone or PC were used to obtain the vital body parameters such as the body temperature of the pig, and the temperature and humidity data of the breeding environment","The hardware design of the temperature measuring system is shown in  
                         . 
                          evaluated the suitability of infrared thermometry by an experiment, which was set up to assess whether it was possible to predict the rectal temperature based on the body surface temperature of pig","The body surface temperatures of the ears, feet, sides and anus were measured in 12 weaned piglets for 45 consecutive days",These temperatures were compared with daily rectal temperatures,"The results clearly demonstrate that there was no reliable prediction of rectal temperatures from these temperatures, but most of them were significantly correlated with rectal temperatures.   used a mercury column thermometer, an electronic thermometer, and an infrared thermometer to measure the temperature of 1000 pigs respectively before slaughter in a slaughterhouse","It was found that the detection rate of the high-temperature pigs was about 93% by comparing the rectal temperature obtained from the mercury column with the surface temperature of the ear root obtained by the infrared thermometer.   utilized infrared camera to detect the temperature of the cow's breast, and established a model of cow's breast temperature change based on the daily data of the cow's breast temperature",The temperature correlation between the ambient and the cow's rectum was obtained,"The breast temperature and ambient temperature parameters can successfully predict by the current breast temperature, which could achieve early detection of cow mastitis","Qin et al. ( ) combined infrared temperature measurement technology with RFID technology to realize the intelligent detection for the temperature of the group of pigs through the deployment of multiple temperature monitoring points, the system structure is shown in  
                         ","Telemetry, wireless sensor network, infrared and other temperature measurement methods, when the system is completed to measure temperature, almost no stress on livestock and poultry, which is in line with the concept of welfare farming, less labor input during measurement, improving work efficiency and facilitating the development of large-scale intensive farming ( ;  )","However, the temperature measurement method using the telemetry and wireless sensor network has high accuracy requirements for temperature data acquisition devices because it cannot directly contact the target","Moreover, in the process of data information transmission, there are signal interference and other problems that will cause data loss ( )","In general, most of the emerging methods for measuring body temperature are automatic or semi-automatic in the process of obtaining body temperature data",Temperature collection equipment can continue to use for a certain number of years,"For large-scale pig breeding companies, emerging body temperature measurement methods are less economical in the face of increasing labor costs",It is in line with the development concept of resource conservation and is increasingly favored by the breeding industry,"Among them, the advantages of infrared temperature measurement technology are more obvious","The temperature measurement accuracy is high, the temperature measurement system has strong stability, the temperature measurement mode is simple, and real-time remote temperature measurement operation can be realized","It can complete the temperature data acquisition in the unmanned environment, does not make the pigs stress, and makes the obtained temperature data more accurate and effective","Compared with contact, implantable and wireless sensor network temperature measurement methods, the development and application prospect of infrared temperature measurement technology is better.  
                          shows the emerging temperature measurement technology mentioned. 3 Research status of infrared technology in the measurement of body temperature of livestock and poultry 
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   3.1 Infrared temperature measurement technology and sensor system Infrared technology was used in military surveillance, industrial inspection, medical diagnosis and other fields firstly","In recent years, it has been applied in the field of agriculture, and has been continuously explored and developed in the fields of crop growth monitoring, agricultural product quality inspection, and livestock and poultry disease detection","According to different methods, infrared technology can be divided into based on point temperature analysis and based on field analysis","The point-based analysis system is represented by the infrared thermometer, and there are hand-held and fixed temperature measuring devices",The field-based temperature measurement system is represented by infrared camera,The development of infrared camera began in the late 1920s,Infrared imaging technology can break through the obstacles of the night environment and broaden the limitations of human vision,It can provide real-time monitoring of all heat radiating objects,Infrared cameras have been widely used since their inception and continue to evolve,"Using infrared camera to get pig body temperature can reduce labor consumption, simple operation process, reduce pig stress, avoid cross-infection of livestock, reduce farming costs, and improve aquaculture production efficiency",It is increasingly favored by the pig breeding industry ( ),Infrared camera is an imaging temperature measuring device,"The object radiates energy outward in the form of electromagnetic waves, and the infrared radiation intensity of different objects is different","The principle of infrared thermal imaging is to use the difference between temperature and emissivity between the target and the surrounding environment to generate different thermal gradients, which shows the infrared radiation energy density distribution map, that is the “thermal image”","Human vision is not sensitive to infrared light, the thermal image seen by humans is that the infrared radiation emitted by the object is converted by the thermal imager",The different temperatures of the object are represented by different colors on the infrared image,"The lighter the color, the higher the temperature of the surface of the object, and the darker the color, the lower the temperature of the surface of the object",Infrared camera converts infrared radiation into visible light image in two steps,"The working principle of infrared camera is shown in  
                         , the first step is to convert the infrared radiation into an electrical signal through an infrared detector",The intensity of the infrared radiation is reflected by the magnitude of the signal,The second step is to display the distribution of the target infrared radiation on the display through the display screen,"The electrical signal completes the transition from electricity to light, and finally the visible image that reflects the target temperature distribution. 3.2 Application of infrared technology in body temperature measurement of livestock and poultry 
                          studied the application of infrared thermal imaging technology in animal metabolism, nutrition, inflammation process, disease, ectoparasite detection and reproduction is described, resulted that infrared thermal imaging is innovative, low-cost, fast and effective","It is sensitive to changes in animal heat patterns, but this method may not be sufficient to determine the cause of animal fever","Soerensen et al. ( ) evaluated the application of infrared temperature measurement technology in the measurement of pig body temperature, explored the relationship between skin, environment and body temperature","It had been found that the optimal skin position that is highly correlated between skin temperature and rectal temperature is likely to be hot windows (the commonly used measuring hot window is shown in the  
                         ) such as the ear, eyes and breasts, however, this relationship may vary with age, stress and biological state, for example Childbirth may change the extent of this correlation, and analyzed the prospects for the development of infrared thermometers in the diagnosis of future pig diseases. 
                          applied infrared camera to the early detection and prediction of animal diseases, and inoculating viral diarrhea strains (24515) to 10 calves, seting up an uninfected control group, collecting images an infrared image is shown  
                         )with infrared features (nose, ear, back) of infected calves and control groups",The blood and salivary cortisol was obtained and immunoglobulin A and other parameters were analyzed to detect whether the cow is ill,"The experiment concluded that the infrared facial scan can detect the temperature increase of infected calves from 1.5 °C to 4 °C (P < 0.01) before 1 week, the which shows that infrared heat measurement can be used for early detection and prediction of burdock disease. 
                          used the rat breast adenocarcinoma 13762 MAT (a tumor that has been used for the identification of anti-angiogenic drugs) as a research object, and explored the feasibility of infrared imaging technology for detecting angiogenesis of malignant tumors","No tumor surface temperature elevation associated with the tumor was observed in the experiment, but some constant and very significant temperature reductions were found to be produced by relatively small tumors (>0.5 cm diameter), yielding a conclusion that temperature changes were independent of tumor size",It is speculated that this phenomenon may be due to the poor vascularity of rapidly growing tumors,"Analysis showed that elevated peripheral temperature of breasts reported in breast cancer patients is unlikely to be caused by tumor growth leading to angiogenesis, possibly due to chronic inflammatory reactions around breast tumors, which means infrared imaging may have a considerable prognosis value","Liu et al. ( ) obtained clinical mastitis, recessive mastitis, and normal lactating cow breast temperature through infrared camera, and found that the differences in breast surface temperature of the three types of lactating cows reached extremely significant levels (P < 0.01)",It provides a viable method for early diagnosis of lactating cow mastitis.   described the use of infrared thermography to detect inflammation of sows' lower extremities to identify different degrees of lameness in sows,"The selected areas ranged from top to bottom: 1 - t bone, 2 - upper jaw bone, 3 - lower jaw bone, and 4 - phalanx ( 
                         )","This shows that infrared thermography has become an effective method for diagnosing animal disease. 
                          found that non-contact infrared thermometry (NIFT) could serve as a valid alternative to manual rectal thermometry in portable germ-free facility without disturbing experimental animals","The development of NIFT body temperature assessment without animal restrictions is clinically beneficial, especially in reproductive piglets, and much less stressful for laboratory procedures in aseptic facilities.   discussed the use of thermal imaging cameras in the field of veterinary medicine as a diagnostic for orthopedic diseases in livestock and poultry, analyzed the significance of infrared thermal imaging as a breeding tool, which can participate in the thermoregulation of the breeding process to increase animal welfare and participate in the process of assisting lactating milk.   used an automated, RFID driven, noninvasive infrared thermography technology to determine bovine respiratory disease (BRD) in cattle","Routine parameters (core temperature, hematology, serum cortisol) and infrared image for 65 cattle of approximately 220 kg were obtained","The acquisition equipment located in a feedlot pen around a water station with RFID driven, non-invasive IRT scanning station was showed in  
                         ","The BRD positive and negative cattle were determined by routine parameters, and the peak temperature of the infrared images was compared","The positive average peak value was 35.7 ± 0.35 °C, and the negative average peak value was 34.9 ± 0.22 °C","The study suggests that infrared imaging technology can be used to wirelessly collect biometric data and help diagnose animal diseases. 
                          studied the feasibility of using infrared cameras to detect foot-and-mouth disease (FMD) virus of cattle","Through analyzing the maximum foot temperatures of healthy, directly inoculated, contact, and vaccine tria cattle were measured, the conclusion was that infrared thermography was a promising screening technology that enabled rapid identification of potentially infected animals during a FMD outbreak for confirmatory diagnostic testing",However the technique needs to be further evaluated to determine the value of IRT in detecting mild clinical symptoms or subclinically infected animals,"Skykes et al. ( ) used digital infrared thermal imaging (DITI) equipment to obtain the temperature of the sow vulva, including the minimum (MIN), maximum (MAX), mean (AVG) and standard deviation (SD) of temperature gradients, ambient (AMB) and rectal temperatures (RT)",The blood samples were analyzed and the serum progesterone (P4) concentration was determined,"The MIN, MAX and AVG of the pig vulva temperature in the thermal image were found to be positively correlated with each other (P < 0.01), and positively correlated with the AMB (P < 0.01)",MAX and AVG were higher in estrus than in diestrus,"There was no significant difference in MIN and SD between estrus and diestrus, and no difference in RT between the stages (P < 0.05), and RT was not significantly associated with vulvar thermal images","Vulva thermal images from defined regions of interest, as shown in  
                         ",It was indicated that digital infrared thermal imaging (DITI) can measure the genital surface temperature during estrus and identify the estrus period of the sow,"Meng et al. ( ) measured the epidermal temperature of the buttocks, vulva and eyes of nearly 1000 normal pregnant sows by infrared camera, and analyzed the correlation of skin temperature of the three parts","It was concluded that the vulva of the estrus is higher and the estrus of the sow can be identified, so that the actual production can be guided through the temperature change",Health assessment of animal is getting more and more attention,"Marcia et al. ( ) tested 24 small cows in 35 days, using infrared cameras to capture infrared images of different areas of the cow, including the left and right eye areas, right and left eyes, left forelimb tail, left forelimb skull, left and right flank and forehead","In all the body regions studied, the thermal imaging forehead temperature has the highest correlation with the rectal temperature","The forehead and left and right flanking temperatures are related to the ambient temperature and humidity closely, which could be used to studied the body temperature regulation and body heat generation in the future.   used the thermal imaging camera to measure the collateral-related cervical injury crown band and hoof skin temperature from the perspective of animal welfare and economic loss","It was found that when the hoof has a lesion, the surface temperature of the squat leg will increase, and an early thermal detection of the lesion can be performed using an infrared camera.   measured the temperature of the pig's head by a thermal imaging camera and detected the early fever of the pig by differential ROI (region of interest)",The specificity of the method in the experiment is about 85% and the sensitivity is 86%.   evaluated the thermal status of newborn piglets by infrared thermal imaging,"After comparing 1695 infrared image data with 915 rectal temperature data, it was found that the maximum temperature (IRmax) of the infrared image was highly correlated with rectal temperature","And the IRmax of the infrared thermogram is mainly located in the ear (27/50), the head area (12/50) and other parts of the armpit area (8/50)",The IRmax of infrared thermal image can be used as a basis for evaluating the thermal status of newborn piglets,"The main applications of infrared thermal imager in livestock and poultry breeding have been showed in  
                         ","Using infrared temperature measurement technology, under the premise of no contact with pigs, the measurement temperature of pig under non-stress condition is obtained, which is closer to the real body temperature of pigs, and the data accuracy is high",Many studies found that the measurement results of infrared temperature measuring equipment are relatively stable within a certain distance range ( ),"Therefore, the consistency of measurement distance should be maintained when using infrared equipment to obtain temperature data multiple times","In addition, the temperature difference between different parts of the pig is very large, the temperature at the anus, eyes, ear roots, and armpits is significantly higher than the temperature at other parts of the body surface ( ;  ;  )",This may be related to the emissivity of different parts of the pig.   analyzed the best infrared measurement site of the pig by studying the emissivity of different parts of the sow to obtain an accurate infrared thermography,"In the actual measurement, the same part of the pig is selected for measurement",Some scholars have found in the process of measuring the body temperature of pigs by infrared equipment: the temperature of pig ear root (that is the junction between the ear and the back) is significantly higher than other parts of the head,"Moreover, there is a clear correlation between the temperature of the ear root and the body temperature of the pig ( ;  )",So the body temperature status of pigs can be characterized by measuring the temperature of the pig ear roots,"In livestock and poultry farming and pig breeding, infrared technology can be used for non-contact body temperature measurement of animals, which could be applied to health assessment and monitoring of pig, and animal disease detection",It has been a technology for disease prevention and treatment,"It can also be used to guide breeding work, to predict and test estrus, to improve the conception rate of sows","Details of infrared temperature measurement technology are shown in  
                         . 3.3 Application and development of infrared image processing technology in livestock and poultry breeding industry With the continuous exploration of the application of infrared technology in the livestock and poultry breeding industry, the infrared image processing technology based on livestock and poultry breeding industry is also exploring and developing constantly","The image processing method based on active shape model proposed by Cootes et al. ( ) which is a method for searching for an object of a specific type in an image.   applied this method to infrared image processing for the first time, expounded the comprehensive applicability of thermal infrared image to this method, and proposed an improved algorithm","Through the image processing, landmark display, shape creation and initialization, 2 million infrared images of cows is handled successfully, which revealed that the direction of further research is to process and analyze the infrared images of moving animal targets","Liu et al. ( ;  ) used the infrared camera and visible light camera to obtain the target pig image at the same time, and proposed an image registration algorithm based on ray contour feature points for multi-source images","Using the characteristics of the infrared image to get a clear outline of the pig's foreground firstly, the problem is transformed from image registration to parameter matching of the feature point set. 50 infrared and visible images had been matched automatically with a registration rate of 94% and the average registration error is <1 pixel, which is better than manual registration","Based on this, a fusion method of visible image and infrared image of pigs based on non-subsampled contour wave is proposed","Through the fusion application of infrared image and visible image, more surface and behavior data of pig are obtained, the body surface temperature of pig ear root could be get in time, flow chart of pig ear root area extraction is showed in  
                         ","The monitoring of abnormal behaviors such as fever and surface ulceration of pigs was achieved, which laid a foundation for the monitoring of abnormal behavior of pigs and the extraction of characteristics of pigs based on multi-source images. 
                          evaluated the health status of piglets using an infrared camera",An automatic extraction of the ear-based temperature algorithm based on the top view is proposed,"Firstly, the head of the piglet was identified by the training classifier, and then the two ear base points were located based on the feature points of the head contour, and the highest temperature in the circle centered on the ear base point was extracted to represent the two ear bases temperature. 100 images were processed by this method, and the extracted temperature was compared with the temperature obtained manually using Fluke Smart View 3.14","For the left and right ear bases, 97% and 98% of the test images had errors in the range of 0.4 °C.   proposed a sow infrared image segmentation algorithm based on improved geometric active contour model","The point operation was used to enhance the contrast, reduced the background interference, and the energy weights were dynamically equalized by the weight function which can change with the overall contrast of the image and the local contrast",Through all of that the 300 thermal infrared images of sows were segmented,"The average time of single infrared image segmentation was 49.67 s, and the correct segmentation rate is 98%",It provided the possibility for further application of infrared video monitoring.   proposed a method for measuring the surface temperature of animals based on Kinect sensor and infrared camera,"The relationship between temperature and angle of view was analyzed, and the mathematical model of compensating the angle of view was established",The correlation coefficient was above 0.99,"At the same time, the depth and infrared thermal image fusion method of the synchronized image were established, and the angle of view of each pixel was calculated","By this method, the temperature measurement error caused by the angle of view can be compensated, and the significant influence of the measurement angle of view on the measured temperature was reduced","There is a certain correlation between temperature and body temperature in the ear region of pigs ( ;  ).   simultaneously acquired infrared and visible light images, and proposed an improved active shape model for detecting pig ear regions based on image fusion","The feature region of the ear root was extracted according to the shape feature, and the average shape initial pose was improved in combination with the head skeleton model","After processing 50 images, comparing segmentation ear region with the manual segmentation ear region, the coincidence degree which >0.8 account for 84%, which showed the detection effect was satisfied","Zhou et al. ( ) described a method based on the improved Otsu algorithm for detecting the infrared image of pigs, using a self-designed infrared image acquisition system to collect images of pigs","Through the improved Otsu algorithm, the image of the ear roots of piglets, finishing pigs and pregnant pigs could be detected","This method can correctly detect pigs with complete ear root feature areas, and 23% of piglets with incomplete root characteristics can be found correctly, 25% of fattening pig images can be detected correctly, 33% of pregnant pig images could be found correctly",It is impossible to detect images of pigs that do not have the characteristics of the ear roots,"Use this method to detect images with inaccurate focus or too few image pixels during the acquisition process, the result is good, which avoid inaccurate detection areas due to relatively low resolution of acquired images",It is a very complicated process to process the infrared image and get the body temperature information of the pig,Effective data acquisition enables image processing to be more efficient and faster,"In the process of infrared image processing, some scholars have found that visible light has a greater impact on the quality of acquired infrared images","Infrared images acquired in low illumination conditions are less noisy when segmented, as the illuminance increases, the noise increases, resulting in unsatisfactory target segmentation","Therefore, attention should be paid to choose the appropriate illuminance when acquiring infrared images","In addition, the ambient temperature also has a great influence on the infrared image, higher noise at higher temperatures","Avoid high temperature and hot weather when acquiring infrared images, and select appropriate ambient temperature for data acquisition","In the process of data processing, the effective extraction of the target area of the infrared image is particularly important","Pigs have obvious shape features, easy to detect feature points, and analyze feature areas",Pig body extraction method based on feature points and feature regions is the most widely used,"Visible light images contain more detailed information than infrared images, which can compensate for the limitations of infrared images","Register and combine infrared and visible images to get more information firstly, then processing fused images to obtain temperature information, this method is also constantly being explored. 4 Conclusion and future trends Body temperature can be used as an important indicator to assess the health status of pigs, diagnose pig diseases, and guide pig breeding",Emerging pig body temperature measurement techniques can be divided into contact and non-contact,"With the development of the concept of welfare farming, non-contact infrared temperature measurement technology will be more and more recognized","At present, the application of infrared temperature measurement technology in the pig breeding industry is still in the initial stage of development",The measuring equipment that has been introduced mainly includes infrared thermometers for point analysis and infrared cameras for field analysis,People are still exploring new application modes of infrared technology in measuring body temperature of pigs,"However, based on the transformation needs of the intensive pig breeding model, the need for welfare farming, and the need for smart agriculture to actively explore, the trend of infrared temperature measurement to replace traditional manual temperature measurement is becoming clearer","At the same time, infrared temperature measurement technology is increasingly prominent in the fields of health monitoring of pigs, disease diagnosis, and identification of sow estrus, and because of its real-time, efficient, convenient and accurate advantages, it is more and more recognized by the breeding industry","Compared with traditional mercury column temperature measuring equipment or electronic temperature measuring equipment, infrared equipment costs much more",It is more difficult to promote and apply in small farms or personal breeding,Some infrared equipment and temperature measuring system are complicated to install,"Moreover, the data collection operation requires more professional workers and a higher level of knowledge",It is not conducive to the promotion in actual breeding,The development of infrared temperature measurement technology in the pig breeding industry will continue to advance with the development of the information society,"High-precision, high-stability, portable, easy-to-operate infrared equipment is currently the most needed for Chinese pig breeding companies","The infrared temperature measuring equipment in actual production application is mainly an infrared thermometer based on point analysis, and the application of the infrared camera is relatively small","However, the infrared camera based on field analysis has higher precision and more comprehensive temperature information","How to reduce costs, improve stability and generate product promotion trends is the key to the future application of infrared cameras in the livestock industry","Many infrared image processing technologies used in other fields have begun to explore the applicability in the pig industry, which further promotes the development of infrared temperature measurement","However, infrared image processing technology generally has problems such as large imaging noise, low image contrast, and narrow gray scale","Infrared imaging is easily affected by non-uniformity and invalid pixels, resulting in low actual resolution",These image processing challenges will still hinder the development of infrared temperature measurement technology in the pig breeding industry in the future,The enhancement of key technologies for infrared image processing will further promote the application of infrared camera temperature measurement in the pig industry,The exploration of the applicability of infrared image processing technology in other fields in the pig breeding industry will further promote the achievement and product development of infrared temperature measurement technology,"Today, with the continuous development of Internet of Things agriculture, the pig breeding industry is also changing in the direction of informationization and automation","The non-contact, real-time and long-distance characteristics of infrared temperature measurement technology represented by infrared camera conform to the concept of IoT agriculture","In the process of development, we continuously explored the combination with other information management technologies and emerging farming technologies, laying a foundation for building a more efficient farming model and breeding process, and its future development prospects are very bright.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0888613x13002867, 1 Introduction Inconsistency is a pervasive and important problem in many applications when information is gathered from multiple sources,"It has been increasingly recognized that measuring inconsistency plays an important role in analyzing inconsistent information in a variety of applications such as belief change  , knowledge bases merging  , ontology management  , requirements engineering  , expert systems in medicine   and intrusion detections in security  ",A growing number of inconsistency measures for knowledge bases have been proposed recently  ,Most of these inconsistency measures focus on classical knowledge bases only,Such measures assume that any two pieces of knowledge in the same knowledge base are equally preferred,"However, some pieces of knowledge are often more preferred than others in some applications, such as belief revision  , knowledge bases merging  , and inconsistency management in requirements engineering  ","More importantly, the preference relation between pieces of knowledge is always considered as useful to resolving inconsistency in these applications","Therefore, proposals for measuring inconsistency in such applications should take into account preferences on knowledge","To illustrate this, consider a scenario about selecting the best paper from submissions for a conference",Suppose that A and B are two conference submissions,Alice and Bob are the two reviewers for these two submissions,"Alice recommended A for the best paper with  , whilst Bob gave the contrary reviewing result with  ","Bob recommended B for the best paper with  , but Alice opposed Bobʼs recommendation with  ","Intuitively, the contradiction between Alice and Bob about A is different from that about B","Evidently, if we ignore recommendations with  , B receives two valid reviews that are contradictory, whilst A has just one review that is positive","Further, if we consider all reviews, A also has two contradictory reviews","But the contradiction between Alice and Bob about A is less sharp than that about B, because Bob opposes Alice w.r.t",A with   instead of  ,"It should be pointed out that while there is a lot of work on utilizing preference information for handling inconsistent knowledge bases (e.g.  ), there are relatively few approaches for measuring inconsistency for knowledge bases using preference","On the other hand, given a preference relation over a knowledge base, the base may be stratified into several strata according to the preference relation between pieces of knowledge, if we assume that any two pieces of knowledge in the same stratum are equally preferred","Moreover, each stratum of a given knowledge base, except for the least preferred stratum, naturally splits the whole knowledge base into two parts: one is the set of formulas more or equally preferred than the stratum, and the other is the set of formulas strictly less preferred than the stratum","In many applications, we need to consider only the first part of the knowledge base instead of the whole base","For example, to manage time effectively and efficiently, an agenda robot needs to classify a set of tasks into four strata, including the  , the  , the  , and the  ",Some of these tasks may contradict to each other,Suppose an agenda robot has detected that the userʼs available time is very limited,"Then it is advisable for the user to focus on only   tasks, i.e., the first one or the first two strata","Roughly speaking, compared to flat knowledge base, the stratum-based structure of a knowledge base makes it possible for the robot to focus on relevant parts in the stratified knowledge base","As a result, we need to measure the inconsistency of a stratified knowledge base stratum by stratum","To address these issues, we propose two approaches to measuring inconsistency for stratified knowledge bases in this paper","The first approach, termed the multi-section inconsistency measure (MSIM for short), provides a framework for capturing the inconsistency occurring at each stratum of a stratified knowledge base","Informally, the multi-section inconsistency measure for a stratified knowledge base is a vector such that the first   components of the vector together exactly capture the inconsistency in the first   strata of the base","In particular, we present two instances in this framework, i.e., the naive MSIM and the stratum-centric MSIM","The former takes advantage of some intuitive inconsistency measures for flat knowledge bases such as the   inconsistency measure presented in  , to characterize inconsistencies at each stratum of a stratified knowledge base","The latter aims to capture the most preferred stratum involved in inconsistencies caused by each stratum of a stratified knowledge base, in which inconsistencies are characterized in terms of minimal inconsistent subsets of each  -cut (the union of the first   strata) of the base","The second approach to measuring inconsistency, termed preference-based approach, focuses on assessing inconsistencies in a whole stratified knowledge base from an integrated perspective","It allows us to define measures by considering the number of formulas involved in inconsistencies as well as the preference levels of these formulas under   models, one of representative paraconsistent models  ","By examining their logical properties, we show that these inconsistency measures indeed take into account the preorder relation on a stratified knowledge base in assessing the inconsistency in the base",Then we provide a systemic analysis of the computational complexity of these measures,"Finally, we use a small but explanatory example to illustrate how to use our measures to monitor the process of inconsistency handling in requirements engineering",The rest of this paper is organized as follows,Section   provides some necessary notations about inconsistency as well as stratified knowledge bases,In Section   we propose the multi-section inconsistency measure and its two instances for stratified knowledge bases,In Section   we propose preference-based inconsistency measures for stratified knowledge bases,In Section   we address logical properties of these inconsistency measures,In Section   we study computational complexity issues about these measures,"In Section  , we present an example to illustrate the application of our approaches in the domain of requirements engineering",In Section   we will compare our approaches with some closely related work,"Finally, we conclude this paper in Section  . 2 Preliminaries In this section we introduce some basics about knowledge bases and inconsistency measures that will be used in the paper",We consider the language   built from a finite set of propositional variables   under logical connectives   and constants   (true) and   (false),"We use   to denote propositional variables, and   to denote formulas. 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   2.1 Stratified knowledge bases Before introducing stratified knowledge bases, we first fix some necessary mathematical notations","By   or   for short, we denote an  -ary vector","In particular, we use   to denote the zero vector  ","The lexicographical ordering relation ⩽ between two vectors   and   of the same size is given as follows:   if  , or there exists   s.t.   and   for each  ","Furthermore,   if and only if   and  ","If   is a vector and   is a real number, then the concatenation operation between   and  , denoted  , is defined as 
                      A binary relation   on some set   is a total preorder relation if it is reflexive, transitive, and total, i.e., for all  , we have that: 
                      A   (stratified KB for short), also termed  , is a pair  , where   is a finite set of propositional formulas and   is a total preorder on   
                         ","For any  ,   means that   is preferred to   in  ","A stratified knowledge base   is often given in the form of a stratification   for   such that 
                      Each   is called the  -th   of the stratification","Throughout this paper, we use   or just   to denote the stratified knowledge base   with   strata","Given a stratified knowledge base  , we call   the  -cut of   for each   
                         ","Obviously, the  -cut of   is exactly  ","We use   to denote the lexicographical cardinality of  , i.e.,  ","Furthermore, for each  , we call the stratified knowledge base   the  -section of  ","Roughly speaking, the  -cut of   is exactly a set consisting of formulas in the first   strata, whilst the  -section conveys the preorder relation among these formulas as well as these formulas","A stratified knowledge base   is inconsistent if   is inconsistent, i.e., there is a formula   such that   and  , where ⊢ is the classical inference relation","If there is no confusion, we use   to denote that   is inconsistent","For example,   is inconsistent because   is inconsistent",The total preorder relation over a stratified knowledge base is conveyed by the  -strata structure of the base,"However, such a structure is not commensurable in many cases, and then it is difficult to compare any two stratified knowledge bases with different strata","To bear this in mind, we focus on only changes that cannot affect the  -strata structure of a stratified knowledge base","That is, we assume the scale of priority levels in a knowledge base always remains unchanged","Given a stratified knowledge base  , a propositional formula   and an integer   ( ), we use   to denote the stratified knowledge base obtained by adding   to the  -th stratum of  , i.e.,  ",Note that such an enlargement of   does not change the  -strata structure of  ,"On the other hand, in order to keep the  -strata structure within a stratified knowledge base, we also allow empty strata in the rest of paper","We call a stratified KB   a sub-KB of another stratified KB  , denoted  , if   for all  ","In particular, for any  , we denote   if  ","We use   to denote the set of all minimal inconsistent subsets of   w.r.t. ⪯, that is,  Here   in   means lexicographical ordering, which is used to define  . 2.2 Inconsistency measures for flat knowledge bases If there is only one stratum in a stratified KB  , i.e.,  , we call   a   or classical knowledge base","In this case, we just write   for simplicity","Obviously, any two formulas in a flat knowledge base are considered equally preferred","For convenience, we assume that a flat knowledge base in this paper is always not empty",In this subsection we recall some major inconsistency measures for flat knowledge bases,"To deal with inconsistent flat KBs, some paraconsistent semantics have been introduced",A well known approach to paraconsistency is based   models used in Priestʼs Logic of Paradox (LP for short)  ,"An   model is a three-valued model with truth values  , in which the third truth value   means that a statement is both true and false in the model",The following material about the   model is largely taken from  ,"An   interpretation (or just interpretation)   in the Logic of Paradox maps each propositional formulas to one of the three truth values  ,  ,   such that  where  ","Then the set of   models of a propositional formula   is defined as  Further, the set of   models of   is defined as  Let   be an   interpretation and   a flat knowledge base","We use   to denote the set of variables of   assigned to   by  , i.e.,  where   is the set of variables of  ","Based on  , the set of minimal models of   w.r.t.   is given as follows: 
                      
                         
                      The normalized minimum number of variables assigned inconsistent truth values in   models of a knowledge base has been considered as an inconsistency measure for the knowledge base in  ","Moreover, as stated in  , such a measure is a special case of the assessments for the degree of contradiction presented in  , which is based on the minimum cost of test actions that ensures a consistency recovery in  . 
                         
                      Note that   is always replaced with   for any individual knowledge base given in examples  . 
                         
                      Furthermore, we use   to denote the non-normalized version of  , i.e.,  It is exactly the degree of contradiction of   presented in the case of the Priestʼs Logic of paradox  ","On the other hand, minimal inconsistent subsets are also used to capture the inconsistency in a flat knowledge base  ","A subset   of  , is called a minimal inconsistent subset of   if   is inconsistent, and none of its proper subsets is inconsistent",A formula of   is called a   if it does not belong to any minimal inconsistent subsets of  ,"We use   to denote the set of minimal inconsistent subsets of  ,  ","The   inconsistency measure presented in   considers the number of minimal inconsistent subsets of a knowledge base as an assessment for the inconsistency in the base. 
                         
                      
                         
                      3 Multi-section inconsistency measures How to articulate inconsistency in a stratified knowledge base has not yet been given much attention in the literature","Intuitively, the preference levels of formulas involved in inconsistency should be taken into account in establishing approaches to measuring inconsistency for stratified knowledge bases","We will propose two approaches to measuring inconsistency for stratified knowledge bases in this and the next section, respectively",The first approach aims to articulate inconsistencies occurring in each section of a stratified knowledge base,The second focuses on assessing inconsistency in a stratified knowledge base from a global perspective,Both the two approaches adopt vectorial measures to capture the inconsistency in a stratified knowledge base,"In particular, the place of each component in a vectorial measure corresponds to one preference level, and then such vectorial measures are appropriate for characterizing inconsistency in a stratified knowledge base","On the other hand, in some applications, a single value-based measure is more attractive than vectorial measures","For the second approach, we also provide a weighted version to capture the inconsistency in a stratified knowledge base by a single value","However, as we illustrated later, the weighted inconsistency measure fails to possess some expected properties","As mentioned earlier, given a stratified knowledge base with   strata, formulas in the first   strata are more preferred than the last   strata for each  ","This provides a usual way to separate a stratified knowledge base into two parts at each preference level  , i.e., the  -section and the formulas strictly less preferred than it","Moreover, these sections are also meaningful for resolving inconsistency in some applications","For example, consider a stratified knowledge base   illustrated in  
                      ","Intuitively, the conflict between   and ¬  is the unique focus in inconsistency measuring, moreover,   has nothing to do with inconsistency in case that we only focus on the most preferred formulas","In contrast, if we focus the first two strata, then   is also involved in inconsistency","Furthermore, we may find that   is involved again in the new inconsistency we meet at the third stratum","Allowing for this, we need to identify where (at which stratum) inconsistencies occur and how severe these inconsistencies are in such cases","That is, an inconsistency measure for a stratified knowledge base should articulate the inconsistency in each section as well as in the whole knowledge base","On the other hand, for each  -section, the  -section of the whole base is also its  -section for all  ","Then it is natural to define such a framework for measuring inconsistency for stratified knowledge bases in the following way. 
                      
                   Informally speaking,   uses a vector instead of a single value to measure the conflicts in   such that the first   components of the vector together describe the conflicts in the  -section of  ",This implies that the multi-section inconsistency measure allows us to characterize conflicts in a stratified knowledge at each preference level or stratum,"Next we introduce two kinds of instances of the multi-section inconsistency measure, i.e., the naive MSIM and the stratum-centric MSIM. 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   3.1 The naive MSIM Given a stratified knowledge base, the  -cut of the base consists of formulas in the first   strata of the base (i.e., all formulas with at least the  -th preference level)",We may consider using all the measures for the degree of inconsistency in each cut of the  -section together to assess inconsistencies in the section,"We call such inconsistency measures naive MSIMs because they do not consider the total preorder relation on formulas within each  -cut. 
                         
                      The naive MSIM for a stratified knowledge base essentially focuses on assessing the inconsistency in each  -cut of the knowledge base","That is, the naive MSIM captures the degree of contradiction among formulas we meet at each stratum",Note that  for all  ,"So, it is a kind of multi-section inconsistency measure","Now we give the following naive MSIM based on  . 
                         
                      
                         
                      Similarly, we may use other inconsistency measures for flat knowledge bases such as   to instantiate the naive MSIM. 3.2 Stratum-centric MSIM The naive MSIM focuses on describing conflicts in a stratified knowledge base by characterizing how each cut of the base inconsistent is","On the other hand, the naive MSIM does not consider the preorder relation between formulas within each  -cut",Then the impact of preference on the inconsistency assessment has been not yet reflected intuitively within each  -cut,"To illustrate this, consider three inconsistent stratified knowledge bases with the commensurable scale  ,  , and  ","Note that the 2-cut of each stratified knowledge base is consistent, moreover,  ",Then   always holds,"However, as illustrated by  
                         , conflicts in the three knowledge bases are very different from each other",Note that different levels of stratum are involved in inconsistencies in the three knowledge bases,"In detail, the formula ¬  with the lowest preference level contradicts the most preferred formula   in   and   in the second stratum of  , respectively, whilst only the formulas with the lowest preference level are involved in the inconsistency of  ","Intuitively,   is the most inconsistent one among the three stratified knowledge bases","To address this, next we introduce a new multi-section inconsistency measure, which is based on strata involved in inconsistency in a stratified knowledge base","We start with minimal inconsistent subsets due to each stratum. 
                         
                      
                         
                      In the above example,   is the only minimal inconsistent subset relevant to  ","Moreover,   is the most preferred stratum involved in  ",Then we can use the level number 2 to characterize the degree of inconsistency partially caused by  ,"In general, we can formalize this notion as follows","Given a stratified KB   and a fixed level   with   ( ), we say that the  -th stratum   ( ) is involved in inconsistency due to   if   for some  ",We use   to denote the minimum level of strata involved in minimal inconsistent subsets due to  ,"For convenience, we set   if  ","Formally, 
                      Recall the three knowledge bases illustrated in  , then   for  ,   for  , and   for  ",It can make a distinction among the three knowledge bases,We can use the number   to characterize the inconsistency due to  ,"However, for technical reasons, we introduce the following multi-section inconsistency measure. 
                         
                      Note that   captures the level of the most preferred stratum involved in inconsistency due to  , and   is a designated value for the case that   brings no new minimal inconsistent subsets","Then the more preferred strata involved in inconsistency due to  , the greater   is","In this sense, it provides an assessment for the level or severity of inconsistency due to  ","Moreover,  for all  ","Then the stratum-centric MSIM for a stratified knowledge base describes the most preferred stratum involved in inconsistency we meet at each stratum in such a way. 
                         
                      Next we introduce some evident observations about  . 
                         
                      
                         
                      Clearly,   means that minimal inconsistent subsets of   have nothing to do with  , i.e.,   brings no new conflicts in   for each  ","In contrast,   means that new conflicts   due to   are exactly conflicts between formulas in  , whilst   states that there are formulas in the more preferred stratum   are involved in new conflicts due to  ","Moreover, the larger   is, the more preferred stratum involved in inconsistency due to  ","In this sense, these observations show that each   of   captures the new conflicts in   due to   within the context of   indeed","We use the following example to illustrate the role of multi-section in characterizing inconsistency in a stratified knowledge base.  
                      4 Preference-based inconsistency measures The multi-section inconsistency measure for a stratified knowledge base focuses on characterizing inconsistency in each section of that base",A complete assessment for the inconsistency in a stratified knowledge base consists of all the characterizations of inconsistency in sections,In some applications we also need to assess the inconsistency in a stratified knowledge base as a whole,"In other words, we hope to use an inconsistency measure to capture all the conflicts in each stratum in an integrated way","Intuitively, such an inconsistency measure should take the preference information between different strata into account as well as the inconsistency in each stratum","Given a flat knowledge base, the   inconsistency measure focuses on counting the minimal number of inconsistent variables rather than inconsistent formulas under   models of the base  ","However, in a stratified knowledge base, its stratum-based structure induces a preference relation on formulas",So the inconsistency measure would be more precise and more natural if the preference information can combined when we count the number of formulas in each stratum that are interpreted inconsistent under a   model,"In order to define such an inconsistency measure, we first introduce the   of a   model. 
                      
                   When the knowledge base is clear from the context,   is abbreviated as  . 
                      
                   Given the above definition, we could use the following notion to measure the inconsistency of a stratified KB. 
                      
                   Intuitively, the   measure for a stratified knowledge base is the minimum rank of its   models. 
                      
                   The following example shows that the preference-based measure   is essentially different from multi-section inconsistency measures. 
                      
                   We must point out   measure   is not the lexicographical cardinality of minimal inconsistent subsets w.r.t.  ","To illustrate this, consider the following example.  
                   Given a   model of  , its rank describes the number of formulas assigned to inconsistent truth values in each stratum under that model",In some applications we are also interested in what proportion of formulas in each stratum involved in inconsistency,"To address this issue, we use a normalized version of the   measure for a stratified knowledge base","To this end, we first introduce the normalized rank of a   model",Let   be a stratified knowledge base and   the rank of the model   of  ,"Then the normalized rank of  , denoted  , is defined as  where   for all  ","Now we are ready to define our normalized version of the preference-based inconsistency measure   for stratified KB based on the normalized rank of a   model. 
                      
                   
                      
                   Roughly speaking,   focuses on counting the number of formulas involved in inconsistency for each stratum of  , whilst the normalized version   aims to capture the ratio of the number of formulas of each stratum involved in inconsistency to the cardinality of the stratum","That is, the two measures capture different aspects of inconsistency in a stratified knowledge base","Then it is not surprised that   behaviors differently from   in supporting different logical properties, as illustrated later",Note that either multi-section inconsistency measures or the preference-based measures for a stratified knowledge base with   strata use  -size vectors to capture the inconsistency in the base,"However, a single inconsistency value is often more attractive to many applications than vectorial inconsistency assessment",A usual way is to use a set of weights to articulate the preference relation on a stratified knowledge bases,"Actually, the choice of weights is rather sensitive to application domains because it is difficult to assure that all the stratified knowledge bases are commensurate","Moreover, as illustrated later, the weight-based approach may fail to support some expected properties for characterizing inconsistency in stratified knowledge bases",We start with the weights compatible with a stratified knowledge base,"A vector   of weights is compatible with a stratified knowledge base   with   strata, if  ","Essentially, we provide no condition about   other than linear order relation on  ",It makes the choice of weights flexible in applications,"Given an   compatible with  , we may define the following weighted version of the   measure for  . 
                      
                   
                      
                   As shown by the following proposition,   is less discriminative than  . 
                      
                   
                      
                   The aim of the weighted   measure is to integrate assessments for strata listed in lexicographical order into a single value",Such an integration based on weights may weaken the difference between preference levels of strata,"This is why   is less discriminative than  . 5 Logical properties To our best knowledge, there is not yet a desirable set of properties designed for inconsistency measures in the case of stratified knowledge bases","However, how to characterize inconsistency measures is still a challenge even in the case of classical knowledge bases","Compared to a growing number of inconsistency measures, there are relatively fewer proposals for developing suitable properties for them","One reason is that it is difficult to describe inconsistent knowledge bases in classical logics due to the principle of explosion, and another one is the diversity of paraconsistent logics  ","Mu et al. have presented that a desirable set of properties for an inconsistency measure should at least contain three aspects, i.e., constraints on the nature of inconsistency, constraints on the change of inconsistency, and special characteristics of the measure  ","Roughly speaking, the first aspect requires that an inconsistency measure can articulate the nature of inconsistent knowledge bases, i.e., it is capable of distinguishing inconsistent knowledge bases from consistent knowledge bases",The second aspect focuses on whether the degree (or amount) of inconsistency in a knowledge base increases when the base is enlarged or strengthened logically,"The last aspect is always associated with the form of inconsistency characterization (e.g., minimal inconsistent subsets or atoms assigned to inconsistent truth values) as well as the underlying structure of knowledge bases (e.g., stratified or not)",The property of   presented in   is considered as one of the intuitive constraints on the nature of inconsistency,It states that any nonnegative inconsistency measure should assign zero to only consistent knowledge bases,"The satisfaction of   for a given measure ensures that the measure is indeed an inconsistency measure, because   makes the inconsistency measure capable of distinguishing inconsistent knowledge bases from consistent knowledge bases  ","Obviously, it is also suitable for inconsistency measures in the case of stratified knowledge bases","Let Inc be an inconsistency measure for stratified knowledge bases, we may express   as follows: 
                   Secondly, the property of   used in   describes that the amount of inconsistency in a knowledge base cannot decrease when the base is enlarged by adding new formulas","In particular, the properties of   and   used in   describe two cases that the amount of inconsistency in a knowledge base remains unchanged when the base is enlarged by adding formulas having nothing to do with some special kinds of inconsistency characterization, respectively","Intuitively, the three properties are also suitable for the case of stratified knowledge bases","We may express them as follows: 
                   Note that   states that free formulas cannot affect the amount of inconsistency in a knowledge base when the inconsistency is characterized in the form of minimal inconsistent subsets, whilst   (also termed   in  ) states the any consistent formulas consisting of new atoms brings no new conflict in a knowledge base","In this sense, the former is appropriate only for characterizing inconsistency measures based on minimal inconsistent subsets","In addition, the property of   adopted in   states that logically stronger formulas bring more conflicts in a knowledge base",It is also intuitive in the case of stratified knowledge bases,"Then we may express it as follows: 
                   Note that the last four properties can be considered as constraints on the change of inconsistency","Roughly speaking, these five slightly adapted properties focus on describing some general characteristics of inconsistency measures for stratified knowledge bases","In other words, these properties do not consider the stratum-based structure or the preference relation over a stratified knowledge base explicitly","To address this, we introduce one new property, which reflects the impact of the priority level of a formula on the inconsistency assessment","It states that a given formula at more preferred stratum makes a stratified knowledge base more inconsistent that the same formula at less preferred stratum. 
                   As mentioned in  ,  ,  ,  , and   are designed for only characterizing inconsistency measures which focus on pure inconsistency (formulas or atoms involved in inconsistency) in a knowledge base","However, as mentioned earlier, in some applications we need also some measures such as the   measure tell us what proportion of formulas or atoms involved in inconsistency","Generally, we call such measures normalized inconsistency measures","It has been shown that normalized inconsistency measures behave differently from other measures, and then none of four properties is suitable for such measures  ","On the other hand,   is also a property only suitable for the inconsistency measures focusing on pure inconsistency","To illustrate this, consider adding a new consistent formula to a stratum, it cannot bring any new conflict, but it can enlarge the stratum",This may decrease the degree of inconsistency (the proportion of inconsistent formulas) we meet at the stratum in some cases,"Allowing for this, we adopt the properties  ,  ,  ,  , and   to characterize all the inconsistency measures for the amount of inconsistency","In addition, we also consider   as a property for the inconsistency measures based on minimal inconsistent subsets","Let us first see what is the relation between the naive MSIM for stratified knowledge bases and the inconsistency measure used in it. 
                      
                   
                      
                   It has been shown that   satisfies  ,  ,  and   
                      ","Evidently, we can get the following corollary from  . 
                      
                   Concerning the second kind of the multi-section inconsistency measure  , we have identified the following properties. 
                      
                   
                      
                   Also, we have obtained that   fails to support the property of  .  
                   
                      
                   However, as analyzed in  , such a failure is largely due to the syntax sensitivity of minimal inconsistent subsets","Now we use the following example to illustrate the change of inconsistency in a knowledge base. 
                      
                   We have obtained that the   measure   possesses all the five properties. 
                      
                   
                      
                   Evidently, we can get the following corollary from  . 
                      
                   
                      
                   
                      
                   
                      
                   Compared to  ,   only supports two intuitive properties, i.e.,   and  , and fails to support  ,  , and  ",This is largely due to weakening of the difference between preference levels of strata in integration process based on weights,"In this sense,   can be used to capture the nature of inconsistency for stratified knowledge bases in essence, whilst   is considered meaningful only in the case of a single value being more attractive","As the normalized version of  , the   measure   possesses the following two intuitive properties. 
                      
                   
                      
                   Moreover,   does not support  ,   and  , as illustrated by the following example. 
                      
                   As mentioned above, none of  ,   and   is designed for characterizing the normalized inconsistency measure which focuses on the proportion of inconsistency in a knowledge base","Therefore, as a normalized version, its failure in supporting  ,   and   cannot be considered as a negative aspect of  ","However, how to develop properties suitable for such normalized inconsistency measures is still an open problem","Finally, we summarize these results in  
                      , in which √ denotes satisfiable properties, × unsatisfiable ones, and − unconcerned ones. 6 Computational complexity Now we turn to the complexity issue","We assume that the reader is familiar with the basics of complexity, in particular the polynomial hierarchy (  , and for all  ,  , and  )  ","Here   (resp.  ) is the class of all functions that can be computed by a polynomial-time Turing machine with an NP oracle (resp. an   oracle), and   is the class of all functions that can be computed in polynomial time by a Turing machine using a number of NP oracles bounded by a logarithmic function of the size of the input","The canonical natural  -complete problem is MAX-WEIGHT SAT, which is given as follows:   
                      ","The problem of MAX SAT, one of the well known natural  -complete problems, is given as follows:   
                      ",Let us recall the complexity result about the   inconsistency measure   in the case of flat knowledge bases,It has been shown that the problem of computing   is  -complete in the case of flat knowledge bases  ,"Note that  then we can get the following result firstly. 
                      
                   We omit the proof of this lemma, because the membership is obvious, and the proof for hardness has no essential difference from that for Theorem 4 in  ","We have identified the following complexity of computing the naive MSIM  . 
                      
                   
                      
                   The following proposition shows that fixing the number of strata can lower the complexity of computing  . 
                      
                   
                      
                   The stratum-centric MSIM   is a multi-section inconsistency measure built upon minimal inconsistent subsets",The main part of computing   is essentially checking whether a formula is involved in minimal inconsistent subsets due to each stratum,"It has been shown that checking whether a set of clauses is a minimal inconsistent subset (also termed Minimally unsatisfiable subformulas or MUS) or not is DP-complete, and checking whether a formula is involved in minimal inconsistent subsets of a knowledge base is in   
                      ",We may obtain the following lower complexity bound for the stratum-centric inconsistency measure,"Its tight complexity bound is still open. 
                      
                   
                      
                   Concerning the   measure  , we have obtained the following complexity results. 
                      
                   
                      
                   Further, we have obtained that fixing the number of strata can lower the upper bound of complexity of computing  . 
                      
                   
                      
                   Clearly, we can obtain the following complexity results from  . 
                      
                   
                      
                   
                      
                   
                      
                   For the   measure  , we identified the following complexity results. 
                      
                   
                      
                   
                      
                   
                      
                   In summary, the complexity of each inconsistency measure built upon   models presented in this paper is  -complete","Moreover, when we fix the number   of strata, its complexity reduce to  -complete","Finally, we summarize the complexity results about these measures in  
                      . 7 An application in requirements engineering Here we use a small but explanatory example in requirements engineering to illustrate the application of our measures for stratified knowledge base",We consider a scenario for eliciting requirements for updating an existing software used in  ,"Moreover, to illustrate the role of our measures in monitoring inconsistency handling process, we adapt slightly the scenario to the case of prioritized requirements by giving a preference relation over requirement statements in the scenario. 
                      
                   8 Related work Inconsistency handling for stratified knowledge bases has been paid much attention in many applications","In this section, we compare the inconsistency measures presented in this paper with some of closely related approaches",Measuring inconsistency has been increasingly considered crucial for effectively resolving inconsistency in some applications  ,Most of proposals presented so far are mainly concerned with measuring inconsistency in flat knowledge bases,Note that any flat knowledge base does not convey any non-trivial preorder relation on the base explicitly,Then inconsistency measures for a flat knowledge base may be built upon either the minimum number of formulas involved in inconsistency or the minimum number of variables assigned to inconsistent truth values in some paraconsistent models  ,"In contrast, for the case of stratified knowledge bases, both the minimal number of formulas involved in inconsistency and the minimum number of inconsistent variables are insufficient to capture the inconsistency in a knowledge base, because the impact of preorder relation over the base on the inconsistency assessment cannot been reflected","We presented two kinds of measures to measuring inconsistency for stratified knowledge bases, i.e., the multi-section inconsistency measure and the preference-based inconsistency measure","The multi-section inconsistency measure provides a framework to capture inconsistency in a stratified knowledge bases by using inconsistency assessments for all sections of the base together, in which inconsistency assessments for sections are defined in an incremental way",Such an inconsistency measure is more appropriate for looking inside an inconsistent stratified knowledge base stratum by stratum,We may take advantage of some inconsistency measures for flat knowledge bases such as   presented in   to assess the inconsistency in each cut of a stratified knowledge base in the framework of multi-section inconsistency measure,We call such an instantiation naive MSIM,Note that any naive MSIM will reduce to the corresponding measures for flat knowledge base when the stratified knowledge base has only one stratum,"In addition, the inconsistency rank presented in   captures the most preferred stratum where an inconsistency occurs","It can be also considered as a sketchy naive MSIM (i.e., the 0–1 vector) in which only the first   consistent cuts are assigned 0","The second instance of multi-section inconsistency measure, termed the stratum-centrical MSIM  , focuses on the most preferred stratum involved in new inconsistencies in each  -section due to  , in which inconsistencies are characterized in the form of minimal inconsistent subsets of each  -cut",It reflects the impact of preorder relation over a base on the inconsistency assessments explicitly,"Note that   may be reduced as the drastic inconsistency measure for flat knowledge bases when the stratified knowledge base has only one stratum, i.e., the it only tells the base is inconsistent ( ) or not ( )","Generally, the naive MSIM is more appropriate for describing how inconsistent each cut of a knowledge base is, whilst the stratum-centrical MSIM is more suitable for uncovering how preferred the strata involved in inconsistency are at each stratum","Compared to the multi-section inconsistency measure, the preference-based inconsistency measure   presented in this paper aims to assess the inconsistency in a stratified knowledge base from a global or integrated perspective directly",Note that both   and   presented in   are built upon   models,"However, the following aspects distinguish   and  ","First of all,   is a   inconsistency measure, whilst   is a   inconsistency measure","Roughly speaking,   for a flat knowledge base focuses on the minimum number of variables assigned to the inconsistent truth value in   models of the base","In contrast,   for a stratified knowledge base is concentrated on the number of formulas of each stratum assigned to the inconsistent truth value in   models of the base, because the preorder relation is given over formulas rather than variables","Second, a   model with minimal number of inconsistent truth values is not necessarily minimal with regard to the lexicographical rank of models used in  , and the converse does also hold","To illustrate this, consider  ","Then  , where  ","However,  , where the model   such that  ",This implies that the two measures focus on different classes of   models,"Third,   cannot be reduced to   in the case that a stratified knowledge base has only one stratum","Lastly,   and its normalized version are syntax sensitive","To illustrate this, consider   and  ","Then  , but 
                   The rank of   models is also different from the lexicographical rank of classical models presented in  ","To illustrate this, consider  ",Consider the classical possible worlds   where  Let   such that  ,"Then  Note that 
                   The measures for prioritized knowledge bases presented in   is also a similar research to our approaches","At first, there is a slight difference between prioritized knowledge bases presented in   and stratified knowledge bases","Informally speaking, for a prioritized knowledge base, each stratum corresponds to a fixed level of preference","For example,   is a prioritized knowledge base under 4-level preference scale, in which there is no formula with the third level","In contrast, each stratum in a stratified knowledge base does not necessarily corresponds to a fixed level of preference",It just expresses the relative preference of the formula in the stratum within the knowledge base,"So,   and   are considered as different prioritized knowledge bases, but the same stratified knowledge base as  ","Second, the measures presented in   are built upon minimal inconsistent subsets of a knowledge base as well as the preference level of formulas involved in inconsistency","Roughly speaking, these inconsistency measures aims to assess inconsistency in a prioritized knowledge base by accumulating the amounts of inconsistency in all minimal inconsistent subsets (also considered as prioritized knowledge bases) of a knowledge base","In contrast, most of inconsistency measures presented in this paper are built upon   models, except the stratum-centrical MSIM  ","However, the stratum-centrical MSIM   is more concerned with the level of the most preferred stratum involved in minimal inconsistent subsets rather than the amount of inconsistency in each minimal inconsistent subsets","Lastly, both the inconsistency measure   presented in   and the preference-based measure   presented in this paper are built upon   models as well as the preference relation over a knowledge base","However, the main difference between   and   is that the latter aims to count inconsistent formulas at each stratum instead of inconsistent variables or atoms",It makes the latter more intuitive to capture the inconsistency in a stratified knowledge base because the preorder relation is explicitly given on set of formulas rather than on the set of variables. 9 Conclusion We have presented two approaches to measuring inconsistency for stratified knowledge bases,Both the two approaches allow us to consider the impact of the total preorder relation over stratified knowledge bases on the inconsistency assessment,"This paper presented the following contributions to measuring inconsistency for knowledge bases: 
                  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0921889015000834," 1 Introduction At present, Parallel Kinematic Machines (PKMs) are primarily used in large setups to perform machining tasks such as milling    or manipulation of heavy components   ; additionally, different configurations of PKM are utilised for manipulations in assembly lines    or accurate positioning systems    for astronomy installations or MEMS","However, recent work has shown that PKMs are also suitable for use in a mobile context, i.e. being moved into a location of intervention to perform various inspection/processing tasks","Yang et al.    have designed a quadruped robot PK machine tool, equipped with one redundant limb that is used only for walking, which can move between fixed pins fitted to the surface to be traversed; these pins react the lateral forces and ensure that the legs fall within a series of known positions, which eliminates the need for referencing the PK structure relative to the base platform","However, due to such configuration, this PKM structure walking machine cannot walk within an unprepared environment","Guy    developed a robot PKM for drilling and riveting, which positions itself on the exterior of a section of aircraft fuselage using a fixed base and actuators attached to the parallel platform that can lift the base while it relocates utilising a PK mechanism","This design uses suction cups to attach the base unit to smooth, relatively flat surfaces; however, it could be noted that this solution cannot cope with complex environments, since the footprint of the system is large and therefore cannot easily avoid obstacles",It also cannot cope with terrains that are non-smooth and is not flexible enough for more general applications than working only on fuselage sections,"Both Guy    and Yang et al.’s    designs have the desired mobility, but are only mobile within limited environments, rendering them ineffective for more general tasks/interventions (e.g. in-situ repair) that might require motion within uneven terrain and/or complex paths of the end-effectors","They also might exhibit a limited accuracy for performing processes such as multi-axis milling or rely on pins built into the environment; hence, neither is suitable to perform accurate automated operations in hazardous or constrained environments","Furthermore, attempts have been made, such as that by Denton   , to implement a tooling solution for performing machining with a standard hexapod robot, which uses an off-the-shelf axially symmetric configuration; this places a limit on the accuracy achievable by such a system and leads to a relatively low useful working volume","As a result, the robot would be required to walk while machining if performing operations on a large area, further reducing accuracy and repeatability",An existing Free-leg Hexapod (FreeHex)    with a PK structure that can be attached directly to the workpiece (without the need for a fixed base) for in-situ processing (e.g. machining) has been reported; this machine must be placed in location by a human operator and calibrated using an innovative methodology involving a set of gauges that must be removed prior to machining   ,"Despite this, the FreeHex was reported to be capable of achieving high accuracy, i.e. repeatable results when machining and proved to meet real industrial application needs, thanks to its design","However, in its current design, it lacks the advantage of being able to reach tight spaces or hazardous locations independently","Consequently, new strategies are required in order to allow this system to walk independently, without compromising the machining capability conferred by a PK configuration","Prior to a redesign of the FreeHex to enable its walking capability, it is important to evaluate the stability of such structures during their motions and analyse suitable gaits in order to select suitable actuators","Therefore, it is important to consider existing walking robots, even though they may not be capable of generating 6-axis tool paths",Walking robots fall into two overall categories: statically stable    and dynamically stable (e.g.   ),"Dynamically stable robots are most analogous to bipedal animals and humans, where balance must be actively maintained",Dynamically stable walking is performed by internally generating an imbalance such that the centre of mass is (usually) in front of the supporting limbs causing the subject to have the tendency to topple forwards,"Thus, one leg is placed in front of the other in order to prevent the system from collapsing and so to enable its advancement","According to McKerrow   , dynamic stability is achieved by continuously moving either the feet or body to maintain balance",Wettergreen and Thorpe    describe an active feedback approach to control system implementation in order to maintain the balance with respect to a required speed,"By contrast, statically stable walking robots rely on maintaining a balanced pose at all stages during motion","This is analogous to the motion of many creatures with four legs and all creatures with six or more; as such, many examples of this type of robot are inspired by animals or insects","For a robot to be able to move autonomously, reasoned decisions need to be made as to where and in what order the feet are placed; this process is referred to as gait generation analysis",The gait of a robot is the set of motions that the legs should go through in order to allow the robot to advance in a specified direction,"According to Wettergreen and Thorpe   , previous work on gait can be classified into four categories: Behavioural, Control (previously explained), Constraint-Based and Rule-Based","Behavioural gait generation is an attempt to mimic the method of determining limb movement used by animals and insects by creating an environment for unconscious reasoning, such as a neural network","Beer et al.    built a walking hexapod to investigate a control network based on the neuroethology of insect locomotion, producing a range of gaits and degrees of robustness in a real robot that match quite closely with simulations","Berns et al.    describe an hierarchical control architecture for a walking hexapod named LAURON utilising neural network techniques; this focuses on active learning within the control system, which proved to be time intensive and not of as much practical use if the environment to be traversed is well defined","Rule-Based gait generation involves assigning a prescribed gait based on the classification of the robot’s environment, unlike the behavioural and control approaches that involved no active planning","As the robot switches between different types of terrain, the system adopts the gait that is most suitable for the current environment","Song    reports on an efficient wave gait that varied foot placements between terrains while retaining gait sequencing; furthermore, this was developed to allow for autonomous crossing of four major different types of obstacle: grade, ditch, step and isolated-wall   ","Kumar    uses a system of control schemes to modify gait parameters including duty factors for the wave gait in order to demonstrate that robot velocity can be varied continuously even with irregular, asymmetric and changing support patterns; however, this approach showed that some problems in switching between gaits could appear","This highlights the main drawbacks of using Rule-based gait generation: (i) difficulty in generating an exhaustive list of environment scenarios; (ii) difficulty in autonomous recognition of which type of environment scenario is most appropriate for the current terrain; (iii) while the robot is transitioning between two environments, it is not fully in either environment, so the system must have a method of coping with fuzzy logic","Cruse    achieves some success in addressing these issues by means of the ‘Cruse Coordination Rules’, which allow the robot to adapt automatically to its environment, producing stable and reliable gait patterns","Roggendorf    compares this with Steinkühler and Cruse’s MMC model    and a modified form of Porta and Celaya’s approach   , but finds that the latter produces the best performance in simulation","Belter    utilises an evolutionary algorithm to generate a tripod gait for the hexapod ‘Ragno’, reporting that there is a strong dependence on the accurate knowledge of the physical parameters in the quality of the produced gait",Buchli    presents an excellent control methodology for the ‘LittleDog’ quadruped incorporating a novel line-based COG trajectory planner which is proven to be effective in real world trials,"For complex and constrained environments, a modified standard gait is not as suitable for avoiding all obstacles",It is in these situations where the fourth category of gait generation is most useful: Constraint-based gait generation—a mid-term planning active searching gait generator,"It operates in the following stages: (i) a complete list of possible moves that the robot legs and platform could make is generated; (ii) this list is reduced by eliminating all motions that are infeasible due to spatial uniqueness (e.g. clashes between legs and legs/other parts of the robot/the environment); (iii) the list is further reduced by eliminating all unstable movements and possibly by using other criteria (such as singularity points); (iv) the list of movements is ordered by a parameter to be optimised such as stability, energy usage or speed",These steps can be carried out for several stages in advance of the robot’s current position,"However, the main problems with this approach are: the list of options (as explained above) grows exponentially with the number of degrees of freedom (DoF) of the robot, as discussed by Latombe   ; due to the processing time, it is impractical to calculate a large number of steps in advance, so the robot may be making apparent good progress, but may be entering a route that does not allow it to reach the final objective safely without turning back and retracing steps (this is known as the Horizon Effect   )","Jayarajan    demonstrate that constraint-based gait generation is still useable by creating a reduced list of movements, considering four leg placements for each body translation; this simplification reduces the optimisation of the whole system, but considerably speeds up the process",The leg workspace (referring to the area available to place a given leg at any instance) and terrain reference frames were later discretised to constrain possible gaits and the search was limited to movement cycles that advanced towards the goal in an attempt to overcome the Horizon Effect,This attempt met with success in a simplified test environment   ,"Subsequently, Pal and Jayarajan improved the techniques by application to a walking hexapod robot    and demonstrated how the functions may be designed to generate common periodic gaits such that the system may be optimised for any general factor   ","These gait generation techniques have so far been mostly applied to walking hexapods that have either two rows of three legs in parallel (monosymmetric, e.g.   ) or equispaced axially symmetric legs (e.g.   )","Driven by the industrial need for a truly 6-axis walking machine tool, the present research builds on the experience of the FreeHex   ",The goal is to produce a machine that can operate as a highly effective and accurate machine tool but also operate as a highly manoeuvrable walking robot able to navigate multiple environments,"As seen in the literature, a walking robot fitted with a spindle is not capable of fully addressing these challenges","Therefore, a PKM with a novel leg layout and architecture is used","The scope of this paper is a theoretical study of the factors relevant to allowing a hexapod PKM with tri-radial symmetry to walk, and specifically to walk on inclined planes of varying angles of elevation","This paper presents a gait methodology based on stability margin criteria and a torque analysis, noting that the WalkingHex, having telescopic legs, is capable of varying the pose and translation of the upper platform, which can be taken into consideration to improve the stability margin and actuation torque of the system","The following aspects are among those that must be considered: the overall design of the hexapod and leg joints, and the measures of gait effectiveness such as stance stability and torque in the leg’s spherical joints","This determination is of key important because the algorithm can be used to avoid high levels of torque that the leg actuation mechanism is incapable of producing when walking in different environments. 2 Schematic description of the WalkingHex The Walking free-leg Hexapod structure (WalkingHex) is the next design evolution of the FreeHex   , so has a structure based on that of a hexapod PKM—known to be able to respond well to requirements for multi-axis machining operations","The legs are attached to the upper platform by actuated spherical joints  , mounted in pairs in a rotationally symmetrical pattern while the length of the legs can be varied (to allow 6-axis movement of the upper platform for machining operations) using prismatic joints   as illustrated in  
                      ; while a solution for actuation of the upper spherical joints has been developed, this is not subject of this communication, which focuses on gait analysis to enable the dimensioning of the motors to be used for such actuation",The feet   are attached to the legs via spherical joints,"Karimi and Nategh show that grouping the hips and feet in pairs as follows provides the best quality workspace for a hexapod PKM   : pairs of joints that are closer together at the top of the legs should not be close at the foot, but the foot should be paired with (and closer to) the foot of the leg whose upper joint is further away (but still consecutive); thus feet are paired   and  ,   and  , etc. as per  ","This leg arrangement is very different to that of the majority of walking hexapod designs, which are typically either monosymmetrical or axially symmetrical, so it has unique properties that must be studied, particularly in relation to walking","In order to optimise the capability of the machining mode of operation and retain the six prismatic actuator system previously employed by the FreeHex, the robot uses these driven joints for walking, alongside a series of actuators for controlling the rotation of the upper spherical joints, thus allowing it to lift its feet off the ground and walk; the torque capability of the proposed actuation system (to comply with design requirements—not detailed here) is limited to 10 N m",The design of the actuation system on the spherical joints is not discussed in the present study and will make the scope of future reporting,"The specifications of the WalkingHex are outlined in  
                      ","This design requires a different approach to gait generation, as it has some unique benefits and drawbacks for walking; nevertheless, such a system needs to be used also for machining operations and the previous research proved to be advantageous","For example, it is possible to tilt and translate the platform in order to shift the centre of mass for the system, making it more stable","It also is able to overcome the issue of motor power versus speed, which can be summarised as follows","In a standard gait, for most statically stable walking robots, there are three kinds of movement that the legs are required to make—(i) lifting/lowering, (ii) translating the leg while the corresponding foot is in the air, and (iii) translating the base link in the direction of advancement","Little force is needed to translate the legs in case (ii), but the speed of translation during this motion is critical to the overall speed of walking as this needs to happen at least twice per gait cycle","Greater force is for case (iii), but speed is less important and may need to be regulated to ensure stability","For most walking robots (with a few notable exceptions), both motions (ii) and (iii) are performed by the same motors, meaning that a compromise must be made between speed and strength of the motors","For the WalkingHex, this compromise is avoided by the use of 2 different sets of motors: the spherical joint actuators performing motion (ii) with speed and the prismatic actuators performing motion (iii) with strength","The design of the WalkingHex also means that it is more difficult to walk in some directions than in others, so it is not a truly omnidirectional walker. 3 Parameters for gait analysis Since this research focuses on a statically stable walking robot that has low walking speed requirements but a high load capacity, the pose must be optimal with respect to the stability margin criteria and the torque produced in the joints (so as not to overload the actuators)","The stability margin provides a measure of how stable the system is in any particular pose, ensuring that the WalkingHex does not topple over","The upper spherical joints are actuated in order to orient the legs, so the magnitude of torques experienced in the joints are needed in order to select suitable actuators to hold the legs at a fixed angle when on the floor and to rotate the joint to orient the legs when in the air","This is particularly important when walking on an inclined plane. 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   3.1 Stability margin criteria At present, a displacement based method has been used to calculate a stability margin for the system, based on the system reported by McGhee   ",Huberty    identifies that this methodology is particularly efficient and effective among the static stability margin techniques,"In order to maintain a stable pose at all times, at least three legs must be touching the ground at any time, with the vertically projected centre of mass placed within the Support Polygon made up by those legs",Mahalingam and Whittaker describe the Support Polygon as the minimum bounding polygon on the ground plane that includes all points of contact between the supporting legs and the ground   ,"For this approximation, the centre of mass and foot locations are projected onto the horizontal ground plane and the minimum perpendicular distance between this projected point and the support polygon gives a measure of the stability of the system","Since the foot positions on an inclined slope are projected onto a lower plane, the effect of a slope in reducing the stability margins is incorporated into the model","For the purposes of a statically stable walker, this measure is largely accurate as the static states are of greatest interest",The following set methodology is proposed for evaluating the displacement stability margin using vector algebra,"The description here is based on the assumption that the robot is using a tripod gait; however, this methodology could be applied to any statically stable gait (where there are at least 3 ft on the ground at any time)","This case is studied since it is the most disadvantageous in terms of torque and stability margin and any torques and margins experienced by the system when utilising gaits with more feet on the ground (pentapod, quadruped) should be more desirable than the tripod equivalent.  The quantity of interest is the perpendicular distance to this point from each side of the support polygon","For example, this is determined for   as follows:   As is evident from these equations, the stability of the system as quantified by displacement is dependent on the location of the centre of mass and feet; however, the location of the centre of mass is a function of platform orientation as well as translation","As such, one novel feature that will be considered for this design will be the effect of platform attitude and translation and hexapod rotation on the stability of the system, particularly when the WalkingHex is located on a slope or uneven surface, which will be discussed in Section   . 3.2 Torque analysis A model was developed in SimMechanics with the purpose of carrying out an analysis of the torques in the spherical joints at the top of each leg, with a view to restricting the gait to positions that the actuation system can handle","When the system has all of the feet on the floor (see  ) and under the assumption that all feet are secured to the floor, the torque in each joint is effectively zero, as each leg constrains one of the six degrees of freedom of the platform, so the actuators on the spherical joints are redundant in this situation and can be disengaged if desired; this is the normal scenario on which and PKM machine tool (e.g","While in walking mode with at least one foot off the ground, when the spherical joint actuators are engaged, there are two types of torque that the actuators must be able to handle: (1) the torque in the upper spherical joint resulting from supporting the mass of the WalkingHex and its payload in its current orientation when the corresponding foot is on the ground and (2) those resulting from the total mass of the leg and foot when raised in the air",The torque arising for the second case is relatively easy to calculate analytically for a given leg elevation angle and tends to be the smaller of the two types of torque,"The torque in the joints at the top of the legs that are on the ground during a particular stage of walking is less straightforward to calculate due to the interaction between the components of the support structure, so MSc ADAMS    was selected as a suitable environment to simulate a simplified model of the hexapod in order to find these torques and check the stability of gait poses, as it is a scriptable multi-body dynamics simulator",Gaits were generated and their static stability was analysed in MATLAB; the poses from these gaits were then passed to ADAMS and simulated individually,"The torques, forces and velocity of the centre of mass (in order to determine if the hexapod had slipped or toppled) were then passed back to MATLAB for analysis","In parallel with this, SimMechanics    was employed to give an estimate of the values as a means of verification. 
                         
                          shows the model constructed in ADAMS in order to test the gaits","The system is modelled parametrically using variable length links to represent the telescopic legs (prismatic joints  ), a plate to represent the platform and its mass, short cylinders to represent the feet and a cylinder to represent the cylindrical payload and its mass","The actuators used for positioning and holding the leg at a constant angle are simulated by constraints placed on the upper spherical joints   that prevent rotation. 
                         
                          shows the SimMechanics configuration used to simulate the WalkingHex","In  (a), the virtualisation of the model can be observed, including the Centre of Mass of each of the components","The feet for which no centre of mass symbol is visible are in contact with the floor, so the mass and size of the foot is absorbed by the ground component as is of no consequence to the model.  (b) shows the block diagram for a single leg: at the top of the leg, a custom joint with the same number and type of degrees of freedom as a spherical joint is used to allow application of a driving condition, representative of the actuator on the upper joint.  (c) shows the full block diagram for the system; the legs are highlighted in blue (i), the platform in green, the feet in orange (ii) and loads on the system in pink (iii)",Readings are taken directly from the custom joints in the leg blocks and exported to the workspace via the beige coloured blocks,The prismatic joints are not simulated directly in SimMechanics and ADAMS as they are set to a predetermined extension at each static stage of operation and may be considered rigid in this part of the model,A simple study using a basic tripod gait was conducted in the SimMechanics and ADAMS environments in order to validate the ADAMS model,"The study has eight Stages as shown in Cartesian co-ordinates in  
                         : The transitions between the stages of this gait are outlined in  
                          specifying which feet are moved at each stage, in which direction and by how much: The results of the two analyses are shown in  
                         ","During Stages 2 and 3, legs 2, 4 and 6 are in the air and the only torque is due to the total mass of the leg and foot and the fact that the leg is not in a neutral vertical position","Legs 1, 3 and 5 are carrying the load of the entire robot, so experience a larger torque","Stages 5 and 6 have the largest torques since the feet are advanced in the direction of motion, while the platform has not been translated; thus producing the lowest stability",A gait with lower maximum torques can be produced by including a stage in which the platform is translated in-between Stages 4 and 5,"The two simulations show a high level of agreement for Stages 2, 3, 5 and 6, but a serious discrepancy for Stages 1, 4, 7 and 8",This is due to a difference in assumptions between the two models,"In the SimMechanics model, the contacts between the feet and the floor are modelled as welded joints; this removes 2 degrees of freedom that are present in the case of non-perfect friction, motion in the   and   directions","In ADAMS, the contact with the floor is modelled by Coulomb friction, so that the feet are only held in place by frictional forces in the   and   directions and the torques in the upper joints that prevent the legs from splaying","This can be demonstrated by considering the special case whereby the feet are directly below the upper spherical joints, meaning that the legs are vertical and the torques must be zero; as the splay of the feet increases, the torques should increase linearly and proportionally","It can be shown using simple trigonometry and assuming symmetry and uniform cylindrical legs that for the case where  :   Substituting in the values from   for   and  :   This case was evaluated using ADAMS in order to verify this behaviour, starting with  , which is the same value as  , as can be seen in Stage 1 of  
                          and finishing at   (Stage 5)",The result of this investigation was that the torque was found to increase linearly with foot spacing radius as:   This represents errors of 1.54 and 1.47% in the gradient and  -intercept respectively; this is a good level of agreement between the analytical model and ADAMS predictions,"The values provided by ADAMS are very similar to the theoretical results and most importantly, they cross the   axis at the same point (0.0975), where  , meaning that the ADAMS model is providing more accurate results for Stages 1, 4, 7 and 8","Apart from these effects, the two sets of results shown in   match closely, which shows that the figures produced by the ADAMS model are very similar to those produced by the SimMechanics model and it is therefore suitable to carry out the required calculations for the optimal step","Due to the identified issues with the SimMechanics model, it was not subsequently used. 3.3 Definition of optimal step for the WalkingHex The effect of varying different system parameters (e.g. foot placement, platform attitude) can be established, in this case, by considering a standard gait ( )","A simulation was conducted in order to investigate how the stability margin and torque in the upper spherical joints vary with changes in foot placement (foot spacing angle and radius, see  
                         ), hexapod rotation (Section   ), platform translation (Section   ) and platform attitude (pitch with respect to the   direction) (Section   ) as the slope of the terrain changes","In this paper, only slopes in the   direction (the direction of advance) will be considered for brevity","Slope angle and platform pitch are taken using opposing sign conventions; in order for the platform to be parallel to a slope of  , the pitch should be  ","The feet are placed on a circle at a fixed radius to ensure that the weight of the robot is evenly distributed; the feet could be placed in any required position, with each foot position given independently in Cartesian co-ordinates, but confining their placement to a circle reduces the number of possible poses to a manageable number","As can be seen in  , Stage 6 is characterised by the highest total torque in the system, so it was chosen as an example for optimisation in this paper",This methodology can be applied in turn to each of the stages of the gait,The examples of results illustrated here for each set of parameters are presented as a diagram showing a sample set of poses and a results graph for each of the minimum stability margin and maximum torque in the upper joints for a pose,"Results for Stages for whose poses cause the WalkingHex to become unstable are omitted from the torque graphs as they cannot be evaluated and set, by convention, to −0.01 for the margin graphs","In order to maintain the same setup for each of the studies, the following values were chosen for the controlled experimental parameters (see  
                         ): The coefficient of friction was held at 0.8 for the course of the following experiments, as this is a close approximation to what the actual value may be for the completed hexapod; however, several simulations were run in order to determine the effect of varying the coefficient of friction",The main result was the shift in the angle of inclination at which the hexapod started to slip,"The angle at which the hexapod slips increases with the coefficient of friction up to 0.7 (0.45 rad, see  
                         ), then remains constant; this is due to the fact that the hexapod becomes unstable at this point, so the failure mode is tumbling rather than slipping","Other than this, the effects were minimal. 
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                         
                      3.3.1 Analysis of foot placement The first parameters to be considered as variables in the system are those pertaining to foot spacing: the foot spacing angle and radius","The first set of results is for the system on a flat surface. 
                            
                            (a) shows that the stability margin increases with increases in both foot spacing radius and angle; the wider the spread (as is intuitive) and closer the opposing pairings of feet, i.e. smaller values of  , as in agreement with Karimi and Nategh   , the more stable the system becomes. 
                            (b), however, shows that increasing the foot spacing radius   also increases the maximum torque,   in the system; such that some of the results at   exceed the   limit","Unsurprisingly, small foot spacing radii produce unstable states; this is evident from the unstable (dark) region on  (a) and the absence of results in this region on  (b)",The smallest stable value of   in both graphs is 0.14 for a foot spacing angle   of  ,"At low radii, the torque does not appear to have any strong dependence on  ; however, at larger radii it becomes apparent that either extreme of   (where 2 ft are close together) is preferable to evenly spaced feet  ","Considering a central value of   and  , a change of ±20% in   value results in ±67% change in   and   with variations between ±16%",A change of ±33% in   results in variations in the order of ±24% of   and   changes by  ,"Both graphs suggest that large   values are beneficial, but there must be a trade off in terms of foot spacing radius to ensure that the hexapod is highly stable while minimising the torque in the upper joints","As such, for further studies, an   of 0.2 m will be used as the standard foot spacing radius, as this offers good stability without exceeding the torque limit","The shape of the unstable results matches well between the two graphs, but shows the stability margin to be slightly optimistic",A safety margin must be applied to the stability margin results in order to ensure that the physical hexapod does not topple,"Since one of the main objectives of this system is to enable walking on inclined surfaces, it is important to consider how the choice of foot placement affects the stability and torque when a slope angle is introduced","The example taken here is that of a   slope.  
                            (a) and (b) show that the shape of the stable regions of the graphs is very similar when on a slope, but with the stability significantly compromised; there are far fewer stable poses than in the previous case and those that remain stable have lower margins","The torques are slightly higher when on a slope than those present on the flat, but by less than 10%, so the effect on torque is considerably weaker than that on stability; however, due to the combined effect, there are now only a few points for which the maximum torque is less than the limiting value","In summary, higher foot spacing angles are always beneficial, choice of foot spacing radius should be a balance between low torques and high margins","Since stability is a greater issue when walking on a sloped surface, methods of improving the stability must be investigated and employed. 3.3.2 Analysis of platform pitch The next parameter under consideration is platform pitch,  ","Since the platform is carrying a large payload with a finite thickness, the centre of mass of the hexapod is located above the centre of the platform; this means that the centre of mass can be shifted in the   or   direction by a small amount by changing the orientation of the platform, in particular, the pitch and roll","According to the choice of coordinate systems specified in  , the pitch has an effect on the   coordinate of the centre of mass, which is important when considering slopes in the   direction","Since slopes in the   direction will not be considered, the roll parameter is of lower importance and will not be included in the examples. 
                            
                             shows a selection of the poses, based on Stage 6, used in this simulation that were repeated over a series of angles of inclination; the platform pitch is varied from   to   over 13 Stages","The minimum stability margin and maximum torque for each stage were measured and plotted in  
                            ; results for the torque are omitted in the case that the hexapod is unstable. 
                            (a) shows that the stability of the system decreases linearly as the slope angle increases and that the stability also decreases linearly with the platform pitch (the platform angle is best opposing the angle of the slope)","Due to the rotation of the hexapod coordinate system when it is placed on the slope, the centre of mass of the system is shifted backwards making the system less stable; the hexapod topples long before it reaches the slipping point","Nevertheless, using WalkingHex unique constructive characteristics, the stability margin is improved by tilting the platform forwards, which moves the centre of mass forwards; however, the system fails to achieve an optimal value as the adjustment is small. 
                            (b) reveals that pitch adjustments have little effect on the highest torque for any given slope; this means that the pitch can be adjusted to increase the stability margin without affecting the maximum torque","It also shows that the stability margin graph is optimistic, as the cut-off point of this set of simulations is at a lower angle for most values of platform pitch","The torque values for all the results are quite high (>9 N m) and increase with slope angle; the maximum walking angle achievable for the proposed system utilising these parameters would be  , a feeble accomplishment","Considering a roughly central value for the stable zone,  ,  : a change of ±50% in   results in ±26% change in   and ±5% change in  ",A change of ±50% in   gives ±15% change in   and an insignificant change in  ,"In conclusion, the stability margin can be improved slightly by tilting the platform without affecting the torques to any degree; an alternative method of improving the pose must be established and used in conjunction to achieve advancement up a slope, particularly since the torques in the system are so close to the 10 N m limit","The stability is not the limiting factor in this case. 3.3.3 Analysis of platform translation A second factor to consider is the translation of the hexapod platform with respect to the central point of the foot spacing circle. 
                            
                             shows a selection of the poses, based on Stage 6, used in this simulation that were repeated over a series of angles of inclination; with platform translations ranging from   to   over 16 Stages. 
                            
                            (a) immediately paints a better picture of the situation for the stability once the platform is translated: the graph is clearly planar either side of a straight line of inflection; this means that for any given slope angle, there is an optimal platform translation and there is a strong, regular dependency","By applying a fit to the maxima, a straight line relationship was determined in order to give the optimal translation for each slope value based on  :   The   value for this fitted line is 0.99, indicating a good level of fit for this relationship. 
                            (b) shows that the torques are minimal in a similar region to which the stability margins are greatest; though in the region between the peaks caused by low stability the torques are weakly dependent on the translation",This allows for optimisation of stability without compromising the level of torque in the system,"A fit was applied to the minima, and the line of best fit is close to that for the stability margin:  The   value for this fitted line is 0.9828, again indicating a good level of fit","From this graph we can see that the system remains stable up to  , provided that the translation is within the optimal range",Climbing any slope of greater degree is not feasible for this design of hexapod utilising a tripod gait,"By repeating this analysis for each stage of the basic gait, an optimal set of translations is produced: 
                            
                             shows that for the optimal gait, the translation of the platform should occur between Stages 3 and 5 and that the initial location of the platform should be −0.009 m in the   direction. 3.3.4 Analysis of hexapod rotation 
                            
                             shows a selection of the poses, based on Stage 6, used in this simulation that were repeated over a series of angles of inclination; the platform pitch is varied from   to   over 73 Stages. 
                            
                             shows that the rotation of the hexapod can have a significant impact on the stability margin and torque in the system, even when the system is on a flat plane  ","There is a significant agreement between the shapes of the stable regions identified in the graphs, as well as lower torques in conjunction with higher stability margins, which is highly desirable","The stability of the system can be extended by at least 10 degrees simply by aligning the orientation correctly; since the system has tri-radial symmetry, it is perhaps not surprising that the pattern repeats every  ",The optimal rotation value for Stage 6 is:  where  ,"Considering a point along the line of optimal alignment  , a change of ±50% in   results in ±6.8% change in   and ±9% change in  ",A change of ±33% in   gives −68% change both ways in   and   changes between +72% and +56%,"However, there is a further consideration in the use of this parameter: it is strongly dependent on the pose of the hexapod, that is, the optimal rotation for Stage 6 is not the same as the optimal rotation for, say, Stage 3","It is also not a parameter that is easily changed in-between stages, and it is in any case undesirable for the hexapod to have to stop and perform a rotation in the middle of an advancement gait as this will considerably slow the advancement","With this in mind, it is worth investigating the optimal angle for other stages in order to measure the effect on the gait as a whole","The example of Stage 3, the most unstable pose in the first half of the gait cycle will be considered here. 
                            
                             shows that the system is far more stable and experiences lower torques at Stage 3 and is therefore able to stand on a steeper slope without toppling","As such, this stage warrants a lower level of consideration in the decision over what angle the hexapod should be rotated through",Both graphs show that at lower slope angles there is only a weak dependency on hexapod rotation and it is not until the slope exceeds   that there is a significant distinction over the rotational cycle,"As with  , there is a   repetition of a sinusoidal relationship for both margin and torque, with favourable conditions lying together (low torque and high margin)",The optimal rotation value for Stage 3 is:   where  ,"This pattern is offset from that of Stage 6 by  , meaning that optimal conditions for both stages cannot be simultaneously achieved","Either a midway compromise must be reached, or the more logical approach is to ignore one optimal condition in favour of the other, considering the relative stability of Stage 3","Since Stage 6 is more disadvantageous than any other stage and the optimal rotation is very similar for Stage 5 (not presented here), the second most disadvantageous pose, there is no further benefit to presenting further rotation analyses for other stages, since only one optimisation can be usefully applied during a gait sequence","Looking closely at  , it can be seen that there is a lip present near the horizontal position (slope angle  =  0); this is due to the fact that the centre of mass is offset slightly forwards in the   direction, so that when the hexapod is tilted slightly, the centre of mass moves backwards, closer to the centre of the foot spacing diameter","Once the angle again increases the centre of mass moves further backwards, past the optimal position and the margin decreases again",Simulations were also conducted to briefly investigate the interaction between foot spacing angle and hexapod rotation on a 0.3491 rad slope,"It was found that increasing the foot spacing angle has a very limited effect on the maximum and minimum stability margin (order of 0.0003 m over 0.3491 rad); perhaps the more interesting result is that the phase of the minimum stability margin undulation seen in  , such that for a 0.1745 rad increase in foot spacing angle, there would be a −0.1745 rad hexapod rotation shift in the peak margin",This means that the two parameters are not independent and can be tuned in conjunction with one another,This is due to the alignment of a point of the stability triangle with the area occupied by the projected centre of mass,"Changing the foot spacing angle, however, has a significant impact on the torques in this case, seeing an increase in torque with foot spacing angle, as well as a phase shift of 0.1745 rad for an increase of 0.1745 rad in foot spacing angle (see  
                            )","This increase in torque is likely due to the increased angle of the legs to the vertical. 3.4 Application to gait generation Initially, it is instructive to evaluate a standard gait, but once the criteria for optimising the gait and a method for evaluating the criteria are established, the system can be optimised using numerical methods","The program could also be implemented as part of a gait planning algorithm, such as the one presented in  
                         ","This proposed gait can be described as follows: the terrain type that the WalkingHex will be moving in (such as flat concrete, steel pipe, inclined plane or grating) and an approximate trajectory are defined, either by user input or by intelligent recognition (which is not in the scope of this paper)",The program then chooses appropriate system parameters from a database of parameters generated from simulations and previous experience and determines a target step length,"At this point, the stability of the WalkingHex is calculated in order to assess whether its current stability is within the required margins; if it is not, the system moves to restore safe values of stability","If at any point this is not possible, the WalkingHex has reached a deadlock and must be retrieved manually","Once stability has been assured, the program uses the system parameters to determine the next target point along the trajectory (stepwise discretisation)",The system then finds the stability margin for motions of each leg (for pentapod gait) or set of legs (other gaits) and the platform by simulating the movements in turn,The optimal motion is selected and performed,This cycle is repeated until the destination is reached. 4 Conclusions The industrial need to perform complex multi-axis processing in-situ large structures/hazardous environments has led to research for the developments of mobile/walking machine tools,"To address this need, this paper reports on a theoretical analysis of a Walking free-leg Hexapod structure (WalkingHex) that represents a key step forward from the previously reported Free-leg Hexapod configuration",The parameters of primary relevance to achieving walking capability were identified and methods of determining numerical values for these variables are described,"Investigations into the effects of foot positioning, platform attitude and translation, and hexapod rotation on a selection of gait stances were conducted","As such, the main contributions of the paper can be summarised as follows:   All the above findings have been unified in a novel algorithm for gait generation for the WalkingHex so that the design/selection of mechatronics systems for this novel self-propelled multi-axis machined tool can be finalised.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s1875952119300394, 1 Introduction There is a growing interest in the field of human-robot interaction (HRI) for the investigation of robots and their emotional abilities through interaction with peers or colleagues on shared tasks  ,"Such   between humans and robots includes social cues which are perceived to elicit physiological affect (e.g., gaze, expressions, gestures, speed, distance)","During such  , humans tend to attribute emotional states to robots  ","The aim of these previous investigations in social and collaborative HRI is to investigate how humans and robots interact together in a shared physical space aimed at accomplishing a goal  , as numerous studies indicate the importance of such social interactions  ","Takayama et. al.   have found a significant effect of pet possession on the proxemic behaviors of humans interacting with robot partners, where individuals who previously owned a pet were willing to get closer to the robot partner",These findings motivate a need for the investigation of elicited physiological affect regarding robot partners interacting collaboratively on a shared task,"Moreover, it motivates a need for a deeper understanding of how elicited physiological affect influences the decision performance in human partners, to inform the design of robot collaborators and serious games","The Theory of Mind reasons that humans perceive and distinguish various social cues to explain events in terms of intents and goals of agents (i.e., their actions) which might affect the elicited emotions  ","Following these propositions, this study set out to understand the mechanism of affective exchange that occurs between human and robot agents, based on the observable social cues  ",Such embodiment provided by the social cues of collaborating robots has a direct link to the role of body information in an intelligent behavior  ,"The robot partners have been designed as autonomous social entities that can feature diverse behavior in the context of this study (i.e., autonomously acting based on a complex algorithm)","Humans perceive collaborating robots depending on the physical interaction, actions, shape, and the environment itself","It was shown that such embodied non-humanoid robots are as engaging as humans, eliciting emotional responses in their human partners  ","As evidence shows that emotions critically influence human decision-making and performance  , this study sets out to find how a small subset of social cues elicits physiological affect in humans collaborating with robots, in an attempt to investigate how these influence the decision performance on serious game tasks",Humans use the mechanisms from   (HHI) to perceive robots as autonomous social agents  ,"These propositions motivate this investigation to take into consideration both HHI and HRI, in the investigation of the elicited physiological affect",This investigation is an extension of the previous study where autonomic non-humanoid robot arms were perceived as social and emotional agents using a small subset of social cues in a serious game  ,"Therefore, this research aims to investigate elicited physiological affect and bring it in relation to the performance on a serious game task","Traditional physical games require a tangible interaction, contrary to the electronic games which are popular in the contemporary research methods  ",The physical aspect of serious games is an important factor since humans perceive robots as physical entities which have access to the virtual domain,"Therefore, this study uses a traditional game which provides a straightforward measurement of performance, where physical and virtual duality is supported through HRI-enhanced serious games","This paper attempts to investigate the effects of physiological affect underlying such human-robot proximate collaboration, as proposed by Ijsselsteijn et. al.  ","More specifically, to bring these effects in relation to the performance on a serious game task in collaborative HRI, by mapping the participants’ physiological responses towards the collaborating non-humanoid robot arms on the arousal/valence axes  ","These insights could provide a deeper understanding of how elicited physiological affect underlies such interaction from the human collaborator perspective, informing the design of more meaningful collaborative serious games that would use the objective measures of physiological affect together with intelligent robot collaborators to potentially increase performance on the shared tasks","The work presented here builds upon the previous findings, where it has been partly described and published  ","This previous publication investigated the social abilities of non-humanoid robot arms to be perceived as social agents with emotional abilities, through a number of subjective questionnaires probing social categories and emotions perceived by the human participants",The current manuscript uses objective physiological measures to disseminates the elicited physiological affect findings in human participants in the interaction with their robot partners and how it influences the decision-making performance,The study was a part of the PsyIntEC ECHORD project (FP7-ICT-231143). 2 Related work The somatic marker theory claims that decisions are aided by emotions in the form of relevant information contained in the bodily states  ,"Recent discoveries motivated the determination of emotions through a combination of factors  , which motivated Russell   to classify them through a combination of their independent components, arousal and valence","In his model, the level of excitement has been represented by arousal, while valence defined whether the current emotional state is positive or negative","The evidence further suggests that people are sensitive to the social cues of collaborating robots   (i.e., gestured motion and speed), where they have been found to elicit emotions in their human partners  ","The previous investigation used variations in gestured motion and speed of the collaborating robots, from a direct path at a fixed speed to a variable speed in gestured motions  ","These previous investigations motivate the use of the collaborators in this study to act as a stimulus eliciting emotional responses, bringing them into relation to the performance on a serious game task. 
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   2.1 Affect detection There is a rich body of physiological literature related to the affect detection and its use in the HRI domain  ","However, only a few studies have used robots as the primary elicitors of affective physiological responses","While the subjective measures (e.g., self-evaluation) always incorporate a potential self-deception, physiological signals are seen as an objective measure of emotional states since they are hard to manipulate intentionally  ","There is substantial evidence that physiological activities associated with affective states can be differentiated and systematically organized, which would allow for the analysis of their effect on the performance in collaborative serious games  . 
                          (ECG) and   (GSR) were employed to measure emotional states of humans in response to robots with more than 80% success rate  ",Studies have found a strong correlation between ECG and physiological valence  ,There have been multiple findings of a linear correlation between GSR and physiological arousal  ,"Even though GSR has been found to indicate physiological valence, the correlations were not always as strong, and greater confidence in interpretations has been obtained by combining GSR with other measures, such as ECG  ","In psychophysiological experiments, it is easy to neglect the recorded baseline as a valuable source of information about the participants","Not only that there are complexities regarding when to measure baseline values, but also the law of initial values limits the extent of possible changes on physiology that can occur during tasks  ","Therefore, higher or lower values during baseline would inherently affect physiological response levels in serious games, where subsequent task levels are calculated against these baseline levels","Baseline is the tonic level of activity during a rest period for a specific physiological modality, and it is measured when participants are not responding to a (known) stimuli  ","This measure is typically taken at the end of a resting period, typically in the last three to five minutes of a ten minute resting period. 2.2 Serious games Serious games are defined as games which are not used for mere entertainment purposes but might be used as tools to collect behavioral data  , as it is the case in this study",They need to captivate and engage players for a specific purpose intended in a ‘serious’ investigation of certain aspects of human endeavors  ,"There are studies investigating the interaction between humans and robots in serious games, but they are sparse","Even though research on interaction in a physical environment is rare, simulated computer agents playing games (e.g., chess or checkers) together with humans are more common  ",Physical collaborators might support higher motivation and better performance in contrast to the traditional collaboration-based digital serious games with a computer (a virtual entity)  ,This is especially true for HRI-enhanced serious games present a physical entity eliciting diverse behaviors and stronger emotional responses in participants for the robot collaborators in serious games  ,"In contrast to the traditional collaboration-based digital serious games where one is playing together with a computer (a virtual entity), HRI-enhanced serious games present a physical entity eliciting diverse behaviors and stronger emotional responses in participants  ","The authors further state that this might support higher motivation, performance, and focus on a task  ",A collaborative serious game task has been used in this study to extend the research in this domain,There have been a number of previous studies which have used robot collaborators aiding human decisions in serious game settings,"Robots were successfully introduced and used in a social interaction context, inside serious games: the ‘Tic-tac-toe’ serious game where the robot and humans move the game pieces on a physical board  ; for the treatment of autism   and stress  ; and in games and other natural social interactions with humans conveying emotions and robots providing feedback  ","The dynamics between performance and physiological affect might deepen our understanding of human collaboration with robots in serious games, through the detection of underlying emotional states  ","It is only when we perceive those physiological signals in such a way, that we can grasp its implications for the broader scope of understanding elicited physiological affect using social cues in the HRI-enhanced serious games","Peck e al.   used a robotic basketball game to learn the preferences of children with  , where the anxiety levels were implicitly measured using physiology",The authors reported an increased performance and higher positive valence between the robot collaborator conditions,Kim et al.   designed a game of ‘Twenty Questions’ as an interactive task with the robot system successfully detecting emotions,Tapus et al.   created a serious game called the ‘Stress Game’ using the robot which continuously monitored the player’s performance to adjust elicited stress and consequently frustrate players,"The authors reported an increased performance for the lower arousal condition. 3 Hypothesis The previous investigations have shown that engaging physical non-humanoid robot collaborators can elicit emotional responses  , which in turn might influence human decision-making and performance on game tasks  ","Furthermore, humans perceive the observable social cues of their collaborators through the mechanisms from HHI   (i.e., gestured motion and speed  )",These might have an effect on elicited emotions  ,"Following up on these findings, this study investigated the influence of human and non-humanoid robot arm collaborators on the performance in a serious game","Thus, this study takes into consideration the influence of physiological affect in response to human and such robot collaborators on the performance in serious games","Therefore, the following hypothesis is presented: 
                   Previous research suggests that robot collaborators elicit physiological emotions in human partners  ","Moreover, a higher motivation, positive emotions, and sufficient arousal are correlated with higher performance in serious games with physical collaborators  ","Csikszentmihalyi and Bosse   argue that previous findings are valid unless a challenge is sufficiently beyond or below one’s abilities, which might generate anxiety or boredom respectively, resulting in lower performance","It has been found that an engaging environment elicits high physiological arousal, while those higher levels of arousal were correlated with a higher performance  ",These studies warrant further investigation of how the performance on a collaborative task is affected by different dimensions of elicited physiological affect in regard to robot collaborators,"To expand on these investigations, the following hypothesis is presented: 
                   While physical collaborators have been found to elicit a higher motivation on a task, which is correlated with positive emotions  , previous studies have not found a strong correlation between physiological valence and performance on a task  ","To expand on these findings, the following hypothesis is postulated for the investigation of the interactive effects of elicited physiological valence and its influence on the performance in a serious game: 
                   4 Methodology 
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                   4.1 Participants This study included 70 participants, 58 were males and 12 females","The age of participants ranged between 19 and 31, with a mean of (23.60   2.34)","Participants were students of Blekinge Institute of Technology, Sweden","Demographic data (i.e., familiarity with the ToH game task, board games in general, previous possession of a pet, and solving mathematical problems) were collected, and the participants were given a movie ticket as a reward for participating","The experiments were carried out by the Game Systems and Interaction Laboratory (GSIL) at Blekinge Institute of Technology, Karlskrona, Sweden","The Ethical Review Board in Lund, Sweden, has approved all experiments (reference number 2012/737) conducted in this study. 4.2 Experimental setup A crossover study with controlled experiments has been conducted in a laboratory setting",The artificial fixture light was used while the temperature was held constant at 23  C   1  C throughout the experiment,The participants were seated in a chair with a fixed height and a predefined position,The height and position were constant during the experiment,The two experimenters were always present in the room while they were completely hidden behind the screen,Safety monitoring was executed through a live video feed from the camera,"In addition, an emergency stopping sequence and additional fail-safe software were introduced in the case of possible software failures. 
                         
                         
                         
                      
                         
                         
                         
                         
                         
                      4.2.1 Main manipulations The Tower of Hanoi (ToH) game was used as the serious game for the study",The aim of the task was to move the disks from the starting to the final configuration,"The participants played the turn-taking ToH game together with a non-humanoid robot arms collaborator and human collaborator, with identical setup between the trials and conditions","Four manipulations were introduced in the experiment (see  
                            , which were: (a) the   condition where the participants were playing the game on their own; (b) the   condition where the participants played together with the human partner emulating the direct robot collaborator condition; (c) the   condition where the robot was always moving in a similar fashion with a direct path at fixed speed; and (d) the   condition where the robot had one additional non-direct random point inserted in its path while moving at varying speeds",The   has been trained to interact the same way with every participant according to a well-rehearsed procedure,"All of the collaborators were following the algorithm and playing optimally on each move. 4.2.2 Study stimuli The ToH was originally a single-player game, wherein the context of collaborative gameplay between humans and robots each took turns to complete the game in the pace they feel most comfortable with","Since an optimal solution to the game existed, the robot arms were able to easily handle the game, while it was a reasonable challenge for most humans","Following the previous discussion on serious games as tools to collect behavioral data which engage players in a ‘serious’ investigation of certain aspects of human endeavors whose purpose is not pure entertainment, the ToH game was considered as a serious game in this scientific exploration of human-robot collaborative interaction",The rules of this mathematical game constitute of three rods and a number of disks of different sizes that can slide onto any rod,The aim of the game is to start from a given configuration of the disks on the leftmost peg and to arrive in a minimal number of moves at the same configuration on the rightmost peg  ,"In the context of this study, all participants started the first move from the beginning configuration with four disks","The individual trials consisted of moving any single disc to a next legal position, interchangeable between the participants and the collaborator until the final configuration of disks was reached on the opposite peg from the start","The participants always started first and the game was linear, which meant that there was always just one possible optimal step to move the disks towards the final configuration","Most of the participants were naïve to the ToH serious game, and it was a reasonable challenge for most of them","The task was simple to automate since the games including only one human participant where social actions are not required (e.g., chess or checkers), make it possible to create autonomous agents that play optimally  ","Therefore, there was just one possible optimal step at every move that progressed the disks towards the final configuration","The participants had a choice to take this optimal step as their next move or to take the non-optimal step to move the disks in any other legal position, which would not necessarily lead towards the final configuration",The optimal step was mandatory for all of the collaborators,The elicitation of physiological affect was achieved through the gestured motions and the speed of the collaborating robot,"More specifically, the gestured motions for the   were composed of a direct path at a fixed speed of 30 cm/s between the two endpoints of a current disc movement","In particular, humans prefer that a robot moves at speed slower than that of a walking human  ","Furthermore, for the   the gestured motions were composed from a random path and speed between 5 cm/s up to 70 cm/s, where a random point in the path between the two endpoints of a current disc movement was generated on-line and inserted","The random points were randomized on each game move robot arm makes, which totals in three virtual positions the robot arm has to follow while making its move",The robot was passing through all the specified positions before having arrived at a final disc movement position,"The experimenter had been trained according to a well-rehearsed script to interact the same way with every participant, in an optimal fashion at every move","Participants were instructed on the rules and trained through a practice game session with the experimenter until they could finish the simple setup with three disks. 4.3 Experiment procedure A demonstration of the experimental setup is shown in  
                         , where a human and the robot are collaborating on the ToH serious game, sharing the same physical space",It shows the experimental setup for human-robot cooperation with all the physiological sensors attached,"Upon the arrival of a participant, the following procedure was employed: 
                      As suggested in the study from  , the four conditions ( ,  ,   and  ) were presented to each participant","For each of the four conditions a participant repeated each condition three times one after the other (thus in all, a total of 12 ToH games were played per participant)",Each experimental session took around 90 min to complete. 4.4 Data collection ECG was measured using two 16-mm Ag/AgCl spot active electrodes in a three-lead unipolar modified chest configuration,"The ground electrode was positioned on the left earlobe, while the other two electrodes were positioned on the right collarbone and the lowest rib on the left side of the chest","Therefore, the recorded ECG signal was amplified, band-pass filtered at 10–40 Hz, and 16-bit digitized, before the analysis was performed to detect highly-positive R-spikes (heartbeats) in the signal and calculate consecutive R-R intervals  ",The interval outliers resulting from artifacts or ectopic myocardial activity were edited and linearly interpolated,"To provide the strongest signal variations, GSR was measured using surface electrodes attached to the palmar surface of the middle phalanges from the middle finger and the index finger of the non-dominant hand (to reduce mechanical pressure susceptibility)",The participants washed their hands with water and soap before the electrode placement,The temperature and humidity were held constant across the sessions because of GSR susceptibility to their influence,"Both physiological signals were acquired using Biosemi Active Two 
                          physiological data acquisition system and its accompanying ActiView 9 software",The sampling rate was fixed at 2048 Hz for all channels,"As described, appropriate amplification and band-pass filtering were performed, and the signal was subsequently down-sampled to 256 Hz upon data reduction. 4.5 Data reduction and analysis The data reduction was performed using Ledalab software for GSR  , while Kubios software   and the HRV Toolkit 
                          were used for ECG","Such data were compared across the condition differences ( ,  ,   and  ) and the individual differences for the same trials",GSR was measured in microsiemens (muS) and analyzed offline,"GSR includes short-term phasic responses to specific stimuli, and relatively stable longer-term tonic levels  ","In continuous stimulus settings, the most common measures of GSR are   (SCL) and   (SCR), where their changes are thought to reflect general changes in autonomic arousal  ","The authors stated that the SCR signal is suitable for assessing the intensity of single (phasic) emotions, but changes in the overall (tonic) level are rather inert, thus valid for the trials longer than two minutes, such as the overall session in this experiment",Changes in arousal within periods shorter than two minutes are not likely indicated using the SCL,This problem is particularly limiting in trials shorter than two minutes,"When the SCL temporal precision is insufficient, the   (NS-SCR) seem to indicate a more promising focus: their number during a given time period is a prominent phasic-based indicator of arousal  ",The raw GSR signal was down-sampled by a factor of eight (from 2048 Hz to 256 Hz) to remove the high-frequency measurement noise and then smoothed by a 25 ms moving average window,The continuous decomposition analysis was performed,The phasic skin conductance detection algorithm used the following heuristics for the valid peak identification for a particular SCR: the slope of the rise to the peak should have exceeded 0.05 muS/min; the amplitude should have exceeded 0.05 muS; and the rise-to-peak time should have exceeded 0.25 s,"The standard threshold was set to a minimum amplitude of 0.05 muS  , because any function of NS-SCR and amplitude remains highly sensitive to the threshold that distinguishes it from noise","Such noise may be a consequence of the instruments’ quality, the environment, or interpersonal differences","Once the phasic responses were identified, the rate of responses was determined",All the signal points that were not included in the response constituted the tonic part of the SCL signal,The slope of the tonic activity was obtained using linear regression,Another feature derived from the tonic response was the mean tonic amplitude,The raw ECG signal was filtered using a high pass filter with a cutoff frequency of 0.1 Hz,"The R-peak detection algorithm performed band-pass filtering of the raw ECG signal, and the signal was then smoothed by a 10 ms moving average window","The peaks were then detected in the resulting signal, and the detection heuristic rules were applied to avoid missing R peaks or detecting multiple peaks for a single heartbeat",These rules included obtaining the amplitude threshold (the difference between a peak and the corresponding inflection point) at which a peak is considered a beat: enforcing a minimum interval of 300 ms and maximum interval of 1500 ms between the peaks; checking for both positive and negative slopes in a peak to ensure that the baseline drift is not misclassified as a peak; and backtracking with the reexamination/interpolation when a missing peak was detected,"Generally, the average change of a heart rate is expected to range between 2 and 15 bpm  ",The chosen interval threshold between the peaks was well above the rate of heart rate change due to the genuine heart acceleration,Data were visually inspected for the artifacts which were subsequently corrected,The R-R intervals were extracted,"The time-domain features of  , such as the mean and standard deviation, were computed from the detected R peaks","Inter-beat interval variability was explored by performing a power spectral analysis using inter-beat interval data to localize the sympathetic and parasympathetic nervous system activities associated with the different frequency bands. 
                          (HRV) is highly correlated with emotions  ",Two measures of HRV are the   in the time domain (SDNN) and the   (LF/HF)  ,"Wang and Huang   stated that SDNN and LF/HF were employed as two dimensions in the physiological valence/arousal model, where evidence revealed that SDNN was a good physiological indicator of valence  ",The total variance of HRV increased with the length of analyzed recordings  ,"Thus, in practice, it was inappropriate to compare the SDNN measures obtained from the recordings of different durations","However, the duration of recordings used to determine the SDNN values (and similarly, the other HRV measures) was standardized to a minimum of 5 min recordings","Generally, the SDNN levels for the participants with positive affect were found to be higher than for negative one  ","For even shorter recordings, main spectral components of the LF/HF ratio were distinguished in a spectrum calculated for the short-term recordings from 2 to 5 min  ","Ectopic beats, arrhythmic events, missing data and noise effects could have altered the estimation of HRV; therefore the proper interpolation (or linear regression or similar algorithms) on the preceding/successive beats on the HRV signal or its auto-correlation function could have reduced this error  ","The previously described interpolation steps were performed in Kubios software, while the variables of Heart rate, HRV, SDNN, and LF/HF ratio were extracted in the HRV Toolkit. 4.6 Hardware and software systems The hardware system contained two Adept Viper S650 
                          6 DOF robot arms with Robotiq Adaptive 2-finger Grippers 
                          as the end effectors","The Adept ACE software was used to control the robot arms, to pick up and drop the specified game disk, and to close/release the grippers",The Microsoft Kinect camera was used to track the moves made by the humans and robots during the ToH game,"A camera was also used for the surveillance of the participants and the robot arms, in case of an emergency","A single PC running Windows 7 was controlling the system. 5 Results Prior to the analysis, both ECG and GSR data were normalized and standardized, while the outliers were removed using z-scores for standardized values of  3.0 or greater","One-way and two-way ANOVA was used to analyze the differences across groups and conditions, while Pearson product-moment correlation index and Spearman’s rank-order correlation were used to analyze correlations in the data",The analysis was performed using SPSS software with the alpha level set at  ,"The data showed no violation of normality, linearity, or homoscedasticity","The participants reported previous experience with robotics on a seven-point Likert scale, where 1 meant ‘no experience’ and 7 meant ‘familiar experience’ (  = 1.7   = 1.095   = 70)","The differences between the participants were analyzed based on the reported values, and the experienced outliers were identified","Therefore, six participants from the experienced outliers group were removed from the analysis to exclude the effects of participants’ familiarity on the experience with the robot collaborators, which resulted in the 64 valid data samples","Moreover, 43 of 70 participants have not had any previous experience with the ToH. 
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                   5.1 Collaborator conditions and performance All of the collaborator conditions were reported as comparable in performance, as there was no significant difference in the total number of moves to reach the final configuration of disks between the human and robot collaborator condition groups ( (1,569) = 3.705,   >.05), shown in  
                          where a higher value reflects worse performance","In contrast, a significantly higher number of moves in the   condition was found ( (3,751) = 20.807,   <.001) since the participants were not expected to know the most optimal solution for the ToH game task",This evidence does not land support for the (H1) regarding the investigation to understand the impact of the collaborator conditions on the performance on a game task,"Therefore, an alternative hypothesis was preferred that there was no significant difference in the performance between a human and robot collaborator conditions. 5.2 Physiological arousal A series of analyses were performed to understand the impact of elicited physiological arousal on the performance on the game task for each collaborator condition (H2a and H2b)","The ToH serious game was found to elicit a high physiological arousal overall (SCL) of 765.209 muS (  = 835.545 muS), normalized against the baseline","Furthermore, there was a significant difference between the collaborator conditions regarding arousal ( (3,744 = 58.881,   <.001), as shown in  
                          and detailed in  
                         ","For simplicity, both   condition groups were merged into a single data sample and compared against the  ","A Tukey post hoc test revealed that the physiological arousal indicator NS-SCR was significantly higher for the   (19.59   8.05,   =.03) compared to the   (17.43   6.76) condition","Moreover, both robot collaborator conditions were significantly higher (18.51   7.49,   <.001) than the   (11.62   9.11) and the   condition (11.01   6.08)","The evidence suggests that robot collaborators elicited a higher physiological arousal compared to the human collaborator, lending support to H2a","Moreover, the   elicited a higher arousal than the   one, but it made no influence on the performance","Interestingly enough, a significant negative correlation was found between physiological arousal indicator NS-SCR values during baseline and the performance ( 
                         (61) = −.286,   =.023)",The evidence suggests that the participants who were more aroused during baseline measurement prior to the task performed better at it,"Furthermore, there was a significant difference between physiological arousal (SCL) during baseline and pet possession ( (2,61) = 13.47,   <.001)",Where the physiological arousal at baseline prior to the task was significantly higher for the participants that owned a pet 7074.54 muS (  = 2797.307 muS) compared to the ones that have not 4966.337 muS (  = 1931.829 muS),"The worse performing participants reported higher arousal values after each round, as a significant positive correlation was found between the number of moves per round and the physiological arousal indicator NS-SCR values (  =.179,   = 739,   <.001)","Considering both this evidence and ones from the previous paragraph, it seems that there is a difference within the elicited physiological arousal that showed an effect on the performance on the game task, while there might be a more complex effect regarding different collaborator conditions","To investigate this problem, a significant interaction was identified between the effects of the collaborator conditions and physiological arousal on the number of moves (performance) on the game task ( (2, 556) = 8.902,   <.001)",The participants were grouped based on lower and higher physiological arousal elicited,"The simple main effects analysis showed that the collaborator conditions significantly affected the performance when physiological arousal was lower (  <.001), with better performance associated with both robot collaborators compared to the  ","Between the robot collaborators, better performance was associated with the   (  <.001)",The presented evidence landed support that there was a difference within the elicited physiological arousal for different collaborator conditions that showed an effect on the performance on the game task (H2b),"Furthermore, a significant interaction was identified between the effects of the collaborator conditions and their order of presentation on the physiological arousal (SCL) ( (1, 396) = 3.815,   =.05), as well as physiological valence (SDNN) ( (1, 405) = 22.509,   <.001)","The simple main effects analysis showed that the order of the robot collaborator conditions significantly affected both physiological values for the robot collaborators (  <.001), with lower valence and higher arousal associated with first administered   following with   order, compared to the higher valence and lower arousal associated with it in the reversed order, as shown in  
                          (a) and (b) respectively. 5.3 Physiological valence A series of analyses were conducted to understand the impact of elicited physiological valence on the performance on the serious game task for each collaborator condition (H3a and H3b)","The ToH serious game was found to elicit a high (positive) physiological valence overall (SDNN) of.624 s (  = 1.07 s), normalized against the baseline","Furthermore, there was a statistically significant difference between the collaborator conditions regarding valence ( (3,732) = 3.575,   =.014), as shown in  
                          and detailed in  ","A Tukey post hoc test revealed that the valence indicator LF/HF was significantly higher for the   (3.081   1.869,   =.008) compared to the   (2.453   1.724)","Both robot collaborator conditions were significantly higher (2.967   1.841,   =.007) than the   condition (2.453   1.724)",There were no statistically significant differences between the   and the   (  >.05),"The evidence suggests that the participants seemed to elicit a higher (positive) physiological valence for the  , compared to the   condition and the  ","These findings suggest that elicited physiological valence was affected by the collaborator conditions, lending support for the H3a","Overall in the experiment, the participants were found to perform the task equally well regardless of the physiological valence found, as no significant correlation was found between the valence indicator LF/HF and the number of moves (  >.05)","These results provided support for the H3b and indicated that while there were differences across the collaborator conditions regarding the elicited physiological valence, this had no effect on the performance. 5.4 Limitation The collaborative task between humans and robots in the context of serious games might have limited the generalizability","Therefore, future studies should investigate further towards any collaborative task context between humans and robots",The participants’ cohort in this study consisted entirely of college undergraduates and graduates,"Therefore, future research should deepen the understanding of different populations interacting with the robot collaborators, as social robots get integrated into various aspects of life","The time on the task for each participant was around 30 min, with 90 min total for the whole experiment","Even though such time was comparable with other similar studies reported, it might have resulted in the fatigue effects in this experiment","Nevertheless, the effect was assumed to be minimal, as results gave evidence of an increased performance during the later trials of the game task for each participant. 6 Discussion This study investigated physiological affect and the performance of participants in a proximate interaction with the human and non-humanoid robot arm collaborators on the serious game task","Furthermore, it examined the effects of elicited physiological arousal and valence on the human-robot collaborative performance on the game task","This investigation was based on the social cues of the collaborating robots (i.e., gestured movement and speed)","The results show that the collaboration with non-humanoid robot partners might be as effective as a collaboration with human ones, where the worst performing individuals were found in the   condition","These findings did not lend support to H1, as there was no significant difference in participants’ performance between any of the collaborator conditions","Motivation could be that, collaboration with physical entities eliciting diverse behaviors and strong emotional responses might have promoted a higher focus on the game task at hand  ","The results further show that both robot collaborators elicited higher physiological arousal than the   one, with the   being significantly higher than the  ","These results support previous investigations on social cues in the context of HRI, where the participants were found to be sensitive to the robots’ social cues regarding physiological arousal in the context of collaborative serious games  ","The results also indicate that high physiological arousal is correlated with worse performance in the context of collaborative serious games, which is motivated by the previous investigations on the connection between arousal and performance on a task  ","In their investigations, the authors state that the performance is positively correlated with physiological arousal up to the point when the level of arousal becomes too high, and the performance decreases",This notion motivated the investigation that there might have been high physiological arousal elicited in this study,"Results show that the ‘lower’ physiological arousal group showed a significant effect of collaborator conditions on the performance, while the ‘higher’ physiological arousal group had no such effect","Taking these findings into consideration, it seems that the performance on the collaborative serious game task is affected by both collaborating partners and elicited physiological arousal, landing support for both H2a and H2b","These findings also indicate that serious games might elicit high physiological arousal, which may have disrupting effects on the performance on the game task, invalidating all the potential benefits of collaborating partners","On another note, better performance was found in the participants who had higher physiological arousal during baseline","Taking the resting nature of baseline measurements, this motivates the notion that these individuals might have been already aroused and motivated to participate in the study, which might have placed them initially at the more optimal position of the arousal-performance bell-shaped curve  , where some amount of arousal is needed for optimal performance on the task","These finding may be further supported by the notion that the individuals who previously owned a pet were found to have higher physiological arousal at baseline, compared to the ones that have not",The autonomous nature of pets might have contributed to the higher arousal and excitement of these individuals for the experiments with the robot partners,The results show that the   elicited higher (positive) physiological valence compared to the   on the serious game task,These findings indicate that people are sensitive to robots’ social cues regarding physiological valence in the context of collaborative serious games,"Nevertheless, physiological arousal was found to have a more profound effect on the performance in the context of collaborative serious games, compared to physiological valence, as the participants performed equally well regardless of the elicited physiological valence",These findings are supported by the previous studies exploring characteristics of robot behavior in HRI  ,"These findings lend support for both H3a and H3b, where collaborative partners influenced the valence on the task, but without affecting the performance","On another hand, for the ‘lower’ arousal participants, better performance was associated with the  , which in turn elicited higher (positive) physiological valence",Results further showed ordering effects where the presentation of collaborator conditions significantly affected elicited physiological affect,The first robot collaborator presented in the experiment was associated with lower valence and higher arousal compared to the second robot collaborator presented,"The participants positively experienced the robot collaborators after they have been already introduced to them for the first time, as well as a relaxing experience","This finding is further motivated by the report on the previous experience with robots where most of the participants were not familiar with them, and this might have been the first time they collaborated with one. 7 Conclusion Contrary to the standard Wizard-of-Oz approach to studies regarding robots and users  , this study was conducted in a realistic setting without an obvious presence of the experimenter",The contributions include the advances in both theoretical and practical understanding of physiological affect in the context of HRI,"As well as, the design of a serious game using non-humanoid robot arm collaborators which elicited physiological affect essential to such context",The motivation for this investigation lies in a direct link between the embodiment of physiological affect and information provided through the social cues of collaborating partners,This link is dependent on physical interaction and a serious game environment itself  ,"Therefore, this research outlines a method for the objective measurement of the physiological affect in collaborative HRI, applied in the context of serious games","Moreover, it supports the notion that even non-humanoid robot collaborators can display social cues and elicit physiological affect in their human partners","Overall, the collaborators in this study created a physiologically arousing and highly (positive) valenced serious game environment","Regarding the H1, the findings indicate that the participants’ performance on the serious game task is comparable between the human and robot collaborator conditions, as the number of moves was consistent across all the collaborator conditions","Therefore, the collaborator condition (i.e., human, robot) may not have affected the performance on the collaborative task in a serious game","Regarding the H2 and H3, this study found evidence of higher physiological affect elicited with the robot collaborators (arousal and valence) in contrast to their human collaborator counterpart","Therefore, elicited physiological arousal and valence may have been affected by the collaborator conditions between humans and robots on the collaborative task in a serious game","Nevertheless, while arousal may affect the performance on such game task, valence may not have such a significant effect","These findings motivate the introduction of autonomous robots as partners in the context of collaborative serious games, where the same performance benefits may be achieved as with using the human ones, which would motivate their introduction as partners on the task","The   condition elicited higher physiological arousal and a (positive) valence, compared to the  ","Moreover, it elicited a higher physiological arousal than the   condition, indicating that the careful design of robot partners might leverage different social cues to elicit target physiological arousal in the context of collaborative serious games","Furthermore, such context may witness a more positive valence elicited when using   partners instead of human ones","This may be important as robots get introduced to different aspects of human lives, possibly as team members  ","Nevertheless, one has to be careful as to which robot partners get introduced to naïve participants, as ordering effects showed lower arousal and more positive valence for the robot collaborator which was introduced after the initial robot condition",It is important to note that people might elicit lower arousal and positive valence for their robot partners after they have been introduced to them once,This might become a prevalent issue as robot partners become pervasive in society,"Furthermore, the results also showed that people who owned a pet were more positive and excited to collaborate with robots on the same task",Such individuals who were initially starting the task with a certain amount of arousal performed better on a decision-making task in serious games,"The current study supports the notion that understanding physiological affect underlying such collaborative HRI from the human perspective, it would be possible to design more personalized serious games with intelligent robots which act together with human partners eliciting relevant physiological affect  ",This may contribute to improving the quality of HRI informing the design of such collaborative serious games,"On the other hand, one has to be careful when designing serious games which elicit high physiological arousal, as such high levels of physiological arousal may be correlated with lower performance  ","In contrast, physiological valence may not have such a significant effect","The results showed that more aroused individuals prior to the task, performed better on the task, giving further evidence for the introduction of robot partners on the task, at least at this novelty stage","Nevertheless, valence proved to be a more complex issue, where individuals who had optimal ‘lower’ arousal on the task were able to benefit from higher ‘positive’ valence and increase their performance","If one considers designing serious game environments that elicit lower physiological arousal using robot instead of human collaborators, than one might witness an increase in the performance on a serious game task","As this study found evidence that the better performance was associated with the robot collaborators compared to the human ones, only for the ‘lower’ physiological arousal group which was the only one showing the statistically significant effect of the collaborator conditions on the performance","Finally, if one considers that robots possess the physical-virtual duality and have access to a game task information, one can clearly see why a choice of robot collaborators in serious games would be a sound choice","Taking a step forward with using the physiological measurements, recognition of affective states is expected by cooperating humans, which may allow them to be aware of their emotions through the presentation of sufficient feedback  ","Future studies should investigate the recognition of participants’ emotions on-line using physiological measurements to adapt the robots’ behavior in a closed-loop social interaction  , as the embodiment is a powerful concept in the development of the adaptive autonomous systems  ",Declaration of Competing Interest The authors declared that there is no conflict of interest.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s0957417416306844," 1 Introduction Clustering plays a basic role in many parts of data engineering, pattern recognition, and image analysis","Some of the most important clustering methods are based on GMM, which in practice accommodates data with distributions that lie around affine subspaces of lower dimensions obtained by principal components (PCA) ( ), see  
                      (a)","However, by the manifold hypothesis, real world data presented in high dimensional spaces are likely to concentrate in the vicinity of non-linear sub-manifolds of lower dimensionality ( )",The classical approach approximates this manifold by a mixture of Gaussian distributions,"Since one non-Gaussian component can be approximated by a mixture of several Gaussians ( ), these clusters are, in practice, represented by combination of Gaussian components","This can be seen as a form of piecewise linear approximation, see  (a)","A similar result gives the Cross Entropy Clustering (CEC) ( ) approach, see  (b)","In our paper we construct a general afCEC (active function Cross-Entropy Clustering) theory, which allows the clustering of data on sub-manifolds of  ","The motivation comes from the observation that it is often profitable to describe non-linear data by smaller numbers of components with more complicated curved shapes to obtain a better fit of data, see   (for more detailed analysis see  )","While developing this theory, we were influenced by the classical Shannon Entropy Theory ( ), Minimum Description Length Principle ( ), Cross-Entropy Clustering ( ), Expectation Maximization (EM), Implicit Function Theorem ( ), and Active curve axis Gaussian Mixture Models ( )","Consequently, we present a theoretically-motivated clustering method that automatically reduces unnecessary clusters and accommodates non-linear structures","Because we have to approximate complicated structures in each step, we have to construct a numerically efficient model","Therefore, we have chosen an approach that allows for the use of an explicit formula in each step",This paper is arranged as follows,"In the next section, we present related works",Then the theoretical background of the density model will be presented (corresponding to the case of one cluster),"In the fourth section, we introduce the theoretical background of the afCECmethod","We prove that the cost function decreases in every iteration, see  ",The last two sections we present numerical experiments. 2 Related works Density based clustering was studied by  ;  ;  ;  ;  ;  ,One of the most important clustering algorithms is based on Gaussian Mixture Models ( ),"It is hard to overestimate the role of GMM in computer science ( ); it includes object detection ( ), object tracking ( ), learning and modeling ( ), feature selection ( ), classification ( ), and statistical background subtraction ( ). 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   2.1 Study of nonlinear data Most of algorithms based on Gaussian distribution can be interpreted as a linear approximation of data (see  )","Since typically data in practice lies around curved structures (manifold hypotheses), algorithms which can approximate curves or manifolds are important",Principal curves and principal surfaces ( ) have been defined as self-consistent smooth curves (or surfaces in  ) which pass through the middle of a  -dimensional probability distribution or data cloud,They give a summary of the data and also serve as an efficient feature extraction tool,Principal curves/surfaces algorithms are typically capable of expressing a single complex manifold,"If the data lies on complicated manifolds in higher dimensional spaces, these algorithms cannot fit a single manifold without special initialization",Another method that attempts to solve the problem of fitting nonlinear manifolds is that of self-organizing maps (SOM) ( ) or self-organizing feature maps (SOFM) ( ),"These methods are types of artificial neural networks which are trained using unsupervised learning to produce a low-dimensional (typically two-dimensional) discretized representation of the input space of the training samples, called a map",Self-organizing maps are different from other artificial neural networks in the sense that they use a neighborhood function to preserve the topological properties of the input space,Kernel methods provide a powerful way of capturing non-linear relations,"One of the most common, kernel PCA (KCPA) ( ), is a non-linear version of principal component analysis (PCA) ( ) that gives an explicit low dimensional space such that the data variance in the feature space is preserved as much as possible",The above approaches focus on finding only a single complex manifold,"In general, they do not focus on the clustering method; furthermore, it is difficult to use them for dealing with clustering problems",Kernel methods and self-organizing maps can be used as a preprocessing for classical clustering methods,In such a way spectral clustering methods was constructed ( ),The classical kernel  -means ( ) is equivalent to KPCA prior to the conventional  -means algorithm,Most of kernel methods consist of two steps: an embedding into a feature space and a classical clustering method used on the data transformed to feature space,"Therefore spectral methods are typically time consuming and use large number of parameters. 2.2 Acagmm 
                          present an adaptation of the Gaussian Mixture Model called the Active curve axis Gaussian Mixture Models (AcaGMM), which uses a nonlinear curved Gaussian probability model in clustering","Since our paper aims to solve the same task as AcaGMM, but in a greater generality, let us first explain what AcaGMM is","In its standard version, it works with data on the plane and adapts to quadratic curves","In other words, AcaGMM uses a wider class than typical Gaussians – namely Gaussians which are curved over parabolas","AcaGMM works well in practice; however, it has major limitations","First of all, the method is not a density model. 
                          Consequently, one can not use the EM procedure to minimize the AcaGMM cost function",The incorrect use of the EM method causes fundamental problems,"The AcaGMM cost function does not necessarily decrease with iterations, which causes problems with the stop condition",We also do not obtain density estimation as in a correctly used EM procedure,Detailed description of AcaGMM model we present in  ,It is possible to partially correct the model by taking in to consideration the Jacobian,"However, even such a modification does not help because AcaGMM uses PCA and regression in each cluster. 
                          Moreover, AcaGMM is naturally restricted to quadratic functions. 
                          Furthermore, the use of the method in dimensions higher then two, although theoretically possible, is impractical from a numerical point of view","In this paper, we propose the Active Function Cross-Entropy Clustering (afCEC) method (see  (d)) which is based on the Cross-Entropy Clustering (CEC) model. 
                          Thanks to the simplicity of the density model, we are able to construct an algorithm which is easy to adapt to the case with a higher dimension","Moreover, the afCECmethod is able to reduce unnecessary clusters","In  
                         , we present the convergence process of afCECwith the initial number of clusters at   which is reduced to  ","The afCEC algorithm uses densities, and therefore we can interpret it as a density estimation, see  
                         ","Consequently, we can compare it with other density based algorithms like GMM or CEC by using the log-likelihood function","Experiments on synthetic data, Chinese characters, data from the UCI repository and from wind turbine monitoring systems (see  ) show that afCEC better describes the intricate structure of data by using fewer parameters. 3 Theory: adapted gaussians In this section, we focus on  -adapted Gaussian distributions, where   is a continuous function",The goal of this approach is to transform a normal distribution (which assumes the intrinsic linearity of the model) to the case of manifolds given by the graph of the function  ,"The above model will be used in the afCECmethod as a representation of each cluster. 
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   3.1 Toy example in  
                      We begin with an illustration of our idea in the two-dimensional case","Let us recall that a two-dimensional Gaussian density with mean   and covariance matrix   is given by the following formula:
 where in the one-dimensional case we have:
 
                      In the case of a “curvilinear” coordinate system, we adapt the Gaussian density to the arbitrary given function  ","For each point, we use the Euclidean distance along the second coordinate to curve   instead of applying the coordinates on the canonical basis
 see  ","Although at first the difference between formulas   and   seems small, the above modification allows us to describe nonlinear data",Level sets of curved Gaussian distributions in the case of different functions in   are presented in  ,"For the convenience of the reader, and to present the basic difference between afCEC and AcaGMM we will briefly describe the AcaGMM model","Since AcaGMM works in the two-dimensional case (in cases with higher dimensions, the authors use PCA to reduce problems to 2D) with parabolas (  for  ), we will restrict our discussion to this situation",Let   for   be given and let  ,"The AcaGMM approach uses the orthogonal projection of the point x onto the parabola  , which is denoted by  (x), and the arc length between  (x) and m, which is denoted by  ( (x), m)","Consequently, the AcaGMM function is given by
 Although this approach is very intuitive, it causes some basic problems",It is very hard (or even impossible) to give explicit formulas for orthogonal projections and arc lengths for more complicated curves in higher dimensional spaces,"Thus calculations are complicated (from a numerical point of view) and, consequently, possible generalizations of AcaGMM are limited","Moreover, the function which was used in AcaGMM, see formula  , is not a density, as the Jacobian of the respective transformation was not included","The practical difference between AcaGMM and our approach in   in the case of one cluster described by a parabola is rather small 
                         , see  
                         
                         ","Nevertheless, our model is more flexible as we can use an arbitrary class of functions in an arbitrary dimension for which the least square methods work, see  . 3.2 
                         -Adapted gaussian density In this subsection, the general notion of the  -adapted Gaussian is presented","Moreover, we show (see  ) that under weak assumptions, the class of adapted Gaussians contains a class of classical normal distributions","Let us recall that the standard Gaussian density in   is defined by
 where m denotes the mean,   is the covariance matrix, and   is the square of the Mahalanobis norm","In our work, we use a multidimensional Gaussian density in a curvilinear coordinate system which is spread along the function   ( -adapted Gaussian density)",We treat one of the variables separately,"In such a case we consider only those   (where   denotes the set of  -dimensional square, symmetrical, and positive define matrices) which have the diagonal block matrix form
 where   and   > 0","For   and   we will use the notation
 
                      Now, we will give the mathematically formal definition of the  -adapted Gaussian function.
 
                      Level sets for  -adapted Gaussian distributions with respect to different types of functions are presented in  ","The  -adapted Gaussian function is a density model, see  , which has profound consequences for the convergence of the minimization procedure.
 
                         
                      In the basic form of the CEC algorithm, we are looking for the optimal Gaussian function in the family of all  -dimensional Gaussian densities  ","In the case of afCEC, we describe each cluster by the  -adapted Gaussian function","Consequently, we need to find optimal density in the class of all curved Gaussians","For the given   we denote the   by
 
                      In the afCEC algorithm, we describe clusters by generalized Gaussian distributions from   where   is in some class of functions (we can use any class of functions for which the regression procedure works) and  ","Therefore, we will need one more definition","For the family   we define
 
                      If   contains linear functions, then the family coincides with the class of all classical Gaussian distributions.
 
                         
                      This implies that afCECis a natural extension of the standard CEC algorithm, because for   containing only linear functionals, we obtain exactly the standard Gaussian densities","On the other hand, for wider classes of functions   we can detect curved clusters, which describe groups concentrated around manifolds which are not necessarily linear","The following observation is a direct corollary from  .
 
                      In the previous considerations, we assumed that one variable was chosen to be dependent","Since, in the case of the  -adaptive Gaussian density, all computations are applied in the canonical basis, we can verify all possible dependent variable choices","Our idea for checking all coordinates in the canonical basis came from the Implicit Function Theorem ( ).
 
                      The above observation explains the intuition connected with checking all coordinates in the canonical basis instead of finding a local coordinate system","Let us now present an example of such a procedure.
 
                      In the above example, we consider only   but for   we have to consider   different possible choices of dependent variables","For the family   we define the family of  -adapted Gaussian distributions with all the possible choices of dependent variables by
 
                      4 Theoretical background of afCEC In this section, the theoretical background of afCECwill be presented","First, we introduce the cost function that will be minimized by the algorithm","Then, we prove that the optimal function describing each cluster can be obtained by the least square regression ( )",We will end by describing the full algorithm of afCEC,Our method is based on the CEC approach,"Therefore, we start with a short introduction to the method (for a more detailed explanation we refer the reader to  )","Since CEC is similar to EM in many aspects, let us first recall that, in general, EM aims to find  
                       and   Gaussian densities (where   is given beforehand and denotes the number of densities for which the convex combination builds the desired density model) such that the convex combination
 optimally approximates the scatter of our data   with respect to the MLE cost function
 The EM procedure consists of the Expectation and Maximization steps","While the Expectation step is relatively simple, the Maximization step usually needs complicated numerical optimization even for relatively simple Gaussian models ( )","A goal of CEC is to minimize the cost function, which is a minor modification of that given in   by substituting the sum with the maximum:
 Instead of focusing on the density estimation as its main task, CEC aims itself directly at the clustering problem","It occurs that at the small cost of having a minimally worse density approximation ( ), we gain speed in implementation 
                       and the ease of using more complicated density models","Roughly speaking, this is more advantageous because the models do not mix with each other since we take the maximum instead of the sum","To apply CEC, we need to introduce the cost function which we want to minimize","In the case of splitting   into   so that we code elements of   using a function from the family of all Gaussian densities   the mean code-length of a randomly chosen element   equals
 where  ",The formula uses the Cross-Entropy of a data set with respect to the family  ,The aim of CEC is to split of dataset   into sets   which minimize the function given in  ,"Our goal is to calculate an explicit formula for the cost function in the case of  -adapted Gaussian densities. 
                      
                      
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                      
                   4.1 Cost function of one cluster In this subsection we will focus on the situation of one cluster  ","In such a case, we usually understand the data as a realization of a random variable, where we use
 as an estimator for the mean and covariance","For   we denote   the set containing vectors from   with removed   coordinate, and   For the function   we put
 
                      As was previously mentioned, CEC uses the Cross-Entropy of data set   with respect to the Gaussian family  .
 
                      The CEC algorithm will be used for a family of  -adapted Gaussian densities","In such a case, the cost function is described by the following theorem.
 
                         
                      As a corollary from the above theorem, we obtain the optimal from the Cross-Entropy point of view function for   which describes a cluster that can be obtained by the least squares method ( ).
 
                      Consequently, we minimize cross-entropy by finding a least squares estimation","Moreover, if   is a set of functions which are invariant under the operations   for any  , it is enough to find  
                         
                      The above theorem guarantees that the cost function is decreasing during iterations when we apply the regression procedure for extracting a function which describes clusters",The analogue of this result does not hold for AcaGMM (PCA is used for finding a local coordinate system),"Consequently, in afCEC(contrary to AcaGMM), we are able to construct a simple stop condition",When we have all of the possible dependent variables we need to find the optimal one,"More precisely, we find parameters which minimize Cross-Entropy respectively to family  
                         
                      In conclusion, for one cluster   we estimate the parameters of the model in two steps","First, we consider all of the possible choices of dependent variables and calculate functions   (corresponding with relations  ), means  
                          and covariances  
                          for  ","More precisely, we find  -adapted Gaussian distributions
 which realize a minimum of cross-entropy
 for  ","Then we determine the optimal dependent variable
 Consequently, our data set is represented by the function, mean, and covariance matrix
 where subscript   denotes the dependent variable in the cluster","The above parameters minimize the cost function of one cluster  . 4.2 Optimization problem and afcec algorithm In the previous subsections we presented the process of describing one cluster by optima parameters: means, covariances, and regression functions by verifying which coordinates should be dependent","Now, we are ready to introduce the afCEC optimization problem.
 
                      Since our cost function is given by cross-entropy, it is bounded from below by the Shannon entropy ( )","Moreover, typically to most clustering problem it has many local minima’s","Our goal is to find the optimal one, which value is close to the total minimum of the cross-entropy function",It is NP-hard problem ( ) to find the optimal clustering for  -means even in the scalar case,"Since our method generalizes CEC, and CEC in the limiting case reduces to  -means  , it is also an NP-hard problem to find the optimal solution","Thus, similarly to classical  -means, result of afCECstrongly depends on the initialization","To avoid poor initializations, we use a  -means++ approach, which was introduced by  ",We also start our algorithm at least ten times and choose the optimal solution,Lloyd’s method uses two steps which are applied simultaneously,"In the first one, we estimate the parameters of means, covariances, and regression functions","It is important to verify all of the choices of dependent variables and use   which minimizes Cross-Entropy
 In the second step, we construct a new division of   by adding the points to the closest cluster, or, more precisely, to the closest curved Gaussian density","Consequently, we assign the point x ∈   to the cluster   such that
 is minimal","We apply the above steps simultaneously until the change of cost function   is small, see  
                         ",We compared the computational times between afCECand alternative methods: CEC implemented in R package   ( ) and GMM from R package   ( ),"We varied the number of data set instances and the dimension of data, see  
                         ",In the case of acaGMM we do not have the original author’s implementation,"Since our implementation of acaGMM gives much worse results then afCEC, we decided to not provide it in the comparison, since it can be caused by the non-optimality of our implementation",For this purpose a simple mouse-like set with 3 well separated Gaussian component was considered,One can observe that afCECgives similar results to   and  ,"In the case of afCECwe have to apply a regression in each cluster therefore it is slightly worst. 5 Experiments In this section, we present a comparison of the afCECwith density based methods AcaGMM, GMM, CEC","Since AcaGMM is not a density model, the log-likelihood function is not well-defined","Nevertheless, by inputing the Jacobian of the AcaGMM transformation, we obtain a valid probability distribution",This modification was applied in order to compare the methods,"To compare the results, we use the standard Bayesian Information Criterion (BIC):   and Akaike Information Criterion (AIC):   where   is the number of parameters in the model,   is the number of points, and   is a maximized value of the log-likelihood function","Consequently, we need a number of parameters which are used in each model","In the case of   AcaGMM uses two scalars for mean, three scalars for covariance matrix, two scalars for parabola, and one for the local coordinate system (obtained by PCA)","On the other hand, in afCECwe do not need scalars for the local coordinate system","Consequently, afCECuses two scalars for mean, three scalars for covariance matrix, and two scalars for parabola. 
                      
                   
                      
                      
                      
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                   
                      
                      
                      
                      
                      
                      
                   5.1 Synthetic data set Let us start from a synthetic data set","First, we report the results of afCEC, AcaGMM, CEC and GMM in the case of a circle-type set, see  
                         .  (e) shows how the log-likelihood function changes when the number of clusters increases from 1 to 10","A similar relation, with respect to the number of parameters 
                          is presented in  (f)","For a similar values of the log-likelihood function, we need two clusters in afCECand AcaGMM and four in GMM and CEC, see  ","In such a case, the BIC criterion shows that algorithms which use the curved densities model fit data better using a smaller number of parameters","A similar situation can be observed in the more complex case of a spiral-type set, see  
                         ","In  
                         , the mean (with standard deviation) and maximum value of Log-likelihood for 100 initializations of algorithms are shown","As we see for similar values of the log-likelihood function, we have to use nine clusters for afCECand AcaGMM and fourteen for GMM and CEC","We present the comparison of algorithms by using BIC and AIC with similar values of the log-likelihood function in  
                         ","Algorithms which are able to adapt to curve type structures (AcaGMM, afCEC) fit data better","More precisely, the log-likelihood function takes a larger value with the same number of parameters, see  (f) and  (f)","Since Log-likelihood increases with the number of classes, we use BIC criterion, which takes into account the number of parameters","In the case of AcaGMM and afCEC, we obtain the optimal value of BIC after about 4–6 iterations","In conclusion, AcaGMM and afCECfit data better in that they yield a higher value of the Log-likelihood function while requiring a lower number of parameters","Algorithms AcaGMM and afCECgive a comparable value of log-likelihood, see  (e) and  (e)","Nevertheless, afCECuses less parameters, see  (f) and  (f)","Moreover, the strong theoretical background of this method guarantees that the cost function decreases in each iteration","Consequently, we obtain a simple stop condition for our method","Chinese characters mainly consist of straight-line strokes (horizontal, vertical) and curve strokes (slash, backslash and many types of hooks)",GMM has already been employed for analyzing the structure of Chinese characters and has achieved commendable performance  ,"However, some lines extracted by GMM may be too short, and it is quite difficult to join these short lines to form semantic strokes due to the ambiguity of joining them together","This problem becomes more serious when analyzing handwritten characters by GMM, and this was the motivation to use AcaGMM to represent Chinese characters","In  
                         , we present a comparison of afCEC, AcaGMM, GMM, and CEC forChinese and Latin characters:   (dog),   (beg),   (father),  (mother),   (fire),   (master), b, R, S",The number of clusters has been determined so as to obtain a similar value of log-likelihood function,At the end of this subsection we present how our method works in the case of segmentation 3D objects,"The effect of afCECon three objects ( ) is shown in  
                         . 5.2 Data from the UCI repository In this subsection we compare  -means, EM, CEC and afCEC with respect to Rand and Jaccard indexes, see  
                         ","In the case of data sets of dimension higher then three, due to computational profitability, we used smaller class of quadratic polynomials of the type
 instead of the class of all quadratic polynomials
 This allows to fit less parameters in each step, which results in smaller risk of overfitting and helps to effectively cluster higher dimensional data","In most cases afCEC gives better results than classical algorithms, which means that data sets represent curve (or manifold) type structures. 5.3 Data from the monitoring systems of wind turbines At the end of this section, we examine the method on data from the monitoring systems of wind turbines ( )",The growing number of that type of systems necessitates an analysis of gigabytes of the data obtained every day,"Apart from the development of several advanced diagnostic methods for this type of machinery, there is a need for a group of methods that can act as “early warning tools”",The idea of this approach could be based on a data driven algorithm that would decide on the similarity of current data to data that are already known,Using density-based clustering algorithm data from a turbine which is in good condition could be described by density estimation,"If there are data points which do not match the density, this means that an unknown operational state of the turbine has occurred and an expert should be informed about the situation","Classical CEC and GMM algorithms have been used in such cases ( ), therefore we verified afCEC in a similar situation","Let us consider data in   covering the period from 04/18/2014 to 04/29/2014, recorded every 1 second by an on-line monitoring system","The data set contained only the basic values that define the operational state of a turbine: wind speed, rotational speed of the rotor, and the AC/DC power generated by the turbine",Negative values of AC power indicated that the power was absorbed,The recorded data were not averaged,"The data set included 985,837 measurements","The data where situated in s lower-dimensional subspace and were strongly correlated in one direction, as the last eigenvalue of the covariance matrix where essentially smaller than the first three: 350102.91, 2006.03, 126.14, 0.76","Therefore, we used PCA (Principal Component Analysis) for extracting the three most important dimensions","The results of the experiment are included in  
                         ",The afCEC method gives a better value of the Log-Likelihood function,"More precisely we need 8–10 clusters in GMM and CEC to obtain a similar approximation as in afCEC with 2–4 groups. 5.4 Comparison with non-density based methods Now we present comparison between afCECand classical approaches dedicated for clustering of nonlinear datasets: kkmeans ( ), SOM ( ), and spectral clustering ( ) (see  
                         )",Kernel methods and self-organizing maps can be used as a preprocessing for classical clustering methods,In such a way spectral clustering methods was constructed ( ),The classical kernel  -means ( ) is equivalent to KPCA prior to the conventional  -means algorithm,Most of kernel methods consist of two steps: an embedding into a feature space and a classical clustering method used on the data transformed to feature space,Therefore spectral methods are typically time consuming and use large number of parameters,In the case of non-density based method we use many internal quality indexes which have been proposed by various authors in order to determine an optimal clustering,"In our work we use BH index introduced by  , DB index proposed by  , SD index ( ) and Dunn index ( )","The first two indexes measure internal consistence of clusters, the next two describe how the clusters are separated","As we see in  
                          afCECmethod gives similar results to other approaches. 6 Conclusion In this paper, the afCEC method for clustering curved data, which uses generalized Gaussian distributions in curvilinear coordinate systems, was presented","The afCEC method has a strong theoretical background, and in particular, the cost function decreases in each iteration ( )","Moreover, afCEC can be used as a density estimation model",Since afCEC is an implementation of the Cross-Entropy clustering approach the method reduces on-line unnecessary clusters,"In practice, the approach gives essentially better results than linear models like GMM or CEC, since we obtain a similar level of the Log-likelihood function by using a smaller number of parameters to describe the model","On the other hand, the results are similar of AcaGMM when we restrict the data to two dimensions and use the quadratic function as the baseline","Appendix A AcaGMM Gaussian model As it was previously mentioned, AcaGMM does not use densities","More precisely, the Jacobian of the transformation was not taken into consideration","However, the EM procedure, which was used in AcaGMM, works with probability distributions","Therefore, from the theoretical point of view the above procedure is incorrect","Moreover, if we want to compare our method by using of the Log-likelihood function we need densities",Let us start from numerical integration of the original AcaGMM function and of the model rescaled by Jacobian correction,"The Simpson method  , on the square   with 50000 segments was used",The integral in the case of AcaGMM is equal to 1.038,After correction we obtain 1 (with a precision of 10 ),Let us consider situation of the AcaGMM model,"Suppose   and   are zero mean independent Gaussian distributions with variances  
                      ,  
                      :
 Moreover, let
 where  ","Let  ( ) represent the Jacobian of the original transformation
 In such a case, we have
 
                   Let us consider the function   expressed as parametric equation   (in the case of AcaGMM it is a parabola)","Using the formula from  , Table 1.) we obtain the orthogonal projection ( ( 
                      ),  ( 
                      )) of point ( 
                      ,  
                      ) on curve  :
 where  
                       and  
                   On the other hand, the arc length of   between zero and ( ( 
                      ),  ( 
                      )) ( , Formula (10)) is given by
 Consequently, we have
 
                      where  ","Our goal is to determine the Jacobian of our transformation, see  
                      ","Let us consider an arbitrary small neighborhood of ( ( 
                      ),  ( 
                      ))","In such a case, the local curvature of   at ( ( 
                      ),  ( 
                      )) is the same as the curvature of the osculating circle 
                       at ( ( 
                      ),  ( 
                      ))","The radius of curvature in the case of parametric form of curve is given by
 Consequently, our goal is to determinate how a set is changing under the influence of the transformation, see  
                      ","A small square neighborhood of the point ( 
                      ,  
                      ) is mapped to a trapezoid (asymptotically when a size of square converges to zero)",This operation is showed in  ,It is easy to see that the square area changes linearly depending on the distance  ,If we consider the situation where   we obtain that our square is collapsed to a point,"Consequently, for points above the curve Jacobian is asymptotically proportional to
 In a natural way, if a point ( 
                      ,  
                      ) is under the curve, the square area is increasing under the influence of the transformation","Therefore, the Jacobian is asymptotically equal to
 
                   Now we have the formula for the Jacobian of AcaGMM transformation, but it depends on the relation between a point and its orthogonal projection","More precisely, we have to verify which formula should be used (or equivalently on which side of parabola a point is found), see  
                      ","We can easily verify where the point ( 
                      ,  
                      ) is in relation to the orthogonal projection ( ( 
                      ),  ( 
                      )) by checking the orientation of a basis containing the normal vector   and the tangent vector ( ′( 
                      ),  ′( 
                      )) at a point ( ( 
                      ),  ( 
                      ))","Consequently, we have to verify the sign of the determinant
 
                   Appendix B Proof of  
                   Let us start with simple Lemma.
 
                      
                   Now we can prove  .
 
                  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
