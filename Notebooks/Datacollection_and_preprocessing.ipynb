{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Datacollection_and_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8x0ZqwY2920"
      },
      "source": [
        "Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P6iYX3Z2ycC"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "import xml.etree.ElementTree as ET\n",
        "import urllib\n",
        "import urllib.request\n",
        "import shutil\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import http.cookiejar\n",
        "from bs4 import BeautifulSoup\n",
        "!pip install elsapy\n",
        "from elsapy.elsclient import ElsClient\n",
        "from elsapy.elsprofile import ElsAuthor, ElsAffil\n",
        "from elsapy.elsdoc import FullDoc, AbsDoc\n",
        "!pip install scidownl\n",
        "!pip install slate3k\n",
        "import slate3k as slate\n",
        "from scidownl.scihub import *\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feEChKX0QKpC"
      },
      "source": [
        "This is a folder in google drive which contains the citations list file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEtNGC6WP2HS",
        "outputId": "e800d003-899c-46b7-ea18-a74fd65b774c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/ProjectNLP', force_remount=True)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /ProjectNLP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uuj2r6tZQMQm"
      },
      "source": [
        "creating this directory to store the downloaded .xml files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hdx34_aQOZq",
        "outputId": "5a67514e-75c0-4f93-c091-2bec756f3926"
      },
      "source": [
        "!mkdir  '/content/NLP'"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/NLP’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFmBEEGSQh9Q"
      },
      "source": [
        "changing the directory to the path that contains the citation list file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZK6XfyWQipk"
      },
      "source": [
        "os.chdir('/ProjectNLP') "
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3-1v5rF3DWX"
      },
      "source": [
        "The task was to get the urls of the articles which have an open access.\n",
        "\n",
        "Steps followed:\n",
        "\n",
        "Go to the following link:\n",
        "\n",
        "https://www.sciencedirect.com/search/advanced?qs=highlights&articleTypes=FLA&accessTypes=openaccess&show=100&offset=3600 \n",
        "\n",
        "\n",
        "*   This link is obtained by filtering out the documents with \"open access\" and keyword \"highlights\"\n",
        "\n",
        "*   Click on Export and select Export citations to text\n",
        "\n",
        "*   We get a list of urls\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o4wXtP73MR2"
      },
      "source": [
        "def extractURLs(fileContent):\n",
        "     urls = re.findall('\\(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', fileContent.lower())\n",
        "\n",
        "    #  for url in urls:\n",
        "    #         print(url)\n",
        "     return urls"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HREAYeEERka"
      },
      "source": [
        "Getting the urls from the citation files and storing it in variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_fpjEYYQxBR"
      },
      "source": [
        "# finalurl=[]\n",
        "# for myFile in os.listdir(os.getcwd()+\"/My Drive/ProjectNLP\"):\n",
        "#   print(myFile)\n",
        "#   myFile = open(os.getcwd()+\"/My Drive/ProjectNLP/\"+ myFile,encoding=\"utf8\")\n",
        "#   fileContent = myFile.read()\n",
        "#   finalurl.append(extractURLs(fileContent))\n",
        "myFile = open(os.getcwd()+\"/My Drive/ProjectNLP/\"+ \"aipubsumu.txt\",encoding=\"utf8\")\n",
        "fileContent = myFile.read()\n",
        "listURL=(extractURLs(fileContent))"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOuJtLZxRraQ"
      },
      "source": [
        "os.chdir('/content')"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtdQpmwnOClg"
      },
      "source": [
        "Pass the API key generated from Elsevier portal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0N8AWk3RxUe"
      },
      "source": [
        "key = '1cdd52bb8881a480836964f7775f0620'\n",
        "client = ElsClient(key)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VBuY5RmHGi5"
      },
      "source": [
        "We pass the extracted URLs which are stored in the list and download the scientific articles in .xml format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFLKVr4CR7Ez"
      },
      "source": [
        "listPII=[]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUKiQkzCR6C6",
        "outputId": "ae1228da-04d6-4d82-dc6d-4952c465542c"
      },
      "source": [
        "for url in listURL:\n",
        "    if url.startswith(\"(http://www.sciencedirect.com\"):\n",
        "        pii = url.replace(\"(http://www.sciencedirect.com/science/article/pii/\", \"\").replace(\")\", \"\").replace(\"\\n\", \"\")\n",
        "        if not pii in listPII:\n",
        "            listPII.append(pii)\n",
        "print(listPII)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['s0957417413009615', 's2590188519300162', 's0957417418306535', 's0888613x13002041', 's0957417419302489', 's107158191630074x', 's0888613x17302335', 's1071581916000021', 's0925231219301961', 's0957417417300751', 's0963868717301798', 's0736584518303831', 's0921889016306285', 's0957417419301903', 's0888613x1400067x', 's0888613x14001133', 's259018851930006x', 's0888613x1300234x', 's0888613x14000607', 's1071581919300552', 's0957417419301812', 's0020025519304864', 's0142061518304319', 's0933365716302950', 's0306437915000459', 's0921889015003000', 's1071581916300866', 's1071581918300016', 's1071581916301380', 's0957417415003590', 's0921889018307474', 's0957417417307698', 's1071581917301404', 's0921889014002164', 's1071581916300453', 's0952197619301253', 's1071581918300971', 's0306437919304909', 's0957417414006472', 's0925231217309864', 's0736584515000666', 's1071581917300988', 's0888613x14000206', 's0950705118301394', 's1875952116300040', 's0888613x14000851', 's0740818818301828', 's2590188519300083', 's1875952117300952', 's0888613x15000857', 's095741741830215x', 's0925231214000265', 's1071581918303471', 's0957417415000238', 's1071581918304312', 's0888613x13002910', 's0888613x14000796', 's0888613x15000912', 's1071581917300320', 's0888613x13002880', 's0888613x14001212', 's2589721719300029', 's0888613x13002867', 's0921889015000834', 's1875952119300394', 's0957417416306844']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfzLdpDbSFAu",
        "outputId": "276debaf-1af4-4ae3-a01f-3cbbe6a70269"
      },
      "source": [
        "len(listPII)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z-vEcRQSNx0",
        "outputId": "74779907-c72f-4430-b72b-04162039ea30"
      },
      "source": [
        "!mkdir '/Test_data'"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/Test_data’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GshQawGySO4_"
      },
      "source": [
        "os.chdir('/Test_data')"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAGPnCy8aG8g"
      },
      "source": [
        "Method to split the text into sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSv0nD_1SRiE"
      },
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def getSentences(text):\n",
        "  sentences=[]\n",
        "  intersent=''\n",
        "  for s in re.split('[.][\\s]([A-Z])',text):\n",
        "    # print(s.split(\" \"))\n",
        "    if len(s)==1:\n",
        "      intersent=s\n",
        "    else:\n",
        "      if len(re.split('[\\s]',intersent+s))>4:  \n",
        "        if not '\\n' in  intersent+s:     \n",
        "          sentences.append(intersent+s)\n",
        "        intersent=''\n",
        "  return sentences  \n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfTthqEPSdNb"
      },
      "source": [
        "def getSentencesHighlights(text):\n",
        "  sentences=[]\n",
        "  intersent=''\n",
        "  for s in re.split('[•]([A-Z])',text):\n",
        "    # print(s.split(\" \"))\n",
        "    if len(s)==1:\n",
        "      intersent=s\n",
        "    else:\n",
        "      if len(re.split('[\\s]',intersent+s))>4:\n",
        "        sentences.append(intersent+s)\n",
        "        intersent=''\n",
        "   \n",
        "  return sentences"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ECnFC9EZHEU"
      },
      "source": [
        "Method to get the \"Highlights\" section from the papers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRMGI_VISjLN"
      },
      "source": [
        "def getHighlights(tree):\n",
        "  root = tree.getroot()\n",
        "  highlightswithoutpreprocess = \"\"\n",
        "  for tag in root.iter():\n",
        "    if (tag.tag == '{http://www.elsevier.com/xml/common/dtd}abstract-sec'):\n",
        "      for t in tag:\n",
        "        if (t.tag == '{http://www.elsevier.com/xml/common/dtd}simple-para'):\n",
        "          for tg in t:\n",
        "            if (tg.tag == '{http://www.elsevier.com/xml/common/dtd}list'):\n",
        "              for g in tg:\n",
        "                if (g.tag == '{http://www.elsevier.com/xml/common/dtd}list-item'):\n",
        "                  for ti in g:\n",
        "                    if (ti.tag == '{http://www.elsevier.com/xml/common/dtd}para'):\n",
        "                      highlightswithoutpreprocess += str(ti.text)\n",
        "                      highlightswithoutpreprocess += \" \"\n",
        "  return highlightswithoutpreprocess\n"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsVJdZjIZfLP"
      },
      "source": [
        "Method to get the \"Introduction\" section from the papers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9o26OcKT9Gj"
      },
      "source": [
        "# sentence_index=[]\n",
        "def getIntro(tree):\n",
        "  root = tree.getroot()\n",
        "  introText=\"\"\n",
        "  isIntroFound = False\n",
        "  for child in root.iter():\n",
        "    if child.tag == '{http://www.elsevier.com/xml/common/dtd}section' and isIntroFound == False:\n",
        "      for introtag in child:\n",
        "        introText+=\" \"+introtag.text\n",
        "        for subintroTag in introtag:\n",
        "          introText+=\" \"+subintroTag.tail\n",
        "      isIntroFound = True \n",
        "  return introText\n",
        "# for s in introText.split(sep='.'):\n",
        "#   if(len(s)>50):\n",
        "#     sentence_index.append(s)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEv5OqqsZYF2"
      },
      "source": [
        "Method to get the \"Abstract\" section from the papers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJtg3ZzVUAIr"
      },
      "source": [
        "def getAbstract(tree):\n",
        "  root = tree.getroot()\n",
        "  # sentence_index=[]\n",
        "  abstract_text = \"\"\n",
        "  for tag in tree.iter():\n",
        "    if(tag.tag == '{http://purl.org/dc/elements/1.1/}description'):\n",
        "      abstract_text+= str(tag.text)\n",
        "  return abstract_text\n",
        "  # abstract_df[filename] = abstract_text  \n",
        "# for s in abstract_text.split (sep = '.'):\n",
        "#   if (len(s)>50):\n",
        "#     sentence_index.append(s)\n"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-Sc4S4rZqnu"
      },
      "source": [
        "Method to get the \"All other\" section from the paper\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYrk7xS3UFiM"
      },
      "source": [
        "def getOC(tree):\n",
        "  # root = tree.getroot()\n",
        "  # oc_text = \"\"\n",
        "  # # sentence_index=[]\n",
        "  # for tag in tree.iter():\n",
        "  #   if (tag.tag in notimptags):\n",
        "  #     continue\n",
        "  #   else:\n",
        "  #     if (tag.text!= None):\n",
        "  #       oc_text+= str(tag.text)\n",
        "  # return oc_text\n",
        "  root = tree.getroot()\n",
        "  introText=\"\"\n",
        "  isIntroFound = False\n",
        "  for child in root.iter():\n",
        "    if child.tag == '{http://www.elsevier.com/xml/common/dtd}section' and isIntroFound == False:\n",
        "      for introtag in child:\n",
        "        introText+=\" \"+introtag.text\n",
        "        for subintroTag in introtag:\n",
        "          introText+=\" \"+subintroTag.tail           \n",
        "  return introText\n",
        "      # other_content_df[filename] = oc_text\n",
        "# for s in oc_text.split(sep = '.'):\n",
        "#   if (len(s)>50) and not s.startswith('\\n') and s[1].isupper():\n",
        "#     sentence_index.append(s)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTZiG7doH7Gf"
      },
      "source": [
        "Method to get the keywords from the paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5W6dtrxUJga"
      },
      "source": [
        "def getKeywords(tree):\n",
        "  keyword=[]\n",
        "  # for tag in tree.iter():\n",
        "  #   if (tag.tag == '{http://www.elsevier.com/xml/common/dtd}bold'): #get bold words\n",
        "  #     keyword.append(tag.text)\n",
        "        # bold_words[filename] = (bold)\n",
        "  # for tag in tree.iter():\n",
        "  #   if (tag.tag == '{http://www.elsevier.com/xml/common/dtd}italic'): #get italic words\n",
        "  #     keyword.append(tag.text)\n",
        "        # italic_words[filename] = (italics)\n",
        "  for tag in tree.iter():\n",
        "    if (tag.tag == '{http://www.elsevier.com/xml/common/dtd}keyword'):  # get keywords\n",
        "      for t in tag:\n",
        "        if (t.tag == '{http://www.elsevier.com/xml/common/dtd}text'):\n",
        "          keyword.append(t.text)\n",
        "                # keyword_main[filename] = (keywords)\n",
        "  # for tag in tree.iter():\n",
        "  #   if(tag.tag == '{http://purl.org/dc/elements/1.1/}title'): # title\n",
        "  #     keyword.append(tag.text)\n",
        "  return keyword\n",
        "        # title_words[filename] = (title)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRSE9OOQUOBG"
      },
      "source": [
        "Abstract={}\n",
        "Highlights={}\n",
        "OtherContent={}\n",
        "OtherContent_Dict={}\n",
        "Abstract_Dict={}\n",
        "Highlights_Dict={}\n",
        "Intro={}\n",
        "Intro_Dict={}\n",
        "keywordmain={}\n",
        "for pii_id in listPII:\n",
        "  try:\n",
        "    pag=urllib.request.urlopen( \"https://api.elsevier.com/content/article/pii/\"+pii_id+\"?apiKey=1cdd52bb8881a480836964f7775f0620\")\n",
        "    tree = ET.parse(pag)    \n",
        "    Abstract_Dict[pii_id]=getAbstract(tree)\n",
        "    Abstract[pii_id]=getSentences(getAbstract(tree))\n",
        "    Highlights_Dict[pii_id]=getHighlights(tree)\n",
        "    Highlights[pii_id]=getSentences(getHighlights(tree))\n",
        "    Intro_Dict[pii_id]=getIntro(tree)\n",
        "    Intro[pii_id]=getSentences(getIntro(tree))\n",
        "    OtherContent_Dict[pii_id]=getOC(tree)\n",
        "    OtherContent[pii_id]=getSentences(getOC(tree))\n",
        "    keywordmain[pii_id]=getKeywords(tree)\n",
        "  except:\n",
        "    print(\"Failed for \"+pii_id)\n"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WEbq38yVBdM",
        "outputId": "d4d1e272-f210-4e62-8967-7db753a3c461"
      },
      "source": [
        "print(Abstract_Dict)\n",
        "print(Highlights_Dict)\n",
        "print(Intro_Dict)\n",
        "print(keywordmain)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'s0957417413009615': '\\n                  This study improves the recognition accuracy and execution time of facial expression recognition system. Various techniques were utilized to achieve this. The face detection component is implemented by the adoption of Viola–Jones descriptor. The detected face is down-sampled by Bessel transform to reduce the feature extraction space to improve processing time then. Gabor feature extraction techniques were employed to extract thousands of facial features which represent various facial deformation patterns. An AdaBoost-based hypothesis is formulated to select a few hundreds of the numerous extracted features to speed up classification. The selected features were fed into a well designed 3-layer neural network classifier that is trained by a back-propagation algorithm. The system is trained and tested with datasets from JAFFE and Yale facial expression databases. An average recognition rate of 96.83% and 92.22% are registered in JAFFE and Yale databases, respectively. The execution time for a 100×100 pixel size is 14.5ms. The general results of the proposed techniques are very encouraging when compared with others.\\n               ', 's2590188519300162': '\\n                  Forecasting future trends of the stock market using the historical data is the exigent demand in the field of academia as well as business. This work has explored the feature optimization capacity of firefly with an evolutionary framework considering the biochemical and social aspects of Firefly algorithm, along with the selection procedure of objective value in evolutionary notion. The performance of the proposed model is evaluated using four different stock market datasets, such as BSE Sensex, NSE Sensex, S&P 500 index and FTSE index. The datasets are regenerated using the proper mathematical formulation of the fundamental part belonging to technical analysis, such as technical indicators and statistical measures. The feature reduction through transformation is carried out on the enhanced dataset before employing the experimented dataset to the prediction models such as Extreme Learning Machine (ELM), Online Sequential Extreme Learning Machine (OSELM) and Recurrent Back Propagation Neural Network (RBPNN). For feature reduction, both statistical and optimized based feature reduction strategies are considered, where Principal Component Analysis (PCA) and Factor Analysis (FA) are examined for statistical based feature reduction and Firefly Optimization (FO), Genetic Algorithm (GA) and Firefly algorithm with evolutionary framework are well thought out for optimized feature reduction techniques. An empirical comparison is established among the experimented prediction models considering all the feature reduction techniques for the time horizon of 1 day, 3 days, 5 days, 7 days, 5 days and 30 days in advance, applying on all the datasets used in this study. From the simulation result, it can be clearly figured out that firefly with evolutionary framework optimized feature reduction applying to OSELM prediction model outperformed over the rest experimented models.\\n               ', 's0957417418306535': '\\n                  Malware creators have been getting their way for too long now. String-based similarity measures can leverage ground truth in a scalable way and can operate at a level of abstraction that is difficult to combat from the code level. At the string level, information theory and, specifically, entropy play an important role related to detecting patterns altered by concealment strategies, such as polymorphism or encryption. Controlling the entropy levels in different parts of a disk resident executable allows an analyst to detect malware or a black hat to evade the detection. This paper shows these two perspectives into two scalable entropy-based tools: EnTS and EEE. EnTS, the detection tool, shows the effectiveness of detecting entropy patterns, achieving 100% precision with 82% accuracy. It outperforms VirusTotal for accuracy on combined Kaggle and VirusShare malware. EEE, the evasion tool, shows the effectiveness of entropy as a concealment strategy, attacking binary-based state of the art detectors. It learns their detection patterns in up to 8 generations of its search process, and increments their false negative rate from range 0–9%, up to the range 90–98.7%.\\n               ', 's0888613x13002041': '\\n                  In real time, one observation always relies on several observations. To improve the forecasting accuracy, all these observations can be incorporated in forecasting models. Therefore, in this study, we have intended to introduce a new Type-2 fuzzy time series model that can utilize more observations in forecasting. Later, this Type-2 model is enhanced by employing particle swarm optimization (PSO) technique. The main motive behind the utilization of the PSO with the Type-2 model is to adjust the lengths of intervals in the universe of discourse that are employed in forecasting, without increasing the number of intervals. The daily stock index price data set of SBI (State Bank of India) is used to evaluate the performance of the proposed model. The proposed model is also validated by forecasting the daily stock index price of Google. Our experimental results demonstrate the effectiveness and robustness of the proposed model in comparison with existing fuzzy time series models and conventional time series models.\\n               ', 's0957417419302489': '\\n                  Latent palmprint identification is a crucial element for both law enforcement and integrated automated fingerprint identification systems because approximately 30% of the imprints found in a crime scene originate from a human’s palms. To find the person whom the palmprint belongs to, forensic experts use systems that automatically compare the imprints found, called latent, against thousands of potential palmprints.\\n                  Identification systems rely on features obtained from the palmprint, and different feature representations to include discriminative information. However, there is no consensus as to which representation allows for a better matching between latent palmprints, and those with a known identity. Furthermore, evaluating the identification performance when matching palmprints obtained when using different representations has not been done fairly. The current manner of evaluating palmprint identification methods uses different datasets, performance measures, and does not allow to discern the contributions of the feature representation and the methods for matching the palmprints. In this study, we have reviewed those features used for latent palmprint identification, and also we propose an evaluation methodology that allows for a fair comparison of minutiae-based features.\\n                  Using our methodology, we evaluated each representation performing more than 5 billion comparisons. Our experiments are done using a dataset that includes information about the matching minutiae according to an expert. We aim with our results to provide a baseline for new research in latent palmprint identification feature representations, allowing for a fair comparison of newly developed representations in the future, which would enhance the whole latent palmprint identification methods. For this purpose, we also publicly provide our dataset, methodology implementation, and the feature representations implementation tested in our experiments.\\n               ', 's107158191630074x': \"\\n                  As many users who are charged with process monitoring need to focus mainly on other work while performing monitoring as a secondary task, monitoring systems that purely rely on visual means are often not well suited for this purpose. Sonification, the presentation of data as (non-speech) sound, has proven in several studies that it can help in guiding the user's attention, especially in scenarios where process monitoring is performed in parallel with a different, main task. However, there are several aspects that have not been investigated in this area so far, for example if a continuous soundscape can guide the user's attention better than one that is based on auditory cues. We have developed a system that allows reproducible research to answer such questions. In this system, the participants’ performance both for the main task (simulated by simple arithmetic problems) and for the secondary task (a simulation of a production process) can be measured in a more fine-grained manner than has been the case for existing research in this field. In a within-subject study (n=18), we compared three monitoring conditions – visual only, visual + auditory alerts and a condition combining the visual mode with continuous sonification of process events based on a forest soundscape. Participants showed significantly higher process monitoring performances in the continuous sonification condition, compared to the other two modes. The performance in the main task was at the same time not significantly affected by the continuous sonification.\\n               \", 's0888613x17302335': '\\n                  Under-reporting occurs in survey data when there is a reason for participants to give a false negative response to a question, e.g. maternal smoking in epidemiological studies. Failing to correct this misreporting introduces biases and it may lead to misinformed decision making. Our work provides methods of correcting for this bias, by reinterpreting it as a missing data problem, and particularly learning from positive and unlabelled data. Focusing on information theoretic approaches we have three key contributions: (1) we provide a method to perform valid independence tests with known power by incorporating prior knowledge over misreporting; (2) we derive corrections for point/interval estimates of the mutual information that capture both relevance and redundancy; and finally, (3) we derive different ways for ranking under-reported risk factors. Furthermore, we show how to use our results in real-world problems and machine learning tasks.\\n               ', 's1071581916000021': '\\n                  Anthropometrics show that the lengths of many human body segments follow a common proportional relationship. To know the length of one body segment – such as a thumb – potentially provides a predictive route to other physical characteristics, such as overall standing height. In this study, we examined whether it is feasible that the length of a person׳s thumb could be revealed from the way in which they complete swipe gestures on a touchscreen-based smartphone.\\n                  From a corpus of approx. 19,000 swipe gestures captured from 178 volunteers, we found that people with longer thumbs complete swipe gestures with shorter completion times, higher speeds and with higher accelerations than people with shorter thumbs. These differences were also observed to exist between our male and female volunteers, along with additional differences in the amount of touch pressure applied to the screen.\\n                  Results are discussed in terms of linking behavioural and physical biometrics.\\n               ', 's0925231219301961': '\\n                  Despite the state-of-the-art performance for medical image segmentation, deep convolutional neural networks (CNNs) have rarely provided uncertainty estimations regarding their segmentation outputs, e.g., model (epistemic) and image-based (aleatoric) uncertainties. In this work, we analyze these different types of uncertainties for CNN-based 2D and 3D medical image segmentation tasks at both pixel level and structure level. We additionally propose a test-time augmentation-based aleatoric uncertainty to analyze the effect of different transformations of the input image on the segmentation output. Test-time augmentation has been previously used to improve segmentation accuracy, yet not been formulated in a consistent mathematical framework. Hence, we also propose a theoretical formulation of test-time augmentation, where a distribution of the prediction is estimated by Monte Carlo simulation with prior distributions of parameters in an image acquisition model that involves image transformations and noise. We compare and combine our proposed aleatoric uncertainty with model uncertainty. Experiments with segmentation of fetal brains and brain tumors from 2D and 3D Magnetic Resonance Images (MRI) showed that 1) the test-time augmentation-based aleatoric uncertainty provides a better uncertainty estimation than calculating the test-time dropout-based model uncertainty alone and helps to reduce overconfident incorrect predictions, and 2) our test-time augmentation outperforms a single-prediction baseline and dropout-based multiple predictions.\\n               ', 's0957417417300751': '\\n                  Deep learning techniques for Sentiment Analysis have become very popular. They provide automatic feature extraction and both richer representation capabilities and better performance than traditional feature based techniques (i.e., surface methods). Traditional surface approaches are based on complex manually extracted features, and this extraction process is a fundamental question in feature driven methods. These long-established approaches can yield strong baselines, and their predictive capabilities can be used in conjunction with the arising deep learning methods. In this paper we seek to improve the performance of deep learning techniques integrating them with traditional surface approaches based on manually extracted features. The contributions of this paper are sixfold. First, we develop a deep learning based sentiment classifier using a word embeddings model and a linear machine learning algorithm. This classifier serves as a baseline to compare to subsequent results. Second, we propose two ensemble techniques which aggregate our baseline classifier with other surface classifiers widely used in Sentiment Analysis. Third, we also propose two models for combining both surface and deep features to merge information from several sources. Fourth, we introduce a taxonomy for classifying the different models found in the literature, as well as the ones we propose. Fifth, we conduct several experiments to compare the performance of these models with the deep learning baseline. For this, we use seven public datasets that were extracted from the microblogging and movie reviews domain. Finally, as a result, a statistical study confirms that the performance of these proposed models surpasses that of our original baseline on F1-Score.\\n               ', 's0963868717301798': '\\n                  In response to the challenge of socializing new IT employees, some IT departments are exploring the incorporation of enterprise social media (hereinafter ESM) as an informal organizational socialization tool. Because this is a relatively new phenomenon, little is known about how ESM facilitate employee socialization. In order to contribute to our understanding of how ESM affects employee socialization, this paper invokes a case study to explore how one organization’s implementation of an ESM for its IT new hire program influenced the socialization process and outcomes. To delve deeply into how the ESM influences socialization, we draw upon technology affordance theory to uncover the various first and second-order affordances actualized by different actor groups and the various outcomes resulting from the affordances. We then identify five generative mechanisms – bureaucracy circumvention, executive perspective, personal development, name recognition, and morale booster – that explain how the actualization of different strands of affordances by various groups of users produces eight different outcomes. Our results provide insights into the different affordances made possible by ESM in the context of a new hire socialization program and how these affordances have repercussions beyond those experienced by the individuals using the ESM. The results have important implications for new hire socialization and technology affordance research.\\n               ', 's0736584518303831': '\\n                  Wire + Arc Additive Manufacturing (WAAM) has proven its capability to build medium to large metallic parts thanks to its high-rate deposition and its potentially unlimited build volume. Moreover, the low-cost equipment and the ability to deposit various metals make WAAM a strong candidate to become a standard industrial process. However, like all Additive Manufacturing (AM) technologies, the key to manufacturing suitable parts lies in the generation of an optimised path that guarantees a uniform defect-free deposition. Most AM technologies have been able to use traditional path strategies derived from CNC machining, but the specificities inherent to the arc deposition make the use of those solutions unreliable across a variety of topologies. Nevertheless, studies have shown that superior results can be achieved by using a feature-based design approach, but developing a path strategy for each new geometry would be a very time-consuming task. Therefore, this paper introduces the Modular Path Planning (MPP) solution that aims to incorporate the modularity of feature-based design into the traditional layer-by-layer strategy. By dividing each layer into individual deposition sections, this method allows users to adapt the path planning to the targeted geometry allowing the construction of a wide variety of complex geometries. This paper also proposes a software implementation that limits user interventions and reduces user inputs to basic CAD modelling operations. Moreover, the MPP has been compared to a traditional path planning solution and used to build a complex part for industry.\\n               ', 's0921889016306285': '\\n                  Since the beginning of robotics, the substitution of human labor has been one of the crucial issues. The focus is on the economic perspective, asking how robotics affects the labor market, and on changes in the work processes of human workers. While there are already some lessons learnt from industrial robotics, the area of service robots has been analyzed to a much lesser extent. First insights into these aspects are of utmost relevance to technology assessment providing policy advice. As conclusions for service robots in general cannot be drawn, we identify criteria for the ex-ante evaluation of service robots in concrete application areas.\\n               ', 's0957417419301903': '\\n                  The main aim of this paper is to investigate how far applying suitably conceived and designed credit scoring models can properly account for the incidence of default and help improve the decision-making process. Four statistical modelling techniques, namely, discriminant analysis, logistic regression, multi-layer feed-forward neural network and probabilistic neural network are used in building credit scoring models for the Indian banking sector. Notably actual misclassification costs are analysed in preference to estimated misclassification costs. Our first-stage scoring models show that sophisticated credit scoring models, in particular probabilistic neural networks, can help to strengthen the decision-making processes by reducing default rates by over 14%. The second-stage of our analysis focuses upon the default cases and substantiates the significance of the timing of default. Moreover, our results reveal that State of residence, equated monthly instalment, net annual income, marital status and loan amount, are the most important predictive variables. The practical implications of this study are that our scoring models could help banks avoid high default rates, rising bad debts, shrinking cash flows and punitive cost-cutting measures.\\n               ', 's0888613x1400067x': '\\n                  Dealing with the large amount of data resulting from association rule mining is a big challenge. The essential issue is how to provide efficient methods for summarizing and representing meaningful discovered knowledge from databases. This paper presents a new approach called multi-tier granule mining to improve the performance of association rule mining. Rather than using patterns, it uses granules to represent knowledge that is implicitly contained in relational databases. This approach also uses multi-tier structures and association mappings to interpret association rules in terms of granules. Consequently, association rules can be quickly assessed and meaningless association rules can be justified according to these association mappings. The experimental results indicate that the proposed approach is promising.\\n               ', 's0888613x14001133': '\\n                  We present a new approach to credal networks, which are graphical models that generalise Bayesian networks to deal with imprecise probabilities. Instead of applying the commonly used notion of strong independence, we replace it by the weaker, asymmetrical notion of epistemic irrelevance. We show how assessments of epistemic irrelevance allow us to construct a global model out of given local uncertainty models, leading to an intuitive expression for the so-called irrelevant natural extension of a credal network. In contrast with Cozman [4], who introduced this notion in terms of credal sets, our main results are presented using the language of sets of desirable gambles. This has allowed us to derive some remarkable properties of the irrelevant natural extension, including marginalisation properties and a tight connection with the notion of independent natural extension. Our perhaps most important result is that the irrelevant natural extension satisfies a collection of epistemic irrelevancies that is induced by AD-separation, an asymmetrical adaptation of d-separation. Both AD-separation and the induced collection of irrelevancies are shown to satisfy all graphoid properties except symmetry.\\n               ', 's259018851930006x': '\\n                  The effective prediction of pavement performance trends can help in achieving the cost-effective management of pavements over their service life. The international roughness index (IRI) is a widely used pavement performance index, which can be considered as a time-dependent variable in terms of scientific modeling. This research aims to develop an innovative IRI prediction model based on fuzzy-trend time-series forecasting and particle swarm optimization (PSO) techniques. Raw datasets extracted from the Long-Term Pavement Performance database are used for model training, testing, and performance assessment. First, IRI values are divided into different granular spaces, which are considered as the principal factor and subfactors. In addition, the multifactor interval division method is proposed according to the principle of the automatic clustering technique. Next, a second-order fuzzy-trend model and fuzzy-trend relationship classification method are proposed to predict the fuzzy-trend of each factor. Then, the fuzzy-trend states for multiple granular spaces are generated while giving full consideration to various uncertainties. Finally, the PSO technique is used to optimize the performance model while carrying out future IRI forecasting. Comparative experiments are performed using more than 20,000 data items from different regions to verify the effectiveness of the proposed method. The experimental results indicate that the proposed method outperforms other approaches including the polynomial fitting, autoregressive integrated moving average, and backpropagation neural network methods in terms of the root mean squared error (0.191) and relative error (6.37%).\\n               ', 's0888613x1300234x': '\\n                  Data semantics plays a fundamental role in computer science, in general, and in computing with words, in particular. The semantics of words arises as a sophisticated problem, since words being actually vague linguistic terms are pieces of information characterized by impreciseness, incompleteness, uncertainty and/or vagueness. The qualitative semantics and the quantitative semantics are two aspects of vague linguistic information, which are closely related. However, the qualitative semantics of linguistic terms, and even the qualitative semantics of the symbolic approaches, seem to be not elaborated on directly in the literature. In this study, we propose an interpretation of the inherent order-based semantics of terms through their qualitative semantics modeled by hedge algebra structures. The quantitative semantics of terms are developed based on the quantification of hedge algebras. With this explicit approach, we propose two concepts of assessment scales to address decision problems: linguistic scales used for representing expert linguistic assessments and semantic linguistic scales based on 4-tuple linguistic representation model, which forms a formalized structure useful for computing with words. An example of a simple multi-criteria decision problem is examined by running a comparative study. We also analyze the main advantages of the proposed approach.\\n               ', 's0888613x14000607': '\\n                  The paper by Eyke Hüllermeier introduces a new set of techniques for learning models from imprecise data. The removal of the uncertainty in the training instances through the input–output relationship described by the model is also considered. This discussion addresses three points of the paper: extension principle-based models, precedence operators between fuzzy losses and possible connections between data disambiguation and data imputation.\\n               ', 's1071581919300552': '\\n                  We review the history of human-automation interaction research, assess its current status and identify future directions. We start by reviewing articles that were published on this topic in the International Journal of Human-Computer Studies during the last 50 years. We find that over the years, automated systems have been used more frequently (1) in time-sensitive or safety-critical settings, (2) in embodied and situated systems, and (3) by non-professional users. Looking to the future, there is a need for human-automation interaction research to focus on (1) issues of function and task allocation between humans and machines, (2) issues of trust, incorrect use, and confusion, (3) the balance between focus, divided attention and attention management, (4) the need for interdisciplinary approaches to cover breadth and depth, (5) regulation and explainability, (6) ethical and social dilemmas, (7) allowing a human and humane experience, and (8) radically different human-automation interaction.\\n               ', 's0957417419301812': '\\n                  Investors utilise social media such as Twitter as a means of sharing news surrounding financials stocks listed on international stock exchanges. Company ticker symbols are used to uniquely identify companies listed on stock exchanges and can be embedded within tweets to create clickable hyperlinks referred to as cashtags, allowing investors to associate their tweets with specific companies. The main limitation is that identical ticker symbols are present on exchanges all over the world, and when searching for such cashtags on Twitter, a stream of tweets is returned which match any company in which the cashtag refers to - we refer to this as a cashtag collision. The presence of colliding cashtags could sow confusion for investors seeking news regarding a specific company. A resolution to this issue would benefit investors who rely on the speediness of tweets for financial information, saving them precious time. We propose a methodology to resolve this problem which combines Natural Language Processing and Data Fusion to construct company-specific corpora to aid in the detection and resolution of colliding cashtags, so that tweets can be classified as being related to a specific stock exchange or not. Supervised machine learning classifiers are trained twice on each tweet – once on a count vectorisation of the tweet text, and again with the assistance of features contained in the company-specific corpora. We validate the cashtag collision methodology by carrying out an experiment involving companies listed on the London Stock Exchange. Results show that several machine learning classifiers benefit from the use of the custom corpora, yielding higher classification accuracy in the prediction and resolution of colliding cashtags.\\n               ', 's0020025519304864': '\\n                  In this paper, we address the hesitant information in enhancement task often caused by differences in image contrast. Enhancement approaches generally use certain filters which generate artifacts or are unable to recover all the objects details in images. Typically, the contrast of an image quantifies a unique ratio between the amounts of black and white through a single pixel. However, contrast is better represented by a group of pixels. We have proposed a novel image enhancement scheme based on intuitionistic hesitant fuzzy sets (IHFSs) for drone images (dronogram) to facilitate better interpretations of target objects. First, a given dronogram is divided into foreground and background areas based on an estimated threshold from which the proposed model measures the amount of black/white intensity levels. Next, we fuzzify both of them and determine the hesitant score indicated by the distance between the two areas for each point in the fuzzy plane. Finally, a hyperbolic operator is adopted for each membership grade to improve the photographic quality leading to enhanced results via defuzzification. The proposed method is tested on a large drone image database. Results demonstrate better contrast enhancement, improved visual quality, and better recognition compared to the state-of-the-art methods.\\n               ', 's0142061518304319': '\\n                  The recognition of partial discharge (PD) sources is an important task of the monitoring and diagnostics of high-voltage components. Nowadays, digital PD measuring systems have the capability of extracting features and form scatter plots with such data sets. Part of an unsupervised PD analysis system is to discover clusters within the data sets and link them to particular PD sources. Due to the nature of PD data sets, clusters may appear very close to each other or even merged hindering the separation of sources. Clustering methods based on spatial density such as the density peak clustering (DPC) method and DBSCAN are suitable approaches to discover clusters within PD data sets. However, their accuracy can be reduced due to the proximity among clusters. In this paper, a new method is presented to improve the accuracy of the DPC method. Our method proposes to partition the data set and later pass the resulting subsets to the DPC method. The partitioning is based on the spatial density of data computed by a smoothed density method (SD). SD has the advantage of being fast and not requiring high computational power. As a final step, a routine is applied to group the sub clusters as per the DPC method having a threshold for the data contour distance as a criterion. This method proved higher accuracy to discover clusters in actual PD data sets. However, the threshold for the data contour distance still needs further research.\\n               ', 's0933365716302950': '\\n                  Objective\\n                  Given the availability of extensive digitized healthcare data from medical records, claims and prescription information, it is now possible to use hypothesis-free, data-driven approaches to mine medical databases for novel insight. The goal of this analysis was to demonstrate the use of artificial intelligence based methods such as Bayesian networks to open up opportunities for creation of new knowledge in management of chronic conditions.\\n               \\n                  Materials and methods\\n                  Hospital level Medicare claims data containing discharge numbers for most common diagnoses were analyzed in a hypothesis-free manner using Bayesian networks learning methodology.\\n               \\n                  Results\\n                  While many interactions identified between discharge rates of diagnoses using this data set are supported by current medical knowledge, a novel interaction linking asthma and renal failure was discovered. This interaction is non-obvious and had not been looked at by the research and clinical communities in epidemiological or clinical data. A plausible pharmacological explanation of this link is proposed together with a verification of the risk significance by conventional statistical analysis.\\n               \\n                  Conclusion\\n                  Potential clinical and molecular pathways defining the relationship between commonly used asthma medications and renal disease are discussed. The study underscores the need for further epidemiological research to validate this novel hypothesis. Validation will lead to advancement in clinical treatment of asthma & bronchitis, thereby, improving patient outcomes and leading to long term cost savings. In summary, this study demonstrates that application of advanced artificial intelligence methods in healthcare has the potential to enhance the quality of care by discovering non-obvious, clinically relevant relationships and enabling timely care intervention.\\n               ', 's0306437915000459': '\\n                  In recent years, monitoring the compliance of business processes with relevant regulations, constraints, and rules during runtime has evolved as major concern in literature and practice. Monitoring not only refers to continuously observing possible compliance violations, but also includes the ability to provide fine-grained feedback and to predict possible compliance violations in the future. The body of literature on business process compliance is large and approaches specifically addressing process monitoring are hard to identify. Moreover, proper means for the systematic comparison of these approaches are missing. Hence, it is unclear which approaches are suitable for particular scenarios. The goal of this paper is to define a framework for Compliance Monitoring Functionalities (CMF) that enables the systematic comparison of existing and new approaches for monitoring compliance rules over business processes during runtime. To define the scope of the framework, at first, related areas are identified and discussed. The CMFs are harvested based on a systematic literature review and five selected case studies. The appropriateness of the selection of CMFs is demonstrated in two ways: (a) a systematic comparison with pattern-based compliance approaches and (b) a classification of existing compliance monitoring approaches using the CMFs. Moreover, the application of the CMFs is showcased using three existing tools that are applied to two realistic data sets. Overall, the CMF framework provides powerful means to position existing and future compliance monitoring approaches.\\n               ', 's0921889015003000': '\\n                  Autonomous systems such as unmanned vehicles are beginning to operate within society. All participants in society are required to follow specific regulations and laws. An autonomous system cannot be an exception. Inevitably an autonomous system will find itself in a situation in which it needs to not only choose to obey a rule or not, but also make a complex ethical decision. However, there exists no obvious way to implement the human understanding of ethical behaviour in computers. Even if we enable autonomous systems to distinguish between more and less ethical alternatives, how can we be sure that they would choose right? We consider autonomous systems with a hybrid architecture in which the highest level of reasoning is executed by a rational (BDI) agent. For such a system, formal verification has been used successfully to prove that specific rules of behaviour are observed when making decisions. We propose a theoretical framework for ethical plan selection that can be formally verified. We implement a rational agent that incorporates a given ethical policy in its plan selection and show that we can formally verify that the agent chooses to execute, to the best of its beliefs, the most ethical available plan.\\n               ', 's1071581916300866': '\\n                  Although computers could offer emotional support as well as task support when aiding a user for a complex task, there is little current understanding of how they might do this. Moreover existing demonstrations of emotional support, though promising, only cover a small number of types of support and investigate a limited number of algorithms designed by hand. In this paper, we present an empirical investigation that starts from first principles, determining different categories of stressors for which emotional support might be useful, different categories of emotional support utterances and promising algorithms for deciding the content and form of textual emotional support messages according to the stressors present. At each stage, the results are validated through empirical experiments with human participants who, for instance, are required to place statements into categories, evaluate possible support messages in different imagined situations and compose their own emotional support from options offered. This development methodology allows us to avoid potentially challenging ethical issues in presenting people with stressful situations. Although our algorithms are attempting to choose emotional support based on the general, “naive” competence of human speakers, we use as a running example situations that can arise when attending a medical emergency and awaiting expert help.\\n               ', 's1071581918300016': \"\\n                  Shaping personalisation in a scenario of tangible, embedded and embodied interaction for cultural heritage involves challenges that go well beyond the requirements of implementing content personalisation for portable mobile guides. Content is coupled with the physical experience of the objects, the space, and the facets of the context—being those personal or social—acquire a more prominent role. This paper presents a personalisation framework to support complex scenarios that combine the physical, the digital, and the social dimensions of a visit. It is based on our experience of collaborating with curators and museum experts to understand and shape personalisation in a way that is meaningful to them and to visitors alike, that is sustainable to implement, and effective in managing the complexity of context-awareness. The proposed approach features a decomposition of personalisation into multiple layers of complexity that involve a blend of customisation on the visitor's initiative or according to the visitor's profile, system context-awareness, and automatic adaptivity computed by the system based on the visitor's behaviour model. We use a number of case studies of implemented exhibitions where this approach was used to illustrate its many facets and how adaptive techniques can be effectively complemented with interaction design, rich narratives and visitors’ choice to create deeply personal experiences. Overarching reflections spanning case studies and prototypes provide evidence of the viability of the proposed framework, and illustrate the final effect of the user experience.\\n               \", 's1071581916301380': '\\n                  The Facebook News Feed prioritizes posts for display by ranking them more prominently in the News Feed, based on users’ past interactions with the system. This study investigated constraints imposed on social interactions by the algorithm, by triggering participants’ awareness of “missed posts” in their Friends’ Timelines that they did not remember seeing before. If the algorithm prioritizes posts from people that users feel closer to and want to stay in touch with, participants should be less likely to report missed posts from close Friends. However, the results showed that relationship closeness had no effect on the likelihood of noticing a missed post, after controlling for how many Facebook Friends participants had and the accuracy of participants’ memories for their Friends’ Facebook activity. Also, missed posts from close Friends were more surprising, even when participants believed that the actions of the system caused the missed posts, indicating that these instances represent participants’ unmet expectations for the behavior of their News Feeds. Because Facebook posts present opportunities for feedback important for social support and maintaining social ties, this could indicate bias in the way the algorithm promotes content that could affect users’ ability to maintain relationships on Facebook. These findings have implications for approaches to improve user control and increase transparency in systems that use algorithmic filtering.\\n               ', 's0957417415003590': '\\n                  The recent developments in cellular networks, along with the increase in services, users and the demand of high quality have raised the Operational Expenditure (OPEX). Self-Organizing Networks (SON) are the solution to reduce these costs. Within SON, self-healing is the functionality that aims to automatically solve problems in the radio access network, at the same time reducing the downtime and the impact on the user experience. Self-healing comprises four main functions: fault detection, root cause analysis, fault compensation and recovery. To perform the root cause analysis (also known as diagnosis), Knowledge-Based Systems (KBS) are commonly used, such as fuzzy logic. In this paper, a novel method for extracting the Knowledge Base for a KBS from solved troubleshooting cases is proposed. This method is based on data mining techniques as opposed to the manual techniques currently used. The data mining problem of extracting knowledge out of LTE troubleshooting information can be considered a Big Data problem. Therefore, the proposed method has been designed so it can be easily scaled up to process a large volume of data with relatively low resources, as opposed to other existing algorithms. Tests show the feasibility and good results obtained by the diagnosis system created by the proposed methodology in LTE networks.\\n               ', 's0921889018307474': '\\n                  Autonomous robots that work in the same environment as humans are preferred to ensure mechanical safety with respect to soft contact with their surroundings and adaptivity to handle various tools and to manage partial malfunctions. To ensure that these requirements for robots are satisfied, this study proposes an approach for obtaining a robot structure and its application to building controller for dynamic motion of a robot. It is assumed that the physical relations between the sensor variables are unknown. On the basis of dependency network construction using mutual information, controllers are generated and tested by finding appropriate causal chains of the sensor variables. The proposed controller generation methods were tested using the control tasks of a musculoskeletal robotic arm. Thus, the proposed controller generation algorithm finds appropriate controllers, and the framework of this generation is robust to the changes in the body of the body.\\n               ', 's0957417417307698': '\\n                  Citizens are actively interacting with their surroundings, especially through social media. Not only do shared posts give important information about what is happening (from the users’ perspective), but also the metadata linked to these posts offer relevant data, such as the GPS-location in Location-based Social Networks (LBSNs). In this paper we introduce a global analysis of the geo-tagged posts in social media which supports (i) the detection of unexpected behavior in the city and (ii) the analysis of the posts to infer what is happening. The former is obtained by applying density-based clustering techniques, whereas the latter is consequence of applying content aggregation techniques. We have applied our methodology to a dataset obtained from Instagram activity in New York City for seven months obtaining promising results. The developed algorithms require very low resources, being able to analyze millions of data-points in commodity hardware in less than one hour without applying complex parallelization techniques. Furthermore, the solution can be easily adapted to other geo-tagged data sources without extra effort.\\n               ', 's1071581917301404': \"\\n                  Increasingly our digital traces are providing new opportunities for self-reflection. In particular, social media (SM) data can be used to support self-reflection, but to what extent is this affected by the form in which SM data is presented? Here, we present three studies where we work with individuals to transform or remediate their SM data into a physical book, a photographic triptych and a film. We describe the editorial decisions that take place as part of the remediation process and show how the transformations allow users to reflect on their digital identity in new ways. We discuss our findings in terms of the application of Goffman's (1959) self-presentation theories to the SM context, showing that a fluid rather than bounded interpretation of our social media spaces may be appropriate. We argue that remediation can contribute to the understanding of digital self and consider the design implications for new SM systems designed to support self-reflection.\\n               \", 's0921889014002164': '\\n                  In this paper, an overview of human–robot interactive communication is presented, covering verbal as well as non-verbal aspects. Following a historical introduction, and motivation towards fluid human–robot communication, ten desiderata are proposed, which provide an organizational axis both of recent as well as of future research on human–robot communication. Then, the ten desiderata are examined in detail, culminating in a unifying discussion, and a forward-looking conclusion.\\n               ', 's1071581916300453': \"\\n                  Few teachers include information and communication technology in the classroom, despite their potential for increasing attention and motivation for students. Educational authoring tools are intended to turn teachers into designers and deliverers of technology-enhanced educational content, and increasing the adoption of these tools is a key element for speeding up this transformation. This paper emphasizes the importance of learnability for preventing rejection or abandonment by of such an authoring tool, and how acceptance is deeply affected by the interaction paradigm and the creation metaphor used in the tool. We present an analysis comparing two design paradigms: the widespread menu-based and choice-guided interaction paradigm versus a consistent metaphor with direct manipulation. The latter was implemented in DEDOS-Editor, a novel authoring tool that allows the creation of diverse educational activities that can be performed on different devices, such as PCs, digital blackboards, tablets, and multitouch surfaces. An experimental study shows the tremendous impact that interface choices have on the tool's learning curve. The results provide the first mapping of the choice of a direct-manipulation interface and its effect on the learning curve's entry point, as well as a consistent interaction metaphor with smoother and fast-growing learning curves. This allows users to complete more tasks and gain more knowledge through experience, in contrast to menu-based interfaces. The initial use of the tool is thus made easier for users with no experience or information about the tool, and the advantages of experience and expertize in facing new challenges are facilitating. This work also highlights the appropriateness of learning curves as a tool for measuring learnability.\\n               \", 's0952197619301253': '\\n                  The segmentation of multivariate temporal series has been studied in a wide range of applications. This study investigates a challenging segmentation problem on traffic engineering, namely, identification of time-of-day breakpoints for pre-fixed traffic signal timing plans. A large number of urban centres have traffic control strategies based on time-of-day intervals. We propose a bilevel optimization model to address simultaneously the segmentation problems and the traffic control problems over these time intervals.\\n                  Efficient memetic algorithms have been developed for the bilevel model based on the hybridization of the particle swarm optimization, genetic algorithms or simulated annealing with the Nelder–Mead method. Numerically the effectiveness of the algorithms using real and synthetic data sets is demonstrated.\\n                  We address the problem of automatically estimating the number of time-of-day segments that can be reliably discovered. We adapt the Bayesian Information Criterion, the PETE algorithm and a novel oriented-problem approach. The experiments show that this last method gives interpretable results about the number of reliably necessary segments from the traffic-engineering perspective.\\n                  The experimental results show that the proposed methodology provides an automatic method to determine the time-of-day segments and timing plans simultaneously.\\n               ', 's1071581918300971': '\\n                  Radio production involves editing speech-based audio using tools that represent sound using simple waveforms. Semantic speech editing systems allow users to edit audio using an automatically generated transcript, which has the potential to improve the production workflow. To investigate this, we developed a semantic audio editor based on a pilot study. Through a contextual qualitative study of five professional radio producers at the BBC, we examined the existing radio production process and evaluated our semantic editor by using it to create programmes that were later broadcast.\\n                  We observed that the participants in our study wrote detailed notes about their recordings and used annotation to mark which parts they wanted to use. They collaborated closely with the presenter of their programme to structure the contents and write narrative elements. Participants reported that they often work away from the office to avoid distractions, and print transcripts so they can work away from screens. They also emphasised that listening is an important part of production, to ensure high sound quality. We found that semantic speech editing with automated speech recognition can be used to improve the radio production workflow, but that annotation, collaboration, portability and listening were not well supported by current semantic speech editing systems. In this paper, we make recommendations on how future semantic speech editing systems can better support the requirements of radio production.\\n               ', 's0306437919304909': '\\n                  Detecting anomalies in process runtime behavior is crucial: they might reflect, on the one side, security breaches and fraudulent behavior and on the other side desired deviations due to, for example, exceptional conditions. Both scenarios yield valuable insights for process analysts and owners, but happen due to different reasons and require a different treatment. Hence a distinction into malign and benign anomalies is required. Existing anomaly detection approaches typically fall short in supporting experts when in need to take this decision. An additional problem are false positives which could result in selecting incorrect countermeasures. This paper proposes a novel anomaly detection approach based on association rule mining. It fosters the explanation of anomalies and the estimation of their severity. In addition, the approach is able to deal with process change and flexible executions which potentially lead to false positives. This facilitates to take the appropriate countermeasure for a malign anomaly and to avoid the possible termination of benign process executions. The feasibility and result quality of the approach are shown by a prototypical implementation and by analyzing real life logs with injected artificial anomalies. The explanatory power of the presented approach is evaluated through a controlled experiment with users.\\n               ', 's0957417414006472': '\\n                  Traditional clustering algorithms do not consider the semantic relationships among words so that cannot accurately represent the meaning of documents. To overcome this problem, introducing semantic information from ontology such as WordNet has been widely used to improve the quality of text clustering. However, there still exist several challenges, such as synonym and polysemy, high dimensionality, extracting core semantics from texts, and assigning appropriate description for the generated clusters. In this paper, we report our attempt towards integrating WordNet with lexical chains to alleviate these problems. The proposed approach exploits ontology hierarchical structure and relations to provide a more accurate assessment of the similarity between terms for word sense disambiguation. Furthermore, we introduce lexical chains to extract a set of semantically related words from texts, which can represent the semantic content of the texts. Although lexical chains have been extensively used in text summarization, their potential impact on text clustering problem has not been fully investigated. Our integrated way can identify the theme of documents based on the disambiguated core features extracted, and in parallel downsize the dimensions of feature space. The experimental results using the proposed framework on reuters-21578 show that clustering performance improves significantly compared to several classical methods.\\n               ', 's0925231217309864': '\\n                  We are seeing an enormous increase in the availability of streaming, time-series data. Largely driven by the rise of connected real-time data sources, this data presents technical challenges and opportunities. One fundamental capability for streaming analytics is to model each stream in an unsupervised fashion and detect unusual, anomalous behaviors in real-time. Early anomaly detection is valuable, yet it can be difficult to execute reliably in practice. Application constraints require systems to process data in real-time, not batches. Streaming data inherently exhibits concept drift, favoring algorithms that learn continuously. Furthermore, the massive number of independent streams in practice requires that anomaly detectors be fully automated. In this paper we propose a novel anomaly detection algorithm that meets these constraints. The technique is based on an online sequence memory algorithm called Hierarchical Temporal Memory (HTM). We also present results using the Numenta Anomaly Benchmark (NAB), a benchmark containing real-world data streams with labeled anomalies. The benchmark, the first of its kind, provides a controlled open-source environment for testing anomaly detection algorithms on streaming data. We present results and analysis for a wide range of algorithms on this benchmark, and discuss future challenges for the emerging field of streaming analytics.\\n               ', 's0736584515000666': '\\n                  The requirement to increase inspection speeds for non-destructive testing (NDT) of composite aerospace parts is common to many manufacturers. The prevalence of complex curved surfaces in the industry provides motivation for the use of 6 axis robots in these inspections. The purpose of this paper is to present work undertaken for the development of a KUKA robot manipulator based automated NDT system. A new software solution is presented that enables flexible trajectory planning to be accomplished for the inspection of complex curved surfaces often encountered in engineering production. The techniques and issues associated with conventional manual inspection techniques and automated systems for the inspection of large complex surfaces were reviewed. This approach has directly influenced the development of a MATLAB toolbox targeted to NDT automation, capable of complex path planning, obstacle avoidance, and external synchronization between robots and associated external NDT systems. This paper highlights the advantages of this software over conventional off-line-programming approaches when applied to NDT measurements. An experimental validation of path trajectory generation, on a large and curved composite aerofoil component, is presented. Comparative metrology experiments were undertaken to evaluate the real path accuracy of the toolbox when inspecting a curved 0.5m2 and a 1.6m2 surface using a KUKA KR16 L6-2 robot. The results have shown that the deviation of the distance between the commanded TCPs and the feedback positions were within 2.7mm. The variance of the standoff between the probe and the scanned surfaces was smaller than the variance obtainable via commercial path-planning software. Tool paths were generated directly on the triangular mesh imported from the CAD models of the inspected components without need for an approximating analytical surface. By implementing full external control of the robotic hardware, it has been possible to synchronise the NDT data collection with positions at all points along the path, and our approach allows for the future development of additional functionality that is specific to NDT inspection problems. For the current NDT application, the deviations from CAD design and the requirements for both coarse and fine inspections, dependent on measured NDT data, demand flexibility in path planning beyond what is currently available from existing off-line robot programming software.\\n               ', 's1071581917300988': '\\n                  The use of the senses of vision and audition as interactive means has dominated the field of Human-Computer Interaction (HCI) for decades, even though nature has provided us with many more senses for perceiving and interacting with the world around us. That said, it has become attractive for HCI researchers and designers to harness touch, taste, and smell in interactive tasks and experience design. In this paper, we present research and design insights gained throughout an interdisciplinary collaboration on a six-week multisensory display – Tate Sensorium – exhibited at the Tate Britain art gallery in London, UK. This is a unique and first time case study on how to design art experiences whilst considering all the senses (i.e., vision, sound, touch, smell, and taste), in particular touch, which we exploited by capitalizing on a novel haptic technology, namely, mid-air haptics. We first describe the overall set up of Tate Sensorium and then move on to describing in detail the design process of the mid-air haptic feedback and its integration with sound for the Full Stop painting by John Latham (1961). This was the first time that mid-air haptic technology was used in a museum context over a prolonged period of time and integrated with sound to enhance the experience of visual art. As part of an interdisciplinary team of curators, sensory designers, sound artists, we selected a total of three variations of the mid-air haptic experience (i.e., haptic patterns), which were alternated at dedicated times throughout the six-week exhibition. We collected questionnaire-based feedback from 2500 visitors and conducted 50 interviews to gain quantitative and qualitative insights on visitors’ experiences and emotional reactions. Whilst the questionnaire results are generally very positive with only a small variation of the visitors’ arousal ratings across the three tactile experiences designed for the Full Stop painting, the interview data shed light on the differences in the visitors’ subjective experiences. Our findings suggest multisensory designers and art curators can ensure a balance between surprising experiences versus the possibility of free exploration for visitors. In addition, participants expressed that experiencing art with the combination of mid-air haptic and sound was immersive and provided an up-lifting experience of touching without touch. We are convinced that the insights gained from this large-scale and real-world field exploration of multisensory experience design exploiting a new and emerging technology provide a solid starting point for the HCI community, creative industries, and art curators to think beyond conventional art experiences. Specifically, our work demonstrates how novel mid-air technology can make art more emotionally engaging and stimulating, especially abstract art that is often open to interpretation.\\n               ', 's0888613x14000206': '\\n                  In this paper we present the proof-theoretical approach to p-adic valued conditional probabilistic logics. We introduce two such logics denoted by \\n                        \\n                           \\n                              CPL\\n                           \\n                           \\n                              \\n                                 \\n                                    Z\\n                                 \\n                                 \\n                                    p\\n                                 \\n                              \\n                           \\n                        \\n                      and \\n                        \\n                           \\n                              CPL\\n                           \\n                           \\n                              \\n                                 \\n                                    Q\\n                                 \\n                                 \\n                                    p\\n                                 \\n                              \\n                           \\n                           \\n                              fin\\n                           \\n                        \\n                     . Each of these logics extends classical propositional logic with a list of binary (conditional probability) operators. Formulas are interpreted in Kripke-like models that are based on p-adic probability spaces. Axiomatic systems with infinitary rules of inference are given and proved to be sound and strongly complete. The decidability of the satisfiability problem for each logic is proved.\\n               ', 's0950705118301394': '\\n                  Human creativity generates novel ideas to solve real-world problems. This thereby grants us the power to transform the surrounding world and extend our human attributes beyond what is currently possible. Creative ideas are not just new and unexpected, but are also successful in providing solutions that are useful, efficient and valuable. Thus, creativity optimizes the use of available resources and increases wealth. The origin of human creativity, however, is poorly understood, and semantic measures that could predict the success of generated ideas are currently unknown. Here, we analyze a dataset of design problem-solving conversations in real-world settings by using 49 semantic measures based on WordNet 3.1 and demonstrate that a divergence of semantic similarity, an increased information content, and a decreased polysemy predict the success of generated ideas. The first feedback from clients also enhances information content and leads to a divergence of successful ideas in creative problem solving. These results advance cognitive science by identifying real-world processes in human problem solving that are relevant to the success of produced solutions and provide tools for real-time monitoring of problem solving, student training and skill acquisition. A selected subset of information content (IC Sánchez–Batet) and semantic similarity (Lin/Sánchez–Batet) measures, which are both statistically powerful and computationally fast, could support the development of technologies for computer-assisted enhancements of human creativity or for the implementation of creativity in machines endowed with general artificial intelligence.\\n                  \\n               ', 's1875952116300040': '\\n                  The pre-show experience is a significant part of the movie industry. Moviegoers, on average arrive 24min before when the previews start. Previews have been a part of the movie experience for more than a hundred years and are a culturally significant aspect of the whole experience. Over the last decade, the pre-movie in-theatre experience has grown to a $600 million industry. This growth continues to accelerate. Since 2012, this industry has increased by 150%. Consequently, there is an industry-wide demand for innovation in the pre-movie area. In this paper, we describe Paths, an innovative multiplayer real-time socially engaging game that we designed, developed and evaluated. An iterative refinement application development methodology was used to create the game. The game may be played on any smartphone and group interactions are viewed on the large theatre screen. This paper also reports on the quasi-experimental mixed method study with repeated measures that was conducted to ascertain the effectiveness of this new game. The results show that Paths is very engaging with elements of suspense, pleasant unpredictability and effective team building and crowd-pleasing characteristics.\\n               ', 's0888613x14000851': 'None', 's0740818818301828': '\\n                  A library is a particular kind of organization. It plays a valuable role and is dedicated mainly to the development and growth of society. Analyzing a library from the perspective of a network of relations and ties, which exist between social and technical network nodes, contributes to a more nuanced assessment of effectiveness. Building on social network analysis and going beyond human relations in a library, this study examines perceptions related to knowledge and skills, resources, and tasks, identified through a survey conducted at the university library in Warsaw. Overall, the analyzed library is characterized by redundancy and congruence of knowledge, resources, and tasks required at the library (organizational) level and at the particular node (employee) level. Analyzing the network efficiency of a library is a new and valuable research design which uses a unique network measurement that should attract more interest in the future. This form of analysis gives managers the tools to dynamize relations and understand the flow, use, and sharing of resources or knowledge within a library context. However, more studies in the public sector would be invaluable in order to formulate new theories or conclusions.\\n               ', 's2590188519300083': '\\n                  Due to the power and impact of social media, unsolved practical issues such as human trafficking, kinship recognition, and clustering family photos from large collections have recently received special attention from researchers. In this paper, we present a new idea for family and non-family photo classification. Unlike existing methods that explore face recognition and biometric features, the proposed method explores the strengths of facial geometric features and texture given by a new fractional-entropy approach for classification. The geometric features include spatial and angle information of facial key points, which give spatial and directional coherence. The texture features extract regular patterns in images. The proposed method then combines the above properties in a new way for classifying family and non-family photos with the help of Convolutional Neural Networks (CNNs). Experimental results on our own as well as benchmark datasets show that the proposed approach outperforms the state-of-the-art methods in terms of classification rate.\\n               ', 's1875952117300952': '\\n                  A cost-effective, easily-accessible neuro-motor rehabilitation solution is proposed that can determine the range of motion and the kinematic ability of participants. A serious game comprising four-scenarios are developed in which the players control an avatar that mirrors the rotations of the upper-limb joints through multi-channel-input devices (Kinect, Myo, FootPedal). Administered functional reach tests (FRT) challenge the player to interact with a 3D-environment while standing or sitting and using the FootPedal which simulates the action of walking whilst body movement is measured concurrently. The FRT’s complexity level is adapted using a Monte Carlo Tree Search algorithm which determines a virtual object’s position based on the proved ability of the user. Twenty-three volunteers were recruited to play the game in 45-min sessions. The data show that the system has a more positive impact on players performance and is more motivating than formal therapy. The visual representation of the trajectory of the objects is shown to increase the perception of the participants voluntary/involuntary upper extremity movement, and the results show a comparable inter-session reliability (acceptable-good) over two repeated sessions. A high Pearson correlation demonstrates the validity of using Kinect and Myo devices in assessing upper-limb rehabilitation, and the timing and the clinically relevant movement data have a higher accuracy when the devices are paired.\\n               ', 's0888613x15000857': '\\n                  Fuzzy linguistic logic programming is a logical system for representing and reasoning with linguistically-expressed human knowledge. In fuzzy linguistic logic programming, up until now, there have been only two methods to compute answers to a query w.r.t. a logic program: (i) by bottom-up iterating the immediate consequence operator \\n                        \\n                           \\n                              T\\n                           \\n                           \\n                              P\\n                           \\n                        \\n                     ; or (ii) by using the procedural semantics. Nevertheless, the former is exhaustive and not goal-oriented. Indeed, it requires computation of the whole least Herbrand model despite the fact that not all the results are required to determine the answer of the query. On the other hand, although the latter is goal-oriented, it may lead to an infinite loop and may recompute atoms in rule bodies. Furthermore, it may not, in general, give a most general answer to a query. In this paper, we develop two query answering procedures which can overcome these problems for fuzzy linguistic logic programming. More precisely, the non-deterministic tabulation procedure is close to the procedural semantics and gives all answers to a query. The deterministic tabulation procedure is more efficient than the non-deterministic one and gives all and only the most general answers to a query. The deterministic procedure also enables threshold computation and top-k retrieval.\\n               ', 's095741741830215x': '\\n                  The aim of this study is to provide an overview the state-of-the-art elements of text classification. For this purpose, we first select and investigate the primary and recent studies and objectives in this field. Next, we examine the state-of-the-art elements of text classification. In the following steps, we qualitatively and quantitatively analyse the related works. Herein, we describe six baseline elements of text classification including data collection, data analysis for labelling, feature construction and weighing, feature selection and projection, training of a classification model, and solution evaluation. This study will help readers acquire the necessary information about these elements and their associated techniques. Thus, we believe that this study will assist other researchers and professionals to propose new studies in the field of text classification.\\n               ', 's0925231214000265': '\\n                  The tensor completion problem is to recover a low-n-rank tensor from a subset of its entries. The main solution strategy has been based on the extensions of trace norm for the minimization of tensor rank via convex optimization. This strategy bears the computational cost required by the singular value decomposition (SVD) which becomes increasingly expensive as the size of the underlying tensor increase. In order to reduce the computational cost, we propose a multi-linear low-n-rank factorization model and apply the nonlinear Gauss–Seidal method that only requires solving a linear least squares problem per iteration to solve this model. Numerical results show that the proposed algorithm can reliably solve a wide range of problems at least several times faster than the trace norm minimization algorithm.\\n               ', 's1071581918303471': \"\\n                  Current trends in society and technology make the concept of interruption a central human computer interaction problem. In this work, a novel soft computing implementation for an Interruption Classifier was designed, developed and evaluated that draws from a user model and real-time observations of the user's actions as s/he works on computer-based tasks to determine ideal times to interact with the user. This research is timely as the number of interruptions people experience daily has grown considerably over the last decade. Thus, systems are needed to manage interruptions by reasoning about ideal timings of interactions.\\n                  This research shows: (1) the classifier incorporates a user model in its’ reasoning process. Most of the research in this area has focused on task-based contextual information when designing systems that reason about interruptions; (2) the classifier performed at 96% accuracy in experimental test scenarios and significantly outperformed other comparable systems; (3) the classifier is implemented using an advanced machine learning technology—an Adaptive Neural-Fuzzy Inference System—this is unique since all other systems use Bayesian Networks or other machine learning tools; (4) the classifier does not require any direct user involvement—in other systems, users must provide interruption annotations while reviewing video sessions so the system can learn; and (5) a promising direction for reasoning about interruptions for free-form tasks–this is largely an unsolved problem.\\n               \", 's0957417415000238': '\\n                  The paper presents the main results of the KOMET (Knowledge and cOntent structuring via METhods of collaborative ontology design) project, which aims to develop a novel paradigm for knowledge structuring based on the interplay between cognitive psychology and ontology engineering. By the knowledge structure (a conceptual model) we define the main domain concepts and relations between them in the form of a graph, map or diagram. This approach considers individual cognitive styles and uses recent advances in knowledge engineering and conceptual structuring; it aims to create new, consistent and structurally holistic knowledge bases for various areas of science and technology. Two stages of research have been completed: research into correlations between the expert’s individual cognitive style and the peculiarities of the expert’s subject domain ontology development; and research into correlations between the expert’s individual cognitive style and the group ontology design (including design accomplished by groups of experts with either similar or different cognitive styles). The results of these research stages can be applied to organizing collaborative ontology design (especially for research and learning purposes), data structuring and other group analytical work. Implications for practice are briefly delineated.\\n               ', 's1071581918304312': '\\n                  Creative engagement with novel musical interfaces can be rewarding for non-musicians. However, designing novel musical interfaces for non-musicians can be challenging because they lack conceptual and technical musical skills. In this paper we explore the effects of task motivation (experiential goal vs utilitarian goal) and user interface mode (whether the content is editable, and whether content can be replayed), on non-musicians’ creative engagement with novel musical interfaces. We show through an empirical study of twenty-four participants that an experiential exploratory goal encourages users’ creative engagement compared to a utilitarian creative goal. We found that being able to replay records is less important when participants have an experiential exploratory goal than when they have a utilitarian creative goal. Results also indicate that allowing people to replay their musical ideas increased some aspects of their creative engagement which was further increased when they were able to edit their creations. We also found that creative engagement increased when the interface supported users in planning ahead. A descriptive model of non-musician’s creative engagement with musical interfaces is proposed including three modes of musicking. An optimal trajectory of creative engagement through these modes is suggested and a description of inferred motivations, output, status and activities during creative processes is discussed. Design implications are proposed for supporting novices’ creative engagement taking into consideration their motivation and skills, and supporting insight and real-time activity.\\n               ', 's0888613x13002910': '\\n                  The specification of conditional probability tables (CPTs) is a difficult task in the construction of probabilistic graphical models. Several types of canonical models have been proposed to ease that difficulty. Noisy-threshold models generalize the two most popular canonical models: the noisy-or and the noisy-and. When using the standard inference techniques the inference complexity is exponential with respect to the number of parents of a variable. More efficient inference techniques can be employed for CPTs that take a special form. CPTs can be viewed as tensors. Tensors can be decomposed into linear combinations of rank-one tensors, where a rank-one tensor is an outer product of vectors. Such decomposition is referred to as Canonical Polyadic (CP) or CANDECOMP-PARAFAC (CP) decomposition. The tensor decomposition offers a compact representation of CPTs which can be efficiently utilized in probabilistic inference. In this paper we propose a CP decomposition of tensors corresponding to CPTs of threshold functions, exactly ℓ-out-of-k functions, and their noisy counterparts. We prove results about the symmetric rank of these tensors in the real and complex domains. The proofs are constructive and provide methods for CP decomposition of these tensors. An analytical and experimental comparison with the parent-divorcing method (which also has a polynomial complexity) shows superiority of the CP decomposition-based method. The experiments were performed on subnetworks of the well-known QMRT-DT network generalized by replacing noisy-or by noisy-threshold models.\\n               ', 's0888613x14000796': '\\n                  The inclusion–exclusion principle is a well-known property in probability theory, and is instrumental in some computational problems such as the evaluation of system reliability or the calculation of the probability of a Boolean formula in diagnosis. However, in the setting of uncertainty theories more general than probability theory, this principle no longer holds in general. It is therefore useful to know for which families of events it continues to hold. This paper investigates this question in the setting of belief functions. After exhibiting original sufficient and necessary conditions for the principle to hold, we illustrate its use on the uncertainty analysis of Boolean and non-Boolean systems in reliability.\\n               ', 's0888613x15000912': '\\n                  In the study, we propose a concept of incremental fuzzy models in which fuzzy rules are aimed at compensating discrepancies resulting because of the use of a certain global yet simple model of general nature (such as e.g., a constant or linear regression). The structure of input data and error discovered through fuzzy clustering is captured in the form of a collection of fuzzy clusters, which helps eliminate (compensate) error produced by the global model. We discuss a detailed architecture of the proposed rule-based model and present its design based on an augmented version of Fuzzy C-Means (FCM). An extended suite of experimental studies offering some comparative analysis is covered as well.\\n               ', 's1071581917300320': '\\n                  The possibility of linking paper to digital information is enhanced by recent developments in printed electronics. In this article we report the design and evaluation of a local newspaper augmented with capacitive touch regions and an embedded Bluetooth chip working with an adjunct device. These allowed the interactive playback of associated audio and the registration of manual voting actions on the web. Design conventions inherited from paper and the web were explored by showing four different versions of an interactive newspaper to 16 community residents. The diverse responses of residents are described, outlining the potential of the approach for local journalism and recommendations for the design of interactive newsprint.\\n               ', 's0888613x13002880': '\\n                  For many problem domains, such as medicine, chain graphs are more attractive than Bayesian networks as they support representing interactions between variables that have no natural direction. In particular, interactions between variables that result from certain feedback mechanisms can be represented by chain graphs. Using qualitative abstractions of probabilistic interactions is also of interest, as these allow focusing on patterns in the interactions rather than on the numerical detail. Such patterns are often known by experts and sufficient for making decisions. So far, qualitative abstractions of probabilistic interactions have only been developed for Bayesian networks in the form of qualitative probabilistic networks. In this paper, such qualitative abstractions are developed for chain graphs with the practical aim of using qualitative knowledge as constraints on the hyperspace of probability distributions. The usefulness of qualitative chain graphs is explored for modelling and reasoning about the interactions between diseases.\\n               ', 's0888613x14001212': '\\n                  A novel technique to classify time series with imprecise hidden Markov models is presented. The learning of these models is achieved by coupling the EM algorithm with the imprecise Dirichlet model. In the stationarity limit, each model corresponds to an imprecise mixture of Gaussian densities, this reducing the problem to the classification of static, imprecise-probabilistic, information. Two classifiers, one based on the expected value of the mixture, the other on the Bhattacharyya distance between pairs of mixtures, are developed. The computation of the bounds of these descriptors with respect to the imprecise quantification of the parameters is reduced to, respectively, linear and quadratic optimization tasks, and hence efficiently solved. Classification is performed by extending the k-nearest neighbors approach to interval-valued data. The classifiers are credal, meaning that multiple class labels can be returned in the output. Experiments on benchmark datasets for computer vision show that these methods achieve the required robustness whilst outperforming other precise and imprecise methods.\\n               ', 's2589721719300029': \"\\n                  Body temperature is an important physiological indicator in the whole process of pig breeding. Temperature measurement is also an effective means to assist in disease diagnosis and pig health monitoring. In the conventional method of measuring body temperature, a mercury column is used to obtain the rectal temperature. The operation of this method is complicated and requires a large amount of labor. This kind of temperature measurement method is contact and can make the pig stressed, which is disadvantageous for the healthy growth of pigs. Therefore, rectal temperature measurement no longer meets the needs of the large-scale pig industry in China's welfare agriculture. In recent years, the emerging pig body temperature detection technologies are electronic temperature measurement technology, infrared temperature measurement technology and so on. Infrared temperature measurement technology has been the main means of measuring the temperature of pig body surface with its advantages of non-contact, long distance and real-time. At present, infrared temperature measurement technology and infrared image processing technology used in pig breeding are still in the exploration stage. Nowadays, the infrared temperature measurement equipment based on point-by-point analysis represented by infrared thermometer and temperature measurement equipment based on full-field analysis represented by infrared thermal imager have been applied to pig breeding industry. These types of temperature measurement are more in line with the needs of the pig breeding industry to transform and upgrade to the automation, in line with the development concept of welfare farming and smart agriculture, and its development prospects are very impressive.\\n               \", 's0888613x13002867': '\\n                  A number of proposals have been proposed for measuring inconsistency for knowledge bases. However, it is rarely investigated how to incorporate preference information into inconsistency measures. This paper presents two approaches to measuring inconsistency for stratified knowledge bases. The first approach, termed the multi-section inconsistency measure (MSIM for short), provides a framework for characterizing the inconsistency at each stratum of a stratified knowledge base. Two instances of MSIM are defined: the naive MSIM and the stratum-centric MSIM. The second approach, termed the preference-based approach, aims to articulate the inconsistency in a stratified knowledge base from a global perspective. This approach allows us to define measures by taking into account the number of formulas involved in inconsistencies as well as the preference levels of these formulas. A set of desirable properties are introduced for inconsistency measures of stratified knowledge bases and studied with respect to the inconsistency measures introduced in the paper. Computational complexity results for these measures are presented. In addition, a simple but explanatory example is given to illustrate the application of the proposed approaches to requirements engineering.\\n               ', 's0921889015000834': '\\n                  The scope of this paper is to present a novel gait methodology in order to obtain an efficient walking capability for an original walking free-leg hexapod structure (WalkingHex) of tri-radial symmetry. Torque in the upper (actuated) spherical joints and stability margin analyses are obtained based on a constraint-driven gait generator. Therefore, the kinematic information of foot pose and angular orientation of the platform are considered as important variables along with the effect that they can produce in different gait cycles. The torque analysis is studied to determine the motor torque requirements for each step of the gait so that the robotic structure yields a stable and achievable pose. In this way, the analysis of torque permits the selection of an optimal gait based on stability margin criteria. Consequently, a gait generating algorithm is proposed for different types of terrain such as flat, ramp or stepped surfaces.\\n               ', 's1875952119300394': '\\n                  Elicited physiological affect in humans collaborating with their robot partners was investigated to determine its influence on decision-making performance in serious games. A turn-taking version of the Tower of Hanoi game was used, where physiological arousal and valence underlying such human-robot proximate collaboration were investigated. A comparable decision performance in the serious game was found between human and non-humanoid robot arm collaborator conditions, while higher physiological affect was found in humans collaborating with such robot collaborators. It is suggested that serious games which are carefully designed to take into consideration the elicited physiological arousal might witness a better decision-making performance and more positive valence using non-humanoid robot partners instead of human ones.\\n               ', 's0957417416306844': '\\n                  Gaussian Mixture Models (GMM) have many applications in density estimation and data clustering. However, the models do not adapt well to curved and strongly nonlinear data, since many Gaussian components are typically needed to appropriately fit the data that lie around the nonlinear manifold.\\n                  To solve this problem we constructed the Active Function Cross-Entropy Clustering (afCEC) method, which uses Gaussians in curvilinear coordinate systems. The method has a few advantages in relation to GMM: it enables easy adaptation to clustering of complicated data sets along with a predefined family of functions and does not need external methods to determine the number of clusters, as it automatically (on-line) reduces the number of groups.\\n                  Experiments on synthetic data, Chinese characters, data from UCI repository and wind turbine monitoring systems show that the proposed nonlinear model typically obtains better results than the classical methods.\\n               '}\n",
            "{'s0957417413009615': '', 's2590188519300162': '', 's0957417418306535': '', 's0888613x13002041': '', 's0957417419302489': '', 's107158191630074x': '', 's0888613x17302335': '', 's1071581916000021': '', 's0925231219301961': '', 's0957417417300751': '', 's0963868717301798': '', 's0736584518303831': '', 's0921889016306285': '', 's0957417419301903': '', 's0888613x1400067x': '', 's0888613x14001133': '', 's259018851930006x': '', 's0888613x1300234x': '', 's0888613x14000607': '', 's1071581919300552': '', 's0957417419301812': '', 's0020025519304864': '', 's0142061518304319': '', 's0933365716302950': '', 's0306437915000459': '', 's0921889015003000': '', 's1071581916300866': '', 's1071581918300016': '', 's1071581916301380': '', 's0957417415003590': '', 's0921889018307474': '', 's0957417417307698': '', 's1071581917301404': '', 's0921889014002164': '', 's1071581916300453': '', 's0952197619301253': '', 's1071581918300971': '', 's0306437919304909': '', 's0957417414006472': '', 's0925231217309864': '', 's0736584515000666': '', 's1071581917300988': '', 's0888613x14000206': '', 's0950705118301394': '', 's1875952116300040': '', 's0888613x14000851': '', 's0740818818301828': '', 's2590188519300083': '', 's1875952117300952': '', 's0888613x15000857': '', 's095741741830215x': '', 's0925231214000265': '', 's1071581918303471': '', 's0957417415000238': '', 's1071581918304312': '', 's0888613x13002910': '', 's0888613x14000796': '', 's0888613x15000912': '', 's1071581917300320': '', 's0888613x13002880': '', 's0888613x14001212': '', 's2589721719300029': '', 's0888613x13002867': '', 's0921889015000834': '', 's1875952119300394': '', 's0957417416306844': ''}\n",
            "{'s0957417413009615': ' 1 Introduction Facial expression is the explicit transformation of the human face due to the automatic responses to the emotional instability. In most situations it is spontaneous and uncontrollable. The automatic facial expression involves the application of an artificial intelligent system to recognize the expressions of the face under any circumstance. Today, the studies of facial expressions have gained keen interest in pattern recognition, computer vision and its related fields. Mainly, such facial expressions are the seven prototypical ones, namely; anger, fear, surprise, sad, disgust, happy, and neutral. Research into automatic facial expression recognition is very important in this modern society of technological age. For instance, the technology is applied in a wide variety of contexts, including robotics, digital signs, mobile applications, and medicine. It is reported that “some robots can operate by first recognizing expressions” of humans ( ). The AIBO robot for instance is a biologically-inspired robot that can show its emotions via an array of LEDs located in the frontal part of the head ( ). In addition to this, the robot can also display ‘happiness’ feeling when it detects a face. In behavioral sciences and medicine for instance, expression recognition is effectively applied for intensive care monitoring ( ). Currently, there are developing systems that are capable of making routine examinations of facial behavior during pain in clinical settings. In infants the Neonatal Facial Coding System (NFCS) has been employed for real-time assessment within 32 to 33 week post-conceptional age infants who are undergoing a heel lance. The technology is being used in more advanced settings to reduce accidents through the implementation of automated detection of driver drowsiness in public transports. This system relays information about the drivers’ emotional states to observers for effective surveillance leading to necessary awareness. The hallmark of every facial expression system is accuracy and to some extent the speed of execution. However most of the existing systems produce poor performances in terms of accuracy; as for execution speed, most of the systems are even silent to give a hint. Some few examples;   proposed a neural based facial expression recognition system that used principal component analysis (PCA) to reduce the feature vectors. The features were fed into a feed-forward neural network that was trained by a back-propagation network. In this system an average recognition of 84.5% was reported on the Yale facial expression database – an achievement which is not very encouraging.   described a neural network classification facial expression recognition system that employs Gabor feature extraction and feature reduction by PCA to distinguish 7-class facial expression recognition on the JAFFE database. In this system they specified 20 inputs, 40 to 60 hidden layers and seven output feed-forward neural networks. Again, the 60–70% recognition accuracy they obtained by their procedure is not encouraging to befit the expectations of a real-time system. Recently,   extracted features of the face using Affine Moment Invariants and performed the classification using feed-forward neural network. The expression recognition obtained was 93.8% on the JAFFE database.   extracted the facial features using a Sobel filter. In their experiment they reserved the maximum connected component to reduce the wrinkles and noises and conducted 7-class classification on JAFFE facial expression database through the application of Elman network with two hidden layers, each layer containing fifteen neurons. With this approach the average accuracy of automatic facial expression recognition is 84.7%.   extracted the expressive face by using Gabor filters, feature reduction by PCA and expression classification by neural network. In this method an average facial recognition of 93.4% was recorded in the JAFFE facial expression database.   also extracted facial features by Gabor techniques and reduced the features by PCA. The expression classifier was neural network and the average expression recognition was 94.5% ± 0.7 on the seven prototypical facial expressions, however the facial expression database was not mentioned. Most of these studies advocate the use of Neural Network as the expression classifier and extracted the facial features by Gabor filters and reduced the features via PCA. The displeasing thing is that all the results were not very encouraging. This study persists in exploring the potentials of neural networks to execute this kind of assignment, trying to esteem some biological constraints, utilizing the capabilities of modular systems. Though many techniques have been used to extract the facial features, Gabor feature extraction remains a high-quality choice; there are other alternatives but they are not very promising. Just a few examples:   utilized the Haar technique to extract facial features which were used as input to the neural network for classifying 8 facial expressions. The Haar wavelet extraction is very fast ( ), however the wavelets are too huge to result to effective classification when used as input to classifiers in facial expression recognition ( ); in other words it is a potential cause to misclassifications and poor performance. Distance-based feature extraction methods are also one of the largely applied techniques used for feature extraction in both 2D and 3D static faces. The idea behind these procedures is that the muscle deformations which are the major causes of changes in facial expression from normal expression results in variations of the Euclidean distances between facial landmarks or points. These points, as well as their distances, have been widely employed for static facial expression analysis ( ). Among the most successful ones is feature extraction based on the Bhattacharyya distance ( ). However, despite some advantages of this method, the degree of computational complexity is unacceptably high. The matching of even a small model shape with a normal image can take half an hour on an eight-processor Sun SPARCServer 1000 ( ). The Patch based feature extraction method is another alternative widely exploited for facial expression biometrics.   for instance represented extracted patches from facial surfaces by sets of closed curves and then applied a Riemannian framework to obtain the shape analysis of the extracted patches. However, the patch-based features also have numerous drawbacks. First, particular representations cannot be applied to other solutions without major modifications: the majority of the techniques have only been utilized to a single class. Also, most methods do not exploit the large amounts of available training data ( ). Thus on the basis of these we still considered Gabor features as the best approach, not because it does not have drawbacks, but the drawbacks can be easily managed. The Gabor filter is a superior model of simple cell receptive fields in cat striate cortex ( ), and it grants exceptional basis for object recognition and face recognition ( ). Again, the Gabor methods are superior to all the above-mentioned methods because it extracts the maximum information from local image regions ( ), and it is invariant against, translation and rotations ( ). In this work, the data were reduced in dimensions by Bessel transform ( ) and then after extraction of the face by Gabor methods, the features were further reduced via an AdaBoost-based ( ) feature reduction technique. The selected features which represented the facial deformation patterns were then fed into a 3-layer feed-forward neural network that is trained by a back-propagation algorithm. It is interesting to note that Bessel down-sampling techniques have never been adopted for facial expression recognitions. Again, the combinations of Bessel down-sampling and the formulated AdaBoost-based algorithm is an innovation that reduces the expression dataset to enhance accuracy and speed. Finally, the construction of the feed-forward neural network is influential to bring about successful results. The rest of the work is arranged as follows. Section   discusses face detection and image down-sampling. Section   discusses Gabor feature extraction. Section   discusses feature selection. Section   discusses the multilayer feed-forward neural network (MFFNN). Results and analysis are presented in Section  . The final conclusions of the work are drawn in Section  .', 's2590188519300162': \" 1 Introduction Predicting the future price of the stock market is extremely important for the investors as knowing the appropriate movement of the stock price will reduce the risk for determining future trends so as to invest money. It's a challenging issue to know the future movements due to its highly fluctuating nature, hence, suitable computational methods are required to forecast the stock price movement. Regarding stock market predictability ( ) many controversies have been rising from decades long. Initially, the movement of the stock price was characterized by the random walk theory ( ) and later the research on price changes was carried out based on the Efficient Market Hypothesis (EMH) ( ). According to their perspective, the future price movement does not depend on the present and past value and also from their point of view future stock price prediction is impossible. In another way, various studies have made effort to experimentally disprove the EMH and the observational evidences have delineated that the stock market can be predicted to some extent. Numerous traditional approaches have been developed by the researchers in the field of forecasting the stock price movements such as Autoregressive Moving Average (ARMA), Autoregressive Integrated Moving Average (ARIMA) etc., but ( ) these methods are having some limitations, they are not capable to handle the nonlinear relationship exist in time series data as they make the assumption of a linear form of the structure of the model. Furthermore, they are presumed to have constant variance, whereas, the financial time series data are very noisy by nature. Later the issues regarding the existence of non linearity in time series data has been solved by the approach of so many machine learning techniques such as an Artificial Neural Network (ANN), Functional Link Artificial Neural Network (FLANN), Support Vector Machine (SVM) etc. In the last decades, numerous researchers have explored the prediction ability of neural networks for financial market prediction. This study emphasizes on increasing the ability of the prediction model through feature reduction techniques. From the immense literature study it has observed that various kinds of neural networks have been used for stock market prediction. Since the literature study hardly divulges the efficiency of Extreme Learning Machine (ELM), Online Sequential Extreme Learning Machine (OSELM) and Recurrent Back Propagation Neural Network (RBPNN) using both statistical and optimized feature reduction techniques in the domain of financial market prediction. In this study, a comparison has made on the outcome of the prediction model considering statistical based feature reduction along with swarm evolutionary based feature reduction technique. The proposed OSELM and firefly with an evolutionary framework strategy model has not used in the financial domain, specifically in stock market prediction and this work basically focused on prediction of the future value of the stock price, which is entirely application oriented and not reported or published yet in the financial domain. An empirical comparison is established among the performance of prediction models using both statistical and optimized feature reduction techniques. For statistical based feature reduction Principal Component Analysis (PCA) and Factor Analysis (FA) and for optimized feature reduction Firefly Optimization (FO), Genetic Algorithm (GA) and Firefly algorithm with evolutionary framework are considered for the experimental work. In the group of heuristic algorithm FO is considered as the most interesting and novel population based heuristic algorithm, which follows the behavior of fireflies. The beauty of firefly is that, each firefly supposed to be attracted ( ) by each other firefly irrespective of their sex, here the attraction between the fireflies is directly proportional to their brightness that means the lesser brighter is attracted towards the more brighter one and here in experimental point of view the brightness is decided by calculating the fitness function. On the other hand to generate new fireflies with better performance genetic algorithm is employed on FO, which is termed as Firefly algorithm with evolutionary framework in this study. The Firefly algorithm with evolutionary framework, which is the combination of FO and GA explores the optimal solutions more exactly than individual FO and GA. All these features reduction techniques are applied to the experimented prediction models such as ELM, OSELM and RBPNN. Empirically the model validation is realized over four stock market datasets such as BSE Sensex, NSE Sensex, S&P 500 index and the FTSE index within a given period of time. The dataset is regenerated using technical indicators and statistical measures, based on the available features such as open price, close price, maximum price, minimum price and change of price considering the window size seven. The proposed work provides better prediction ability than the other evaluated learning techniques and prediction models used in this experimental work. The rest of the paper is organized as follows:   contains the related work which basically focuses on the promising techniques used by various researchers in financial domain to get best possible results.   covers the details of data samples and the dataset regeneration part.   introduces all the predictive models in details and   covers the particulars of statistical and optimized based feature reduction techniques.   provides the result analysis, which includes a schematic layout of the proposed prediction model, parameter setup, detailed steps of the proposed algorithm, the analysis of the experimental outcome in the form of simulation graph and the training result in MSE along with the performance evaluation measures. Finally,   and   present the overall work analysis and conclusions respectively.\", 's0957417418306535': ' 1 Introduction Arms races alternate between incremental and disruptive moves like the stockpiling of armaments and the invention of airplanes. The malware detection/evasion arms race is no exception. Its history exhibits periods of minor moves and counter-moves like tweaking malware to avoid known signature of disruptive moves like the transition to polymorphic concealment. Our core contribution is to show how to use search to restrict the adversary to only making disruptive moves. Given an evasion or detection technique, we use machine learning to search for transformations that produce variants that force the adversary to make expensive, disruptive moves. The specific detection and evasion techniques we consider use information theoretic entropy. To conceal their malware, black hats often rewrite it. Polymorphism hides malware by encoding it and decoding it at runtime. Because it is trivially semantic preserving, it is the dominant way black hats conceal their malware — in 2016, Webroot reported that 97% of malware is polymorphic ( ). There are two main classes of polymorphism: those that encrypt the malware and those that compress it. Both encryption and compression increase the entropy of their input, when forced to produce output whose size is not much larger than the input. Neither Trojans that are large relative to their hosts nor disk-resident malware that hide themselves polymorphically can violate this constraint without risking becoming too large and therefore less viable. In this case, polymorphism introduces an entropy signature ( ) that distinguishes it from many classes of benign-ware. The promise of entropy as a malware detector is that it works on executables as binary strings, without needing pre-processing, disassembly, dynamic analysis, reverse engineering, or manual analysis. Structural Entropy ( ) ( ) took the first step toward effectively exploiting entropy to detect malware when treating executables solely as binary strings. When a file is separated into chunks, the   of that file is its per-chunk entropy.   computes the entropy signature of a file over small, fixed size chunks. After computing a file’s entropy signature,   segments this signature into sequences of chunks, treats these segments as symbols, then uses Levenshtein to compare the resulting string against the segmentation strings it extracts from other files. It has two severe limitations. First, it does not scale: it relies on the pairwise comparison ( ( \\n                      )) of a suspicious program with a zoo of known malware and benign-ware. Second, its decomposition of an entropy signature into segments, whose entropy it compares, relies on six parameters, three of which are implicit and baked into   technique itself ( ). The authors do not elucidate the settings of these implicit parameters, nor is it obvious from first principles how they should be set and whether they can be learned. There are other three explicit parameters: the chunk size that divides the file into blocks to measure their entropy, the number of chunks or blocks used and a noise threshold. To overcome these limitations, we present a disruptive detection move called EnTS (Entropy Time Series), a new entropy-based malware detector. EnTS scales and requires only three explicit parameters that can be learned from a corpus. EnTS constructs a metric space for entropy signatures. EnTS considers an entropy signature as a time series, then applies wavelet analysis to clean the signature and extract a simplified signature from the amplitude and longitudinal variation in a file’s entropy. EnTS then treats this signature as a point. In this way, EnTS constructs a metric space, while   resorts to segmentation and then to the pairwise computation of edit distance to construct its metric space ( ). Eliminating  ’s segmentation step removes the implicit parameters that   requires. The zoo defines EnTS’ three explicit parameters: the chunk size, number of chunks, and noise threshold ( ). We designed EnTS to defeat polymorphism. On a corpus that includes the Kaggle and the VirusShare training sets and an equal number of benign-ware from   , EnTS achieves 82% accuracy when maximizing 100% precision and 93.9% accuracy when maximizing accuracy. EnTS surpasses the quality of   in terms of scalability: it is more than 15 times faster and linear in its time and memory consumption in contrast to  ’s  ( \\n                      ). EnTS’ accuracy is between 2 to 5 points higher than  ’s in all cases. EnTS is also good at detecting metamorphic malware, not just polymorphic, because metamorphic malware often has compressed or encrypted regions. It detect all the metamorphic variants from our test corpus. EnTS outperforms all 56 VirusTotal AV engines \\n                       applied to the same data, the best of which achieved only 40.6% accuracy. EnTS’ time complexity is linear in the number of files being classified; it is 3000 times faster than its main competitor in accuracy, another information theoretic technique named normalized compression distance ( ) that we built as a baseline ( ). Clearly, the next disruptive evasion to defeat EnTS must control the entropy of packed regions. For this task, we developed   (the evolutionary packer or ‘El Empaquetador Evolutivo’).   is a polymorphic engine that controls the entropy of packed regions under a tight space budget, increasing the size of its input at most 1% ( ).   creates a packed variant with a specific entropy signal by creating chunks with a specific entropy and injecting into the packed binary. To decide position, entropy, and size of these chunks,   leverages evolutionary computation.   is an instance of adversarial machine learning; it uses search to exploit the vulnerabilities of a entropy-based detector in order to fool it ( ). \\n                       defeats  , EnTS, and other state of the art binary-based detection techniques ( ).   explodes the false negative (FN) rates of these techniques: Prior to  ’s application, these technique’s FN rates range over [0%–9.4%]; after  ’s application, their FN rates range over [90.8–98.7%].   rapidly learns to defeat these tools; It takes two generations to defeat the weakest ones and eight to defeat the strongest. To its credit, EnTS resists   better than the other techniques ( ). EnTS embeds binaries into entropy metric space and uses machine learning to detect malware and to advance the state of the art. Defeating it requires  , a disruptive entropy-based evasion technique that uses search to control the entropy of the binary it is concealing. The main contributions of this paper are: \\n                   EnTS,  , and the corpus on which we evaluated them will be available online \\n                      \\n                  ', 's0888613x13002041': \" 1 Introduction Based on fuzzy time series concept, first forecasting model was introduced by Song and Chissom  . They presented the fuzzy time series model by means of fuzzy relational equations involving max–min composition operation, and applied the model to forecast the enrollments in the University of Alabama. In 1996, Chen   used simplified arithmetic operations avoiding the complicated max–min operations, and their method produced better results. Later, many studies provided some improvements in Song and Chissom method in terms of the following issues: \\n                   To enhance the accuracy in forecasted values, many researchers recently proposed various fuzzy time series models. For example, Wong et al.   proposed an adaptive time-variant model that automatically adapts the analysis window size of fuzzy time series based on the predictive accuracy in the training phase and uses heuristic rules to determine forecasting values in the testing phase. To extend the applicability of the univariate models to accommodate multiple variables and to improve forecasting result, Huarng et al.   proposed a multivariate heuristic function, which can be integrated with univariate fuzzy time series models to form a multivariate model. Bai et al.   proposed a heuristic time-invariant forecasting model in which there is a trend predictor for indicating trend of the increase or decrease in time series. To resolve the problem associated with determination of length of intervals and data defuzzification, Singh and Borah   proposed two-factors high-order fuzzy time series model. The proposed model is based on the hybridization of ANN with fuzzy time series. Singh and Borah   presented a new model based on hybridization of fuzzy time series theory with ANN. In this model, to defuzzify the fuzzified time series values and to obtain the forecasted results, an ANN based architecture is developed, and incorporated in the model. The application of fuzzy time series in financial forecasting has also attracted many researchers' attention in the recent years. In recent years, many researchers focus on designing the models for TAIEX   and TIFEX   forecasting. Their applications are limited to deal with either one-factor or two-factors time series data sets. For the stock index forecasting, Huarng and Yu   show that the forecasting accuracy can be improved by including more observations (e.g.,  , and  ) in the models. All the models proposed by researchers above are based on Type-1 fuzzy set concept except the model proposed by Huarng and Yu   in 2005, which is based on Type-2 fuzzy set concept. Researchers employ Type-2 fuzzy set concept (which is an extension of Type-1 fuzzy set) in various domains such as control system design and modeling  , because Type-2 fuzzy set systems are much more powerful than Type-1 fuzzy sets systems to represent highly nonlinear and/or uncertain systems  . Nowadays, Type-2 fuzzy set concept is successfully applied in time series forecasting  . In this study, we aim to propose an improved fuzzy time series model by employing M-factors time series data set. To deal with these factors together, we design a model based on Type-2 fuzzy time series concept, which is an improvement over the existing Type-2 model proposed by Huarng and Yu  . Later, to enhance the forecasting accuracy, we hybridize the PSO algorithm with the proposed Type-2 model. The daily stock index price data set of SBI is employed for the experimental purpose, which consists of 4-factors, viz., “Open”, “High”, “Low” and “Close” factors/variables. After that, performance of the hybrid model is evaluated, which demonstrates its effectiveness over conventional fuzzy time series models and non-fuzzy time series models. The proposed model is also validated by forecasting the stock index price of Google. This paper is organized as follows. In Section  , we present the problem definitions. In Section  , we review the theory of fuzzy set with an overview of fuzzy time series. In Section  , the particle swarm optimization is introduced. In Section  , we present the algorithm and defuzzification process for the Type-2 model. In Section  , we explain the proposed Type-2 model. In Section  , we provide the details of the new proposed hybrid forecasting model. In Section  , we discuss some statistical parameters, which are used to check the robustness the model. The performance of the model is assessed and presented in Section  . Finally, the concluding remarks and future works are discussed in Section  .\", 's0957417419302489': ' 1 Introduction A person’s palm skin presents a particular line pattern that is permanent and unique ( ). Given these characteristics, an imprint of the palm’s line pattern, called palmprint, can be used to identify an individual uniquely ( ). Identification is possible even from palmprints obtained from the imprint left when the palm comes into contact with a surface, called latent palmprints ( ). Latent palmprint identification is paramount for law enforcement as approximately 30% of the prints found at a crime scene are from palms ( ), and palmprints contain more information than fingerprints ( ). Given its importance, in this study, we survey the current state of the art of latent palmprint identification, focusing on the feature representations used for identification. Palmprint identification involves the comparison of a given palmprint, called a query, against a collection of existing palmprints taken under controlled conditions, called templates, each of which has been associated with a known identity. The palmprints are compared by key features that are extracted from the palmprint, such as palmprint direction ( ), or the principal lines of the palm ( ). These features can be represented in different ways to include information from the palmprint to allow for increased discerning capabilities when making the identification between pairs of query and template palmprints. For each pairwise comparison, a matching algorithm compares the feature extracted from both palmprints and emits a similarity score. Supposedly, the comparison that results in the highest similarity indicates the identity of the query that is under investigation ( ). Because the performance of the matching algorithm depends on the feature representation to be compared, there is an interest in determining which features allow to obtain better performance ( ). Latent palmprint identification is a difficult problem in palmprint identification. As the latent palmprints are not obtained under controlled conditions, they could present distortion or degradation, depending on the surface where they were found ( ). Furthermore, latent palmprints commonly do not contain information from the whole palm region. Most methods for palmprint identification perform full-to-full palmprint comparisons and require to find an area of interest to align the palmprints. However, these methods do not apply when carrying out a partial-to-full comparison, which is the common case in latent palmprint identification ( ). The methods that can be used for the latent case are those where the features are not from a specific area of the palm, such as line patterns of the hand, or the salient points of these lines, called minutiae. In this study, we survey the features used in the different methods for latent palmprint identification, the representations of the features, and the reported identification performance that each method has attained. In our review, we found that methods that use minutiae supposedly obtain better results. Besides, we found that there is no standard evaluation methodology and that the existing evaluations are conducted using different datasets. Furthermore, existing methodologies do not allow determining whether the differences in performance are due to the matching algorithms or the feature representations used. Given the lack of a standard evaluation methodology, the reported performance results are not comparable, and thus, we cannot determine which method and feature representation for latent palmprint identification, if any, is better. Forensic experts ascertain the identity of individuals involved in crime scenes by using features, such as minutia, extracted from palmprints. Given the number of prints that have to be compared to determine the possible identity based on a latent palmprint, it is impossible for an expert to perform all the comparisons manually. Thus, experts rely on Automated Fingerprint Identification Systems (AFIS) for comparing an unknown latent print against a dataset of template prints containing known identities, which return a ranked collection of possible suspects to be analyzed by experts manually. The combination of manual matching done by an expert and the matching done by an AFIS has shown that can outperform the expert and the system alone ( ), as happens in other areas such as face recognition ( ). Given the importance of AFIS to aid the decisions of experts in determining the identity of possible crime suspects, it is necessary to evaluate each part of the identification systems separately, with the aim of determining which of them could be improved for obtaining better identification results. In this paper, we address the lack of a standard methodology for fairly evaluating feature representations, which from our point of view should provide all the necessary materials for independent replication and evaluation of palmprint features publicly. In latent palmprint identification, minutiae are the most commonly used feature, which is also used by forensics experts, and the methods using them have the highest reported performance. To compare the matching performance when using different minutiae representations and to determine which one allows to obtain a better performance, in this paper, we propose a new methodology to fairly evaluate minutiae-based feature representations used in the context of latent palmprint identification. To achieve this, the proposed methodology measures the matching performance using the similarity scores obtained when comparing the minutiae representations in the query palmprints against those in the templates. Thus, our evaluation is performed independently of the global matching algorithms proposed in each latent palmprint identification method. By our evaluation methodology, we show that no minutiae representation allows to obtain a better performance than the others, by always obtaining the highest similarity scores for matching minutiae pairs. Because our results were obtained using a public latent palmprint dataset ( ), they serve to establish a baseline for future developments in this area. Our results show that the performance in latent palmprint identification can be enhanced further with the development of minutiae representations that better capture discriminative information about the palmprints. Although we focus on evaluating the feature representations, further work involves evaluating different matching algorithms independently of the features and their representations or determining whether representations can be combined in a manner that improves the performance. The contributions of this paper are as follows: (i) an in-depth review of the features used in the context of latent palmprint identification, and their representations, (ii) the introduction of a methodology to evaluate palmprint feature representations based on minutiae, and (iii) experimental results that allow for a fair comparison of the performance obtained by using different minutiae-based representations used in latent palmprint identification.', 's107158191630074x': \" 1 Introduction Business processes such as in manufacturing and logistics or in administration, but also technical processes like in robotics, are becoming increasingly complex while they are at the same time more and more automated, computerized and monitored in real-time ( ). This is true for processes in many domains, but especially for industrial productions, where a delayed delivery of raw materials can lead to a standstill in production and thus high loss of profit. On the one hand the increasing amount of data offer an enormous potential to better monitor and control processes. On the other hand it puts increasing pressure on monitoring personnel who need to observe processes. The status quo in large-scale process monitoring is heavily focused on control centers where users observe production on multiple screens, using both video features as well as schematic overviews of process and machines/facilities, charts/graphs, textual descriptions and alerts ( ). Especially in smaller- and medium-sized production companies, there are often no dedicated personnel charged with full-time monitoring, but instead engineers and supervisors need to primarily perform other tasks, yet monitor the process’ status at the same time. However, especially in such peripheral or   scenarios where the attention is focused on a primary task and other information is monitored indirectly at the same time, visual means are not well suited, as pointed out by  . Meanwhile, maintenance experts have been using the auditory sense to identify or anticipate possible machine problems, a technique referred to as vibration analysis, for a long time. Crucial vibration properties are amplitude, frequency, phase and modulation ( ). Therefore, traditional production monitoring is still considered to be a holistic approach, covering the visual, auditory and even olfactory sense, even though automation has enhanced   vibration analysis in the recent years ( ). In modern production settings, sound is typically only used as a means to convey warnings and alerts, e.g., to convey an alarm situation when a machine broke down or a predefined threshold had been exceeded ( ). In a production scenario, this could for instance be the case when the stock level of a resource has dropped below a critical level, or when a temperature sensor of a machine measures a critical temperature, indicating imminent machine failure ( ). However, this type of auditory display has several drawbacks: on the one hand, if rules that define alert triggering thresholds are defined too  , i.e., requiring strong evidence before issuing positive classifications, potentially critical situations such as machine failures might occur without issuing an alert. On the other hand, if the values are defined too  , i.e. risking high false positive rates, the resulting flood of (in many cases unnecessary) alerts and alarms might lead to an information overload of the user, or to the situation that the user stops to take the alerts as serious as they are. Furthermore, in many scenarios engineers are not able to define all states and values that might lead to a critical situation beforehand. Levels and values that might constitute a critical state are often complex to decide, as e.g., the question if a specific parameter value constitutes a critical situation or not often depends on the context, given by various other parameters. But even if all possibly critical situations are covered by alerts and alarms, in most cases operators might prefer to be informed even   a situation might become critical, thus enabling them to  ,   and   the problem. A constant awareness of states and values through an   might enable such an anticipation of critical situations. Thus, we suggest to use the mentioned tradition of auditory monitoring as a leverage effect by supplementing state-of-the-art visual process monitoring with techniques from sonification. Sonification is the systematic, reproducible and thus scientific method for representing data as (mostly non-speech) sound in an auditory display ( ). Well-known examples of sonifications are the Geiger counter for displaying radioactive radiation, or the auditory parking aid which conveys the distance to the vehicle or obstacle behind as pulse rate of a beep sound. Beyond these very basic and simple types, sonification researchers have developed a plethora of approaches to represent more complex data such as multivariate time series (e.g. EEG and ECG), or spatio-temporal data (e.g. images and well logs), and also general high-dimensional data distributions. Sonification has several key advantages that makes it suitable especially for the application area of real-time process monitoring, like our ability to process audio faster than visuals or the fact that we easily habituate to static sound sources, yet that we are at the same time very sensitive to changes ( ). For these reasons sonification promises a solution to the aforementioned challenges of state-of-the art process monitoring. However, there are several open questions when it comes to supporting users in monitoring as a secondary task that concern the sonification design and as well as how different types of sound-enhanced process monitoring affect attention and concentration in main- and secondary task, which we tackled with this paper. Our main research goals were (a) to find out if a continuous, soundscape-based sonification of individual production steps can support users better in monitoring as a secondary task than a purely-visual solution, or one that is based on auditory alerts. Other open research questions were (b) to what extent the three different conditions distract users from their main task, (c) how users rate the three different conditions concerning relevant aspects such as pleasingness, helpfulnesses, intrusiveness or exhaustiveness. Answering those research questions poses several challenges, such as simulating the potential users' main- and secondary task in such a way, that they are both cognitively demanding and thus binding the undivided attention, while at the same time allowing for an easy and reliable measurement of task performance in a fine-grained manner. As there are no standardized environments that fit these requirements, we have developed the SoProMon system (Sonification for Process monitoring), that is a hard-/software system for reproducible research in sonification for peripheral monitoring, particularly for the investigation of attention allocation in dual-task-settings. The system has already been presented in  , and consists primarily of a main task console to bind the user's attention by presenting simple arithmetic problems and a simulated production process that requires different types of user interactions (see  ). Based on the SoProMon system, we conducted an extensive experiment in a within-subject design (n=18), whose results contribute to answering the mentioned open research questions, and thus to advancing research in this area (see  ) in the following ways: \\n                   As industrial production is an area, in which it is especially crucial to monitor processes in real-time and that can probably be intuitively understood also by domain novices, the secondary task of this experiment is based on a simulated production processes. However, as the experiment design aimed at quite fundamental questions of attention allocation, the results should be generic enough to be transferable to monitoring scenarios in other domains as well. \\n                       The details on the current state-of-the-art concerning research in sonification for (peripheral) process monitoring as well as on the open research issues that we tried to tackle with the experiment can be found in  . The hypotheses derived from the literature which we tried to tackle with the experiment are described in  , followed by an introduction into the SoProMon system ( ) and the methodology of our experiment ( ). Experimental results will be presented in   and discussed in  , followed by overall conclusive considerations.\", 's0888613x17302335': ' 1 Introduction \\n                       usually occurs in survey data, and it refers to respondents that under-report the answer to a question, for example due to a perceived social stigma  . A famous example is maternal smoking during pregnancy, which is a key risk factor for adverse offspring outcomes including preterm birth and low birth weight (LBW). Like many health behaviours, accurate measurement of smoking habits can be difficult and expensive during pregnancy. For that reason, many studies use   data, e.g. Wright et al.  . Given that most smokers know their habit to be harmful, both to themselves and their unborn child, there are strong motivations for women to under-report or deny their smoking status  . As such, the frequency of smokers in a sample is expected to be significantly lower than would be expected according to expert knowledge, derived for example from blood test result. Gorber et al.   presented a comprehensive analysis of the literature and compared the prevalence estimates of smoking based on self-reported data against the prevalence estimates based on directly measured smoking biomarkers. According to this analysis, self-reported smoking is generally under-reported in such a way that the true smoking figures may be underestimated by up to 47%. Estimating the association between an   (UR) variable and another will be biased in a manner that is specific to the degree/pattern of UR. Thus, any policy decisions made on the basis of such a biased result will be questionable. For example, government policies on tobacco control, e.g.  , maybe ill-formed if they do not take into account UR. Maternal smoking and alcohol consumption are our focus for this paper, but there are many important health applications where   for UR are needed – e.g. HIV prevalence  . One method to correct UR bias is to spend time and resources to manually identify individuals that are likely to have misreported, and ignore/correct their testimony, i.e. identify smokers by performing cotinine blood tests. Unfortunately, in many applications it is impossible to completely correct the misreported cases. For example, Gorber et al.   present some possible flaws of the biochemical markers that identify smoking. As an alternative to this, authors in medical statistics treat it as a problem of  , and combine data with a prior belief of the pattern of misclassification. They use this prior knowledge to derive corrected estimators for the log-odds ratio  , and the relative-risk  , or to suggest ways for performing tests of independence  . These solutions suffer from a number of weaknesses, which are addressed in this paper. For testing independence, they do not control both types of error (false positive/false negative). For estimating effect sizes, the suggested solutions are naturally only applied to estimate the correlation between a binary UR variable and a binary target variable – multi-class target variables with more than two categories are handled via a one-vs-one or one-vs-all strategy. Furthermore, ranking of variables in relation to a target – a common need in feature selection and other machine learning tasks – is not straightforward. Our strategy to overcome these limitations is to provide an information theoretic insight for the under-reporting problem, by disentangling three intimately related activities:  ,  , and   of features. Our main goal is to derive corrected estimators for the   (MI), a measure of effect size widely used in machine learning applications with several interesting properties  . To achieve our goal we reinterpret the challenge not as dealing with misclassification and biased data, but as a problem of  , and particularly learning from positive and unlabelled data  . By this interpretation, we present solutions using a graphical representation called   or  graph  , which is a tool to naturally incorporate a prior belief over the misreporting at the population (or appropriate sub-demographic) level. Furthermore, with our work, we show how to correct MI for under-reporting by examining independence properties observable via the  graph representation. In this paper, we present the following novel contributions \\n                       in relation to UR variables:  \\n                       \\n                       presents two different methods for information theoretic feature ranking in the presence of UR variables, by using our suggested estimators. Section   presents Corrected-MIM, a univariate approach that provides rankings and captures only the relevance, while Section   presents Corrected-mRMR, a multivariate method that captures both the relevance and redundancy between the features. All the above contributions are novel, with the exceptions of Sections   and  , which have been published in a conference paper  . Furthermore, we provide further experimental results in two applications. Firstly,   \\n                       derives rankings of risk factors related to low birth weight infants using a case study of 13,776 births in northern England, where we demonstrate some significant false conclusions that might be drawn when ranking variables without the correction factors. Finally,   \\n                       presents a machine learning application where we derive rankings of features when training/test distributions differ.', 's1071581916000021': ' 1 Introduction The ability to accurately link a physical person to their activities conducted in the digital realm is an increasing challenge for our intelligence and law enforcement agencies. The present work was inspired through collaboration with the large multidisciplinary project Super Identity \\n                       whose overarching aim is to model links between a wide range of physical/digital identity measures in order to build new identity attribution techniques. The theme of this paper was inspired by a need to address a specific real-world problem: how to identify criminals who conduct their activities using temporary burner mobile phones that are unregistered to them by name. Our main objective is to establish whether touch interaction dynamics can be leveraged to provide usable evidence as to the physical characteristics of their creator. In this paper, we present initial findings from an exploration into whether it is feasible to infer a single specific physical characteristic of a person – specifically the length of their thumb – from the way in which they perform a common smartphone interaction gesture, the ‘swipe’. If such a link could be made, we suggest that by following a route through known proportional relationships between that digit length and other human measurements, we would conceivably be able to infer various other physical characteristics of the person who created the gestures. To provide an example, to possess a long thumb suggests a longer hand length, a longer hand length suggests a longer forearm length and a longer forearm length suggests a taller standing height. \\n                      \\n                      \\n                      \\n                      \\n                      \\n                   \\n                      \\n                      \\n                      \\n                      \\n                      \\n                   \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                   \\n                      \\n                      \\n                      \\n                      \\n                      \\n                  ', 's0925231219301961': ' 1 Introduction Segmentation of medical images is an essential task for many applications such as anatomical structure modeling, tumor growth measurement, surgical planing and treatment assessment\\xa0 . Despite the breadth and depth of current research, it is very challenging to achieve accurate and reliable segmentation results for many targets\\xa0 . This is often due to poor image quality, inhomogeneous appearances brought by pathology, various imaging protocols and large variations of the segmentation target among patients. Therefore, uncertainty estimation of segmentation results is critical for understanding how reliable the segmentations are. For example, for many images, the segmentation results of pixels near the boundary are likely to be uncertain because of the low contrast between the segmentation target and surrounding tissues, where uncertainty information of the segmentation can be used to indicate potential mis-segmented regions or guide user interactions for refinement\\xa0 . In recent years, deep learning with convolutional neural networks (CNN) has achieved the state-of-the-art performance for many medical image segmentation tasks\\xa0 . Despite their impressive performance and the ability of automatic feature learning, these approaches do not by default provide uncertainty estimation for their segmentation results. In addition, having access to a large training set plays an important role for deep CNNs to achieve human-level performance\\xa0 . However, for medical image segmentation tasks, collecting a very large dataset with pixel-wise annotations for training is usually difficult and time-consuming. As a result, current medical image segmentation methods based on deep CNNs use relatively small datasets compared with those for natural image recognition  . This is likely to introduce more uncertain predictions for the segmentation results, and also leads to uncertainty of downstream analysis, such as volumetric measurement of the target. Therefore, uncertainty estimation is highly desired for deep CNN-based medical image segmentation methods. Several works have investigated uncertainty estimation for deep neural networks\\xa0 . They focused mainly on image classification or regression tasks, where the prediction outputs are high-level image labels or bounding box parameters, therefore uncertainty estimation is usually only given for the high-level predictions. In contrast, pixel-wise predictions are involved in segmentation tasks, therefore pixel-wise uncertainty estimation is highly desirable. In addition, in most interactive segmentation cases, pixel-wise uncertainty information is more helpful for intelligently guiding the user to give interactions. However, previous works have rarely demonstrated uncertainty estimation for deep CNN-based medical image segmentation. As suggested by Kendall and Gal  , there are two major types of predictive uncertainties for deep CNNs:   uncertainty and   uncertainty.   uncertainty is also known as model uncertainty that can be explained away given enough training data, while   uncertainty depends on noise or randomness in the input testing image. In contrast to previous works focusing mainly on classification or regression-related uncertainty estimation, and recent works of\\xa0Nair et\\xa0al.   and Roy et\\xa0al.   investigating only test-time dropout-based ( ) uncertainty for segmentation, we extensively investigate different kinds of uncertainties for CNN-based medical image segmentation, including not only   but also   uncertainties for this task. We also propose a more general estimation of   uncertainty that is related to not only image noise but also spatial transformations of the input, considering different possible poses of the object during image acquisition. To obtain the transformation-related uncertainty, we augment the input image at test time, and obtain an estimation of the distribution of the prediction based on test-time augmentation. Test-time augmentation (e.g., rotation, scaling, flipping) has been recently used to improve performance of image classification\\xa0  and nodule detection\\xa0 . \\xa0Ayhan and Berens   also showed its utility for uncertainty estimation in a fundus image classification task. However, the previous works have not provided a mathematical or theoretical formulation for this. Motivated by these observations, we propose a mathematical formulation for test-time augmentation, and analyze its performance for the general \\xa0  uncertainty estimation in medical image segmentation tasks. In the proposed formulation, we represent an image as a result of an acquisition process which involves geometric transformations and image noise. We model the hidden parameters of the image acquisition process with prior distributions, and predict the distribution of the output segmentation for a test image with a Monte Carlo sampling process. With the samples from the distribution of the predictive output based on the same pre-trained CNN, the variance/entropy can be calculated for these samples, which provides an estimation of the   uncertainty for the segmentation. The contribution of this work is three-fold. First, we propose a theoretical formulation of test-time augmentation for deep learning. Test-time augmentation has not been mathematically formulated by existing works, and our proposed mathematical formulation is general for image recognition tasks. Second, with the proposed formulation of test-time augmentation, we propose a general   uncertainty estimation for medical image segmentation, where the uncertainty comes from not only image noise but also spatial transformations. Third, we analyze different types of uncertainty estimation for the deep CNN-based segmentation, and validate the superiority of the proposed general   uncertainty with both 2D and 3D segmentation tasks.', 's0957417417300751': ' 1 Introduction The growth of user-generated content in web sites and social networks, such as Twitter, Amazon, and Trip Advisor, has led to an increasing power of social networks for expressing opinions about services, products or events, among others. This tendency, combined with the fast spreading nature of content online, has turned online opinions into a very valuable asset. In this context, many Natural Language Processing (NLP) tasks are being used in order to analyze this massive information. In particular, Sentiment Analysis (SA) is an increasingly growing task ( ), whose goal is the classification of opinions and sentiments expressed in text, generated by a human party. The dominant approaches in sentiment analysis are based on   techniques (  \\n                      ). Traditional approaches frequently use the Bag Of Words (BOW) model, where a document is mapped to a feature vector, and then classified by machine learning techniques. Although the BOW approach is simple and quite efficient, a great deal of the information from the original natural language is lost ( ), e.g., word order is disrupted and syntactic structures are broken. Therefore, various types of features have been exploited, such as higher order  -grams ( ). Another kind of feature that can be used is Part Of Speech (POS) tagging, which is commonly used during a syntactic analysis process, as described in  . Some authors refer to this kind of features as   forms, as they consist in lexical and syntactical information that relies on the pattern of the text, rather than on its semantic aspect. Some prior information about sentiment can also be used in the analysis. For instance, by adding individual word polarity to the previously described features ( ). This prior knowledge usually takes the form of  , which have to be gathered. Sentiment lexicons are used as a source of subjective sentiment knowledge, where this knowledge is added to the previously described features ( ). The use of lexicon-based techniques has a number of advantages ( ). First, the linguistic content can be taken into account through mechanisms such as sentiment valence shifting ( ) considering both intensifiers (e.g. very bad) and negations (e.g. not happy). In addition, sentiment orientation of lexical entities can be differentiated based on their characteristics. Moreover, language-dependent characteristics can be included in these approaches. Nevertheless, lexicon-based approaches have several drawbacks: the need of a lexicon that is consistent and reliable ( ), as well as the variability of opinion words across domains ( ), contexts ( ) and languages ( ). These dependencies make it hard to maintain domain independent lexicons ( ). In general, extracting complex features from text, figuring out which features are relevant, and selecting a classification algorithm are fundamental questions in the machine learning driven methods ( ). Traditional approaches rely on manual feature engineering, which is time consuming. On the other hand, deep learning is a promising alternative to traditional methods. It has shown excellent performance in NLP tasks, including Sentiment Analysis ( ). The main idea of deep learning techniques is to learn complex features extracted from data with minimum external contribution ( ) using deep neural networks ( ). These algorithms do not need to be passed manually crafted features: they automatically learn new complex features. Nevertheless, a characteristic feature of deep learning approaches is that they need large amounts of data to perform well ( ). Both automatic feature extraction and availability of resources are very important when comparing the traditional machine learning approach and deep learning techniques. However, it is not clear whether the domain specialization capacity of traditional approaches can be surpassed with the generalization capacity of deep learning based models in all NLP tasks, or if it is possible to successfully combine these two techniques in a wide range of applications. In this paper, we propose a combination of these two main sentiment analysis approaches through several ensemble models in which the information provided by many kinds of features is aggregated. In particular, this work considers an ensemble of classifiers, where several sentiment classifiers trained with different kinds of features are combined, and an ensemble of features, where the combination is made at the feature level. In order to study the complementarity of the proposed models, we use six public test datasets from two different domains: Twitter and movie reviews. Moreover, we performed a statistical study on the results of these ensemble models in comparison to a deep learning baseline we have also developed. We also present the complexity of the proposed ensemble models. Besides, we present a taxonomy that classifies the models found in the literature and the ones proposed in this work. With our proposal we seek answers to the following questions, using the empirical results we have obtained as basis:\\n \\n                   The rest of the paper is organized as follows.   shows previous work on both ensemble techniques and deep learning approaches.   describes the proposed taxonomy for classifying ensemble methods that merge surface and deep features, whereas   addresses the proposed classifier and ensemble models. In  , we describe the designed experimental setup. Experimental results are presented and analyzed in  . Finally,   draws conclusions from previous results and outlines the future work.', 's0963868717301798': ' 1 Introduction The challenge of socializing newcomers has become an ever more pressing issue for organizations as the nature of work has increasingly shifted from long term employment within a single employer marked by slow but steady upward progression to more short-term positions and lateral movements across a variety of different organizations ( ). With organizational affiliation waning, occupational affiliation has been on the rise. Whereas in the 1970s, workers were more likely to change their occupation than their employer, by the early 1990s, changing employers had become more common than changing occupations ( ). Information technology (henceforth, IT) workers are among those who demonstrate greater occupational than organizational loyalty ( ). The problem of employee flight is substantial: the cost of losing an employee is up to 3 times the employee’s salary ( ). According to an IT staffing company, the direct and indirect costs incurred by organizations in replacing a single employee who makes $60,000 per year reach approximately $150,000 ( ). The lack of organizational loyalty is important not just in terms of the costs an organization faces in hiring and training replacements, but also in the productivity losses incurred when well-trained IT workers leave a project before completion and the team must either redistribute the work or integrate a new member. So significant is the problem of IT talent and retention, that the issue has been rated by CIOs as the 2nd or 3rd most important issue facing IT leaders for five consecutive years in the SIM survey on IT issues and Trends ( ). One way that organizations may increase employee loyalty to the organization is through socialization programs ( ). Facing large numbers of new IT workers entering the workforce ( ) as well as the challenge of integrating experienced workers, IT departments are showing increased interest in socialization programs designed not just to train new employees in task-related skills, but also to instill a sense of loyalty to the organization in hopes of increasing the organizational affiliation of its IT workforce. Given the costs associated with hiring and training new IT employees as well as the loss in productivity incurred when valuable employees leave, the issue of effectively socializing new IT employees is of strategic importance to IT departments. Socialization is the process whereby newly hired employees learn the beliefs, values, orientations, behaviors, social knowledge, and work place skills necessary to successfully fulfill their new organizational roles and responsibilities ( ). Socialization leads to positive outcomes such as better job performance, less stress, higher job satisfaction, and reduction in intent to leave ( ). While the benefits of socialization are clear, the means of achieving effective socialization are complex with many tools and techniques available. Historically, socialization programs have relied upon formal onsite orientation sessions, offsite training sessions, buddy systems, mentoring programs, and business trips with co-workers ( ). Recently, organizations have begun implementing enterprise social media (ESM) as an informal organizational socialization tool. Social media allows users to create, edit and exchange web-based content ( ), thereby enabling organizations and employees to foster relationships, share knowledge and collaborate ( ). ESM have a role to play in organizational innovation, operations, and human relations ( ). Considering the potential role of ESM in an organization’s IS strategy is important for organizations that wish to realize business value from ESM ( ). Academic and practitioner research has encouraged IS managers to develop a social media strategy based on the capabilities of social media platforms to manage interpersonal networks and share content. These capabilities are well-suited for socialization programs ( ). Organizations have begun using ESM systems to help new employees learn about their jobs, their colleagues, and the organization ( ). ESM enables fast and extensive knowledge sharing and facilitates open conversations ( ) both of which can foster new hire socialization. Moreover, ESM provide various opportunities such as self-marketing and relationship building that extend beyond the embedded functions and features of the technology and that may hold important ramifications for new hire socialization and, in essence, make the socialization process an “open” one. Much as ESM has been shown to enable open strategizing with a resultant sense of community and stronger organizational commitment ( ), ESM may enable open socialization wherein active participation may result in a strong sense of community and commitment. However, the multivocality enabled through ESM in which more voices are heard and more messages are generated ( ) may shift the control of organizational communication away from central, largely senior, sources to employees who have access to, and choose to engage with, the ESM. While such participation changes the rhetorical practice of organizations, in a sense democratizing the practice ( ), it may also create conflicts and tensions ( ). For example, in the context of open strategy, ESM has been shown to create tensions between the participatory practices of the technology and the existing managerial practices ( ). Such tensions might also be created in the application of ESM to organizational socialization practices. Formal socialization programs have been carefully scripted by senior management to convey a desired organizational message, culture, and mission. The introduction of ESM as informal socialization tools has the potential to threaten this careful scripting and disrupt the cultural norms of the organization. ESM thus have both the potential to foster a greater sense of community and organizational commitment, but also the potential to create tensions. Given the strategic importance of socialization in the current organizational context of decreasing organizational commitment marked by frequent job changes, ESM for socialization are strategically important systems and must be mindfully implemented in order to produce effective results. Despite the strategic importance of ESM systems in organizations (Gartner  ) and the strategic importance of attracting, training and retaining a skilled IT workforce ( ), few studies to date have investigated the use of ESM for employee socialization ( ). In order to contribute to our understanding of how ESM affects employee socialization, this paper invokes a case study of an organization that had recently incorporated ESM into its IT new hire program. Drawing upon technology affordance theory as our lens, we address the following research question: how do ESM affordances influence the socialization of IT new hires? This paper is organized as follows. We first provide the theoretical foundation. We then present the method, a case description and the analysis followed by the implications, limitations, and conclusion.', 's0736584518303831': ' 1 Introduction In the past 30 years, Additive Manufacturing (AM) has gradually evolved from prototype applications to parts production by improving manufacturability and reducing lead time  . Even though AM is already used in many commercial processes, its full potential might appear in the near future, bringing a significant societal impact  . Among numerous AM technologies, Wire + Arc Additive Manufacturing (WAAM) stands out, especially in the field of medium to large metallic deposition. Indeed, by combining arc welding tools with standard robotic manipulators, WAAM provides a potentially unlimited build volume and a high-rate deposition of various metals, such as steel, aluminium alloys or titanium alloys  . Post-processing consolidation treatments like Hot Isostatic Pressing (HIP), which reduces porosity and lack of fusion, can be difficult to apply to large components due to the absence of sufficiently-big HIPing facilities. For this reason, defect-free deposition is essential to build primary structures that require high-structural integrity. Ding et\\xa0al.   have shown that, in WAAM, the quality of deposition is fundamentally linked to the tool path strategy used. Therefore, the WAAM technology requires a dedicated software approach to generate optimised paths, thus guaranteeing uniform deposition and ultimately enabling a complete commercial solution. In fact, many studies have focused on this particular topic from which two mains approaches can be distinguished. The first approach is to slice a geometry and to generate a path using the same path planning strategy, for each resulting layer. Although this solution has been successfully used on other AM process such as FDM  , it is not directly applicable to WAAM, which has specific requirements inherent to arc welding deposition. Indeed as Ding et\\xa0al. describe in their research  , several path characteristics such as discontinuities, sharp turns and overlaps contribute to an unstable deposition that, layer after layer, can lead to a catastrophic failure. These limits have been understood for a long time, in fact, early studies   have designed path planning strategies for WAAM that generate continuous paths. Unfortunately, removing discontinuities increases other factors like sharp turns. For these reasons, Ding et\\xa0al. introduced several path planning strategies   limiting simultaneously all the faulty factors in a path to improve deposition. Nevertheless, in this approach, all the proposed solutions apply the same path planning strategy regardless of the layer shape. Yet, the higher the topological complexity of a geometry, the more discontinuities and sharp turns are likely to appear. Thus, the resulting quality can vary substantially according to the geometry. The alternative approach is the feature-based design introduced by Kazanas et\\xa0al.  . In their research, they demonstrated WAAM’s ability to build complex parts like enclosed structures by designing a path strategy that fits the requirement of this particular targeted shape. This solution has been then followed by the development of cross structures  , T-crossing features   and more recently, multi-directional pipe joints   ( \\n                      ). Thus, this approach has shown that designing a path strategy ad hoc for a given topology guarantees the deposition quality; however, this solution requires a time-consuming path design research for each new part, which is incompatible with the purpose of AM. Furthermore, one must bear in mind the fundamental differences between powder-bed AM and directed-energy deposition AM. In the former, the layer height is fixed by the downward movement of the build platform and the consistency between thickness of the sliced layers in pre-processing, and thickness of the layer built is somehow always ensured. The latter, instead, is closer to micro-casting, and numerous factors can influence the shape of the deposited bead (width and height). One of these factors is the local variation in the part geometry. This means that even if the same set of parameters is used, the resulting bead geometry can vary. Imagine a linear deposit; in such a case there is a balance between energy introduced, energy conducted away, energy used to melt the wire, and energy used to melt the underlying material. In this steady state, the resulting geometry does not change. However, when that linear structure changes into an intersection, the energy balance is disturbed; more heat is conducted away; the melt pool would shrink resulting in thinner wall width, and larger layer height, if no compensation is applied to the process parameters. Therefore it is absolutely essential that parameters are changed ad hoc to compensate for such variation and to ensure that the geometry obtained is the same as that expected per sliced CAD file, and no errors are accumulated throughout the build. This is also why simple reverse-machining strategies, which fill the sliced layers, cannot be applied. To tackle these challenges, this paper introduces a new approach to generate paths for WAAM of complex 3D geometries. The proposed solution, called Modular Path Planning (MPP), integrates the adaptability of the feature-based design into a more efficient layer-by-layer path planning solution. Thus, it will be shown how this solution guarantees a uniform layer deposition, leading to high-quality part building, and with limited effort in the pre-processing stage. The following Section presents the MPP concept and defines the rules and the decomposition process to guarantee the uniform deposition of a layer. Then,   describes the MPP implementation that reduces user inputs to basic CAD modelling operations. To describe the entire solution, an application example is presented in  .   compares the MPP to a traditional path solution and shows its ability to build complex parts for industry. Finally, in  , the benefits and limits of the presented solution are discussed, followed, in   by the conclusion and the presentation of future work.', 's0921889016306285': ' 1 Introduction In the early days of using robots, German technology assessment of robotics and automation was in particular characterized by studies on the impact on the labor market. Back then, the basic principle of automation was strictly applied, i.e.,\\xa0the work processes were divided into individual action sequences in order to examine which of these sequences could be automated. As a result, a manufacturing process was established in which automatable action sequences were performed by machines while non-automatable tasks continued to be performed by humans. The main objective was the substitution of human labor in order to achieve an increase in efficiency through labor cost savings. Assembly lines were developed in which automated operations could be optimally coordinated, particularly in the automotive industry. The non-automatable actions to be performed by human workers constituted the so-called residual activities which were partly integrated in these assembly lines or remained as upstream or downstream tasks  . Today, due to major progress made in programming as well as further technological advances in engineering, robotic systems can increasingly take over non-standardized tasks previously reserved for humans—and at economically feasible costs. As a consequence, automation is no longer restricted to the production of standardized products in industry but increasingly becomes part of value creation processes in the service sector. Hence, the former paradigms of automated manufacturing have been put into perspective again. The focus has – at least partially – shifted from substitution to cooperation between human and machine. Often the aggregate of tasks performed by humans is designed in such a way that they no longer merely represent “residual activities” of automation. “Exaggerated” automation strategies were modified as to create meaningful tasks for humans, which can often be done in teamwork. Again, the economic cost–benefitanalysis also plays a key role  . Today, the robot is increasingly able to perform not only manual and routine cognitive tasks but also non-routine manual and cognitive tasks. As a result, the general areas of application for robots broaden and they can be used both to substitute even more job profiles than before and help mitigate shortages within the labor market. Due to their capabilities, robots also increasingly act as collaborators of human labor. Overall, this means that robots do not necessarily substitute human labor, but complement it and, in specific areas, make it even more productive. In the following, this paper will elaborate on these considerations and demonstrate how technical progress can enable not only a transition from industrial to service robotics but also a shift in the relationship between human and machine from a formerly substitutional to a complementary one. These considerations will be linked to the so-called capital-skill complementarity hypothesis which addresses the relationship between physical capital and different types of skills (cf.  ). This perspective addresses both challenges and opportunities for human labor resulting from technological change. The inherent complexities are highly relevant for any technology assessment of robotic systems. On the one hand, the use of robots is aimed at optimal implementation of technical possibilities in order to achieve economic gains. On the other hand, it must be investigated from a work science perspective what exactly the tasks are that humans are supposed to perform in cooperation with machines. This paper addresses some key issues related to service robotics, which in this connection represents the “natural” further development of industrial robotics. Already in 1994, the Fraunhofer Institute for Manufacturing Engineering and Automation (Fraunhofer IPA) – one of the key players in the field of robotics – phrased the following definition of service robots, which is still valid today  : \\n                      \\n                   In service robotics, the circumstances differ from those of industrial robotics because the environment can only very rarely be completely redesigned with regard to the use of robots (for example, an entire stable for milking robots). That means that the robot system must be able to react flexibly to different environments in which services can be rendered. In that respect, cooperation with humans becomes more multi-faceted. When the service is rendered to people, e.g. hair-washing, the challenge for the robot system is even greater. This paper aims to describe main lines of argumentation of two disciplinary approaches, namely (labor) economics and work science. The goal is to derive some general observations about the assessment of service robots and to outline first findings on technology assessment of specific service robot systems. In this respect, this paper is an important preparation for the contextual case-by-case analysis of service robots.', 's0957417419301903': ' 1 Introduction At a time when even the largest banks are not immune to distress, credit decision-making is crucially important. The Reserve Bank of India (RBI) and the Finance Ministry has thus far externally controlled and regulated the banking sector. Deregulation and the decoupling of state control pose new challenges, and intense competition is placing the survival of all but the fittest and the most efficient in doubt. Commercial banks are accordingly striving to adjust to a new economic and technological environment. Sound credit scoring models form an integral part of this adjustment process. This motivates our present purpose which is to propose suitably conceived and designed credit scoring models for personal loans with due allowance for the incidence of default. The novel contribution of the present paper consists in integrating two stages of the decision process with reference to the Indian banking sector. Firstly, we build credit scoring models for our unique sample of personal loans, provided by one of the largest Indian banks. The sample includes a significant number of bad debts that is consonant with the current and evolving profile of personal indebtedness. Secondly, we explore in detail the characteristics of the defaulters in our sample. This feature is particularly important given the recent history of rising bad debt. In both stages, we identify the key predictor variables to be used in building models. Further, we evaluate our models by using   misclassification costs. The sharp increase in household leverage ratios in recent years shown in  \\n                      a (Leverage Ratios in India) portrays the increase in borrowers’ vulnerability.  b (Growth of Personal Loans and Housing Loans) shows the muted growth of personal loans over recent years up to the end of 2010. However, the year ended March 2011 saw the increase of 17% portrayed in  \\n                      , against only 4.12% in the year ended March 2010. The rate slightly decreases in the next two years, 2012 and 2013, which is commensurate with the increase of non-performing assets reported on Indian banks’ balance sheets ( ). It should be emphasised that at the end of March 2014 retail credit has increased driven primarily by housing loans, personal loans and auto loans representing 47%, 36% and 14%, of gross credit respectively ( ). Indian market credit bureaux, for example Credit Information Bureau India Limited ( ), collect credit data for the banking industry. CIBIL maintains a repository of the credit history of all commercial and consumer borrowers in the country and it provides information to any bank to facilitate their credit granting decisions. CIBIL’s Consumer Credit Bureau deals with the credit history of individual customers while the Commercial Credit Bureau maintains the credit history of non-individual clients such as corporates. CIBIL provides credit information as distinct from opinions and does not classify any client’s loan as being in default unless the lender has already classified it as such. While many research papers have discussed credit scoring models for developed countries ( ), relatively few have focused on building such models for developing and emerging markets ( ). While these have addressed a wide range of cases none, to the authors’ knowledge, have examined the Indian banking sector. Given the sensitivity of data access is significant. Particularly, in the light of past financial crises, banks become increasingly risk reverse due to security and clients data protection laws. Small samples are widely used in building scoring models in the literature, as this issue is well recognised (see for example  ). For instance, consumer loan applications models are regularly built using around1,000 observations or less (see for example  ). In building scoring models, statistical techniques such as discriminant analysis and logistic regression are widely used ( ). The logistic regression model does not necessarily require the assumptions of the discriminant analysis model and may prove to be more robust in practical applications. Other classification techniques such as classification and regression tree, k-nearest neighbour and support vector machines are also in common use ( ). Various neural networks, including artificial neural networks, multilayer perceptron neural networks and back-propagation neural networks, have also been used in building scoring models ( ). Amongst these probabilistic neural networks provide results which are significantly more accurate in building personal loan scoring models (see,  ). Comparisons between traditional and advanced scoring techniques have been the subject of numerous studies ( ). A substantial number of these studies demonstrate the superiority of neural networks over conventional techniques ( ). However, there is still a role for conventional techniques such as discriminant analysis and logistic regression in building scoring models for personal loans (see for example,  ). In this paper four statistical modelling techniques are applied to analyse bank personal loans using a data-set provided by an Indian bank. As motivated by the above literature these are discriminant analysis, logistic regression, multi-layer feed-forward neural networks and probabilistic neural networks. Three different criteria namely correct classification rate, error rates and   misclassification cost are used to compare the effectiveness and predictive capabilities of different models. Moreover, in this paper   misclassification costs, provided by the bank’s own credit officials, are used in preference to the more conventionally used estimated misclassification costs. This underscores the novelty of our contribution. The layout of this paper is organised as follows:   reviews the current guidance note on credit risk management by RBI.   addresses research methodology and data sources.   discusses the empirical results.   concludes and discusses the opportunities for further research.', 's0888613x1400067x': \" 1 Introduction Association mining consists of two phases: pattern mining and rule generation. Many efficient algorithms have been developed for pattern mining, however, due to the huge number of patterns generated by the mining process, the challenging issue for pattern mining is not efficiency but interpretability  . Frequent closed patterns partially alleviate the redundancy problem. Recently, many experiments   have proved that frequent closed patterns are a good alternative to terms or  -grams for representing text features. Several approaches for post-processing of patterns have also been proposed recently: pattern compression  , pattern deploying   and pattern summarization   have been proposed to summarize patterns. The phase of rule generation finds interesting rules based on discovered patterns and a minimum confidence. This phase is also a time consuming process that can generate many redundant rules. The approaches for pruning redundant rules can be roughly divided into two categories, the subjective based approaches and objective approaches. The former approaches are to find rules that satisfy some constraints or templates  . The latter approaches construct concise representations of rules without applying user-dependent constraints  . The use of closed patterns has been found to greatly reduce the number of extracted rules; however, a considerable amount of redundancy still remains  . There are several obstacles to overcome when using association mining in real applications  . The first problem is the overwhelmingly large volume of discovered patterns, rules and false discoveries. Various methods have been proposed to tackle this, some of which we now briefly describe. Frequent association mining has been extended to multilevel association mining, which uses concept hierarchies or taxonomy trees to find rules  . The leaves of a taxonomy tree represent items at the lowest level of abstraction. Using a top-down strategy, at each level, frequent patterns are calculated based on accumulated counts. Recently, mining flipping correlations   has been proposed to find positive and negative correlations in taxonomy trees. Another paradigm is the filtered-top-  association discovery   which used three parameters: a user specified measure of how potential interesting an association is, filters for discarding inappropriate associations, and   the number of associations to be discovered. The second problem is the lack of semantic information along with the mining process because most algorithms are developed for transaction databases. The third problem is the insufficiency of knowledge coverage due to the use of two approximation phases (a minimum support for the pattern mining phase and a minimum confidence for the rule generation phase) that miss some specific patterns or rules (e.g., the low-support problem, in which a large pattern is more specific but has a very low support  ). Based on the above overview, the increasing size of patterns or rules rapidly becomes unmanageable, and users are looking forward to new interpretation methods to process and “understand” the knowledge in data sets. Currently there are several different approaches for the interpretation of discovered knowledge based on some sorts of semantic annotations: an OLAP based visualization method  , a generating semantic annotation method  , flow graphs   and multi-tier structures  . Multi-tier structures use “granules” instead of “patterns” and “rules”, and defined meaningless rules based on the relationship between long rules and their general ones (short rules). In this paper, we continue to develop multi-tier structures for the interpretation of association rules in databases in order to provide a new solution for the challenging issue. To illustrate the relationship between patterns and granules, we present a method to interpret granules in terms of patterns, and prove that decision rules and max closed patterns are mutually corresponding. We also indicate that small closed patterns can be interpreted as smaller granules. Moreover, we formalize concepts of association mappings and present efficient algorithms for the constructions of multi-tier structures. Finally, experiments on a real dataset and Foodmart 2005 data collection have been conducted and the results show that the proposed approach is promising. The remainder of the paper is structured as follows: Section   discusses related work, Section   introduces basic definitions, Section   presents a method for the interpretation of granules in terms of patterns, Section   discusses the concept of multi-tier structures and presents a method to estimate patterns' support based on granules, Section   presents definitions and properties for association mappings, Section   proposes efficient algorithms for the construction of multi-tier structures based on association mappings, Section   evaluates the proposed approach and lastly, Section   presents the conclusion.\", 's0888613x14001133': \" 1 Introduction A   \\n                       is a probabilistic graphical model   that is popular in fields such as statistics, machine learning and artificial intelligence. It identifies the nodes of a Directed Acyclic Graph (DAG) with random variables and interprets the graphical structure of the DAG as an assessment of the independencies amongst these variables; nodes that are not connected represent variables that are conditionally independent of each other. By exploiting these independencies, a global uncertainty model can be constructed easily out of local ones, allowing for a compact representation of the model. Efficient algorithms have been developed for performing inferences in such Bayesian networks, leading to their successful application in a multitude of real-life problems  . Despite their success, Bayesian networks have an important limitation: the construction of a Bayesian network requires the exact specification of a conditional probability distribution for all variables in the network. In case of limited data or disagreeing and/or partial expert opinions, this requirement is clearly unrealistic and renders the resulting model arbitrary; see Ref.   for numerous other arguments against this ‘precision requirement’. In order to avoid those problems, one can use the theory of  , which, simply put, are Bayesian networks that allow for imprecisely specified local models. Initially, these were taken to be   \\n                       (closed and convex sets of probability distributions), which explains the terminology. However, as the theory progressed, other imprecise-probabilistic models such as lower previsions   and sets of desirable gambles entered the field as well  . In the present paper, we use the very general theory of   \\n                      . The key idea of this theory is that a subject's beliefs about the unknown outcome of an experiment can be modelled by means of the bets on this outcome—referred to as gambles—that he is willing to accept. Although sets of desirable gambles are not as well known as other (imprecise) probability models, they have definite advantages. To begin with, they are more expressive than most—if not all—other imprecise-probabilistic models, including the theories of credal sets and coherent lower previsions  : every set of desirable gambles has an associated lower prevision and credal set, but—in general—the original set of desirable gambles cannot be recovered from these derived models. A particularly interesting consequence of this added expressiveness is that conditioning on events with (lower or upper) probability zero becomes non-problematic  . Secondly, sets of desirable gambles are strongly connected to classical propositional logic  , thereby providing a unified language for both logic and probability. Thirdly, they have the advantage of being operational, meaning that there is a practical way of constructing a model that represents a subject's beliefs  . \\n                       And finally, our experience tells us that it is usually easier to construct proofs in the geometrically flavoured language of coherent sets of desirable gambles than in other, perhaps more familiar frameworks. Three main kinds of credal networks can be distinguished, the difference between them being the notion of independence they adopt: strong independence, epistemic independence or epistemic irrelevance; see Cozman's pioneering work   for an overview. In a precise-probabilistic context, all these approaches coincide and reduce to a Bayesian network. Credal networks under strong independence are by far the most popular ones; see Refs.   for some nice overviews, containing numerous references to both theoretical results, algorithms and applications. In contrast, credal networks under epistemic independence have received almost no attention  , a situation which is likely to persist due to their computational intractability. The current paper deals with the remaining option: credal networks under epistemic irrelevance. Let us start by stating some of their advantages. \\n                   Despite these advantages, credal networks under epistemic irrelevance have received relatively little attention so far; to our knowledge, Refs.   are the main contributions to the field. One of the main persisting problems is that—except for networks that are sufficiently small or have a tree topology—no efficient, exact or even approximate inference algorithm is known. We believe that this is to a great extent due to a profound lack of known theoretical properties. In the present paper, we start to remedy this situation by providing a firm theoretical foundation for credal networks under epistemic irrelevance. We begin in Section   by providing a short introduction to the theory of sets of desirable gambles. We then go on to introduce and discuss important concepts such as directed acyclic graphs and epistemic irrelevance in Section  , and use these in Section   to show how assessments of epistemic irrelevance can be combined with given local sets of desirable gambles to construct a joint model for a credal network under epistemic irrelevance. We call this the   and prove that it is the most conservative coherent model that extends the local models and expresses all conditional irrelevancies encoded in the network. In the remainder of the paper, we develop some remarkable properties of this irrelevant natural extension. Section   presents what we consider to be our main technical achievement: a very general factorisation result and a closely related marginalisation property. In Section  , we develop a tight connection with the   \\n                       and show that it corresponds to a special case of the irrelevant natural extension. Our perhaps most important result is presented in Section  : the irrelevant natural extension satisfies separation properties similar to the ones that are induced by d-separation in Bayesian networks. We introduce an asymmetrical version of d-separation, called  , and show that it implies epistemic irrelevance. Furthermore, since AD-separation is shown to satisfy all asymmetric graphoid properties (all graphoid properties except symmetry), the induced set of epistemic irrelevancies does so as well. We conclude the paper in Section  , comment on how to translate our results to the framework of coherent lower previsions, discuss some algorithmic applications and present future avenues of research. In order to make our main argumentation as readable as possible, all technical proofs are collected in  . We should note that some of our results have already been published in an earlier conference version of this paper  . The current version gives a more detailed exposition of these results, provides them with proofs—which were omitted in the conference version—, and extends them; notable examples of additional results are the connections with independent natural extension, presented in Section  .\", 's259018851930006x': ' 1 Introduction In transportation engineering, the topmost consideration should be safety. Therefore, professional scientific effort should be made by both private and public sectors to comprehensively interpret service conditions and the changing mechanism of road roughness levels. The international roughness index (IRI) was initially proposed in a research project conducted by the University of Michigan ( ) for monitoring the overall roughness condition of certain pavements. The IRI mainly measures the longitudinal profile condition of a travelled roadway according to the vehicle vibration condition. The most widely used units for measuring the IRI are meter per kilometer and inch per mile ( ). Over the past few decades, scholars across the world have made significant effort to study the changing IRI mechanisms, which have been then used to analyze pavement performance deterioration trends ( ).   analyzed the IRI characteristics of cracked and seated concrete pavements and thereby provided maintenance and rehabilitation guidance for pavement management. However, they did not consider the climate conditions, which have significant influence on the roughness progression, especially for hot-mix asphalt overlay pavements ( ).   investigated the roughness condition of low-volume roads using multiple regression analysis.   predicted the IRI over time using Markov analysis, thus providing a probabilistic IRI prediction method instead of a deterministic one.   proposed an exponential IRI regression model by modeling two parameters α and β and used the model to predict IRI trends over time. However, the effectiveness and robustness of the abovementioned approaches are not convincing because most regression analysis methods assume that the dataset follows certain distributions. Typical IRI prediction models have been developed by the transportation departments of Mississippi and Washington as part of the Long-Term Pavement Performance (LTPP) program and by other state agencies based on the Mechanistic Empirical Pavement Design Guide ( ). The development of these models is a remarkable achievement, and they are still being used for long-term pavement design and management. However, because these models are based on data-driven approaches, they still have the intrinsic drawbacks of statistic-based methods ( ). Thus, the performance of these models relies significantly on the quality and characteristics of the data being used. In addition, traditional approaches are influenced by the low accuracy of data acquisition, and hence, they usually fail to handle the ambiguity and uncertainty inherent in raw datasets ( ). With the advancement of artificial intelligence (AI) techniques, several engineering fields are focusing on using AI approaches to facilitate research and practice tasks. In the field of pavement management, the initial attempt was made by  , who proposed an intelligent method based on artificial neural networks (ANNs) to analyze the deterioration trend of pavements through IRI prediction modeling. Thereafter, researchers have used not only ANNs but also other AI techniques such as support vector machines ( ) to investigate pavement roughness progression. However, these methods consider the IRI as a dependent indicator that is influenced by several other pavement conditions, traffic loading, and environmental factors at a fixed observation time, thereby ignoring the impact of time on the IRI. Time is an invisible space variable that can have an abstract existence and can contain a comprehensive combination of all other visible, traceable, and concrete influences. Hence, this variable has a direct influence on the deterioration of pavement performance, and hence, it affects the IRI. Therefore, fuzzy time-series methods should be investigated to bridge the gaps in current literature with regard to intelligent IRI predictions. Fuzzy time-series prediction methods have been successfully applied to several fields such as stock prediction and weather prediction, and these applications serve as invaluable reference for the IRI prediction research conducted in this study. Considering the background described above, the present study aims to develop an IRI prediction approach based on an innovative fuzzy time-series analysis. To fulfill this purpose, multifactor and multigranularity analyses are considered for time-series forecasting. First, the automatic clustering technique (ACT) is adopted to generate a series interval within each granular space, which should be defined in advance. Next, a second-order fuzzy-trend matrix (SFTM) and fuzzy-trend relationship classification (FTRC) method are proposed to predict the fuzzy-trend of each factor. Then, the fuzzy-trend states of multiple granular spaces are generated while giving full consideration to various uncertainties. Finally, the particle swarm optimization (PSO) technique is employed to forecast the optimal IRI value. Comparative experiments are performed to evaluate the effectiveness of the proposed method.', 's0888613x1300234x': \" 1 Introduction Computing with words (CWW) was introduced by Zadeh   as a methodology for reasoning and computing with human-sourced information described in natural language, of which the idea was actually rooted from his previous work on linguistic variables, fuzzy constraints and fuzzy if-then rules  . During the last decade or so, CWW has attracted considerable attention of the fuzzy set community. In order to establish a mechanism for automated reasoning, computing or decision-making with words, it is necessary to establish appropriate mathematical models for representing linguistic information and perceptions, which would be able to capture certain semantic characteristics of words that underline the way human beings reason or make decisions using natural language. This is truly a challenging task because of the flexibility (e.g., context/culture dependency) of as well as fuzziness and uncertainty associated with semantic characteristics of words used in human reasoning. All of those make the process very difficult to model and represent the meaning of words and perceptions in order to carry out meaningful computing. This makes CWW a comprehensive research area being open to interpretations and different instantiations, as intensively discussed in  . So far, there have been numerous models developed for CWW with applications to a spectrum of practical human-centric problems. From the perspective of modeling and reasoning, Zadeh's seminal work   is the first one that provides a general framework for CWW in which the use of fuzzy sets becomes crucial as they provide a means of modeling the fuzziness inherent in natural language utterances. Computational mechanism underlying the CWW is essentially based on the so-called extension principle in association with generalized rules of inference in fuzzy logic. Interestingly, in  , Lawry proposed an alternative approach to CWW based on mass assignment theory   and probability theory and provided a mechanism for reasoning with linguistic descriptions endowed with imprecise probabilities that avoids the computational complexity incurred by applying the extension principle in Zadeh's theory of CWW. As a matter of fact, Zadeh's general framework could be further developed and detailed for various applications to human-centered modeling and reasoning problems in practice. As effective and efficient paradigm serving for handling imprecise information in natural language, the authors of   expect that CWW can find applications in the areas related to computational linguistics, and argue that CWW could have high implementation potential in natural language generation, processing or understanding, illustrating by an application of their proposal to linguistic data summaries. From the perspective of decision-making application, over the last decade much work has been done so as to develop CWW approaches for solving decision problems involving vague and imprecise information. Typically, in decision-making applications, CWW is mainly involved with the problem of how to represent and aggregate linguistic information in decision making. In recognizing that “ ”, Mendel   proposed to use type-2 fuzzy sets for modeling words in CWW for assisting people in making subjective judgments. While most early methods for dealing with linguistic information in decision-making were making use of fuzzy sets as a means of modeling linguistic terms and the corresponding CWW models were based on Zadeh's extension principle, e.g.,  . Clearly, the computing results coming by using these methods, classified as semantic models  , come in the form of fuzzy sets that in general do not match exactly pre-defined fuzzy sets of linguistic terms and, therefore, a linguistic approximation process must be applied to obtain linguistic recommendations for the problem at hand. Consequently, such linguistic approximations may result in loss of information and lack of precision of the final results. This has motivated Herrera and Martínez   to propose a so-called 2-tuple linguistic representation model as a tool for CWW, which aims at overcoming the limitation of the loss of information caused by the process of linguistic approximation in fuzzy set-based approaches. The 2-tuple linguistic model has been also discussed in decision making problems present in various application areas, including group decision making, distributed intelligent agent systems, information filtering, information retrieval, and engineering management  . In the 2-tuple linguistic representation model or its variations such as proportional 2-tuple representation model  , which are classified as symbolic models  , each set of the linguistic terms under consideration, denoted by  , is assumed to be strictly ordered, i.e., we have  . In this study the set   is called a linguistic scale. Basically, symbolic models aimed to map a linguistic term set into an appropriate numerical scale and then computation for linguistic information aggregation is performed over this numerical scale so that many available numerical aggregation operators can be applied in a direct manner. Finally, the computing results are converted back to linguistic 2-tuples. As we have observed, in the fuzzy-set-based approaches, the semantic representation of words makes them to become complicated in terms of underlying computations and, in addition, the ordering relationships between terms of the scale become blurred as well. While in the symbolic-model-based approaches, although they allow directly performing computations on the set of linguistic values in which only a totally ordered structure is assumed by mapping the set of linguistic values into suitable numerical scale, we might be losing much of the information we have purposely been keeping at the structural phase of linguistic decision problems. Note that the use of a linguistic approach is only necessary when the information in decision situations cannot be assessed precisely in a quantitative form (i.e., in terms of numerical values). Moreover, let us note again that most linguistic scales used in the previous studies of linguistic decision analysis are assumed to be totally ordered. That is, one can recognize an order between the terms in the scales based on the qualitative semantics of terms, called inherent order-based semantics, which are directly associated with the string expression of terms regarded as their syntax. Obviously, this qualitative semantics of terms is present in any natural language. When experts provide their linguistic assessments, they focus on the ordering semantics of terms completed in comparison with some other terms of the linguistic scales. Therefore, we may refer here to qualitative linguistic scales to indicate that the linguistic terms of the scales must convey their inherent order based qualitative semantics. Quantitative semantics of terms is required from computational standpoint and it is a basis to establish a computational structure or semantics domain that will be called in this study semantic linguistic scale. It is obvious that there is no explicit connection between the qualitative linguistic scales and their respective quantitative ones while, as pointed out by Zadeh in  , one of the key rationales underlying CWW is “much of human knowledge is described in natural language”, and hence words are objects of CWW. Therefore, it is naturally desirable to establish formal linkages between the former scales, connected immediately to the respective linguistic scales, with the latter ones. However, this can be done only when linguistic variable domains can be formalized in a manner that the qualitative semantics of terms determines their quantitative semantics, similarly as a guideline mentioned by Mendel in  , “A word must lead to a membership function rather than a membership function leading to a word”. These linkages can be established by following the idea of the denotational semantics found in programming languages, in which every phrase (linguistic label), as a syntax expression, in a language is interpreted as a denotation, which is often a mathematical object  . That is that “The idea of denotational semantics is to associate an appropriate mathematical object, such as a number, a tuple, or a function, with each phrase of the language”   and, then, semantic worlds or domains “are ‘sets’ of mathematical objects of a particular form”  . In particular, they may be mathematical structures, which are suitable for developing semantic linguistic scales with expected computation features. Since hedge algebras are mathematical models of linguistic term-domains, their elements combined with their quantitative characteristics can be utilized to represent the semantics of linguistic terms by 4-tuple linguistic representation, which may be considered as a generalization of the linguistic representation of linguistic labels in the 2-tuple approaches. Based on this we can develop semantic linguistic scale viewed as “semantic domains” in the above sense of the linguistic terms in linguistic scales. Note that, as being objects of computation in CWW, terms (words) and propositions drawn from a natural language together with their meaning and fuzziness/vagueness representation seem to play an important role in the development of CWW paradigm  . Therefore, such a connection between two semantics could result in the linguistic computational models developed in a more interpretable and convincing fashion. However, it seems that this observation has been overlooked in the previously developed models for linguistic decision-making. In light of the discussion presented above, the main objectives of this paper are as follows: \\n                   The paper is organized as follows. Section   discusses the essential qualitative semantics of terms and the semantic linguistic scales that are useful from computational perspective, and then proposes some requirements for their construction. In Section  , the order-based qualitative semantics of terms will be discussed based on the basis of order-based structure of hedge algebras. Quantitative semantic aspects of linguistic terms previously developed will be reviewed to offer necessary background knowledge for the study. The concept of a 4-tuple semantic representation model and 4-tuple semantic linguistic scales exploiting numeric quantitative semantics and interval-semantics of terms are introduced and systematically developed in Section  . To show the advantages of the 4-tuple semantic linguistic scales, a comparative study based on a simple multi-criteria decision problem is examined in Section  . Finally, some conclusions are presented in the last section.\", 's0888613x14000607': ' 1 Introduction The paper   describes an appealing method for learning models from imprecise data that improves extension principle-based approaches. The empirical loss of a model on interval-valued datasets is defined as the lowest loss over all the possible crisp instantiations (selections) of the uncertain items in the training data. The loss function of a model with fuzzy data is defined as an average over the different level cuts of the data. The model with a best empirical loss is searched for, thus a minimin criterion is adopted. It is shown that this strategy is related to the optimization of certain loss functions used in machine learning. This discussion focuses in on three particular aspects of the paper where further developments may be possible: extension principle-based models, the use of the aforementioned minimin criterion and possible links between data disambiguation and data imputation.', 's1071581919300552': ' 1 Introduction The concepts of automation, and mechanized and automated work have been around for decades. According to the Britannica encyclopedia, automation is “  ( ). The above definition of automation does not involve the requirement of a computer processor. However, many modern forms of automated (or sometimes: autonomous) machines, such as power plant monitoring devices, automated cars, drones, robots, and chatbots, do involve computers. These computer-automated systems are used by humans, and humans are expected to remain essential contributors to artificial systems and automated systems in the future ( ). The study of human-computer interaction, or more specifically human-automation interaction, therefore continues to remain relevant as automated systems are used to support more and more everyday activities, overseen by non-technical and non-professional end-users. In this special issue to celebrate the 50th anniversary of the International Journal of Human-Computer Studies, and its predecessor the International Journal of Man-Machine Studies (from now on collectively referred to as IJHCS), we review the contributions that IJHCS has made towards the study of human-automation interaction. We therefore analyze published work from the journal to distill historic trends. Our analysis shows that human-automation interaction is a field that keeps expanding into new domains and contexts (what we refer to as “breadth”), and also keeps improving its performance within domains and contexts (what we refer to as “depth”). Given these expansions, and the exposure to more contexts and to a wider and more diverse group of end-users, there is a potential for the broader human-computer interaction community to contribute skills and knowledge to create and evaluate safe, engaging, and productive automated systems. We close our analysis by discussing eight trends that we deem of particular relevance for this community, classified in two segments. First, we discuss trends that have been around for a while but continue to remain important: (1) function and task allocation between humans and machines, (2) trust, incorrect use, and confusion, and (3) the balance between focus, divided attention and attention management. Then, we discuss emerging themes: (4) the need for interdisciplinary approaches to cover breadth and depth, (5) regulation and explainability, (6) ethical and social dilemmas, (7) allowing a human and humane experience, and (8) radically different human-automation interaction.', 's0957417419301812': \" 1 Introduction Investors make use of many online discussion channels when deciding to make investments on stock markets. Such information is presented within Financial Discussion Boards (FDBs), news corporations (e.g. Financial Times), broker agency websites, and social media platforms. Recently, Twitter has become a popular platform for investors to disseminate stock market information and discussion ( ). Many large organisations are also using Twitter as a platform to obtain and share information relating to their products and services ( ). Companies are identified on stock markets through the use of ticker symbols, which are typically one to four characters in length (depending on the exchange) and are unique to an exchange, e.g. the TSCO ticker refers to Tesco PLC on the London Stock Exchange (LSE). The use of these ticker symbols within tweets on Twitter are referred to as cashtags and allow investors to participate in discussions and view news regarding a specific company at a moment's notice ( ). Cashtags are clickable links embedded within tweets which mimic the company's ticker symbol, prefixed with a dollar-symbol (e.g. $TSCO cashtag on Twitter refers to Tesco PLC) ( ). Cashtags were originally introduced by Stocktwits \\n                       to allow users to link companies with their posts. Twitter introduced the feature of cashtags in 2012 to allow their users to associate specific companies with their tweets ( ). A tweet can contain multiple cashtags, with the only limitation being the character limit imposed upon Tweets, which was recently increased to 280 characters. The main limitation of cashtags is that they are susceptible to colliding with an identical cashtag belonging to a company listed on another exchange, a phenomenon we refer to as a cashtag collision. As tweets are typically short in length, they can be an indispensable tool for investors to discuss recent events relating to companies. The presence of colliding cashtags, however, can result in investors having to decide if the tweets returned via their cashtag search actually relates to the company in which they are interested in. Investors not aware that Twitter does not distinguish multiple companies over different stock exchanges with identical ticker symbols could have made investments based on information which is not pertinent to the company in which they thought it was. This is even more problematic if investors use automatic analysis tools to measure the popularity of a certain cashtag or other social media metrics. Throughout this paper we refer to a cashtag collision as one of two scenarios: (1) two identical tickers which refer to different companies (e.g. $TSCO refers to Tesco PLC on the LSE, but also refers to the Tractor Supply Company on the NASDAQ) and (2) two identical tickers which refer to the same company which has multiple listings on different exchanges (e.g. $VOD refers to Vodafone Group PLC on both the LSE and the NASDAQ). We anticipate that the second scenario will be particularly difficult to detect and resolve, as the same company which is listed on multiple exchanges does not have many features which can distinguish them apart (e.g. VOD on both exchanges will have the same company name and CEO). The issue of colliding ticker symbols is not just isolated to Twitter, several other news websites which depend on the automatic assignment of news articles to specific companies based on their ticker symbols can also suffer from incorrect assignment of news articles. Yahoo! Finance, for example, incorrectly associates Tesco PLC's (LSE) Regulatory News Service (RNS) statements with the Tractor Supply Company (NASDAQ), which could sow confusion for potential investors who depend on such news sources. This paper introduces a novel methodology for the detection and resolution of colliding cashtags on Twitter. We train traditional supervised machine learning algorithms twice on each tweet to classify if a tweet relates a specific exchange-listed company or not. One classifier is trained on a sparse vector of the tweet text alone, while a second classifier is trained on both the sparse vector and other features contained within a company-specific corpus. The cashtag collision resolution methodology introduced in this paper is a generalised approach which can be applied to any stock market. We validate the cashtag collision resolution methodology by carrying out an experiment involving companies listed on the LSE (discussed in detail in  ). The main contributions of this paper can therefore be summarised as follows: \\n                   These contributions address a problem which has yet to be discussed within the literature. Several previous works involving the analysis of cashtags could have been susceptible to incorrect analysis and results due to the subtlety of colliding cashtags. The remainder of this paper is organised as follows:   introduces the main motivation of this paper, challenges associated with colliding cashtags, and the research questions we aim to answer.   explores the related work involving cashtags, disambiguation on Twitter, data fusion, and the use of custom corpora.   provides an overview of an experiment which has been designed to validate the cashtag collision resolution methodology.   provides an overview of the data used in this experiment.   introduces the company corpora creation and data fusion methodology.   provides a high-level exploratory analysis of the data.   details the cashtag collision resolution methodology for classifying a tweet as belonging to a specific exchange or not.   discusses the results of the experiment.   draws a conclusion and proposes future work relating to cashtag collisions.\", 's0020025519304864': ' 1 Introduction I n computer vision, contrast enhancement is the only method that is used to improve the visibility of underexposed image details. Due to the limitation in the imaging sensors (viz. camera), low illumination, poor quality imaging acquisition systems and improper settings (viz. lenses), the contrast of the captured images can be distinct from ground-truth. For a better human perception and understanding, improvement in the quality of the acquired images is a mandatory requirement. To obtain an enhanced image with better details from these low dynamic range images, contrast enhancement is performed on the basis of image processing criteria\\xa0 . Presently, the application of unmanned aerial vehicles (UAVs viz. Drone) is rapidly growing in various fields such as surveillance, military, agriculture, etc. in incommodious environments. Object detection and tracking are very challenging tasks in the field of intelligent transportation and automatic monitoring\\xa0 . Typically, the targets often include vehicles, vessels, crafts, traffic signs, cars or forest; such targets are specifically highly informative, which can be used to improve the accuracy of target tracking (e.g., vessel shape and grouping activity are used in order to differentiate and predict the vessel), and prior spatial information aid to recognize the detection of target objects (viz. vessels, trees, house)\\xa0 . Due to the expensive nature of drone devices, the processing cost is very high for fast dynamic motion and transmission. Moreover, the projected drone images can be deteriorated by various noises, degraded normal illumination and visual quality\\xa0 . Consequently, it is essential to design and implement a drone efficient image enhancement algorithm with higher accuracy to understand and recognize target objects so as to address the better discrimination of specific objects in dronogram without increasing the burden of the hardware costs\\xa0 . In past few decades, researchers have tried to solve the exact contrast enhancement problem for colour imaging using several approaches\\xa0 . Improved contrast and image details are often required in a wide range of computer vision applications but a generic solution to this problem has not been discovered. Several enhancement methods have been suggested such as spatial filtering, histogram equalization (HE), wavelet decomposition, or soft computing (fuzzy sets, neuro-fuzzy, convolutional neural networks) theory\\xa0 . All these methods are broadly classified into spatial-domain and transformation-domain methods. Detailed reviews can be found in\\xa0 . \\n                       These methods are built on the straightforward manipulation of pixel intensities in source images\\xa0 . Mainly, statistical based sub-band filtering approaches have been utilized to enhance underexposed and low illumination in target images by suppressing noise\\xa0 , but these techniques perhaps cause edge blurring and detail loss\\xa0 . Thus, adaptive enhancement approaches are beneficial to improve the contrast while maintaining edges and details of dronograms\\xa0 . The adaptive density weighted contrast\\xa0 , or first derivative and local statistics\\xa0  are common adaptive based approaches. Though histogram equalization (HE) techniques retain the predominant view in the field of enhancement, they possibly result in unnecessary contrast enhancement or over-fit brightness effect resulting from the lack of constraint on the level of enhancement\\xa0 . Thus, some techniques are investigated to overcome those lacks, e.g., the adaptive HE (AHE), and contrast limited AHE (CLAHE)\\xa0  in literature. Unsharp masking (UM) is useful for enhancing exact details of dronograms, but increases noise and exceeds steep details at the same moment\\xa0 . Later, few revised methods such as the rational UM\\xa0  and nonlinear UM (NUM)\\xa0  have been suggested to overcome those problems. Recently, deep learning is achieving impressive state-of-the art performance for different image processing tasks such as image segmentation, image enhancement etc.\\xa0 . Deep convolutional neural network architecture is quickly becoming prominent in image processing since it provides the ability to efficiently encode spectral and spatial information based on the input image data, without any prepossessing step. It consists of multiple interconnected layers and learns a hierarchical feature representation from raw pixel-data. It discovers features at multiple levels of representations. Several researchers have suggested various deep learning based image enhancement methods such as Deep Bilateral Learning\\xa0 , deep convolutional neural network based image enhancement\\xa0 , Dehaznet for image enhancement\\xa0 , MSR-net\\xa0  etc. All the deep learning based image enhancement methods produce remarkable results for real time images and low illuminated images. Therefore, deep learning method is most robust but lots of synthetic data as well as high computational resources are required to perform the deep learning algorithms. Sometimes, deep learning approaches suffer from high bias and over-fitting problem due to the nature of data. \\n                       In this case, all existing techniques are designed based on the multi-scale representation or Fourier transform approach, which often uses an input/output transformation that varies with the regional feature of a dronogram. Firstly, multi-scale representation-based schemes decompose an image into a level of multi-scale sub-bands by using the contourlet, discrete dyadic wavelet, complex wavelets or shearlet\\xa0 . However, most wavelet based schemes are unable to preserve both the contour and geometry of edges in images\\xa0  whereas contourlet and shearlet are better for preserving image details. Moreover, traditional transformation-domain techniques can produce artefacts such as undesirable blocking effects\\xa0 , or enhance image uniformly, but incompletely enhance all regional image details/regions \\xa0 . Still, implementation and run-time costs for contourlet or shearlet are complicated issues in real-time applications\\xa0 . Since uncertainty and vagueness are undoubtedly created during the acquisition or transmission of images, a reliable model interpreting such images should use the personal experience to determine heuristically. Although, this can be construed by the classical mathematical modeling\\xa0 . Hence, fuzzy sets (FSs), logical FSs, type-I FSs, type-II FSs, or intuitionistic FSs (IFSs), have been used to improve the contrast and visual quality of images because they are knowledge-based systems. These fuzzy techniques efficiently process faulty data collected from imprecision and vagueness\\xa0 . In most of the cases, the grey-level range of a contrast enhanced image using type-I FSs is relatively consistent, and inadequate to enhance the corrupted images with short grey levels and low-intensity values\\xa0 . Type-II FSs are tough to practice and insufficient to address the exact hesitancy in ambiguity\\xa0 . Consequently, image enhancement is basically a challenging task in the domain of image processing. Lots of researchers have suggested various methods to enhance low-contrasted images and most of them perform quite efficiently\\xa0 . Most of the existing methods cannot achieve the desired enhancement result for underexposed and low-illuminated images and typically suffer from deficiency with robustness and accuracy. Here, we propose an appropriate and fittest model to correct the intensity distribution of pixels in the image, which is suitable for human perception. To plan a dronogram (drone image) enhancement scheme, it is motivating to select intuitionistic fuzzy hesitant sets (IFHSs) theory because IFHS take into account more uncertainties in the form of membership function that is more bound to the aspects of human decision-making, in comparison to traditional FSs\\xa0 . The IFHS is well-known for its power to measure hesitant quantity while addressing uncertainty in image information\\xa0 . Inspired by literature study, a novel dronogram enhancement algorithm motivated by the IFHS theory has been developed in this paper. The proposed scheme is designed by using the hyperbolic regularization approach in the intuitionistic fuzzy hesitant set. We have formulated a different membership grades generator to construct the intuitionistic fuzzy hesitant set to measure the hesitant score in fuzzy membership grades under certain fuzzy enhancing criterion. The proposed framework has been divided into two main stages. First, we have separated the background-foreground areas utilizing the global threshold and constructed membership functions in the fuzzy domain. Secondly, a nonlinear real-valued hyperbolic function is applied to modify and adjust both the memberships in background-foreground areas to enhance source drone images. To increase the clarity of the dim image for object detection task, we have utilized adaptive threshold instead of direct or fixed threshold for the separation of foreground/ background of the image. Since low illuminated images are ambiguous in nature, we have applied hesitant score through intuitionistic fuzzy set for the enhancement task. The designed algorithm involves few parameters and thus the suggested scheme is automated and does not require any expert knowledge. Experimental results verify that the designed method is an efficient and simple way to improve the contrast and visual clarity in the dronogram. More detailed contributions are given below: \\n                   The rest of this paper is structured as follows: In  , we present the preliminaries to the related work. In  , we illustrate the designed dronogram enhancement algorithm in details. In  , we give experimental results and discussions. Conclusions and perspectives are presented in  .', 's0142061518304319': ' 1 Introduction Partial discharge (PD) measurement is a powerful tool for monitoring and diagnostics of the insulation of high-voltage equipment. Over the decades the technique has evolved significantly but the main challenge still remains the same: to classify, identify and recognize each of the active PD sources in the insulation that might be seen mixed or overlapped in the conventional phase-resolved PD patterns, causing identification problems. With modern digital PD measuring systems, a collection of features from the pulse shape can be extracted and then used for classification (or separation) and identification purposes. The main challenge is the selection of suitable features from the shape of the PD pulses. In  , current peak I , charge Q and energy E were proposed and tested as suitable clustering parameters under the rationing that those parameters are correlated with the physics of the discharge and that they are more resilient to variations in the acquisition parameters of the digital recording card used for the measurements  . Techniques such as the spectral power cluster  , equivalent time and bandwidth cluster  , frequency-energy characteristics of the pulses by Wavelets  , principal components analysis and t-Distributed stochastic neighbor embedding technique and morphological gradient of cumulative energy   have been widely researched for feature extraction and classification of PD sources. Pairwise plotting of these features, if properly tailored, results in data sets that can be clustered and in turn each cluster linked to a PD source. Literature commonly classifies the clustering techniques into five classes  : distribution-based algorithms, hierarchical-based algorithms, density-based algorithms and grid-based algorithms, the particular choice of one family or another depending greatly on the shape, density, anomalies and priori knowledge of the data sets  . Density-based algorithms stand out as the most suitable family of algorithms for clustering PD data sets due to their remarkable ability of discovering clusters of arbitrary shape, without the previous knowledge of the existing number of clusters. Examples of density-based algorithms are DENCLUE (DENsity ClUstEring)  , OPTICS (Ordering Points to Identify the Clustering Structure)  , DBSCAN (Density Based Spatial Clustering of Applications with Noise)   and DPC (the density peak clustering)  . DBSCAN and DPC are widely known and has been extensively researched to overcome their main drawbacks  : reduced accuracy with data sets having varied densities and high computational cost with large data sets; a minimum complexity of O( \\n                      ). On the other hand, DPC has been prized for being fast and efficient since it finds high-density peaks in a non-iterative   manner and only one parameter has to be tuned  . Joining others efforts to find improvements for the DPC method  , this paper takes the aim at tackling the issue derived from the assumption that the cluster centers in the data sets are separated by a relatively large distance. The knowledge of real world data sets extracted from partial discharge measurements brings up awareness of closeness between clusters. Therefore, a method is proposed in which the PD data sets are first split into subsets by means of the smoothed density method (SD)  , and then each subset is passed to the DPC method. This partition of data sets is done based on the spatial density ρ. Unlike DPC method, the SD method computes ρ from a two-dimensional histogram which limits the complexity of the whole algorithm. A final step involves undoing the partition of the data sets for which a solve-a-puzzle-like (SPL) routine is defined. As the subsets are being put together back again, a criterion for distances between subset contours is applied resulting on the ability of the algorithm to discover clusters that are close to each other. In addition, the results that will be discussed in this paper show that this technique proves effective not only to discover clusters even when they have very dissimilar densities and scatters, but also our SPL algorithm makes the whole algorithm little affected by pseudo cluster centers found by the DPC method, also referred to as the “decision graph fraud”  . The following parts of this paper are defined as follows:   describes the characteristics of the six data sets used to test the algorithms. These data sets as well as the Matlab implementation of the algorithm can be found in  .   provides a description of DPC and SD methods and the adjustments to the SD method to split the data sets into subsets.   defines the SPL routine and the concept of data contour. Finally, the performance of the proposed algorithm is evaluated, and further comments are given in  .', 's0933365716302950': ' 1 Introduction In 1960 the United States spent 5.2% of gross domestic product (GDP) on healthcare and by 2004 that number has risen to 16%. Current predictions have healthcare expenditures exceeding 18.7% of GDP today and forecast to increase to over 20% by 2025. There is no doubt of a healthcare crisis unraveling in the United States and in other parts of the world, driven by the rapid increase in the prevalence of chronic diseases such as diabetes and kidney disease. In addition to increasing prevalence of certain diseases, this increase in health care costs can be attributed to new and prohibitively expensive therapeutics for cancer and other disorders, and ever escalating costs of patient mismanagement and malpractice. It is apparent that the returns from the current paradigm of innovation in healthcare have diminished dramatically. Traditional approaches of hypothesis driven clinical research are becoming less effective in translating empirical data to successful health outcomes. Given the rapid growth in the rates of accumulation of healthcare data, every vertical of the global healthcare industry from therapeutics research and development to epidemiology and precision medicine would benefit from data driven, unbiased approaches. Mathematical and statistical tools developed in the field of artificial intelligence (AI) and machine learning are well poised to assist clinical researchers in deciphering complex predictive patterns in healthcare data. With government incentives offered to clinical organizations to transition from paper based patient information to well-structured and managed digital form, there has been a tremendous explosion in the availability of patient-centric healthcare data. Such data can be leveraged to open up new avenues in advancing healthcare by improving patient care and creating new efficiencies in delivering care  . Understanding variation in treatment outcomes due to patient specific molecular and clinical factors   is essential to the practical implementation of and adoption of precision medicine in clinical practice. The predominant focus of the analytical efforts in health care has been structured management and compilation of disparate data modalities and data mining with the objective of testing hypotheses generated through reasoning. These efforts are limited by the current thinking in medicine and biology, and often ignore unbiased, observational and “unknown biological” evidence. A data-driven hypothesis generation approach creates a paradigm shift in healthcare by leading to discovery of new and often surprising trends in clinical outcomes. Identification of non-obvious correlations and causal relationships may lead to a significant mitigation of side effects, need for additional medications, reduction of hospitalizations, decrease in unnecessary care, loss of income for patients, and eventually in the overall change in clinical practice. This study demonstrates how high-level, publicly available healthcare data, in particular, data coming from the United States Centers for Medicare and Medicaid Services (CMS) can be leveraged to generate profound and surprisingly powerful hypotheses in patient management and disease outcome by employing advanced learning methods such as Bayesian networks   (BNs). BNs or Bayesian artificial intelligence is a mathematical framework for learning probabilistic cause and effect relationships directly from data. The factor or variable relationships learned as directed acyclic graphs. Learning BNs in large data sets is typically an NP-hard optimization problem where an enormous number of possible graphs are attempted in a heuristically driven process of finding locally optimal structures. In this case study, the Diagnosis Related Group (DRG) codes are the factors and the provider specific discharge numbers are the observations. The approach presented here is a data analytics pipeline developed to work with population claims data on the   scale. A network of cause and effect relationships between medical diagnoses was learned in a purely data-driven manner from previously published billing data from CMS. A follow up network analysis revealed novel comorbidities, disease progression factors and the sequence of disease diagnoses for a number of chronic diseases. The findings were confirmed by subsequent association-based statistical analysis using logistic regression models. BNs are a powerful tool that can be readily used in the analysis of big data and presents a dynamic approach to the hypothesis driven epidemiological research. These newly discovered non-trivial relationships might have a significant impact on clinical disease management. The increased utilization of technologies such as AI, in healthcare research, is likely to have a significant impact on clinical disease management and global healthcare efficiency.', 's0306437915000459': ' 1 Introduction Business process compliance emerged as hot topic in research during the last few years. In essence, several approaches have been developed to formally and (semi-) automatically prove that business processes comply with relevant constraints such as regulations, laws, or guidelines. An example constraint from the medical domain would be “The patient has to be informed about the risks of a surgery before the surgery takes place”. In practice, compliance checks are often conducted manually and hence perceived as a burden  , although their importance is undoubted. The need to check for compliance of business processes based on a set of constraints may emerge in different phases of the process life cycle  . During design time, the compliance of a process model with a set of constraints is checked. At runtime, the progress of a potentially large number of process instances is monitored to detect or even predict compliance violations. For this, typically, terms such as compliance monitoring or online auditing are used. Finally, processes can be diagnosed for compliance violations in a   or offline manner, i.e., after process instance execution has been finished. This paper is dedicated to compliance monitoring as this is crucial for the timely detection and prediction of compliance violations as well as for the provision of reactive and pro-active countermeasures on compliance violations  . Further, in realistic settings, the existence of a complete process model for compliance checks cannot always be assumed. In fact, business processes are often implemented in a rather implicit manner and executed over different information systems (e.g., Enterprise Resource Planning (ERP) or Customer Relationship Management (CRM) tools) as depicted in  \\n                      . Although there are similarities between design time/  analysis and compliance monitoring (see also  ), this paper will focus on the latter in order to provide a clear scope. Typically, compliance requirements on business processes stem from different sources such as laws, regulations, or guidelines that are often available as textual descriptions. An important task towards compliance monitoring is the interpretation of these requirements as compliance objectives and the subsequent specification as compliance rules or constraints (note that, in this paper, we will use both terms interchangeably). As shown in  , the specified compliance rules will be verified over the process execution events. The results of compliance monitoring can be visualized and reported back to users in different ways, ranging from notifications on violations to fine-grained feedback on reasons for violations, or even the prediction of possible and unavoidable future violations. In general, compliance monitoring approaches are driven by two factors: (1) the   that is used to specify the compliance requirements and (2) the   the compliance checks are based on. Due to the possible heterogeneity of the data sources employed, an integrated target event format is desirable. \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                   \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                   \\n                      \\n                      \\n                      \\n                      \\n                      \\n                  ', 's0921889015003000': ' 1 Introduction Autonomous systems are increasingly required in various practical applications, including unmanned aircraft, driver-less cars, healthcare robots, manufacturing robots, etc. In all of these cases it is easy to imagine a situation where an autonomous system causes harm to people or property, as a result of an error in its engineering, or an unfortunate combination of circumstances. Therefore, if such autonomous systems are to operate within society, we must be able to trust that their behaviour complies with the legal, social, and ethical norms of that society. Determining the trustworthiness of technology in this respect is usually delegated to a regulatory body, such as the Federal Aviation Administration (for aircraft in the USA) or the Vehicle Certification Authority (for road vehicles in the UK). The process is known as  , and is used to determine the safety and reliability of safety-critical technology, including aircraft, road vehicles, nuclear reactors, pharmaceuticals, etc. For non-autonomous systems, such as cars or manned aircraft, it is assumed that the operator of the system will satisfy the ethical standards of society,  , the pilot of a civilian aircraft does not intend to use the aircraft to commit murder, and will, if necessary, disregard legal restrictions for ethical reasons,  , the pilot will disregard the Rules of the Air in order to preserve human life. These assumptions are an unavoidable result of the opacity of human behaviour; it is extremely difficult to pre-determine the behaviour of a human being. However, autonomous systems are far more transparent, and can be engineered to meet requirements. Typically these requirements are technical (“an aircraft must be able to fly at 10,000\\xa0feet”) or legal (“a car must have visible registration markings”), but in the case of autonomous systems some requirements may be ethical ( , “an autonomous unmanned aircraft will never choose to do something dangerous unless it has no other option”). Such ethical requirements may prove essential for an autonomous system to be certified by a regulatory body, since ethical autonomy is obviously desirable. Machine ethics is an emerging discipline concerned with ensuring that the behaviour of machines towards humans and other machines they interact with is ethical\\xa0  . It is an open question whether machines are, or will ever be, moral agents,  , in possession of an innate ability to distinguish between right or wrong. However, it is necessary to enable them to adhere to our human understanding of morality, despite there exists no obvious or easy way to accomplish this\\xa0  . If we assume that an autonomous system can be capable of moral agency, and possibly even be a better moral agent than a person, the goal of machine ethics is to enable machines to  . Notable works in this area are\\xa0  . Within this sub-area of machine ethics a lot of the questions traditionally studied in moral philosophy are reiterated but now from a computational perspective. The focus of research lies on automated extraction and identification of ethical guidelines for conduct, as well as on automated solving of ethical ambiguities and problems. These systems are often developed with the intention to be used to aid ethical decision-making by people. If we assume that an autonomous system is   capable of moral agency, then the goal of machine ethics is to ensure that machines  . This is done by developing methods for ethically constraining the actions of machines\\xa0  . Within this subarea of machine ethics, research focuses on identifying ethical principles that a system should not violate during its operation and developing methods for embedding consideration of these ethical principles in the decision-making process of the machine. Examples of work in this area are\\xa0  . We are interested in representing and embedding consideration for ethical principles in the decision-making process of an autonomous system in a way that is amenable to certification. The work on ethically constraining actions of autonomous machines in\\xa0   focuses on machines used in military operations and methods for stopping the autonomous machine from performing any action that is deemed unethical, but it does not consider circumstances where no ethical action is possible. Our focus in this paper is on civil applications. We propose a method for selecting among unethical actions, when no ethical action is possible, and for proving that a machine only behaves unethically, by choosing a minimally unethical course of action, if it has no ethical choice. \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                   \\n                      \\n                      \\n                      \\n                  ', 's1071581916300866': \" 1 Introduction Computer systems nowadays support humans in many different tasks. Their superior abilities in searching, book-keeping and summarisation render them irreplaceable in many complex situations. However computer systems in general only address the informational needs of a human being. For a stressful task, a human supporter will not only provide information but also attempt to alleviate the undesirable emotions being experienced by the task performer. This is a feature that is only beginning to be addressed in computer systems, typically in the context of virtual agents. Virtual agents are computer-generated virtual characters that interact intelligently with users typically taking on roles that normally performed by humans such as coaches, tutors or customer representatives. By   we refer to communications from a supporter to a task performer that do not provide concrete help with the details of the task but attempt to address the emotions that are being invoked by the task. We focus on emotions arising from the stressful nature of the task. For instance, a supporter might reassure “Don't worry”, show empathy “I understand that you are feeling frustrated”, praise “you are doing a great job” or encourage “you can do this”. Human beings seem to be remarkably successful at giving emotional support. At least, we trust them to give emotional support in key situations, often with very little training. Moreover, they are able to adapt their support to the type of situation being experienced, which is important as support provided in the wrong context can have a detrimental effect (cf.  , as reported in  ). This paper is about initial attempts to produce a computer algorithm able to capture some aspects of this human behaviour, in particular able to adapt emotional support to different stressors. In order to ask the question “what emotional support should be given in this situation?”, we start by addressing some more fundamental questions: The task is then that of modelling the human behaviour as a computer algorithm \\n                       in particular deciding: \\n                   The work of this paper was inspired by the MIME project ( ), which investigated the development of a computer aid for Community First Responders (CFRs) attending medical emergencies. CFRs are volunteers with limited medical training who attend medical emergencies, particularly in remote and rural areas, while an ambulance is en route. The computer aid enabled CFRs to measure and monitor the key medical parameters of the casualty, enter information about their observations and actions, and generated a handover report for the ambulance personnel when they arrived. Although the task of a CFR is known to be stressful in a number of ways, the MIME system only addressed the provision of factual information to the CFRs. Inspired by this, the research described in this paper asked the question “what sort of emotional support might a computer provide to people experiencing the kinds of stressors CFRs experience?”. Following a review of related work in  , in   we produce and validate textual scenarios depicting individual stressors, crowd source a corpus of emotional support statements, and reliably categorise these statements into emotional support categories. In  , we use the statements and scenarios to investigate what emotional support people offer to other people experiencing different stressors. Based on this, we develop three emotional support algorithms and evaluate these, leading to a refined algorithm and a further evaluation. An overview of this process is illustrated in  \\n                      .   concludes the paper and provides indications for future work.\", 's1071581918300016': ' 1 Introduction In a scenario of digital content delivery for the cultural heritage sector—either online or onsite—to adjust what is presented to the visitor is seen as essential to accommodate different visit motivations, expectations, and needs ( ). Within the meSch project, \\n                       we addressed the challenges of supporting a personally meaningful, sensorily rich, and socially expanded visitor experience through tangible, embedded and embodied interaction ( ). We envisage a cultural space filled with smart objects, each with their own stories embedded therein. Content will be revealed if and when conditions are right, e.g. visitors have picked up an object on display to inspect it, or a group has reached a certain location, or another smart object is close by. Visitors can continue their visit online—via a personalised interaction—to experience heritage in a novel way that combines the material and the digital. To create such a hybrid experience requires a personalisation infrastructure able to span the digital-physical divide. This in turn requires reconsidering how personalisation is done, which features should be applied and when—e.g. on-site or on-line—and, overall, how multiple contact points of the same institution can be orchestrated in a seamless extended and memorable experience. ‘Personalisation’ is a broad term that encompasses three types of system behaviour ( ):   (also called  , the term we use hereafter) offers users a number of options to set up the application/system the way they like it;   is the ability of the system to sense the current state of the environment and to respond accordingly  implies the system maintains a dynamic model of the on-going interaction and dynamically changes its own behaviour to adapt to the changing situation. When applied to a scenario of tangible interaction, the concept of personalisation widens, as the interaction between the user and the system expands to include smart objects and networks of sensors, e.g. visitors hold smart objects or move in reactive spaces. The meaning of customisation, context-awareness and adaptivity must then be extended to include physical aspects. A visitor choosing a smart replica that holds one of many stories makes a choice of customisation—the visit is shaped by the replica that triggers specific content. A system that senses the presence of the visitor and their current visit preferences shows context-awareness that combines the physical and the digital. Finally, a system that offers tangible interaction shows an adaptive behaviour when it uses the dynamic model of the visit to craft a personalised souvenir tailored to what that specific visitor did. These few examples show how personalisation must be reinterpreted when the physical aspects become part of the experience. This paper presents a multilayer framework to support personalisation across the physical and the digital. In collaboration with curators and museum experts, we set out to understand personalisation in a way that is meaningful to heritage and its visitors, that is sustainable for curators to implement, and effective in managing the complexity of hybrid experiences. To deliver such a complex personalisation service the overarching framework has: (i) to reuse the main functionalities in different contexts (e.g. onsite vs. online interaction); (ii) to facilitate porting applications to different sites, hardware devices, and heritage domains; and (iii) to implement personalisation for both content and interaction. It has to be an easy-to-use tool for curators who can, autonomously, create new stories and interpretations, as well as modify current exhibitions ( ). The paper is structured as follows.   gives an overview of the field of personalisation for cultural heritage and the new opportunities offered by tangible and embodied interaction. Interventions in museums and outdoor cultural heritage sites developed as part of the meSch project are illustrated in  ; they show a breath of multisensorial personalised experiences in both content and interaction.   reports a collaborative study with curators that questions the meaning of personalisation and the different features that must be taken into account.   discusses the personalisation framework and how complementary approaches allow for content creation to be controlled by curators while the delivery in context is controlled by the system. We also discuss how exhibition design choices that grant visitors some control on tailoring their experience (customisation) can be more effective than automatic logging and complex events processing (adaptivity).   concludes the paper with reflections on how different forms of customisation, context-awareness and adaptivity are supported by the proposed framework and their effect on the user experience.', 's1071581916301380': \" 1 Introduction Many socio-technical systems, including search engines, news aggregators, and social media sites, employ personalization algorithms to rank and filter the content displayed to users. The Facebook News Feed Algorithm is one example, designed to “deliver the right content to the right people at the right time, so they don't miss the stories that are important to them” ( ). The algorithm acts as a constraint on the information available for users to pay attention to, by generating a personalized ranking of posts for each user. The ranking is based on quantified signals such as how often Facebook Friends interact with each others’ posts, overall post-level engagement, and the type of content in the post. For example, a post can be “surfaced” by the algorithm for a given user—moved higher up in the ranking—based on how much other users are interacting with it ( ). Personalization algorithms are designed to reduce information overload and improve the user experience by connecting users with the information the system predicts they are likely to want to see, based on their past interactions with the system. The Facebook News Feed presents a unique opportunity to study the potential effects of a filtering algorithm for end users in an extremely popular \\n                       socio-technical system in which complex interactions between user and algorithm behavior determine the constraints on content visibility ( ). Algorithms are often thought of as neutral gatekeepers because they are computer code, assumed to be free from human bias ( ). However, rules designed to promote some information necessarily make other information less visible, imparting a very real “threat of invisibility” upon those whose contributions are not evaluated favorably by the algorithm ( ). Personalized content filtering may restrict the subset of Friends whose posts appear in a user's News Feed to only those who they interact with frequently, or who post information similar to what they have read in the past ( ).   defined algorithmic bias as systematically and inappropriately denying opportunities or assigning undesirable outcomes. By constraining which posts and thereby which Friends users can most easily interact with, choices made by the system which are invisible to users may be just as important to study and understand as the visible aspects, especially for the identification of possible bias. \\n                       recently argued that the work an algorithm does looks very different to the end user experiencing the effects of this work than it does to the system designers and operators responsible for creating, maintaining, and updating the algorithm. It is therefore important to investigate users’ experiences interacting with algorithmic filtering, to understand systematic, unexpected patterns of system behavior. This paper presents a study in which Facebook users engaged in a task where they might experience algorithm effects firsthand, by visiting a Friend's Timeline and noticing posts that they did not remember seeing in their own News Feeds. Because Facebook users can navigate directly to their Friends’ Timelines and view a reverse-chronological list of another user's past posts, they can use the system to find posts that may not have appeared in their News Feeds. This task triggered an “infrastructural inversion” ( ), making invisible aspects of the way the system works visible to participants ( ), in order to measure their expectations for what they should see in their News Feeds. The study investigated whether the prevalence of missed posts might vary according to the closeness of participants’ relationship with each Friend, and whether that would have any bearing on how surprised participants were about missing posts from particular people. Surprise is evidence of an unmet expectation ( ), indicating that a particular missed post was one the user would expect to see in the News Feed. Results show that individuals are likely to have missed posts from Friends in their News Feeds regardless of how close the relationship is, after controlling for factors like participants’ memories of their Friends’ past posting behavior and their perceptions of how recently the Friends were active. Even frequent Facebook users who accurately remembered when Friends’ latest posts were created still encountered missed posts, indicating that some of the missed posts participants identified were likely due to the algorithm and not to their own attention and memory. Participants were more surprised about missed posts from close Friends, and from Friends they felt like they saw often in their News Feeds. In addition, believing that the system caused missed posts, as opposed to their own behavior, was related to more surprise. The infrastructural inversion method used in this study created an opportunity for users to notice aspects of the system's behavior that would have been invisible otherwise. The conditions under which participants experienced surprise reveal that participants believed the system would prioritize posts from close friends, and these beliefs were strongest for those who thought the system took an active role in choosing which posts to display. This study highlights a possible consequence of offloading the work of choosing which posts are attention-worthy onto the algorithm, by identifying a pattern of opportunities for interaction that users did not know they were missing. Because even passive consumption of posts on Facebook can strengthen ties between Friends ( ), these results suggest that choices the system makes regarding visibility and invisibility of posts could have consequences for real relationships.\", 's0957417415003590': ' 1 Introduction In recent years, mobile communications have grown both in traffic volume and offered services. This has increased the expectations for quality of service. In this scenario, service providers are pressed to increase their competitiveness, and therefore to increase quality and reduce costs in the maintenance of their networks. This task can only be achieved by increasing the degree of automation of the network. The Next Generation Mobile Networks (NGMN) Alliance ( ) defined   as a set of principles and concepts to add automation to mobile networks. Recently, these ideas were applied to   networks by the 3rd Generation Partnership Project (3GPP) ( ) in the form of use cases and specific SON functionalities. The SON concept is composed by three fields:  ,   and  . This paper is focused on self-healing, which includes all the functionality targeted towards automating troubleshooting in the radio access network (RAN). Currently, the task of RAN troubleshooting is manually done, so the ability of automating it is a great competitive advantage for operators. The benefits of self-healing are numerous, since it will offload troubleshooting experts of repetitive maintenance work and let them focus on improving the network. It also reduces the downtime of the network, therefore increasing the quality perceived by the users. Self-healing is composed of four main tasks: fault detection, root cause analysis (diagnosis), compensation and fault recovery ( ). Currently there are no commercial implementations of these functions, although some research effort has been recently made. The reason of this shortage of implementations, according to the COMMUNE project ( ) is the high degree of uncertainty in diagnosis in the RAN of mobile networks. The COMMUNE project uses a case based reasoning algorithm ( ) where a vector of Performance Indicators (PIs) is compared against a database of known problems. The cause will be the same as the case of the nearest stored problem. In   a Bayesian Network (BN) is used to do the diagnosis. To implement the system, it is required that an expert sets the parameters of the BN, so a Knowledge Acquisition Tool is also presented in  . The UniverSelf project ( ) combines BNs with case-based reasoning ( ) for the diagnosis process. Apart from the previous references in diagnosis, research in self-healing has also been extended to detection ( ) and compensation ( ), which are out of the scope of this paper. This paper proposes a method for learning troubleshooting rules for diagnosis methods based on Fuzzy Logic Controllers (FLCs) ( ). FLCs use fuzzy logic ( ) to assign fuzzy values to numerical variables, and by applying fuzzy rules, they obtain the value of an output variable (e.g. a parameter value) or a given action. FLCs have been used for diagnosis in other fields, such as industrial processes ( ), machinery operation ( ) or medical diagnosis( ). Although FLCs have been used in mobile networks for self-optimization ( ), there are no previous references proposing its application to self-healing. The implementation of the diagnosis process is done by Knowledge-Based Systems (KBS) ( ) such as BNs and FLCs. KBS are composed of two main parts: the Inference Engine (IE) and the Knowledge Base (KB). This separation permits the algorithm to be used in a variety of situations by changing only the KB. Still, the construction of the KB, or Knowledge Acquisition (KA) ( ) is a major research topic. This is where the previous KA proposals in literature for diagnosis in wireless networks fail to deliver convincing results. The KA process involves the troubleshooting experts in a time consuming process ( ). It is based on the fact that the knowledge is contained in the experience of the expert. An alternative approach ( ) considers that the knowledge applied by the experts in the troubleshooting process will also be contained in its results. Every pair composed of the PIs and the fault cause and/or action(s) to be taken provided by the expert contains information about what aspects are observed by the expert and how they are related to each other. Therefore, a problem database ( ) can be created, where each problem is saved along with its diagnosis; and this database will hold the expert’s knowledge implicitly. Data mining (DM) consists of the discovery of patterns in large data sets through the application of specific algorithms ( ). The result of DM is a model of the studied system. DM is used to process information from sensor networks ( ), in scientific data collection ( ) or computer science ( ). DM is also used in commercial applications to find marketing trends ( ). Modern data collection and monitoring systems produce large amounts of data containing valuable knowledge, but due to the huge amount of information, this knowledge is hidden and needs to be extracted and easily visualized. In fact, the traditional DM techniques are often insufficient or ineffective for the large amount of available data. This leads to the development of Big Data techniques ( ), which deal with databases that have special requirements due to one or more of the following factors, known as the  : \\n                   In this work, a DM algorithm for obtaining fuzzy rules in the “  …” form for the diagnosis of the RAN of LTE networks is proposed, which relates certain behaviors in the PIs of a sector at a given time to the possible problem that is present in it. The rules reflect the knowledge of the experts that is implicitly contained in the results of their work (a database containing the solved problems). Other learning algorithms have been proposed for fuzzy rules in other fields, but none of them have been adapted to mobile communication networks ( ). This algorithm has been designed to be easily parallelizable in order to fit the velocity requirements described earlier and to minimize the memory footprint so a large volume of data can be processed. The remaining of this paper is organized as follows. In Section   the traditional processes of troubleshooting is presented, along with the use of KBS for automating it. In Section   the proposed DM algorithm for automating the KA process is presented. Afterwards, in Section  , the system is tested and its performance is compared with another learning algorithm. Finally, the conclusions are discussed in Section  .', 's0921889018307474': ' 1 Introduction It is desirable that autonomous robots should work closely with humans in areas such as housework and nursing care. The robots that operate in environments related to daily human life should be flexible in terms of both hardware and software. With respect to hardware, soft actuators are expected to be more suitable than conventional electromagnetic motors because they ensure soft contact with both humans and environments because of which robots with soft actuators have gained research interest  . With respect to software, it is preferable that the designers of such robots should ensure that their controllers are easy to use in different and changing situations. This study presents a flexible learning control scheme that can be applied not only to electromagnetic motor systems but also to soft actuator systems such as artificial muscles. To this end, we focus on the ability of a robot controller to adapt to the variations in robot structure that are beyond the conventional notions of link structure and serial link manipulators. The physical properties of both the internal components and the external environment of a robot may be altered. Therefore, the physical properties of the components of a robot, such as sensors, actuators, links, attached end-effectors, and tools used by the robot, as instances of internal and external factors, can be unknown or unpredictable. In this study, we present a novel framework of motor-control learning that focuses on identifying the relations among sensor variables and the application of this novel framework to controller construction. The basic concept of controller construction was presented in  , however, the automatic controller generator is extended for adapting it to more general cases in which the target variables can be directly realized without feedback and in which more various controllers can be created. The remainder of the paper is organized as follows. The relations between the presented concept of controller construction and related work are discussed in Section\\xa0 . After defining the problem in Section\\xa0 , the proposed controller construction method is presented in Section\\xa0 . Further, the controller generation is evaluated using the simulation in Section\\xa0 , which is followed by the discussion and conclusion presented in Section\\xa0 .', 's0957417417307698': ' 1 Introduction Nowadays, users are the main source of alternative sensor information in a city, although this huge source of information is often overlooked. Being ubiquitously connected to the Internet with their mobile phones, they intensively use services which promote user generated content such as Online Social Networks (OSNs), one of the most massively alternatives employed. Content in OSNs is a combination of text/images (e.g. a user post, a reply to other users posts, etc.) and meta-data information (number of likes, stars of user posts, number of posts made by the user, GPS-location, etc.). When using a GPS-enabled device, users also add a very valuable information: from where the post is shared. Thus, by analyzing the geo-located posts it is possible to know what is happening and where it is happening ( ). This is especially relevant in OSNs adapted for fast consumption (e.g. microblogging or image messaging) in which the time lapse between an event and its appearance in the platform is very low. Our previous work ( ) introduces an approach to take advantage of the information given by the geo-located posts shared in social media. Abnormal location patterns were detected in the urban area under study, such as unusual city states or dynamics. The input data of the model was restricted to the posts’ geolocation. This information was employed in order to find out when the shared posts in a specific area at that time of the day and that day of the week can be considered usual or an outlier (too much or too many). For this to be possible, a density-based clustering technique was applied. After a training stage which obtained the usual pulse of the city, the technique allowed the detection of abnormal behaviors on-the-fly. The work introduced in this paper supplements our previous findings. Here we set up a two-folded approach. Once the abnormal location pattern is detected, it identifies what is going on, where and when. Taking the set of posts which lead to a geo-anomaly as an activity seed, our new proposal enlarges the focus to all those posts which are considered linked to the seed. Opposite to pure NLP (Natural Language Processing) for geo-dependent topic modeling ( ), we apply Content Aggregation Models as the one in   to identify meaningful threads of content that reflect what is happening in the area under study in a timely fashion. We do this in order to react to potential threats as soon as possible. The paper is organized as follows.   summarizes other research proposal that are relevant for our work.   overviews the proposed methodology of the events detection system. In   we describe the dataset and the reasons behind our selection of Instagram as data source.   details the main aspects that have focused the evaluation of our proposal, whereas in   we enumerate the obtained results after the different experiments performed. The results are discussed in   and, finally, in   we outline the conclusions and future work.', 's1071581917301404': ' 1 Introduction New forms of social media (SM), provide the means to generate and share multiple digital identities, but the resulting identity landscape is complex and the data underpinning digital personhood are fragmented, offering little in the sense of a coherent individual life story or presence. New SM can embrace both individual expressions of self as well as other reflections of self that can be difficult to control, often leading to a form of ‘context collapse’ where private aspects of the digital self may leak out to an inappropriate audience (e.g.  ). Increasingly, researchers are recognising that the management of digital identity can present a burden and new solutions are being explored that support people in understanding and shaping their digital selves (e.g.  ). Many of us have a digital identity but fail to fully understand how it is represented and interpreted in the digital realm ( ). This paper reports on three converging studies that   data drawn down from SM platforms, asking whether such transformations can be used to enhance understanding of the digital self. Following   and  , we ask what different digital storytelling media offer individuals, SM researchers and designers in terms of the construction, evolution and consumption of the digital self and the processes involved in digital self-management. We discuss the context and process of digital self-formation before considering the spaces where the digital self resides and describing what happens when SM is transformed into new, often tangible and primarily visual forms. We then present the findings from three studies where we work with individuals to transform or remediate their SM into different types of visual storytelling media (a physical book, three photographs presented as a triptych, and a film). We find that this remediation of personal digital data allows users to reflect on their digital identity in new ways. We note that current assumptions about the bounded nature of SM sites are overly simplistic and that there is a need to develop more sophisticated SM use and management practices. We offer a curation framework which can be used guide the design of systems that promote self-reflection and self-presentation and that would support improved digital literacy.', 's0921889014002164': ' 1 Introduction: historical overview While the first modern-day industrial robot, Unimate, began work on the General Motors assembly line in 1961, and was conceived in 1954 by George Devol\\xa0  , the concept of a robot has a very long history, starting in mythology and folklore, and the first mechanical predecessors (automata) having been constructed in Ancient Times. For example, in Greek mythology, the God Hephaestus is reputed to have made mechanical servants from gold (  in p. 114, and\\xa0   verse 18.419). Furthermore, a rich tradition of designing and building mechanical, pneumatic or hydraulic automata also exists: from the automata of Ancient Egyptian temples, to the mechanical pigeon of the Pythagorean Archytas of Tarantum circa 400 BC\\xa0  , to the accounts of earlier automata found in the Lie Zi text in China in 300 BC\\xa0  , to the devices of Heron of Alexandria\\xa0   in the 1st century. The Islamic world also plays an important role in the development of automata; Al-Jazari, an Arab inventor, designed and constructed numerous automatic machines, and is even reputed to have devised the first programmable humanoid robot in 1206 AD\\xa0  . The word “robot”, a Slavic word meaning servitude, was first used in this context by the Czech author Karel Capek in 1921\\xa0  . However, regarding robots with natural-language conversational abilities, it was not until the 1990s that the first pioneering systems started to appear. Despite the long history of mythology and automata, and the fact that even the mythological handmaidens of Hephaestus were reputed to have been given a voice\\xa0  , and despite the fact that the first general-purpose electronic speech synthesizer was developed by Noriko Omeda in Japan in 1968\\xa0  , it was not until the early 1990s that conversational robots such as MAIA\\xa0  , RHINO\\xa0  , and AESOP\\xa0   appeared. These robots cover a range of intended application domains; for example, MAIA was intended to carry objects and deliver them, while RHINO is a museum guide robot, and AESOP a surgical robot. In more detail, the early systems include Polly, a robotic guide that could give tours in offices\\xa0  . Polly had very simple interaction capacities; it could perceive human feet waving a “tour wanted” signal, and then it would just use pre-determined phrases during the tour itself. A slightly more advanced system was TJ\\xa0  . TJ could verbally respond to simple commands, such as “go left”, albeit through a keyboard. RHINO, on the other hand\\xa0  , could respond to tour-start commands, but then, again, just offered a pre-programmed tour with fixed programmer-defined verbal descriptions. Regarding mobile assistant robots with conversational capabilities in the 1990s, a classic system is MAIA\\xa0  , obeying simple commands, and carrying objects around places, as well as the mobile office assistant which could not only deliver parcels but also guide visitors described in\\xa0  , and the similar in functionality Japanese-language robot Jijo-2\\xa0  . Finally, an important book from the period is\\xa0  , which is characteristic of the traditional natural-language semantics-inspired theoretical approaches to the problem of human–robot communication, and also of the great gap between the theoretical proposals and the actual implemented systems of this early decade. What is common to all the above early systems is that they share a number of limitations. First, all of them only accept a fixed and small number of simple  , and they respond with a set of  . Second, the only   (in the sense of Searle\\xa0  ) that they can handle are requests. Third, the dialogue they support is clearly not flexibly  ; in most cases it is just human-initiative. Four, they do not really support  , i.e.\\xa0language about their physical situations and events that are happening around them; except for a fixed number of canned location names in a few cases. Five, they are not able to handle  ; i.e.\\xa0emotion-carrying prosody is neither recognized nor generated. Six, their   \\xa0   capabilities are almost non-existent; for example, gestures, gait, facial expressions, and head nods are neither recognized nor produced. And seventh, their dialogue systems are usually effectively stimulus–response or stimulus-state-response systems; i.e.\\xa0no real   or purposeful dialogue generation is taking place, and certainly not in conjunction with the motor planning subsystems of the robot. Last but quite importantly, no real  , off-line or on-the-fly is taking place in these systems; verbal behaviours have to be prescribed. All of these shortcomings of the early systems of the 1990s, effectively have become desiderata for the next two decades of research: the 2000s and 2010s, which we are in at the moment. Thus, in this paper, we will start by providing a discussion giving motivation to the need for existence of interactive robots with natural human–robot communication capabilities, and then we will enlist a number of desiderata for such systems, which have also effectively become areas of active research in the last decade. Then, we will examine these desiderata one by one, and discuss the research that has taken place towards their fulfilment. Special consideration will be given to the so-called “symbol grounding problem”\\xa0  , which is central to most endeavours towards natural language communication with physically embodied agents, such as robots. Finally, after a discussion of the most important open problems for the future, we will provide a concise conclusion.', 's1071581916300453': \" 1 Introduction Information and Communication Technology (ICT) offers many potential possibilities to teachers for creating educational activities that students can perform in the classroom. “The Survey of Schools: ICT in Education” ( ) sheds light on the use of ICT in the classroom. First, most students think that using computers for learning is interesting, they are motivated to learn with them, and they are used to performing tasks with electronic devices at home. Second, school heads and teachers agree about the relevance of ICT use in different learning activities and acknowledge that technologies motivate students. However, despite the significant differences between EU countries regarding ICT and electronic equipment use in the classrooms, “on average across the EU countries […], only between 20 and 25% of students are taught by digitally confident and supportive teachers having high access to ICT and facing low obstacles to their use at school.” This information points out the need for teachers who are confident and supportive in effectively using ICT infrastructure and exploiting their potential. In agreement with  , we believe that this poor use of technology in the classroom is mainly due to teacher limited knowledge of existing technological resources. This problem is worsened by the severe scarcity of educational tools (although this is slowly being solved by the inclusion of tablets in classrooms), as well as the rigidity of available tools, which do not comply with teacher needs in most cases ( ). One approach to this issue is to steer development of applications specifically designed to support teaching in a particular area of knowledge. However, developing a final application is a daunting task involving a costly and time-consuming process of analysis, planning, coding, and testing (and later support) to target a particular need. Targeting needs poses a new problem of applications not being adaptable enough to teacher needs ( ). In this situation, technical expertize is needed to add new requirements to a certain application, which will require teachers to have a strong technological background or technical support. Usually, teachers do not have technical skills, and since having technical support is commonly expensive, teachers will end up abandoning ICT ( ). Putting the creative power in the teacher hands requires authoring tools with which teachers can continuously and smoothly improve the creation process. In this study, we aim to highlight the factors affecting the learnability of an authoring tool, which is understood as “the capability of the software product to enable the user to learn its application” (ISO/IEC 9126-1, 2000). As   suggested, we strongly believe that learnability is one of the critical factors for making teachers adopt a specific technology. In designing an authoring tool, we should improve the familiarity (“the extent to which a user's knowledge and experience in other domains can be applied when interacting with a new system”), the predictability (“support for the user to determine the effect of future action based on past interaction history”), and the generalizability (“support for the user to extend knowledge of specific interaction within and across applications to other similar situations”) ( ). In this sense, learnability can be optimized by measuring the teacher's performance in designing educational activities over time, which can allow us to identify flaws or characteristics of the developed tool and improve it to ease the learning curve. Teachers show different learning curves when using different authoring tools that are affected by both the user interface design ( ) and the teacher's background skills and prior knowledge ( ). Thus, design decisions will affect how easy teachers find the tool upon first use, how fast they acquire skills, and how competent they become after using the tool for a fair amount of time. These effects can in turn be observed in the learning curve of each user. We were thus motivated to design and implement DEDOS-Editor, a novel authoring tool meant to provide a consistent design metaphor to smooth the learning curve for teachers. Our approach capitalizes on the creativity and expertize of teachers in the creation process, as well as the group dimension of the educational system by allowing teachers to access, use, and build over content developed by other teachers. In the design of DEDOS-Editor, we focused on improving learnability through a direct-manipulation interaction style ( ) and a consistent interaction metaphor ( ) for the creation of educational activities. We started with the following questions: \\n                   To test our hypotheses, we performed a controlled experimental study with bachelor's students aged 21–30 years old in primary education and early childhood education (future teachers) at Universidad Rey Juan Carlos, Spain. The experiment compares DEDOS-Editor with an authoring tool that can be seen as a fair example of those available. After reviewing the state of the art, we chose JClic-Author ( ), which is the most popular educational creation tool for personal computers and digital blackboards in Spain. Both tools were compared to collect information about their learnability, which helps us to comprehend teacher struggles when first facing a new tool in their classrooms and the main reasons they stop using them. This paper is structured as follows. In  , we analyze the current educational authoring tools and whether they support collaborative learning using different devices.   discusses how design principles affect the learning curve and which designing principles we have followed.   detail the experimental study, the characteristics of the participants, the methodology, and the results. Finally,   summarize the discussion and conclude the work.\", 's0952197619301253': ' 1 Introduction The study of the dynamic aspects of traffic is essential for a proper modelling of traffic and its related phenomena, like traffic congestion among others. Intelligent Transportation Systems (ITS) address this level of uncertainty through advanced monitoring systems of the traffic network in real time, which makes possible to determine the system state and to respond to unexpected situations. The implementation of these systems (traffic responsive) requires significant economic investment and complex maintenance processes, which means that at the present moment, the number of urban areas equipped with these systems remains small. Many studies and methods for traffic planning and control in urban networks are based on the assumption of time-of-day (TOD) intervals which determine dynamic congestion patterns. A TOD is a period of time where traffic dynamics can be considered stationary, so it is possible to assume that traffic in this interval follows a specific behaviour, which is defined through a pattern. Once TODs have been determined, timing plans are developed and optimized for each TOD using heuristic and metaheuristic algorithms. Typically, between three and five plans are run in a given day. Traditionally, TOD determination and optimization plans are tasks that has been performed visually by traffic engineers making it challenging and subjective. Currently, there are two possibilities to address the problem of defining control strategies for traffic networks based on TOD intervals. \\n                      \\n                   Determining TOD intervals has been widely addressed in the literature using mainly cluster analysis techniques, among others. Nevertheless, the applied methods do not consider the so-called   (see\\xa0 ). The omission of these constraints leads to a noisy and infeasible detection of TOD intervals as well as transitions between different TODs with a high cost, in terms of quality of modelling. These noisy TODs are clusters which do not follow an intuitive TOD scheme as the majority of clusters. For that reason, these clusters have to be re-assigned to other TODs, since if it is not done, the transition between these unfeasible clusters and the other ones will be highly expensive and the obtaining of a larger number of clusters will produce worst results. The re-assigning process of unclean clusters was made again by traffic engineers. The time-domain-constrained data clustering problem tackles a clustering problem in which data are labelled with the time where they were gathered. Furthermore, the time-domain constraints impose that the obtained clusters need to be contiguous in time. That is, if two data are grouped in the same cluster then all data with a time label between both data must be assigned to the same cluster. Thus, time series segmentation is solved through the partition of a time series into homogeneous clusters which are close in terms of time, solving the problem of noisy clusters and minimizing the costs of transitions between clusters. A second challenge for TOD determination and traffic light optimization is the development of a bilevel optimization model which addresses simultaneously the definition of control strategies. However, it implies a high computational cost, derived from the two hard optimization tasks of determining TODs and optimizing timing plans. In order to solve successfully the model, memetic algorithms which combine the advantages of metaheuristics and local search strategies must be developed, in order to build more accuracy and faster algorithms which achieve an optimum trade-off between exploration and exploitation. Hybridizations of metaheuristics and local search algorithms have been proven that outperform the results of them individually (see\\xa0 ,\\xa0 ,\\xa0 , and\\xa0 ). Moreover, there is not in the literature an automated methodology for determining TOD breakpoints and optimizing traffic signal times in each segment. For this reason, in this paper the problem is formalized following a simultaneous methodology through an approach given in\\xa0 . The model is formed to simultaneously determine the TODs and the traffic control strategy. In the upper level the TODs breakpoints are determined by optimization and the lower level problem is represented by a traffic control problem. The integration of both levels avoids local optima in the TOD breakpoints pursued. This paper focuses on a control strategy based on timing plans for intersections but can be easily extended to methods using the whole traffic network. The main contributions of the paper are: \\n                      \\n                   The article is organized as follows. Section\\xa0  reviews traffic signal control and optimization based on TODs. In Section\\xa0  the bilevel model is formulated. In Section\\xa0  the memetic computing for its resolution is described. The automatic determination of the number of TODs is carried out in Section\\xa0 . In Section\\xa0  the numerical experiments over real and synthetic data are carried out and finally the conclusions obtained are analysed.', 's1071581918300971': ' 1 Introduction Speech is a natural form of communication which is rich in information. Since the early twentieth century, radio broadcasting has been used to transmit and consume speech-based content. Today, radio listenership remains high and podcasting continues to grow in popularity. Although many radio programmes are still broadcast live, a large proportion are pre-recorded and put together using audio editing software. Efficient navigation and editing of speech is crucial to the radio production process. However, unlike text, speech must be consumed sequentially, and does not naturally support visual search techniques ( ). Audio editing interfaces display a visual representation of the amplitude of the sound, called a ‘waveform’. This gives users some ability to visually search and scan audio content. Although the waveform is useful for many editing tasks, it displays very limited information, especially when zoomed out ( ). Semantic audio analysis technology can be used to extract higher-level information from the sound, such as whether it is speech or music ( ), where different people are speaking ( ) or an approximate transcript of what they are saying. Presenting this information to the user could allow them to navigate and edit audio content much more efficiently ( ). We are interested in investigating whether semantic speech editing can be used for radio production, and understanding what effect semantic editing and automatic speech recognition (ASR) transcripts have on the production process. In this paper, we describe the design and development of   – a semantic speech editing interface for radio production. We explain how we used our system to run a contextual user study that evaluated semantic speech editing for the production of real radio programmes at the BBC. We find that semantic speech editing can be used to improve the radio production workflow. We learn about the effect of semantic speech interfaces on the production workflow, and identify opportunities to better address the needs of producers.', 's0306437919304909': ' 1 Introduction Information security, while being one of the top priorities for companies, is currently shifting from prevention-only approaches to detection and response — according to Gartner\\xa0 . One analyst states:  \\xa0 ; predicting a   billion worldwide spending on information security by 2020. The vehicle to integrate processes, users, and technology and hence to drive digitalization efforts in companies are executable processes implemented through Process-Aware Information Systems (PAIS). Security problems in PAIS can be found based on anomaly detection approaches, i.e.,\\xa0by revealing anomalous process execution behavior which can indicate fraud, misuse, or unknown attacks, cf.\\xa0 . Typically, whenever anomalous behavior is identified an   is sent to a security expert. Subsequently, the expert determines the alarm’s root cause to choose an appropriate anomaly  , such as, terminating an anomalous process, ignoring a false alarm, or manually correcting process execution behavior, cf.\\xa0 . The survey on security in PAIS\\xa0 , however, shows that the majority of existing approaches focuses on prevention and only few approaches tackle detection and response, particularly at runtime and change time of processes. \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                   \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                  ', 's0957417414006472': ' 1 Introduction Text clustering is a useful technique that aims at organizing large document collections into smaller meaningful and manageable groups, which plays an important role in information retrieval, browsing and comprehension. Traditional clustering algorithms are usually relying on the BOW (Bag of Words) approach, and an obvious disadvantage of the BOW is that it ignores the semantic relationship among words so that cannot accurately represent the meaning of documents. As the rapid growth of text documents, the textual data have become diversity of vocabulary, they are high-dimensional, and carry also semantic information. Therefore, text clustering techniques that can correctly represent the theme of documents and improve clustering performance, ideally process data with a small size, are greatly needed. Recently, a number of semantic-based approaches are being developed. WordNet ( ), which is one of the most widely used thesauruses for English, has been extensively used to improve the quality of text clustering with its semantic relations of terms ( ). However, there still exist several challenges for the clustering results. (1) Synonym and polysemy problems. There has been much work done on the use of ontology to replace the original terms in a document by the most appropriate ontology concept for the solution of these problems; this process is known as word sense disambiguation (WSD). This approach, however, has not proven to be as useful as first hoped. For example, approaches that expand the feature space by replacing a term with its potential concepts only increase the feature space without necessarily improving clustering performance ( ). (2) High-dimensional term features. High dimension of feature space may increase the processing time and diminish the clustering performance, which is a key problem in text clustering. Most current techniques usually rely on matrix operation methods such as LSI ( ), ICA ( ), and LDA ( ) to deal with this problem. Unfortunately, these models need too much computation. Although there also exist a few techniques have considered semantic information ( ), they have various weaknesses. For example, they do not explicitly and systematically consider the theme of a document. (3) Extract core semantics from texts. Existing dimension-reduced methods may remove some topic features, which results in the semantic content of a document is decomposed and cannot be reflected. It is desirable to extract a subset of the disambiguated terms with their relations (known as the core semantic features) that are “cluster-aware”, which leads to improving the clustering accuracy with a reduced number of terms. (4) Assign distinguished and meaningful description for the generated clusters. In order to conveniently recognize the content of each cluster, it is necessarily to assign concise and descriptive labels to help analysts to interpret the result. Nevertheless, good solutions of assigning topic labels to clusters for ease of analysis, recognition, and interpretation are still rare. This paper attempts to alleviate mentioned above problems, its contributions can be summarized as follows. \\n                   The rest of the paper is organized as follows: Section   reviews some related works. Section   presents a modified similarity measure based on WordNet for word sense disambiguation. In Section  , we describe how to extract core semantics by using lexical chains. Section   details the experiments that evaluate our method and the analysis of results. Finally, we conclude this work and show its implications in Section  .', 's0925231217309864': ' 1 Introduction With sensors pervading our everyday lives, we are seeing an exponential increase in the availability of streaming, time-series data. Largely driven by the rise of the Internet of Things (IoT) and connected real-time data sources, we now have an enormous number of applications with sensors that produce important data that changes over time. Analyzing these streams effectively can provide valuable insights for any use case and application. The detection of anomalies in real-time streaming data has practical and significant applications across many industries. Use cases such as preventative maintenance, fraud prevention, fault detection, and monitoring can be found throughout numerous industries such as finance, IT, security, medical, energy, e-commerce, agriculture, and social media. Detecting anomalies can give actionable information in critical scenarios, but reliable solutions do not yet exist. To this end, we propose a novel and robust solution to tackle the challenges presented by real-time anomaly detection. Consistent with  , we define an   as a point in time where the behavior of the system is unusual and significantly different from previous, normal behavior. An anomaly may signify a negative change in the system, like a fluctuation in the turbine rotation frequency of a jet engine, possibly indicating an imminent failure. An anomaly can also be positive, like an abnormally high number of web clicks on a new product page, implying stronger than normal demand. Either way, anomalies in data identify abnormal behavior with potentially useful information. Anomalies can be  , where an individual data instance can be considered anomalous with respect to the rest of data, independent of where it occurs in the data stream, like the first and third anomalous spikes in  \\n                      . An anomaly can also be  , or  , if the temporal sequence of data is relevant; i.e., a data instance is anomalous only in a specific temporal context, but not otherwise. Temporal anomalies, such as the middle anomaly of  , are often subtle and hard to detect in real data streams. Detecting temporal anomalies in practical applications is valuable as they can serve as an early warning for problems with the underlying system. \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                   \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                      \\n                   \\n                      \\n                      \\n                      \\n                  ', 's0736584515000666': ' 1 Introduction Non-destructive testing (NDT) is a highly multidisciplinary group of analysis techniques used throughout science and industry to evaluate the properties of materials, and/or to ensure the integrity of components/structures, without causing damage to them  . In civil aerospace manufacturing, the increasing deployment of composite materials demands a high integrity and traceability of NDT measurements, combined with a rapid throughput of data. Modern components increasingly present challenging shapes and geometries for inspection. Using traditional manual inspection approaches produce a time-consuming bottleneck in the industrial production   and this limitation provides the fundamental motivation for increased automation. Modern Computer-Aided Design (CAD) is used extensively in composite manufacture. Additionally, where it was once necessary to construct large items from many smaller parts, Computer-Aided Manufacturing (CAM) now allows these large items to be produced easily from one piece of raw material (through traditional subtractive approaches, or built up using more recent additive manufacturing processes  ). As a result, large components with complex geometries are becoming very common in modern structures, and the aerospace industry is a typical field, where wide complex shaped parts are very frequently used. Moreover the use of composite materials, which are notoriously challenging to inspect  , is becoming widespread in the construction of new generations of civilian aircraft. To cope with future demand projections for these operations, it is therefore essential to overcome the current NDT bottleneck, which traditionally can be the slowest aspect in a production process. A fundamental issue with composites manufacturing compared to conventional light alloy materials lies in the process variability. Often parts that are designed as identical, will have significant deviations from CAD, and also may change shape when removed from the mould. This presents a significant challenge for precision NDT measurement deployment which must be flexible to accommodate these manufacturing issues. For these reasons, NDT inspection is often performed manually by technicians who typically have to position and move appropriate probes over the contour of the sample surfaces. Manual scanning requires trained technicians and results in a very slow inspection process for large samples. The repeatability of a test can be challenging in structures where complex setups are necessary to perform the inspection (e.g. orientation of the probe, constant standoff, etc.)  . While manual scanning may remain a valid approach around the edges of a structure, or the edges of holes in a structure, developing reliable automated solutions has become an industry priority to drive down inspection times. The fundamental aims of automation within the inspection process are to minimise downtimes due to the higher achievable speed, and to minimise variability due to human factors. Semi-automated inspection systems have been developed to overcome some of the shortcomings with manual inspection techniques, using both mobile and fixed robotic platforms. The use of linear manipulators and bridge designs has, for a number of years, provided the most stable conditions in terms of positioning accuracy  . The use of these systems to inspect parts with noncomplex shapes (plates, cylinders or cones) is widespread; typically, they are specific machines, which are used to inspect identically shaped and/or sized parts. More recently, many manufacturers of industrial robots have produced robotic manipulators with excellent positional accuracy and repeatability. An industrial robot is defined as an automatically controlled, reprogrammable, multipurpose manipulator, programmable in three or more axes  . In the spectrum of robot manipulators, some modern robots have suitable attributes to develop automated NDT systems and cope with the challenging situations seen in the aerospace industry  . They present precise mechanical systems, the possibility to accurately master each joint, and the ability to export positional data at frequencies up to 500 Hz. Some applications of 6-axis robotic arms in the NDT field have been published during the last few years and there is a growing interest in using such automation solutions with many manufacturers within the aerospace sector  . Exploring the current state of the art, RABIT is a group of systems developed by TECNATOM S.A., in collaboration with KUKA Robots Ibérica, that first approached the possibility of incorporating the use of industrial robots in NDT applications  . These systems boast the capability of using the potential of industrial robots and integrating them in an overall inspection apparatus, bringing together all the hardware and software required to plan and configure ultrasonic inspections. Off-the-shelf robotic arms were also used in the Laser Ultrasound for Composite InspEction (LUCIE) system, addressed to inspect large curved surfaces such as the inside of aircraft fuselage, by means of ultrasound generated by laser  . Genesis Systems Group has developed the NSpect family of Robotic Non-Destructive Inspection cells. Incorporating the FlawInspecta technology, developed by Diagnostic Sonar in conjunction with National Instruments, the NSpect systems employ a KUKA 6DOF robot arm to perform ultrasonic inspection using either an immersion tank, or a recirculating water couplant. General Electric (GE) has also investigated the integration of phased array UT with off-the-shelf industrial robots for the inspection of aerospace composites  . Despite these previous efforts, there remain challenges to be addressed before fully automated NDT inspection of complex geometry composite parts becomes commonplace. The key challenges include generation and in-process modification of the robot tool-path, high speed NDT data collection, integration of surface metrology measurements, and overall visualisation of measurement results in a user friendly fashion. Collaborations driving this vision include TWI Technology Centre (Wales), which is currently carrying out a 3-year project, called IntACom, on behalf of its sponsors; its objective is to achieve a fourfold increase in the throughput of aerospace components  . Additionally the UK RCNDE consortium conducts research into integration of metrology with NDT inspection  . Both these consortia have identified the requirement for optimal tool path generation over complex curved surfaces, and the current article describes joint work between these groups to develop a novel approach to a flexible robotic toolpath generation using a user friendly MATLAB toolbox. This new toolbox provides a low cost research based approach to robot path planning for NDT applications, and provides a platform for future development of highly specific NDT inspection challenges.', 's1071581917300988': \" 1 Introduction Humans are equipped with multiple senses to perceive and interact with their environment. However, in HCI, vision and hearing have been the dominant senses, and our sense of touch, taste, and smell have often been described as secondary, as the lower senses ( ). HCI researchers and practitioners are however increasingly fascinated by the opportunities that touch, smell, and taste can offer to enrich HCI. Recent examples of such experiences include the novel olfactory display by  , taste-based gaming by  , olfactory in-car interaction by  , digital flavour experiences by  , and the added value of haptic feedback for audio-visual content by  . In particular, there has been a growing interest in uncovering the specificities of haptic experience design ( ) and the unique features of haptic stimulation that would allow the creation of emotionally engaging and meaningful experiences ( ). With the advent of novel touchless technologies that enable the creation of tactile stimuli without physical contact (e.g., ( ), a novel design space for tactile experiences has been opening up ( ). Most notably, it has been demonstrated that mid-air haptic stimulation can be used to convey emotions to the user ( ). This research has motivated further investigations of the design possibilities for creating novel mid-air haptics experiences ( ). Here we extend the use of mid-air haptics stimulation in the context of a museum, moving beyond a controlled laboratory environment to investigate the effect of multisensory stimulation on users’ experience of art. Museums and art galleries have always been in the forefront of integrating and stimulating multiple human senses, not only to explore new ways of representing arts, but also to increase the wider public interest in the artifacts being displayed.   showed that the use of touch specimens, sounds, and smells to complement the object along with interactive components (e.g., role playing induction device) and dynamic displays can have a strong influence on visitors’ experiences, especially creating a strong sense of flow – being fully immersed and focused in a task ( ). Another intriguing work that relates to multisensory museum experiences is the Jorvik Viking Centre ( ), where multisensory stimuli were used to enrich the experience of a tour concerning the Viking past of the city of York. This experience allowed visitors to touch historical objects (Viking Age artefacts), taste the unsalted, dried cod of the Viking diet, smell the aroma of the corresponding displayed objects, see the animals and inhabitants of the Viking city, and listen to the Viking sagas. More focused on the sense of touch,   presented how visitors could see and feel virtual 3D artworks (e.g., statues) using a haptic device that was connected to the user's right index finger to provide haptic feedback. This use of technology enabled users to touch and feel the contours and stiffness of the artwork. Despite the increasing interest in the different senses as interaction modalities in HCI and related disciplines and professions (e.g., art curators, sensory designers), there is only a limited understanding of how to systematically design multisensory art experiences that are emotionally stimulating. Moreover, there also seems to be a lack of understanding on how to integrate different sensory stimuli in a meaningful way to enrich user experiences with technology ( ), including art pieces.   replicated the work of   and pointed out the mismatches in the amount of time and space people spent in viewing artworks in a laboratory versus a museum context. Specifically, museum visitors had longer viewing time than was mostly realized in lab contexts, as well as longer viewing time when attending in groups of people. Additionally, this work uncovered a positive correlation between size of artwork and the viewing distance. These findings emphasize the fact that there is a need to carry out museum related investigations in the actual environment of a museum. Only through an in-situ approach, the intended users who have an intuitive interest and knowledge about art environments, are reached and can provide valuable feedback on the multisensory design and integration efforts. Building on these prior works, in this paper, we present research and design efforts carried out as part of a six-week multisensory art display – Tate Sensorium – in an actual museum environment (i.e., Tate Britain art gallery). For the first time, mid-air haptic technology was used in a museum context to enhance the experience of a painting (i.e., the   by John Latham) through its integration with sound. The multisensory integration of touch and sound aimed to aid the communication of emotions and meaning hidden in the painting:   (see  b). In collaboration with a creative team of art curators and sensory designers, the specific experience for the   painting was created. A total of three variations of the experience were created, keeping the sound the same but changing the mid-air haptic pattern to investigate the effect of the sense of touch on the visitors’ art experience (see illustrated in   and described in  ). We hypothesized that museum visitors would enjoy more experience involving the pattern specifically designed for Tate Sensorium (Tate pattern, the most sophisticated and purposeful designed experience), followed by the experience involving the Circle pattern (congruent with the visual appearance of the painting) and finally the Line pattern (incongruent with the visual appearance of the painting). Visitors’ experiences were assessed through a short questionnaire at the end of the Tate Sensorium experience and through interviews to deepen our understanding on the subjective differences of sensory enhanced art experiences. In the following sections, we first provide a review of related work on multisensory research and design in museums, followed by a general overview on the multisensory art display – Tate Sensorium in the Tate Britain art gallery. We include the description of the exhibited art pieces and sensory design space. We then focus on the work around the   painting and the design and development of the mid-air haptic patterns as part of the specific touch-sound integration. We provide a detailed description of the data collection process and the insights from the analysis of 2500 questionnaires and 50 interviews. We conclude with a discussion of our findings with respect to the lessons learnt, limitations and future opportunities for designing multisensory experiences outside the boundary of a laboratory environment.\", 's0888613x14000206': \" 1 Introduction There are strong historical links between probability theory and mathematical logic. In the last decades more and more areas of these subjects have been very closely connected in investigations of logical systems called probabilistic logics with a broad range of possible application areas (learning from data  , causal reasoning  , multi-agent systems  , robotics  , logic programming  , etc.). There are numerous proposals for probabilistic logics  , mainly based on the standard Kolmogorov's (measure theoretical) approach to probabilities which are real-valued measures that are typically required to be  -additive. However, a number of alternatives to the probability measures have been studied (for reference see  ). Many of them differ from the standard probability measures in two respects. First, the ‘infinitary’ condition of  -additivity is replaced by finite-additivity. Second, ranges of probabilities are extensions of the rational numbers different from the field of reals  . It turns out that non-Archimedean fields are particularly useful since they contain infinite and infinitesimal numbers. The two most important examples of non-Archimedean fields are the fields of hyperreal numbers   and the fields of  -adic numbers  , for any prime number  . While   is a non-Archimedean ordered field, the  -adic field cannot be ordered in an appropriate way, but in some sense, one can say that it contains infinite and infinitesimal elements. Namely, one can define infinitesimal in the following way: if   is a field of characteristic 0 equipped with the norm  , then   infinitesimal if for every positive integer  ,  . According to this approach, each  -adic number   such that  ,   and   for   is infinitesimal. In addition to these alternatives it is worth to mention an approach (that goes back to de Finetti, Renyi and Popper among others) which considers conditional probability and conditional events as basic notions, not derived from the notion of unconditional probability. Coletti and Scozzafava's book   includes a rich elaboration of different issues of reasoning with coherent conditional probability, i.e. conditional probability in de Finetti's sense. Recently a lot of research has been done by using probabilistic constraints based on the coherence principle and has got remarkable influence in the field of reasoning with uncertainty. We may cite   as some of the relevant references. In   A. Khrennikov presents  -adic probability theory and discusses its applications in physics (especially quantum mechanics). Namely, it is well known that any non-trivial norm on the field of rational numbers   is equivalent to either the usual real absolute value or a  -adic norm  , for some prime   \\n                      . By completing the field of rational numbers we obtain the field of real numbers or a field  . On the other hand, values of relative frequencies of random experiments are rational numbers. Therefore, to estimate the probability of the corresponding event we can use frequencies in the field of real numbers, but we can also use numbers in  , for some prime number  . So, we can choose   as the range of the corresponding probability measure. Thus, the  -adic approach to probability theory is an alternative to the standard real-valued probabilities which gives the opportunity to work: \\n                   Following the concepts of Khrennikov's approach to probability and techniques that are developed in a range of papers on probabilistic logic  , in   the authors have developed the propositional probability logic   which is an extension of classical propositional logic with modal-like operators  , where the intended meaning of   is that the probability of   is in the ball  . As the corresponding semantics, probabilistic Kripke-like models are introduced. These models are based on a  -adic probability space  , where   is a nonempty set,   is a subalgebra of subsets of   and   is an additive function normalized by the condition  . Now, we introduce two probabilistic logics by extending classical propositional logic with probability operators of the form  , with the intended meaning of   that the conditional probability (of truthfulness) of   given   is in the  -adic ball  . As the corresponding semantics we use models introduced for the logic  . One of the essential conditions for  -adic measures is the  : If   is a field of subsets of some set   then, for every  \\n                       In   this condition is ensured by reducing the range of probabilities to an arbitrarily large (but fixed) ball  , where   is a fixed integer. When handling conditional probabilities there is a need for multiplying  -adic numbers. Since arbitrary ball   is not closed under multiplication, these balls are no longer useful for logics with conditional probabilities. Here we might proceed in two ways. First, we can choose the unit ball   as the range of probabilities since it is closed under multiplication. The main part of this paper (Sections  ,  ,  ,  ) concerns the logic  , built on this strategy (  denotes the ring of  -adic integers). In the second approach, also presented in this paper, we can build formulas over a finite set of propositional letters, and retain   as the range of probabilities. In this case, we compute the supremum of finitely many numbers of the form  ,  , which is again a finite number. The corresponding logic   is presented in Section  . Following the strategy of the companion work  , we give axiomatic systems for the logics   and   which are sound and strongly complete with respect to the proposed semantics. Since this is one of the first papers that discuss  -adic probability logics, we would like also to present our motivation to introduce this formalism. There are numerous fields where  -adic numbers and analysis are useful: for example in mathematics in  -adic probability  , in physics  , in biology in modelling of the genom and the genetic code  , and in cognitive processes  , etc. Thus, our aim is to try to address some of the related issues and to illustrate the ways in which the proposed logics can be used to represent uncertain knowledge and to perform the corresponding inferences. Below, in Section  , we analyze several examples. The first one is related to formalization of brain working with information coded by  -adic numbers. Then, using our logic, we describe an experiment in which relative frequencies oscillate with respect to the real, but converge with respect to the  -adic, metric. Finally, we consider the possibility to express in our framework different properties of default reasoning systems. The rest of the paper is organized as follows. In Section   we present syntax and semantics of  . Section   presents axioms and inference rules of  . In Section   we prove the corresponding soundness and completeness theorems. Decidability of   is discussed in Section  . In Section   we consider a modification of the logic  , denoted  , which might be interesting for practical purposes. Some possible applications of the presented logics are discussed in Section  , while concluding remarks are given in Section  . Finally, a short introduction into fields of  -adic numbers is given in  , while more comprehensive overview can be found in  .\", 's0950705118301394': ' 1 Introduction Creativity is the intellectual ability to create, invent, and discover, which brings novel relations, entities, and/or unexpected solutions into existence  . Creative thinking involves cognition (the mental act of acquiring knowledge and understanding through thought, experience, and senses), production, and evaluation  . We first become aware of the problems with which we are confronted, then produce solutions to those problems, and finally evaluate how good our solutions are. Each act of creation involves all three processes—cognition, production, and evaluation  . According to J. P. Guilford, who first introduced the terms convergence and divergence in the context of creative thinking, productive thinking can be divided into convergent and divergent thinking; the former which can generate one correct answer, and the latter which goes off in different directions without producing a unique answer  . Although currently there is no general consensus on the definition of convergent and divergent thinking, modern theories of creativity tend to have the following perspectives. Convergent thinking is regarded as analytical and conducive to disregarding causal relationships between items already thought to be related, whereas divergent thinking is viewed as associative and conducive to unearthing similarities or correlations between items that were not thought to be related previously  . Both convergent and divergent thinking are used to model the structure of intellect  . With regard to the nature of intelligence and originality, two general problem-solving behaviors were identified, those of the converger and those of the diverger, who exhibit convergent and divergent styles of reasoning/thinking, respectively  . The distinction between convergent and divergent thinkers is done based on the dimensions of scoring high on closed-ended intelligence tests versus scoring high on open-ended tests of word meanings or object uses  . The converger/diverger distinction also applies in cognitive styles and learning strategies  . Dual-processing accounts of human thinking see convergent and divergent styles as reflective/analytic and reflexive/intuitive, respectively  , which is in line with current theories of creative cognition involving generation and exploration phases  . The convergent thinking style is assumed to induce a systematic, focused processing mode, whereas divergent thinking is suspected to induce a holistic, flexible task processing mode  . Psychological accounts that consider convergent and divergent production as separate and independent dimensions of human cognitive ability allow one to think of creative problem solvers as divergers rather than convergers  , and to associate creativity with divergent thought that combines distant concepts together  . Focusing only on either convergent or divergent thinking, however, may inhibit the full understanding of creativity  . Viewing convergent production as a rational and logical process, and divergent production as an intuitive and imaginative process, creates the danger of oversimplification and confusion between intelligence and creativity. Instead, it should be recognized that there are parallel aspects or lines of thought that come together toward the end of the design process, making the design а matter of integration  . Since convergent and divergent thinking frequently occur together in a total act of problem solving  , creativity may demand not only divergent thinking, but also convergent thinking  . For example, deliberate techniques to activate human imagination rely on the elimination of criticism in favor of the divergent generation of a higher number of ideas. The process of deferred judgment in problem solving defers the evaluation of ideas and options until a maximum number of ideas are produced, thereby separating divergent thinking from subsequent convergent thinking  . This sequence of divergent and convergent thinking is classified as ideation-evaluation, where ideation refers to nonjudgmental imaginative thinking and evaluation to an application of judgment to the generated options during ideation  . Such accounts of creativity treat divergence and convergence as subsequent and iterated processes  , particularly in that order. More recent accounts of creativity, however, highlight the interwoven role of both convergent and divergent thinking  . This interweaving has been identified in two ways. The analytic approach to creative problem solving based on linkography showed that convergent and divergent thinking are so frequent at the cognitive scale that they occur concurrently in the ideation phase of creative design  . The computational approach demonstrated that a computer program (comRAT-C), which uses consecutive divergence and convergence, generates results on a common creativity test comparable to the results obtained with humans  . Hence, the creative problem solver or designer may need to learn, articulate, and use both convergent and divergent skills in equal proportions  . The concurrent occurrence of convergent and divergent thinking in creative problem solving raises several important questions. Is it possible to evaluate convergence and divergence in problem-solving conversations in an objective manner? How do convergence and divergence relate to different participants in a problem-solving activity? Are there particular moments in the process of real-world problem solving where a definitive change from convergence to divergence, or vice versa, occurs? How do convergence and divergence relate to the success of different ideas that are generated and developed in the process of problem solving? Could semantic measures predict the future success of generated ideas, and can they be reverse-engineered to steer generated ideas toward success in technological applications, such as in computer-assisted enhancements of human creativity or implementations of creativity in machines endowed with artificial intelligence? We hypothesized that semantic measures can be used to evaluate convergence and divergence in creative thinking, changes in convergence/divergence can be detected in regard to different features of the problem-solving process, including participant roles, successfulness of ideas, first feedback from client, or first evaluation by client or instructor, and semantic measures can be identified whose dynamics reliably predicts the success or failure of generated ideas. To test our hypotheses we analyzed the transcripts of design review conversations recorded in real-world educational settings at Purdue University, West Lafayette, Indiana, in 2013  . The conversations between design students, instructors, and real clients, with regard to a given design task, consisted of up to 5 sessions ( \\n                      ) that included the generation of ideas by the student, external feedback from the client, first evaluation by the client or instructor, and evaluation of the ideas by the client. The problem-solving conversations were analyzed in terms of participant role, successfulness of ideas, first feedback from client, or first evaluation by client or instructor using the average values of 49 semantic measures quantifying the level of abstraction (1 measure), polysemy (1 measure) or information content (7 measures) of each noun, or the semantic similarity (40 measures) between any two nouns in the constructed semantic networks based on WordNet 3.1  .', 's1875952116300040': ' 1 Introduction The pre-show period in the film industry is widely recognized as a good opportunity for advertising. Theatregoers often arrive early to see a movie and furthermore, there are few distractions once they are settled in their seats. This captive audience setting is what advertising companies aim to capitalize on to reach and influence as many attendees as possible  . However, an increasing number of moviegoers are becoming dissatisfied and disengaged during the pre-feature period, partly due to the fact that they are aware of their captive status  . For example, patrons may choose to use their smartphones instead of watching the pre-show or decide to not even go to the theatre at all. Patrons are very aware of the diversity of entertainment options available. In fact, many theatregoers have home theatre environments that negatively influence their movie-going behaviour  . Movie theatres have a challenge to provide additional value that leverages the social properties of cinema attendance  . However, now is the right time for the pre-show period to harness the opportunity of social networking and personal interactive technologies  . The pre-show period can be reshaped to provide a socially enticing and personally engaging experience instead of driving theatregoers away  . \\n                      \\n                      \\n                      \\n                  ', 's0888613x14000851': '', 's0740818818301828': \" 1 Introduction A public university library is usually open to everybody, not only to students, and users may access the library in its traditional or electronic (open access) forms. There are many institutions of this type and user choice is determined by many factors, the most important being the richness, variety, and size of resources at their disposal. These key attributes create competition among libraries, who apply for various donations and grants, including governmental ones, both in the domestic and international markets. Maintaining an advanced and modern information and communication infrastructure is crucial to a library's development, and this would not be possible without external finance. Because a library plays a vital role in the educational development of society, the way it functions is of primary importance too. As it constitutes an organizational entity, at its essence, as in the case of any other organization, is the library's socio-technical system, the complexity of which, and its corresponding business processes, make analysis interesting. This socio-technical system is composed of people (staff), whereas the technical system covers processes, tasks, knowledge and resources used by the employees in their work. So, although a library is a public organization, it fits in well with the general conditions of market competitiveness.\", 's2590188519300083': ' 1 Introduction The evolution of communication technologies, such as Facebook, Google+, Twitter, Instagram, Flicker and WhatsApp, help people to interconnect quickly ( ). One such example is photo-sharing services for social networking. By taking advantage of the advancements in mobile digital camera technologies, people can easily take photos when they find something interesting and upload them to a social media platform to share exciting moments with their friends, families and colleagues ( ). As a result, one can expect large collections, which is evident, as the uploaded photo count was “about 4.5 million daily” according to the report in  . In addition, the development of multimedia technologies and cost effective CCTV cameras for surveillance applications produce diversified images or videos at a larger scale. This leads to a huge collection with a high degree of diversity and unstructured data ( ). For instance, some sample images of family and non-family photos chosen from our dataset are shown in  \\n                      (a) and (b), respectively, where we can see each image has its own variety of foreground (face regions) and background information. In this context, face recognition alone may be insufficient to identify family or non-family photos. This is because the recognition methods developed might not work well for images which contain faces with multiple emotions, postures and actions. This makes the problem of finding photos that belong to the same family complex and challenging. As a result, family photo classification/identification can play a vital role in finding a solution to unsolved issues such as human trafficking, kinship recognition, and the problem of identifying/locating refugees ( ). Hence, there is an urgent need for developing an intelligent expert system for tackling the above-mentioned challenges. There are methods for identifying humans, facial expressions and emotions based on biometric features, which can be used for family and non-family image identification ( ). However, one major challenge of biometric systems is the variability in characteristics of the biometric of each individual. For example, the human face is complex, with features that change over time. In addition, facial features change due to variations in illumination, head pose, facial expression, cosmetics, aging, and occlusion because of beards or glasses ( ). In addition, most of the methods require cropped face images for achieving better results ( ). Therefore, recognition-based systems may not be suitable for family and non-family photo classification because the images can have unconstrained backgrounds and multiple faces with numerous emotions or expressions ( ). Hence, we can conclude that we need an expert and robust system that can cope with background complexities and issues of multiple faces with different emotions and expressions. In this work, we propose to find a solution for family and non-family photo classification based on the characteristics defined below for family and non-family images in  . In the case of family photos, it is expected that \\n                   In the case of non-family photos, it is expected that \\n                  ', 's1875952117300952': ' 1 Introduction Stroke, brain injury and multiple sclerosis are the leading causes of most disabilities in adults which result in neuro-motor deficits. This can affect postural control and balance and cause difficulties in independent daily life  . Physiopathologic research has demonstrated that such difficulties include utilising hands for timed grasp, holding, buttoning, reaching, balancing or/and walking  . It is of vital importance to provide opportunities for patients to relearn or improve basic skills by doing exercises that help them to restore appropriate physical functionality. The recent availability of inexpensive off-the-shelf sensors (such as the Apple iPad, Nintendo Wii, Nintendo DS, Microsoft Kinect and balance board) have opened up new exciting perspectives to assess the practical capabilities for home-based rehabilitation and to improve exercise capacity  . These devices have received attention from the academic community in many disciplines including; health, robotics, biomechanics, and engineering  . Some of these devices have been used by researchers to develop rehabilitation tools, but they lack sufficient data acquisition capability   or are expensive, time-consuming and require extensive technical expertise  . The Kinect v2 however is a relatively cheap, easily configurable off-the-shelf device capable of accurately tracking gestures and joint positioning  . It detects the position, orientation and angular velocity of a players’ 25-joints through use of an infrared emitter and a colour camera which forms part of a skeleton tracking system to mirror the location of the player’s joints.   report that those body parts that are obstructed from the Kinect’s direct line of vision cannot be tracked, whilst   conclude that the Kinect v1 system can accurately measure gross unique characteristics.   have conducted a comparative study on motion tracking between Kinect and the OptiTrack optical systems and their work shows that Kinect can achieve a comparable motion tracking performance. Previous work exclusively using the Kinect in a rehabilitation context includes its use in the recovery of spinal muscular atrophy   who report a significant improvement in patient motivation, and   who devised a quantitative assessment of exercises performed by trauma brain injury patients. A number of researchers have utilised the Kinect in conjunction with other devices. These (expensive) studies include use of MoCap and treadmill system for rehabilitation and gesture recognition   and integration with a multiple camera 3D-motion analysis system  . The latter study demonstrated that the Kinect can validly assess postural control in a clinical setting. In the present study we aim to bring a novel solution to the problems related to traditional physiotherapy and rehabilitation by using a cost-effective serious game where the core device is the Kinect v2 but integrated with a Myo armband. The wireless Myo armband (Thalmic Lab) \\n                       is a motion capture device that collects inputs from the user’s skin. The Myo is made of eight medical grade stanless steel electromyography (EMG) sensors that detect the electric impulses in the muscles. The armband is connected via a Bluetooth USB adapter that records/collects real-time data with a high accuracy and precision. The Kinect-Myo apparatus is also linked to a Saitek FootPedal device \\n                       which is connected to the computer via a USB port and a seated or standing player in order to simulate walking via the avatar; foot resistance is adjustable/configurable according to the required level of difficulty. Use of the FootPedal is reported here solely for information as it will be the subject of future development and is not the focus of the present study. This multi-input system outputs high quality data and provides the convenience of wireless transfer to provide a superior clinical grade source of medical data for muscular performance compared to alternative hardware studies. The players’ input is simultaneously transferred into a simulated virtual 3D-park via the Unity game engine with all files and data stored locally on a hard drive. A Monte Carlo Tree Search algorithm (MCTS) generates virtual objects in the 3D-space and adapts game difficulty to the player’s ability in real-time.  \\n                       illustrates the architecture, peripherals, and algorithm used to design the system. Four game scenarios are developed; “Fruit-Collection” to grasp/ release virtual fruits in a virtual basket, “Button-Press” to reach/press virtual buttons for 3-s (the duration reflects an appropriate balance between playability and difficulty), “Sling-Shot” to knock-down virtual boxes and “Fruit-Collection with the FootPedal” to navigate between pre-established key points and collect virtual objects  . The frequencies of the entire data collection were normalised to facilitate comparison between Kinect and Myo when continuously estimating and comparing arm orientation. The reliability and accuracy of the devices in measuring functional and clinically relevant movements of the upper limb were also investigated. Use of a FootPedal is not significant to this study and only reported as work in progress for future developments to the game.', 's0888613x15000857': ' 1 Introduction Humans mostly use words (in natural languages), which are inherently vague and qualitative in nature, to describe real world information, to analyze, to reason and to make decisions. Human knowledge is often expressed linguistically and thus involves fuzzy predicates. Also, if human knowledge is expressed linguistically, different linguistic hedges are commonly used to express different levels of emphasis. For example, different degrees of tallness can be expressed using terms such as “very tall” and “rather tall”. In addition, in the real world applications, there are situations in which information may not be assessed in a quantitative form (i.e., in terms of numbers), but rather in a qualitative one (e.g., in terms of linguistic terms). This may arise for different reasons. In some cases, due to its nature, the information may be unquantifiable and thus can be stated only in linguistic terms. For example, when evaluating the “comfort” or “design” of a car, we may be led to use linguistic terms such as “good”, “medium” and “bad”. In other cases, precise quantitative information may not be stated since either it is unavailable or the cost of computation is too high, so a linguistic “approximate value” may be acceptable. For instance, when the rotation speed of an electric motor is evaluated, linguistic terms, e.g., “very large”, “large” and “medium”, may be used instead of numerical values. Therefore, there is a natural demand for formalisms that can directly work with linguistic terms and make use of linguistic hedges since such systems make it easier to represent and reason with linguistically-expressed human knowledge. Logic programming (LP) is a well-established, coherent formalism for knowledge representation and reasoning since it has a semantics in the sense of Tarski and sound and complete (or weakly complete) proof procedures, which are understandable by humans in terms of problem reduction and are computer-implementable  . Traditional LP has a serious limitation in that it cannot cope with the issues of vagueness and uncertainty into which fall most modes of human reasoning. Fuzzy set theory   and its derived disciplines such as fuzzy logic   and possibilistic logic   are well known for providing the most widely-adopted techniques for managing vagueness and uncertainty. There is substantial literature on LP frameworks extended by these techniques to handle vagueness and uncertainty in knowledge representation and reasoning  . Fuzzy linguistic logic programming (FLLP), introduced in  , is an LP framework for managing vagueness in linguistically-expressed human knowledge, where truth of vague sentences is given in linguistic terms, and linguistic hedges can be used to express different levels of emphasis. It is an LP framework without negation under fuzzy logic in the narrow sense (FLn)  . In FLLP, each fact or rule is graded to a certain degree specified by a linguistic truth value, and linguistic hedges can be used as unary connectives in rule bodies. For example, a statement “(A car is considered good if it is   reliable and consumes   little fuel) is  ” can be represented by the following rule:  where  ,  ,  ,  ,  , and   stand for  ,  ,  ,  ,  , and  , respectively. Most concepts and results of traditional definite LP can have a counterpart in the framework. The linguistic truth values are generated from primary terms   and   using hedges (e.g.,   and  ) as unary operations  . The linguistic truth values and hedges have several intuitive natural semantic properties such as  . Some approximate semantic equivalence in natural language holds in the framework. The procedural semantics of FLLP can directly manipulate linguistic terms to compute answers to queries. Thus, the framework can provide a computational approach to human reasoning in the presence of vagueness. According to Zadeh  , linguistic hedges play a twofold role: in the generation of values of a linguistic variable from primary (atomic) terms and in the modification of fuzzy predicates. In FLLP, hedges are used to generate linguistic truth values from the primary terms. Also, since hedges are allowed to be unary connectives in rule bodies, they can play the role of predicate modifiers. In FLLP, up until now, there have been only two methods to compute answers to a query w.r.t. a logic program: (i) by bottom-up iterating the immediate consequence operator  ; or (ii) by using the procedural semantics. Nevertheless, on one hand, the former is exhaustive and not goal-oriented. Indeed, it requires computation of the whole least Herbrand model despite the fact that not all the results are required to determine the answer of the query. On the other hand, although the latter is goal-oriented, it may lead to an infinite loop (if the program is recursive) and may recompute atoms (subgoals) in rule bodies. Moreover, it typically does not give a most general answer to a given query. In the literature of logic programming, the   (also called   or  ) technique can be utilized to overcome these problems  . The underlying idea is that subgoals and their associated answers are stored in an appropriate data space, called the  , so that they can be reused when a repeated subgoal appears during the resolution process. Whenever such a repeated call occurs, the associated answers of the subgoal are retrieved from the table instead of being re-evaluated against the program clauses. Tabulation-based query answering procedures are able to reduce the search space, avoid looping, and have better termination properties than SLD-based procedures  . There have been several attempts at utilizing the tabulation technique to improve the efficiency and termination properties of the procedural semantics for logic programming dealing with vagueness and uncertainty, resulting in so-called  . The procedures generate a set of  , called a  , and each tree computes answers for an atom. The procedures in   and   deal with propositional residuated logic programs and propositional multi-adjoint logic programs, whereas the one in   is for first-order residuated logic programs. The procedure in   is non-deterministic and not very efficient since it still creates redundant trees and nodes. Moreover, its termination remains open. In this paper, we first adapt the non-deterministic tabulation proof procedure in   for FLLP. The purpose of this is to ease the subsequent proofs of soundness and completeness. We show the termination of the non-deterministic procedure and prove its soundness and completeness. In addition to still generating answers to queries that are less general, the non-deterministic procedure also creates redundant trees and nodes. To overcome these problems, we then propose a deterministic procedure which is more efficient than the non-deterministic one in terms of the number and size of trees created. Moreover, the non-deterministic procedure gives all and only the most general answers to a given query. The tabulation rules of the deterministic procedure are an improvement of those of the non-deterministic one. We determine the priority orders in which the tabulation rules are selected to apply and the nodes are selected for application of each tabulation rule. The deterministic procedure is proved to terminate and to be sound and complete. We also discuss the applicability of the deterministic procedure in problems such as threshold computation and top-  retrieval which can be used, for example, in a case when the number of answers becomes large due to a very large set of facts in the logic program. This paper is a substantially extended version of a conference paper  , in which we propose the deterministic procedure, show its termination, and prove its soundness. The remainder of the paper is organized as follows. Section   gives an overview of FLLP. Section   presents the non-deterministic procedure, defining non-deterministic tabulation rules and the non-deterministic procedure, showing its termination, and proving its soundness and completeness. Section   presents the deterministic procedure, defining deterministic tabulation rules and the deterministic procedure, showing its termination and soundness, proving its completeness, giving an example of how it works, and discussing its applicability in threshold computation and top-  retrieval. Section   discusses related work. Section   concludes the paper.   gives all proofs of lemmas and theorems of the paper.', 's095741741830215x': ' 1 Introduction Text classification is a construction problem of models which can classify new documents into pre-defined classes\\xa0( ). Currently, it is a sophisticated process involving not only the training of models, but also numerous additional procedures, e.g. data pre-processing, transformation, and dimensionality reduction. Text classification remains a prominent research topic, utilising various techniques and their combinations in complex systems. Furthermore, researchers are either developing new classification systems or improving the existing ones, including their elements to yield better results, i.e. a higher computational efficiency\\xa0( ). Literature overviews of text classification usually reveal its crucial elements, techniques, and solutions, proposing the further development of this research area. Nevertheless, the existing reviews are still useful as they address the significant problems of text classification\\xa0( ). However, these works are slightly outdated as they do not include the latest studies. Furthermore, their explanation of text classification has some limitations, for example, they lay emphasis only on machine learning techniques or algorithms, omit some essential elements of text classification, or focus on a particular research domain\\xa0( ). We reiterate here that these are excellent works, which are still useful to the research community. However, with the increasing interest in the area of text classification, we need the most recent systematic overview to better understand what has been achieved in this field. In this study, we aim to overcome the difficulties mentioned above. Moreover, the article presents a latest and holistic summary of text classification. We direct significant effort to generate a research map for text classification to assist in recognising its main elements by examining both the most recent and former studies. More specifically, in addition to the understandable requirement to complement the existing reviews, the objectives of this study are as follows:\\n \\n                   According to the best of our knowledge, there are no similar recent studies in the form of an overview of the investigated field. Furthermore, we believe that this study significantly systematises and enhances the knowledge regarding the modelling of classification systems. The results of the text classification process with its elements are particularly relevant. Moreover, we show that it is possible to identify, explore, and develop new aspects of text classification or alternatively upgrade its existing components. In addition, our study constitutes a relevant and modern complement to the current reviews. The paper is structured as follows.   presents a comprehensive description of the existing reviews. Next,   describes the text classification process and explains the review procedure. Then,   explains the problems, objective, and components of text classification via a qualitative analysis.   introduces a quantitative analysis of the text classification journals, including conference proceedings. Finally,   concludes the research study.', 's0925231214000265': ' 1 Introduction A tensor is a multidimensional array which is the higher-order generalization of vector and matrix. It has many applications in information science, computer vision and graph analysis  . In the real world, the size and the amount of redundancy of the data increase fast, and nearly all of the existing high-dimensional real world data either have the natural form of tensor (e.g. multi-channel images and videos) or can be grouped into the form of tensor (e.g. tensor face  ). Therefore, challenges come up in many areas when one confronts with the high-dimensional real world data. Tensor decomposition is a popular tool for high-dimensional data processing, analysis and visualization. Two particular tensor decomposition methods can be considered as higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP)   and the Tucker  . Tensor decomposition gives a concise representation of the underlying structure of tensor, revealing when the tensor data can be modeled as lying close to a low-dimensional subspace. Although useful, they are not as powerful. For general tensors, tensor decomposition does not deliver best low rank approximation, which will limit its applications. In this paper, we will try to recover a low-n-rank tensor from a subset of its entries. This problem is called the tensor completion problem. It is also called missing value estimation problem of tensors. The problem in computer vision and graphics is known as image and video in-painting problem  . The key factor to solve this problem is how to build up the relationship between the known elements and the unknown ones. Owing to this reason, the algorithms for completing tensors can be coarsely divided into local algorithms and global algorithms. Local algorithms   assume that the further apart two points are, the smaller their dependence is and the missing entries mainly depend on their neighbors. Thus, the local algorithms can only exploit the information of the adjacent entries. However, sometimes the values of the missing entries depend on the entries which are far away and the local algorithms cannot take advantage of a global property of tensors. Therefore, in order to utilize the information of tensors as much as possible, it is necessary to develop global algorithms that can directly capture the complete information of tensors to solve the tensor completion problem. In the two-dimensional case, i.e. the matrix, the rank is a powerful tool to capture the global information and can be directly determined. But for the high-dimensional case, i.e. the tensor, there is no polynomial algorithm to determine the rank of a specific given tensor. Recently, based on the extensions of trace norm for the minimization of tensor rank, some global algorithms   solving the tensor completion problem via convex optimization have been proposed. Liu et al.   first proposed the definition of the trace norm of an  -mode tensor as  . And similar to matrix completion, the tensor completion was formulated as a convex optimization problem. For tackling this problem, they developed a relaxation technique to separate the dependant relationships and used the block coordinate descent (BCD) method to achieve a globally optimal solution. The contribution of this paper is realized at the methodological level by considering a more general kind of the tensor completion problem. By the extension of the concept of Shatten-q norm for matrix, Signoretto et al.   defined tensor Shatten-{ ,  } norm, which is formulated as   and consistent with that for matrix. Compared to the trace norm defined in  , Shatten-{ ,  } norm is a more general tensor norm and the trace norm of tensor can be seen as a special case of Shatten-{ ,  } norm  . Though the general tensor Shatten-{ ,  } norm was defined in this paper, they mainly focused on the special case trace norm of tensor in their algorithm. Similar to the above two works, Gandy et al.   used the n-rank of a tensor as sparsity measurement and tried to find the tensor of lowest n-rank that satisfies some linear constraints. In their algorithm, the tensor completion was converted into a multi-linear convex optimization problem. Based on the Douglas–Rachford splitting technique   and the alternating direction method of multipliers  , trace norm was introduced as the convex envelope of the n-rank and an efficient algorithm to solve the multi-linear convex optimization problem was proposed. In these trace norm based algorithms, they consider the tensor completion problem as recovering a low-n-rank tensor from a subset of its entries, that is, where   are  -mode tensors with identical size in each mode. The elements of   in the set   are given, while the remaining elements are missing.   is the mode-  unfolding of  .   denotes the trace norm defined by the sum of all singular values of the matrix. On the other hand, Zhang et al.   exploited the recently proposed tensor-singular value decomposition (t-SVD)   that is a group theoretic framework to solve the tensor compression and recovery problem. They first constructed novel tensor-rank like measures to characterize informational and structural complexity of tensor. The core strategy of all these algorithms to achieve the optimal solution is the same as that they estimate the variables sequentially, followed by certain refinement in each iteration. Although the details of the solution procedure in each algorithm are different, unfortunately, the refinement in each iteration of all these algorithms requires computing singular value decompositions(SVD) that a task is increasingly costly as the tensor size and n-ranks increase. It is therefore desirable to exploit an alternative algorithm more efficient in solving tensor completion problem. In this paper, a new global algorithm for tensor completion called tensor completion via a multi-linear low-n-rank factorization model (TC-MLFM) is proposed. As the size and structure of each mode of the given tensor are not always the same (e.g. RGB images), the new algorithm combines n-ranks of each tensor mode by weighted parameters. However, the problem is that the function is generally NP-hard and hard to approximate due to the non-convex optimization of  . To solve this problem, we use n-rank factorization optimization problem to substitute  . The new function is solvable and considered as our model. With the new weighted objective model, the proposed algorithm can utilize the mode information of the tensor with choice. To solve this model, a minimization method based on the nonlinear Gauss–Seidal method   that only requires solving a linear least squares problem per iteration is applied. By adopting this method along each mode of the tensor other than minimizing the trace norm in Eq.  , the new algorithm can avoid the SVD computational strategy and reliably solve a wide range of tensor completion problems much faster than the trace norm minimization algorithm. The rest of the paper is organized as follows.   presents some notations and basic properties of tensors. In  , we review the definition of Tucker decomposition and tensor n-rank, which suggests that a low-n-rank tensor is a low rank matrix when appropriately unfolded.   discusses the detailed process of the proposed algorithm.   reports experimental results of our algorithm on simulated data and image completion. Finally,   provides some concluding remarks.', 's1071581918303471': \" 1 Introduction Determining when to interrupt a user at appropriate times as s/he performs computer-based tasks is an ongoing problem ( ). From an algorithmic perspective, it is difficult to determine the precise time to interrupt a user. This is because there are several subproblems that need to be solved to be confident that an interruption will be beneficial to the user. Some subproblems include: i) determining the intent (or goal) of the user as s/he is performing the task; ii) determining the task difficulty ( ); iii) determining the user's current cognitive load ( ); iv) estimating the cost of the interruption and the resumption lag time ( ); and v) incorporating personal user characteristics, such as sensitivity to being interrupted, distractibility level, etc. ( ). A solution to these problems is needed to make accurate decisions about the timing of interruptions. Since interruption is a key human-computer interaction problem, systems must be developed to manage interruptions in terms of reasoning about ideal timings of interruptions. In designing the classifier, the following desirable characteristics were identified: \\n                   \\n                      \\n                      \\n                      \\n                  \", 's0957417415000238': ' 1 Introduction One of the main objectives of research and learning processes is achieving maximal effectiveness from the creation, transfer and dissemination of new knowledge. This effectiveness can be measured by the quality and speed of memorization of the principal concepts of a particular domain and of the relationship between these concepts. Wide evidence exists that the visual thinking used to address the subject of study is positively connected with the quality and speed of memorization, and thus with the effectiveness of knowledge dissemination. Visualization is working as a cognitive tool that facilitates communication both in teacher/learner interaction and within research communities. Special interest in such graphical forms of knowledge codification can be observed in education science, especially within learning where the students are engaged in group knowledge sharing and co-creation processes with continuous feedback. Mutual understanding and mentalization in research is of special interest in collective study or discovery. One of the most productive methods of research and learning collaboration promises to be group ontology design. An ontology is a set of definitions we make in understanding and viewing the world ( ). The specific problem being addressed in this work deals with the problem of improving the quality of group or collective ontologies. We are also interested in filling the gaps in understanding the group ontology design process specifics, such as the causes of differentiation between the form and the content of individual ontologies. During the last decade, visual knowledge representation has become one of the key considerations in knowledge engineering methodology, and it is strongly associated with ontology design and development. These ontologies, which form a conceptual skeleton of the modeled domain, might serve various purposes such as better understanding, knowledge creation, knowledge sharing and reusing, collaborative learning, problem solving, seeking advice, or developing competences by learning from peers ( ). Recently, the ontological engineering perspective has gained interest in many research domains, such as medicine, business and computer science ( ). These studies rely heavily on theory and tools from knowledge engineering analysis that already has a long-standing tradition in the knowledge-based systems domain ( ). The largest number of knowledge engineering research articles has been generated around the theme of descriptive logics and formal foundations of ontology design ( ). Our work, however, emphasizes the informal approach based on human-centred ontology design processes, an aspect neglected by most of the existing approaches. Several attempts have been made to bridge this gap and ease the overall ontology development process, such as HCOME – Human-Centered Ontology Engineering MEthodology, by  ; and human-centred ontology design, by  . The tools and techniques developed in the domain of ontology engineering can be applied fruitfully in the field of knowledge structuring and design ( ) and semantic web applications ( ). The idea of using ontologies and visual structuring in research description and introduction has been discussed in many works ( ) and is now being implemented in several research projects and software tools ( ). This paper presents the main results of the KOMET (Knowledge and cOntent structuring via METhods of collaborative ontology design) project which was devoted to developing methods that use group visual ontology design in educational purposes, with regard to the respondents’ individual cognitive styles. The group ontology design was tested in the medical domain by a smaller group ( ) and computer science (informatics) domain by a larger group of participants ( ). In the larger group of 79 respondents, all the participants were graduate students of the School of Computer Science of Saint Petersburg Polytechnic University. Almost all had 1–2 years’ experience of research in computer science, and were in their fifth year of study, on the Masters programme. The domain “computer science” was chosen as all the students are young professionals in this area. We use the term synonymously with “informatics”. The paper is organized as follows. First, it describes the concept of ontology, with an emphasis on the visual approach to ontology design. Section   concentrates on the theoretical background, with sub-Section   describing ergonomic metrics and their purpose and sub-Section   providing an overview of cognitive styles and the tests used to assess them. Section   presents our human-centred research paradigm and framework, and Section   the results obtained in the study of the relationship between cognitive styles and the peculiarities of individual development of ontologies. Section   introduces the main results of collective ontology development, taking into account the cognitive styles of participants. Finally, some conclusions are drawn and future work is outlined.', 's1071581918304312': ' 1 Introduction In the past few decades Human-Computer Interaction research has moved beyond concerns of usability to study experience related topics such as beauty, enjoyment, fun, emotion, and engagement ( ). Indeed, engagement has been identified as one of the most desirable and essential experiential qualities of HCI activities ( ). Within studies of engagement, creative engagement has been identified as a sustained and intrinsically rewarding engagement experience ( ). This is where a user is engaged in an active, reflective and constructive cognitive process in pursuing a creative outcome with an interactive system ( ). In this way creative engagement emphasizes users’ creative experience over their creative output, and helps to make an interactive experience a ‘memorable one, rather than a ‘pretty one ( ). As a relatively new and elusive concept in HCI, the challenge for studying creative engagement include the lack of an agreed definition and positioning within the broader context of HCI. This is partly because most previous discussion took place within the context of interactive arts ( ) and education ( ), resulting in a lack of design suggestions for supporting creative engagement in other domains. There is also a lack of evaluation criteria as creative experiences should not be evaluated solely on the quality of the contributions or the output as the creative output is valued on a personal level ( ). Of particular interest to this paper is the challenge of how to design support for novices’ creative engagement and to inform future design of such systems. \\n                      \\n                      \\n                      \\n                   \\n                      \\n                      \\n                      \\n                   \\n                      \\n                      \\n                      \\n                      \\n                      \\n                  ', 's0888613x13002910': \" 1 Introduction Bayesian networks   provide a popular framework for modeling and decision-making under uncertainty. Since most of our decisions are based on information that is (at least partially) uncertain, Bayesian networks were applied in many diverse domains. Their fundamental advantage over different frameworks is their ability to divide the modeling problem into two basic stages: first, the structure of the modeled domain is described using a graph and secondly, the numerical values describing the quantitative relationship between model variables are provided. The model is either built by domain experts or automatically learned from collected data; and possibly created using a process that combines both ways. The key property of Bayesian networks that allows them to be applied in domains with up to hundreds of variables is the decomposability of the joint probability distribution they represent. The structure of a Bayesian network is defined by an acyclic directed graph  , where   is the set of directed edges, i.e.,  . The joint probability of a Bayesian network is defined for all configurations   of discrete variables   as  where   denotes the set of parents of node   in the graph  , i.e.,  . Using a common shorthand for a formula valid for all configurations of variables we can write formula   as \\n                   Formula   allows efficient computations of probabilistic queries  for all values   of all  ,  . The computational complexity of this task when the popular junction tree method   is used is exponential with respect to the size   of a largest clique   of the triangulated moralized graph   of  , see   for details. The value   is called the treewidth of  . If the treewidth is not large, then computationally efficient probabilistic inference is possible. This allows the application of Bayesian networks in domains with hundreds of variables, where a naive computation with the full joint probability table would not be tractable. Unfortunately, the treewidth is large in some applications, for example, some variables  ,   may have a very large parent set ( ). In such cases the exact inference with the standard junction tree method is not tractable. One solution is to resort to approximate inference methods, e.g., to Monte-Carlo methods  , the Pearl polytree algorithm   applied to Bayesian networks with loops, variational methods  , inference using probability trees  , or binary probability trees  . Based on our experience with different applications of Bayesian networks we believe that the conditional probability tables (CPTs), which are the basic building blocks of Bayesian networks, may often have a simple structure, for example, they correspond to a noisy functional dependence. This property should be exploited not only when building a Bayesian network model but also during the probabilistic inference. Various methods that can exploit the local structure of CPTs were proposed. An early example is the Quickscore algorithm   exploiting noisy-or relations in the Quick Medical Reference model. Olesen et al.   proposed the so-called parent-divorcing method. Heckerman and Breese   use a temporal model transformation. Zhang and Poole   introduced deputy variables that are used to create heterogeneous factorizations in which the factors are combined either by multiplication or by a combination operator. Takikawa and D'Ambrosio   used auxiliary variables, which allowed them to transform an additive factorization into a multiplicative factorization. The additions are achieved by the marginalization of the intermediate variables. Díez and Galán   pointed out that the transformation of noisy-max can be done using a single variable. CP tensor decomposition, called tensor rank-one decomposition in  , is a generalization of Díez and Galán's decomposition. It is based on the CP decomposition of tensors  . Savicky and Vomlel   described CP tensor decompositions of several canonical models – noisy-max, noisy-min, noisy-add, noisy-xor. Previous research reveals that for certain models the computational savings achieved by this transformation are very large – instead of a representation that is exponential with respect to the number of variables in a CPT we get a representation that is only quadratic  . The application of the CP tensor decomposition allows application of Bayesian networks in domains where exact probabilistic inference would not otherwise be possible. We believe this may have a substantial impact on the quality of decision-making in complex domains such as: medical decision support systems and health monitoring  , troubleshooting complex devices  , etc. This paper is organized as follows. Conditional probability tables with a local structure are discussed in Section  . In Section   we introduce the necessary tensor notation, define tensors of the exactly  -out-of-  and threshold functions, and present their basic properties. Sections   and   represent the main original contribution of this paper. We propose methods for the decomposition of tensors of the threshold and exactly  -out-of-  functions in the real and complex domains and prove results about the symmetric rank of these tensors. In Section   we analytically compare the CP decomposition and the parent-divorcing method using an exemplary class of models. In Section   we present experimental comparisons of the CP decomposition method with the standard junction tree method and the parent-divorcing method. The experiments are performed on a generalized version of the QMR-DT network. In Section   we briefly review other methods exploiting local structure of CPTs. We outline how the CP decomposition can be combined with weighted model counting. Major proofs are moved to  .\", 's0888613x14000796': ' 1 Introduction Probability theory is the most well-known approach to model uncertainty. However, even when the existence of a single probability measure is assumed, it often happens that its distribution is only partially known. This is particularly the case in the presence of severe uncertainty (few samples, imprecise or unreliable data, etc.) or when subjective beliefs are elicited (e.g., from experts). Some authors use a selection principle that brings us back to a precise distribution (e.g., maximum entropy  ), but other ones   have argued that in some situations involving imprecision or incompleteness, uncertainty cannot be modelled faithfully by a single probability measure. The same authors have advocated the need for frameworks accommodating imprecision, their efforts resulting in different frameworks such as possibility theory  , belief functions  , imprecise probabilities  , info-gap theory  , etc. that are formally connected  . Regardless of interpretive issues, the formal setting of belief functions offers a good compromise between expressiveness and calculability, as it is more general than probability theory, yet in many cases remains more tractable than imprecise probability approaches. Nevertheless using belief functions is often more computationally demanding than using probabilities. Indeed, its higher level of generality prevents the use of some properties, valid in probability theory, that help simplify calculations. This is the case, for instance, for the well-known and useful inclusion–exclusion principle (also known as Sylvester–Poincaré equality). Given a space  , a probability measure   over this space and any collection   of measurable subsets of  , the inclusion–exclusion principle states that  where   is the cardinality of  . This equality allows us to easily compute the probability of  , when the events   are stochastically independent, or when their intersections are disjoint. This principle has been applied to numerous problems, including the evaluation of the reliability of complex systems. It does not hold for belief functions, and only an inequality remains. However, it is useful to investigate whether or not an equality can be restored for specific families   of events, in particular the ones encountered in applications to diagnosis and reliability. The main contribution of this paper is to give a positive answer to this question and to provide conditions characterising the families of events for which the inclusion–exclusion principle still holds in the belief function setting. This paper is organised as follows. First, Section   provides sufficient and necessary conditions under which the inclusion–exclusion principle holds for belief functions in general spaces; it is explained why the question may be more difficult for the conjugate plausibility functions. Section   then studies how the results apply to the practically interesting case where events   and focal elements are Cartesian products in a multidimensional space. Section   investigates the particular case of binary spaces, and considers the calculation of the degree of belief and plausibility of a Boolean formula expressed in Disjunctive Normal Form (DNF). Section   then shows that specific events described by means of monotone functions over a Cartesian product of totally ordered discrete spaces meet the conditions for the inclusion–exclusion principle to hold. Section   is devoted to illustrative applications of the preceding results to the field of reliability analysis (both for the binary and non-binary cases), in which the use of belief functions is natural and the need for efficient computation schemes is an important issue. Finally, Section   compares our results with those obtained when assuming stochastic independence between ill-known probabilities, displaying those cases for which these results coincide and those for which they disagree. This work extends the results concerning the computation of uncertainty bounds within the belief function framework previously presented in  . In particular, we provide full proofs as well as additional examples. We also discuss the application of the inclusion/exclusion principle to plausibilities, as well as a comparison of our approach with other types of independence notions proposed for imprecise probabilities (two issues not tackled in  ).', 's0888613x15000912': ' 1 Introduction Rule-based models are composed of collections of rules, which serve as local models describing complex relationships existing in system modeling. Takagi–Sugeno rules   assume the well-known form  where   is a fuzzy set of condition of the  -th rule and formed in the input space and   is a certain local model standing in the conclusion part are commonly encountered in fuzzy modeling. There is a significant number of design procedures of rule-based models  . In general, the design of the fuzzy model is carried out in two phases: (i) formation of condition parts of the rules – fuzzy sets built by fuzzy clustering, and (ii) determination of the conclusion parts of the rules; here we are concerned with a parametric optimization and resort the analytical solutions of the ensuing estimation problem. Quite commonly we encounter the local models assuming a simple form being e.g., linear functions. The design process is well documented in the literature  . Each of these two design phases comes with various augmentations. In  \\n                      , we offer several highlights of the visible representatives of the fuzzy models, specify their development strategies and look at the optimization tools supporting the construction of the fuzzy model. Irrespectively of the diversity of approaches, fuzzy rule-based models share a visible commonality: complex phenomena are modeled   through a series of local models (which are less complex than a single global model). The differences lie in a way in which the rules are formed, how local (typically linear) models are constructed and how aggregation of the rules is completed. While rule-based models are about local modeling of the overall system, incremental fuzzy models follow a radically different line of thought. We consider a global model coming in a form commonly encountered in the literature (say, a linear regression model) while the rule-based model is constructed to enhance – compensate for some discrepancies of the global model. The individual rules of the incremental model are developed in a way so that the deficiencies of the global model are eliminated or substantially reduced. In this way, the incremental model improves the quality of the global model. Some initial studies in this area were reported in  . This incremental rule-based approach to system modeling exhibits some tangible advantages: (i) we rely on the well-known and commonly acceptable model, such as linear regression and offer its improvement, (ii) the methodology of fuzzy rule-based modeling fully applies here so we take advantage of the existing design strategies of fuzzy models, (iii) the two-phase design process is well-motivated. The objective of the study is to develop a detailed concept of incremental fuzzy models, propose algorithms, and demonstrate the performance of the model with the use of synthetic and publicly available data sets. The study is structured as follows. In Section  , we present a structure of the incremental fuzzy model showing how the rules are formed to account for the existing discrepancies of the global model. In Section  , we discussed augmented fuzzy clustering (extended version of the Fuzzy C-Means), which allows to articulate a structure of data given a directional character of the data used in system modeling. Experimental studies are reported in Section   in which both synthetic and publicly available data sets are considered. In the paper, we adhere to the standard notation and symbols used commonly in rule-based models and fuzzy sets. In particular, capital letters are used to denote fuzzy sets, while vectors are shown in boldface.', 's1071581917300320': \" 1 Introduction In the New York Journal of 2nd June 1897, Mark Twain wrote “The report of my death was an exaggeration”. The same might be said about printed newspapers themselves, despite falling circulation, as some readers switch to on-line news content consumed on computers, tablets and phones (c.f.  ). This situation is one of the latest manifestations of an old debate about the future of paper and which has variously raged and rumbled on since the earliest presentation of textual information on computer screens. Comparisons of paper versus screen reading took place in the context of a debate about the paperless office, and the prospects of dedicated reading devices that might eventually replace the printed book ( ). Predictions of new forms of reading in hypertext and hypermedia followed, and eventually came true in the realization of the world-wide-web ( ). New technologies for the presentation and distribution of electronic books, magazines and newspapers were eventually commercialized by companies like Amazon and Apple. And the rise of user-generated content on the web, shifted the balance of power from publishers to individual authors, allowing a wider range of people to create and share information in either screen-based or paper form. Despite these cataclysmic changes in the authorship and form of media content, paper has proved stubbornly resilient in the face of potential obsolescence. The paperless office never materialized because people simply moved backwards and forwards between paper and screen-based materials to make use of the different affordances of each ( ). Reading the web simply became a new form of literacy alongside conventional reading ( ). Hence printed materials of all kinds continue to be used with new generation laptops, e-books and tablets for different reading and writing activities, specialized by content. In fact, they are often used simultaneously or serially in paper-and-screen environments such as architect's offices, transport control spaces and news rooms, where information is compared from different sources, and passed between people ( ). The co-existence and use of both paper and digital content has led several authors to call for the creation of closer links between the two, through technologies of   ( ). Interactive paper is the opposite of electronic paper because instead of making existing devices simulate the properties of paper, it   ( ). A number of different technologies exist for linking paper and digital information together, including 1D and 2D barcodes, RFID and NFC tags, optical page recognition and finger tracking, digitizing tablets, ultrasonic, optical and inductive pens, and capacitive touch regions ( ). Perhaps because of this variety of core technologies, the applications of interactive paper are many and varied (see related work below) and principles for the successful design of interactive paper are lacking. In fact   conclude their review of the area by listing this as one of the major challenges for the field:  . Taking this observation as our starting point, we focus here on a single technology and application domain for interactive paper that we think has great promise, and test out multiple designs in order to extract design principles and recommendations for that domain. The technology comprises capacitive touch points that can sense the position of fingers and thumbs on a printed page through changes in electrical conductivity at the point of contact. We use this interaction technique in the   domain to invoke associated speech or other sounds from printed newspapers and to register subjective responses in the form of ‘votes’. In the rest of the paper we briefly explain our reasons for focussing on newspapers, and summarise related work on augmented paper before introducing a new framework for understanding interactive paper experiences at multiple levels. We then describe the interactive newsprint platform and documents, and the way they were tested in a study to elicit reader responses, before summarizing those responses and their design implications.\", 's0888613x13002880': ' 1 Introduction Probabilistic graphical models (PGMs) have been shown to be convenient and intuitive formalisms to capture the probabilistic independence information in many application fields. In a PGM, random variables are modelled as vertices connected by edges in a graph. These connections reflect the probabilistic dependences and independences between variables and one can associate a probability distribution to the graph that is faithful in some way to the dependences and independences. Popular PGMs include models based on undirected graphs (UGs), i.e.,  , and based on directed acyclic graphs (DAGs), i.e.,   \\n                      . However, both undirected and directed graphs have certain undesirable limitations when representing independence information for an actual problem domain. Hybrid graphs, containing both directed and undirected edges, such as  , offer an elegant generalisation of both Markov and Bayesian networks  . A chain graph (CG) uses potentials rather than straight probabilities to represent the probability distribution of variables and is, therefore, often seen as a blackbox model. Nevertheless, chain graphs have been shown to model equilibrium systems  , which occur in many areas including biology, physics, chemistry, and economics. In fact, it was shown that particular sets of conditional independence statements, which cannot be modelled by a Bayesian network, can indeed be modelled with a chain graph; the ideal gas law and the price and demand model in economics are examples  . On the other hand, Bayesian networks have the advantage that both structure and parameters can be assessed from either expert knowledge, data, or both, which renders Bayesian networks whitebox rather than blackbox models. For the more expressive chain graphs, it is much more difficult to exploit human knowledge in assessing their parameters, and, as a consequence, these models do not share all the advantages of Bayesian networks as whitebox models. One of the aims of the research described in this paper is to come up with ways to move chain graphs closer to whitebox models, in particular by the use of qualitative probabilistic abstractions. Probabilistic information is available in different forms, ranging from numerical, quantitative probabilistic values (possibly with a confidence interval) to qualitative information. Qualitative abstractions of Bayesian networks, called   (QPNs)  , offer a useful method for exploiting qualitative constraints in assessing probabilistic information. Qualitative information in QPNs may consist of qualitative influences and synergies, and independence information. While it is well known that QPN theory has its limitation when it comes to qualitative reasoning – the main reason why QPN theory is not used in actual systems – qualitative knowledge may be quite useful when looked at as offering constraints that should be taken into account when estimating a probability distribution. Some algorithms have been proposed in the past to derive bounds   and qualitative influences   in the presence of both quantitative and qualitative knowledge, with applications in e.g. computer vision  . Furthermore, it has been proposed to derive marginal probability distributions in the presence of such hybrid knowledge  . If exact probabilistic information is not required, then such distributions, also called second-order distributions, \\n                       provide insight into the domain and could, e.g., be used to make decisions. In the next section, we will first argue why chain graphs provide a good starting point for modelling feedback mechanisms. Here we explore three realistic examples drawn from the medical field. The needed theoretical basis underlying the work presented in this paper is provided in Section  . In Section  , we extend the known QPN theory towards chain graphs, which we call   (QCGs). In particular, we will formally discuss qualitative relationships, compare these to the relationships in QPNs, and prove their most important properties. In Section  , we show that sign propagation, a qualitative variant of belief propagation, can be amended to qualitative chain graphs. In Section  , we also demonstrate their usefulness in semi-qualitative reasoning and present experimental results supporting this claim. Although examples were drawn from the field of medicine, which offers a rich source of qualitative modelling, the results will be of value to many other domains. The work is rounded off by conclusions and plans for future research in Section  .', 's0888613x14001212': ' 1 Introduction The theory of   \\n                       extends the Bayesian theory of subjective probability to cope with sets of distributions. This potentially provides more robust and realistic models of uncertainty. These ideas have been applied to classification and a number of classifiers based on imprecise probabilities are already available. Most of these approaches are based on graphical models, whose parameters are imprecisely quantified with a set of priors by means of the   \\n                      . The first attempt in this direction is the   \\n                      , which generalizes the naive Bayes classifier to imprecisely specified probabilities. Each prior in the imprecise Dirichlet model defines a precise classifier. When two precise classifiers of this kind assign a different class label to the same instance, the imprecise classifier returns both labels, and the instance is said to be  . Conversely, when a single label is returned, this is independent of the prior. Classifiers of this kind, possibly returning multiple class labels in the output, are called  . \\n                       The separation between prior-dependent and other instances induced by a credal classifier typically corresponds to a distinction between hard and easy-to-classify instances, with the accuracy of a precise classifier significantly lower on the prior-dependent instances rather than on the prior-independent ones. In this sense, credal classifiers are suitable as preprocessing tools, assigning the right class label to prior-independent instances and partially suspending the judgement otherwise. Despite the relatively large number of credal classifiers proposed in the literature, no credal models specifically intended to classify temporal data have been developed so far. \\n                       This is cumbersome since, on the other side, dynamical models such as Markov chains and   (HMMs) have been already extended to imprecise probabilities to model non-stationary dynamic processes  . As a matter of fact, HMMs in their precise formulation have been often applied to classification of time series (e.g.,  ), while no similar attempts have been made in the imprecise case. This can be partially explained by the lack of algorithms to learn imprecise-probabilistic models from incomplete data (e.g., because referred to hidden variables) and, more marginally, by the lack of suitable inference algorithms. It therefore seems natural to merge these two lines of research and develop credal classifiers for time series based on imprecise HMMs. To achieve that, we first show how to learn an imprecise HMM from a discrete-time sequence. The technique, already tested in previous works   combines the imprecise Dirichlet model with the popular EM algorithm, generally used to learn precise HMMs. After this step, each sequence is associated with an imprecise HMM. In the limit of infinitely long models, HMMs might converge to a condition of  , even in the imprecise case  . A major claim of this paper is that, in this limit, the model becomes considerably simpler without losing valuable information for classification purposes. In the stationarity limit, the imprecise HMM becomes an   (i.e., with multiple specification of the weights) of Gaussian densities over the observable variable (i.e., the joint observation of the features). Two novel algorithms are proposed to perform classification with these models. The first, called IHMM-E, evaluates the mixture expected value, which becomes a static attribute for a standard classification setup. The second, called IHMM-B, uses the Bhattacharyya distance between two mixtures as a descriptor of the dissimilarity level between sequences. Being associated with imprecise-probabilistic models, those descriptors cannot be precisely evaluated and only their lower and upper bounds with respect to the constraints on the parameters can be evaluated. This is done efficiently by solving a linear (for IHMM-E) and a quadratic (for IHMM-B) optimization task. After this step, IHMM-E summarizes the sequence as an interval-valued observation in the feature space. To classify this kind of information, a generalization of the   algorithm to support multivariate interval data is developed. The same approach can be used to process the interval-valued (univariate) distances between sequences returned by IHMM-B. Both algorithms are credal classifiers for time series, possibly assigning more than a single class label to a sequence. Performances are tested on some of the most important computer vision benchmarks. The methods we propose achieve the required robustness in the evaluation of the class labels to be assigned to a sequence and outperform alternative imprecise methods with respect to state-of-the-art metrics   to compare performances of credal and traditional classifiers. The performance is also good when compared with  , the state-of-the-art approach to the classification of time series. The reason is the high dimensionality of the computer vision data: dynamic time warping is less effective when coping with multivariate data  , while the methods in this paper are almost unaffected by the dimensionality of the features. The paper is organized as follows. In Section  , we introduce the basic ideas in the special case of precise HMMs obtained from univariate data. Then, in Section  , we define imprecise HMMs and discuss the learning of these models from multivariate data. The new algorithms IHMM-E and IHMM-B are detailed in Sections   and  . A summary of the two methods together with a discussion about their computational complexity and the performance evaluation are in Section  . Experiments and conclusive remarks are in Sections   and  .', 's2589721719300029': \" 1 Introduction The pig industry has long been the backbone of China's livestock and poultry farming. In recent years, China's pork production ranks first in the world, accounting for about 50% of the world's total production. About 700 million pigs are slaughtered each year. Pork production accounts for about 64% of all meat production ( ;  ). According to statistics, the annual output of pigs, cattle, sheep and poultry in 2017 was 84.31\\u202fmillion\\u202ftons, of which pork production was 53.4\\u202fmillion\\u202ftons. Therefore, pork industry is the absolute pillar of China's meat industry ( ). Although the proportion of pigs in the livestock and poultry industry is in an absolute position, the China's pig industry is still lagging behind. The pig industry is dominated by decentralized farmers, and the industrialization and modernization of the aquaculture industry is relatively low. In 2016, the contribution rate of pigs with a size of <500 to the market pork accounted for about 55%. More than half of them have problems caused by poor farming environment, poor farming technology, suffering from various diseases, feed self-matching and management disorder ( ). The informal pig breeding model not only produces a great waste of resources, but also causes frequent occurrence of various pig diseases and increases the risk of pig breeding process. The industrial upgrading of the pig breeding industry from the decentralized farming model to the intensive modernization is very urgent. In recent years, the concept of welfare farming has been increasingly valued by the industry and the health of pigs has received increasing attention. However, due to various problems in resource management of pig breeding enterprises, such as poor farming pig houses, poor breeding environment and non-standard farming techniques, they are common in free-range farmers or small-scale pig farms, which have led to insufficient disease prevention and disease monitoring in pigs, the diagnosis and treatment of diseases are not timely ( ). At this stage, effective prevention and diagnosis of pig disease is still a major issue related to the economic benefits of pig breeding enterprises. Timely access to body temperature data can help alert and diagnose pig disease, real-time and fast access to pig body temperature is of great significance to the stable and healthy development of pig breeding enterprises, which also could promote the development of welfare farming concept in the pig breeding industry. With the progress of China's urbanization construction and the need for environmental protection, the pig breeding model began to accelerate the evolution from individual decentralized farming to large-scale farming, and pig breeding industry is beginning to enter a new trend ( ). The development of the agricultural Internet of Things (IoT) has made it possible to perceive the body temperature information of pigs in real time. With the development of industrialization and intensive farming mode, the scale of breeding has been expanded continuously, and the problems of environmental pollution in pig houses have begun to become prominent. Severe environmental pollution has a series of effects on the growth, development and reproduction of pigs, and the deteriorating environment of pig breeding has made the health of pigs increasingly serious. In the high-density breeding environment with frequent diseases and frequent epidemics, monitoring the body temperature of pigs in real time is conducive to grasping the health status of pigs at any time. It is important to the prevention and diagnosis of pig diseases, which becomes a key task for intensive farming.\", 's0888613x13002867': ' 1 Introduction Inconsistency is a pervasive and important problem in many applications when information is gathered from multiple sources. It has been increasingly recognized that measuring inconsistency plays an important role in analyzing inconsistent information in a variety of applications such as belief change  , knowledge bases merging  , ontology management  , requirements engineering  , expert systems in medicine   and intrusion detections in security  . A growing number of inconsistency measures for knowledge bases have been proposed recently  . Most of these inconsistency measures focus on classical knowledge bases only. Such measures assume that any two pieces of knowledge in the same knowledge base are equally preferred. However, some pieces of knowledge are often more preferred than others in some applications, such as belief revision  , knowledge bases merging  , and inconsistency management in requirements engineering  . More importantly, the preference relation between pieces of knowledge is always considered as useful to resolving inconsistency in these applications. Therefore, proposals for measuring inconsistency in such applications should take into account preferences on knowledge. To illustrate this, consider a scenario about selecting the best paper from submissions for a conference. Suppose that A and B are two conference submissions. Alice and Bob are the two reviewers for these two submissions. Alice recommended A for the best paper with  , whilst Bob gave the contrary reviewing result with  . Bob recommended B for the best paper with  , but Alice opposed Bobʼs recommendation with  . Intuitively, the contradiction between Alice and Bob about A is different from that about B. Evidently, if we ignore recommendations with  , B receives two valid reviews that are contradictory, whilst A has just one review that is positive. Further, if we consider all reviews, A also has two contradictory reviews. But the contradiction between Alice and Bob about A is less sharp than that about B, because Bob opposes Alice w.r.t. A with   instead of  . It should be pointed out that while there is a lot of work on utilizing preference information for handling inconsistent knowledge bases (e.g.  ), there are relatively few approaches for measuring inconsistency for knowledge bases using preference. On the other hand, given a preference relation over a knowledge base, the base may be stratified into several strata according to the preference relation between pieces of knowledge, if we assume that any two pieces of knowledge in the same stratum are equally preferred. Moreover, each stratum of a given knowledge base, except for the least preferred stratum, naturally splits the whole knowledge base into two parts: one is the set of formulas more or equally preferred than the stratum, and the other is the set of formulas strictly less preferred than the stratum. In many applications, we need to consider only the first part of the knowledge base instead of the whole base. For example, to manage time effectively and efficiently, an agenda robot needs to classify a set of tasks into four strata, including the  , the  , the  , and the  . Some of these tasks may contradict to each other. Suppose an agenda robot has detected that the userʼs available time is very limited. Then it is advisable for the user to focus on only   tasks, i.e., the first one or the first two strata. Roughly speaking, compared to flat knowledge base, the stratum-based structure of a knowledge base makes it possible for the robot to focus on relevant parts in the stratified knowledge base. As a result, we need to measure the inconsistency of a stratified knowledge base stratum by stratum. To address these issues, we propose two approaches to measuring inconsistency for stratified knowledge bases in this paper. The first approach, termed the multi-section inconsistency measure (MSIM for short), provides a framework for capturing the inconsistency occurring at each stratum of a stratified knowledge base. Informally, the multi-section inconsistency measure for a stratified knowledge base is a vector such that the first   components of the vector together exactly capture the inconsistency in the first   strata of the base. In particular, we present two instances in this framework, i.e., the naive MSIM and the stratum-centric MSIM. The former takes advantage of some intuitive inconsistency measures for flat knowledge bases such as the   inconsistency measure presented in  , to characterize inconsistencies at each stratum of a stratified knowledge base. The latter aims to capture the most preferred stratum involved in inconsistencies caused by each stratum of a stratified knowledge base, in which inconsistencies are characterized in terms of minimal inconsistent subsets of each  -cut (the union of the first   strata) of the base. The second approach to measuring inconsistency, termed preference-based approach, focuses on assessing inconsistencies in a whole stratified knowledge base from an integrated perspective. It allows us to define measures by considering the number of formulas involved in inconsistencies as well as the preference levels of these formulas under   models, one of representative paraconsistent models  . By examining their logical properties, we show that these inconsistency measures indeed take into account the preorder relation on a stratified knowledge base in assessing the inconsistency in the base. Then we provide a systemic analysis of the computational complexity of these measures. Finally, we use a small but explanatory example to illustrate how to use our measures to monitor the process of inconsistency handling in requirements engineering. The rest of this paper is organized as follows. Section   provides some necessary notations about inconsistency as well as stratified knowledge bases. In Section   we propose the multi-section inconsistency measure and its two instances for stratified knowledge bases. In Section   we propose preference-based inconsistency measures for stratified knowledge bases. In Section   we address logical properties of these inconsistency measures. In Section   we study computational complexity issues about these measures. In Section  , we present an example to illustrate the application of our approaches in the domain of requirements engineering. In Section   we will compare our approaches with some closely related work. Finally, we conclude this paper in Section  .', 's0921889015000834': ' 1 Introduction At present, Parallel Kinematic Machines (PKMs) are primarily used in large setups to perform machining tasks such as milling\\xa0   or manipulation of heavy components\\xa0  ; additionally, different configurations of PKM are utilised for manipulations in assembly lines\\xa0   or accurate positioning systems\\xa0   for astronomy installations or MEMS. However, recent work has shown that PKMs are also suitable for use in a mobile context, i.e.\\xa0being moved into a location of intervention to perform various inspection/processing tasks. Yang et\\xa0al.\\xa0   have designed a quadruped robot PK machine tool, equipped with one redundant limb that is used only for walking, which can move between fixed pins fitted to the surface to be traversed; these pins react the lateral forces and ensure that the legs fall within a series of known positions, which eliminates the need for referencing the PK structure relative to the base platform. However, due to such configuration, this PKM structure walking machine cannot walk within an unprepared environment. Guy\\xa0   developed a robot PKM for drilling and riveting, which positions itself on the exterior of a section of aircraft fuselage using a fixed base and actuators attached to the parallel platform that can lift the base while it relocates utilising a PK mechanism. This design uses suction cups to attach the base unit to smooth, relatively flat surfaces; however, it could be noted that this solution cannot cope with complex environments, since the footprint of the system is large and therefore cannot easily avoid obstacles. It also cannot cope with terrains that are non-smooth and is not flexible enough for more general applications than working only on fuselage sections. Both Guy\\xa0   and Yang et\\xa0al.’s\\xa0   designs have the desired mobility, but are only mobile within limited environments, rendering them ineffective for more general tasks/interventions (e.g.\\xa0in-situ repair) that might require motion within uneven terrain and/or complex paths of the end-effectors. They also might exhibit a limited accuracy for performing processes such as multi-axis milling or rely on pins built into the environment; hence, neither is suitable to perform accurate automated operations in hazardous or constrained environments. Furthermore, attempts have been made, such as that by Denton\\xa0  , to implement a tooling solution for performing machining with a standard hexapod robot, which uses an off-the-shelf axially symmetric configuration; this places a limit on the accuracy achievable by such a system and leads to a relatively low useful working volume. As a result, the robot would be required to walk while machining if performing operations on a large area, further reducing accuracy and repeatability. An existing Free-leg Hexapod (FreeHex)\\xa0   with a PK structure that can be attached directly to the workpiece (without the need for a fixed base) for in-situ processing (e.g.\\xa0machining) has been reported; this machine must be placed in location by a human operator and calibrated using an innovative methodology involving a set of gauges that must be removed prior to machining\\xa0  . Despite this, the FreeHex was reported to be capable of achieving high accuracy, i.e.\\xa0repeatable results when machining and proved to meet real industrial application needs, thanks to its design. However, in its current design, it lacks the advantage of being able to reach tight spaces or hazardous locations independently. Consequently, new strategies are required in order to allow this system to walk independently, without compromising the machining capability conferred by a PK configuration. Prior to a redesign of the FreeHex to enable its walking capability, it is important to evaluate the stability of such structures during their motions and analyse suitable gaits in order to select suitable actuators. Therefore, it is important to consider existing walking robots, even though they may not be capable of generating 6-axis tool paths. Walking robots fall into two overall categories: statically stable\\xa0   and dynamically stable (e.g.\\xa0  ). Dynamically stable robots are most analogous to bipedal animals and humans, where balance must be actively maintained. Dynamically stable walking is performed by internally generating an imbalance such that the centre of mass is (usually) in front of the supporting limbs causing the subject to have the tendency to topple forwards. Thus, one leg is placed in front of the other in order to prevent the system from collapsing and so to enable its advancement. According to McKerrow\\xa0  , dynamic stability is achieved by continuously moving either the feet or body to maintain balance. Wettergreen and Thorpe\\xa0   describe an active feedback approach to control system implementation in order to maintain the balance with respect to a required speed. By contrast, statically stable walking robots rely on maintaining a balanced pose at all stages during motion. This is analogous to the motion of many creatures with four legs and all creatures with six or more; as such, many examples of this type of robot are inspired by animals or insects. For a robot to be able to move autonomously, reasoned decisions need to be made as to where and in what order the feet are placed; this process is referred to as gait generation analysis. The gait of a robot is the set of motions that the legs should go through in order to allow the robot to advance in a specified direction. According to Wettergreen and Thorpe\\xa0  , previous work on gait can be classified into four categories: Behavioural, Control (previously explained), Constraint-Based and Rule-Based. Behavioural gait generation is an attempt to mimic the method of determining limb movement used by animals and insects by creating an environment for unconscious reasoning, such as a neural network. Beer et\\xa0al.\\xa0   built a walking hexapod to investigate a control network based on the neuroethology of insect locomotion, producing a range of gaits and degrees of robustness in a real robot that match quite closely with simulations. Berns et\\xa0al.\\xa0   describe an hierarchical control architecture for a walking hexapod named LAURON utilising neural network techniques; this focuses on active learning within the control system, which proved to be time intensive and not of as much practical use if the environment to be traversed is well defined. Rule-Based gait generation involves assigning a prescribed gait based on the classification of the robot’s environment, unlike the behavioural and control approaches that involved no active planning. As the robot switches between different types of terrain, the system adopts the gait that is most suitable for the current environment. Song\\xa0   reports on an efficient wave gait that varied foot placements between terrains while retaining gait sequencing; furthermore, this was developed to allow for autonomous crossing of four major different types of obstacle: grade, ditch, step and isolated-wall\\xa0  . Kumar\\xa0   uses a system of control schemes to modify gait parameters including duty factors for the wave gait in order to demonstrate that robot velocity can be varied continuously even with irregular, asymmetric and changing support patterns; however, this approach showed that some problems in switching between gaits could appear. This highlights the main drawbacks of using Rule-based gait generation: (i) difficulty in generating an exhaustive list of environment scenarios; (ii) difficulty in autonomous recognition of which type of environment scenario is most appropriate for the current terrain; (iii) while the robot is transitioning between two environments, it is not fully in either environment, so the system must have a method of coping with fuzzy logic. Cruse\\xa0   achieves some success in addressing these issues by means of the ‘Cruse Coordination Rules’, which allow the robot to adapt automatically to its environment, producing stable and reliable gait patterns. Roggendorf\\xa0   compares this with Steinkühler and Cruse’s MMC model\\xa0   and a modified form of Porta and Celaya’s approach\\xa0  , but finds that the latter produces the best performance in simulation. Belter\\xa0   utilises an evolutionary algorithm to generate a tripod gait for the hexapod ‘Ragno’, reporting that there is a strong dependence on the accurate knowledge of the physical parameters in the quality of the produced gait. Buchli\\xa0   presents an excellent control methodology for the ‘LittleDog’ quadruped incorporating a novel line-based COG trajectory planner which is proven to be effective in real world trials. For complex and constrained environments, a modified standard gait is not as suitable for avoiding all obstacles. It is in these situations where the fourth category of gait generation is most useful: Constraint-based gait generation—a mid-term planning active searching gait generator. It operates in the following stages: (i) a complete list of possible moves that the robot legs and platform could make is generated; (ii) this list is reduced by eliminating all motions that are infeasible due to spatial uniqueness (e.g.\\xa0clashes between legs and legs/other parts of the robot/the environment); (iii) the list is further reduced by eliminating all unstable movements and possibly by using other criteria (such as singularity points); (iv) the list of movements is ordered by a parameter to be optimised such as stability, energy usage or speed. These steps can be carried out for several stages in advance of the robot’s current position. However, the main problems with this approach are: the list of options (as explained above) grows exponentially with the number of degrees of freedom (DoF) of the robot, as discussed by Latombe\\xa0  ; due to the processing time, it is impractical to calculate a large number of steps in advance, so the robot may be making apparent good progress, but may be entering a route that does not allow it to reach the final objective safely without turning back and retracing steps (this is known as the Horizon Effect\\xa0  ). P.K. Pal and K. Jayarajan\\xa0   demonstrate that constraint-based gait generation is still useable by creating a reduced list of movements, considering four leg placements for each body translation; this simplification reduces the optimisation of the whole system, but considerably speeds up the process. The leg workspace (referring to the area available to place a given leg at any instance) and terrain reference frames were later discretised to constrain possible gaits and the search was limited to movement cycles that advanced towards the goal in an attempt to overcome the Horizon Effect. This attempt met with success in a simplified test environment\\xa0  . Subsequently, Pal and Jayarajan improved the techniques by application to a walking hexapod robot\\xa0   and demonstrated how the functions may be designed to generate common periodic gaits such that the system may be optimised for any general factor\\xa0  . These gait generation techniques have so far been mostly applied to walking hexapods that have either two rows of three legs in parallel (monosymmetric, e.g.\\xa0  ) or equispaced axially symmetric legs (e.g.\\xa0  ). Driven by the industrial need for a truly 6-axis walking machine tool, the present research builds on the experience of the FreeHex\\xa0  . The goal is to produce a machine that can operate as a highly effective and accurate machine tool but also operate as a highly manoeuvrable walking robot able to navigate multiple environments. As seen in the literature, a walking robot fitted with a spindle is not capable of fully addressing these challenges. Therefore, a PKM with a novel leg layout and architecture is used. The scope of this paper is a theoretical study of the factors relevant to allowing a hexapod PKM with tri-radial symmetry to walk, and specifically to walk on inclined planes of varying angles of elevation. This paper presents a gait methodology based on stability margin criteria and a torque analysis, noting that the WalkingHex, having telescopic legs, is capable of varying the pose and translation of the upper platform, which can be taken into consideration to improve the stability margin and actuation torque of the system. The following aspects are among those that must be considered: the overall design of the hexapod and leg joints, and the measures of gait effectiveness such as stance stability and torque in the leg’s spherical joints. This determination is of key important because the algorithm can be used to avoid high levels of torque that the leg actuation mechanism is incapable of producing when walking in different environments.', 's1875952119300394': ' 1 Introduction There is a growing interest in the field of human-robot interaction (HRI) for the investigation of robots and their emotional abilities through interaction with peers or colleagues on shared tasks  . Such   between humans and robots includes social cues which are perceived to elicit physiological affect (e.g., gaze, expressions, gestures, speed, distance). During such  , humans tend to attribute emotional states to robots  . The aim of these previous investigations in social and collaborative HRI is to investigate how humans and robots interact together in a shared physical space aimed at accomplishing a goal  , as numerous studies indicate the importance of such social interactions  . Takayama et. al.   have found a significant effect of pet possession on the proxemic behaviors of humans interacting with robot partners, where individuals who previously owned a pet were willing to get closer to the robot partner. These findings motivate a need for the investigation of elicited physiological affect regarding robot partners interacting collaboratively on a shared task. Moreover, it motivates a need for a deeper understanding of how elicited physiological affect influences the decision performance in human partners, to inform the design of robot collaborators and serious games. The Theory of Mind reasons that humans perceive and distinguish various social cues to explain events in terms of intents and goals of agents (i.e., their actions) which might affect the elicited emotions  . Following these propositions, this study set out to understand the mechanism of affective exchange that occurs between human and robot agents, based on the observable social cues  . Such embodiment provided by the social cues of collaborating robots has a direct link to the role of body information in an intelligent behavior  . The robot partners have been designed as autonomous social entities that can feature diverse behavior in the context of this study (i.e., autonomously acting based on a complex algorithm). Humans perceive collaborating robots depending on the physical interaction, actions, shape, and the environment itself. It was shown that such embodied non-humanoid robots are as engaging as humans, eliciting emotional responses in their human partners  . As evidence shows that emotions critically influence human decision-making and performance  , this study sets out to find how a small subset of social cues elicits physiological affect in humans collaborating with robots, in an attempt to investigate how these influence the decision performance on serious game tasks. Humans use the mechanisms from   (HHI) to perceive robots as autonomous social agents  . These propositions motivate this investigation to take into consideration both HHI and HRI, in the investigation of the elicited physiological affect. This investigation is an extension of the previous study where autonomic non-humanoid robot arms were perceived as social and emotional agents using a small subset of social cues in a serious game  . Therefore, this research aims to investigate elicited physiological affect and bring it in relation to the performance on a serious game task. Traditional physical games require a tangible interaction, contrary to the electronic games which are popular in the contemporary research methods  . The physical aspect of serious games is an important factor since humans perceive robots as physical entities which have access to the virtual domain. Therefore, this study uses a traditional game which provides a straightforward measurement of performance, where physical and virtual duality is supported through HRI-enhanced serious games. This paper attempts to investigate the effects of physiological affect underlying such human-robot proximate collaboration, as proposed by Ijsselsteijn et. al.  . More specifically, to bring these effects in relation to the performance on a serious game task in collaborative HRI, by mapping the participants’ physiological responses towards the collaborating non-humanoid robot arms on the arousal/valence axes  . These insights could provide a deeper understanding of how elicited physiological affect underlies such interaction from the human collaborator perspective, informing the design of more meaningful collaborative serious games that would use the objective measures of physiological affect together with intelligent robot collaborators to potentially increase performance on the shared tasks. The work presented here builds upon the previous findings, where it has been partly described and published  . This previous publication investigated the social abilities of non-humanoid robot arms to be perceived as social agents with emotional abilities, through a number of subjective questionnaires probing social categories and emotions perceived by the human participants. The current manuscript uses objective physiological measures to disseminates the elicited physiological affect findings in human participants in the interaction with their robot partners and how it influences the decision-making performance. The study was a part of the PsyIntEC ECHORD project (FP7-ICT-231143).', 's0957417416306844': ' 1 Introduction Clustering plays a basic role in many parts of data engineering, pattern recognition, and image analysis. Some of the most important clustering methods are based on GMM, which in practice accommodates data with distributions that lie around affine subspaces of lower dimensions obtained by principal components (PCA)\\xa0( ), see  \\n                      (a). However, by the manifold hypothesis, real world data presented in high dimensional spaces are likely to concentrate in the vicinity of non-linear sub-manifolds of lower dimensionality ( ). The classical approach approximates this manifold by a mixture of Gaussian distributions. Since one non-Gaussian component can be approximated by a mixture of several Gaussians ( ), these clusters are, in practice, represented by combination of Gaussian components. This can be seen as a form of piecewise linear approximation, see  (a). A similar result gives the Cross Entropy Clustering (CEC) ( ) approach, see  (b). In our paper we construct a general afCEC (active function Cross-Entropy Clustering) theory, which allows the clustering of data on sub-manifolds of\\xa0 . The motivation comes from the observation that it is often profitable to describe non-linear data by smaller numbers of components with more complicated curved shapes to obtain a better fit of data, see   (for more detailed analysis see  ). While developing this theory, we were influenced by the classical Shannon Entropy Theory\\xa0( ), Minimum Description Length Principle\\xa0( ), Cross-Entropy Clustering ( ), Expectation Maximization (EM), Implicit Function Theorem\\xa0( ), and Active curve axis Gaussian Mixture Models ( ). Consequently, we present a theoretically-motivated clustering method that automatically reduces unnecessary clusters and accommodates non-linear structures. Because we have to approximate complicated structures in each step, we have to construct a numerically efficient model. Therefore, we have chosen an approach that allows for the use of an explicit formula in each step. This paper is arranged as follows. In the next section, we present related works. Then the theoretical background of the density model will be presented (corresponding to the case of one cluster). In the fourth section, we introduce the theoretical background of the afCECmethod. We prove that the cost function decreases in every iteration, see  . The last two sections we present numerical experiments.'}\n",
            "{'s0957417413009615': ['Facial expression recognition', 'Bessel transform', 'Gabor feature', 'AdaBoost', 'MFFNN'], 's2590188519300162': ['Stock market prediction', 'Extreme Learning Machine', 'Online Sequential Extreme Learning Machine', 'Factor analysis', 'Firefly Optimization', 'Firefly with evolutionary framework'], 's0957417418306535': ['Malware', 'Information theory', 'Entropy', 'Time series', 'Packing', 'Adversarial learning'], 's0888613x13002041': ['Stock index forecasting', 'Type-1 fuzzy time series', 'Type-2 fuzzy time series', 'Particle swarm optimization', 'Defuzzification'], 's0957417419302489': [], 's107158191630074x': ['Sonification', 'Multi-modal display', 'Process monitoring', 'Quantitative study', 'Dual task', 'Industrial monitoring'], 's0888613x17302335': ['Under-reporting', 'Misclassification bias', 'Missing data', 'Mutual information'], 's1071581916000021': ['Touchscreen gestures', 'Behavioural biometrics', 'Physical biometrics'], 's0925231219301961': ['Uncertainty estimation', 'Convolutional neural networks', 'Medical image segmentation', 'Data augmentation'], 's0957417417300751': ['Ensemble', 'Deep learning', 'Sentiment analysis', 'Machine learning', 'Natural language processing'], 's0963868717301798': ['Enterprise social media', 'Technology affordances', 'Generative mechanisms', 'Organizational socialization'], 's0736584518303831': ['WAAM', 'Wire and Arc Additive Manufacturing', 'Path planning', 'Toolpath generation'], 's0921889016306285': ['Service robots', 'Technology assessment', 'Aggregate production function', 'Labor substitution', 'Labor complementarity'], 's0957417419301903': ['Credit', 'Indian banks', 'Neural networks', 'Actual misclassification costs', 'Timing of Default'], 's0888613x1400067x': ['Association rule mining', 'Decision rules', 'Granule mining', 'Rough sets'], 's0888613x14001133': ['Credal networks', 'Epistemic irrelevance', 'AD-separation', 'Irrelevant natural extension', 'Graphoid properties', 'Sets of desirable gambles'], 's259018851930006x': ['International roughness index', 'Fuzzy time series', 'Multigranularity', 'Automatic clustering', 'Particle swarm optimization'], 's0888613x1300234x': ['Computing with words', 'Fuzzy decision making', 'Order-based semantics of terms', 'Hedge algebras', '4-tuple linguistic representation model', 'Semantic linguistic scales'], 's0888613x14000607': ['Imprecise data', 'Fuzzy data', 'Machine learning', 'Loss functions', 'Classification', 'Regression'], 's1071581919300552': ['Automation', 'Human-automation interaction', 'Safety-critical systems', 'Autonomous agents', 'Embodied systems', 'Situated systems', 'Divided attention', 'Ethics', 'Robotics, Automated vehicles'], 's0957417419301812': ['Cashtag collision', 'Twitter', 'Stock market', 'Data fusion', 'Machine learning', 'Natural language processing'], 's0020025519304864': ['Visual enhancement', 'Drone image', 'Fuzzy set', 'Intuitionistic fuzzy set', 'Hesitant set', 'Hesitant score.'], 's0142061518304319': ['Partial discharge', 'DPC', 'Clustering', 'Spatial density'], 's0933365716302950': [], 's0306437915000459': ['Business process compliance', 'Compliance monitoring', 'Operational support'], 's0921889015003000': ['Autonomous systems', 'Ethics', 'BDI programs', 'Formal verification'], 's1071581916300866': ['Stress', 'Emotional support', 'Adaptation'], 's1071581918300016': ['Cultural heritage', 'Personalisation', 'Customisation', 'Context-awareness', 'Adaptivity', 'Embodied and tangible interaction'], 's1071581916301380': ['Algorithmic curation', 'Socio-technical systems', 'Relationships', 'Facebook News Feed'], 's0957417415003590': ['Self-healing', 'Self-Organizing Networks', 'LTE', 'Data mining', 'Data driven learning', 'Supervised learning', 'Fault management', 'Fuzzy systems', 'Big Data'], 's0921889018307474': ['Adaptivity', 'Control', 'Mutual information', 'Structure estimation'], 's0957417417307698': ['Data mining', 'Crowd detection', 'Density-based clustering', 'Content aggregation', 'Event detection'], 's1071581917301404': ['Social media', 'Remediation', 'Context collapse', 'Transformation', 'Digital identity', 'Reflection'], 's0921889014002164': ['Verbal', 'Non-verbal', 'Human–robot interaction', 'Human–robot communication', 'Survey'], 's1071581916300453': ['Authoring tools and methods', 'Human-computer interface', 'Evaluation of CAL systems'], 's0952197619301253': ['Traffic signal control', 'Time-of-day intervals', 'Evolutionary computation', 'Clustering', 'Multivariate time series analysis'], 's1071581918300971': ['Audio', 'Speech', 'Radio', 'Editing'], 's0306437919304909': ['Anomaly detection', 'Process runtime behavior', 'Root cause', 'Association rule mining', 'Process change'], 's0957417414006472': ['Text clustering', 'WordNet', 'Lexical chains', 'Core semantic features'], 's0925231217309864': ['Anomaly detection', 'Hierarchical Temporal Memory', 'Streaming data', 'Unsupervised learning', 'Concept drift', 'Benchmark dataset'], 's0736584515000666': ['Robotics', 'Path-planning', 'Automated non-destructive inspections'], 's1071581917300988': ['Multisensory experience', 'Museum', 'Mid-air haptic feedback', 'Taste', 'Smell', 'Sound', 'Emotion', 'User experience', 'Art gallery', 'Art exhibition'], 's0888613x14000206': ['\\n                     ', 'Conditional probability'], 's0950705118301394': ['Creativity', 'Divergence', 'Semantic networks', 'Similarity', 'WordNet'], 's1875952116300040': ['Multiplayer real-time games', 'Human computer interaction', 'Context-aware computing', 'Mobile computing', 'Social engaging games'], 's0888613x14000851': [], 's0740818818301828': [], 's2590188519300083': ['Face recognition', 'Facial points', 'Facial geometric features', 'Fractional entropy', 'Convolutional neural networks', 'Family photo classification'], 's1875952117300952': ['Kinect v2', 'Monte Carlo Tree Search (MCTS)', 'Myo armband', 'FootPedal'], 's0888613x15000857': ['Fuzzy logic programming', 'Tabulation proof procedure', 'Linguistic truth value', 'Hedge connective', 'Threshold computation', 'Top-k retrieval'], 's095741741830215x': ['Text classification', 'Document classification', 'Text classification overview', 'Document classification overview'], 's0925231214000265': ['Tensor completion', 'Multi-linear low-n-rank factorization', 'Nonlinear Gauss–Seidal method', 'Singular value decomposition'], 's1071581918303471': ['Human computer interaction', 'Interruption', 'Machine learning', 'Soft computing applications', 'Situationally appropriate interaction', 'Workload'], 's0957417415000238': ['Ontologies', 'Knowledge engineering', 'Cognitive style', 'Conceptual structuring', 'Collaborative design', 'Cognitive ergonomics'], 's1071581918304312': ['Creative engagement', 'Creativity support', 'Musical interface', 'Novice'], 's0888613x13002910': ['Bayesian networks', 'Probabilistic inference', 'Candecomp-Parafac tensor decomposition', 'Symmetric tensor rank'], 's0888613x14000796': ['Belief function', 'Inclusion–exclusion principle', 'Reliability analysis', 'Boolean formula', 'Independence'], 's0888613x15000912': ['Incremental model', 'Rule-based structures', 'Augmented clustering', 'Linear models'], 's1071581917300320': ['Augmented paper', 'Printed electronics', 'Interactive paper', 'Design principles', 'Interactive newsprint'], 's0888613x13002880': ['Probabilistic chain graphs', 'Qualitative reasoning', 'Inference'], 's0888613x14001212': ['Multivariate time series', 'Credal classification', 'Hidden Markov models', 'Gaussian mixtures', 'Imprecise probability', 'Bhattacharyya distance'], 's2589721719300029': ['Pig body temperature measurement', 'Non-contact', 'Infrared image processing'], 's0888613x13002867': ['Inconsistency measure', 'Stratified knowledge base'], 's0921889015000834': ['Hexapod', 'Walking robot', '6-axis milling machine', 'Robotised machine tool'], 's1875952119300394': ['Autonomous robots', 'Serious games', 'Collaborative play', 'Robot-assisted play', 'Emotions', 'Physiology', 'ECG', 'GSR', 'Affect'], 's0957417416306844': ['Clustering', 'Gaussian mixture models', 'Expectation maximization', 'Cross-Entropy Clustering', 'Active curve axis gaussian mixture model']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0amAYwSa7a1"
      },
      "source": [
        "Method for tokenisation, removing stopwords and lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ie4cGvybAvc"
      },
      "source": [
        "def getValidTokens(sent):\n",
        "  stopWords = set(stopwords.words('english'))\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  words = tokenizer.tokenize(sent)\n",
        "  wordsFiltered = []\n",
        "\n",
        "  for w in words:\n",
        "      w = str(w).lower()\n",
        "      if w not in stopWords:\n",
        "          wordsFiltered.append((w))\n",
        "  wordnet_lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_words = [wordnet_lemmatizer.lemmatize(token, pos=\"v\") for token in wordsFiltered]\n",
        "  return lemmatized_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxHfEgxGO2bE"
      },
      "source": [
        "Downloading the papers as the pdf files and separting the content of paper into \"Abstract\", \"Highlights\", \"Introduction\" and \"All other\" section\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA1vA6SSzlHJ"
      },
      "source": [
        "def getAbstractHighlightsKeywords(pii_id):\n",
        "  keywordsres=[]\n",
        "  xml_url='https://www.sciencedirect.com/science/article/abs/pii/'+pii_id\n",
        "\n",
        "\n",
        "  site= \"http://www.nseindia.com/live_market/dynaContent/live_watch/get_quote/getHistoricalData.jsp?symbol=JPASSOCIAT&fromDate=1-JAN-2012&toDate=1-AUG-2012&datePeriod=unselected&hiddDwnld=true\"\n",
        "  hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "        'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
        "        'Accept-Encoding': 'none',\n",
        "        'Accept-Language': 'en-US,en;q=0.8',\n",
        "        'Connection': 'keep-alive'}\n",
        "\n",
        "  req = urllib.request.Request(xml_url, headers=hdr)\n",
        "\n",
        "  try:\n",
        "      page = urllib.request.urlopen(req)\n",
        "  except (urllib.HTTPError, e):\n",
        "      print (e.fp.read())\n",
        "\n",
        "  content = page.read()\n",
        "  soup = BeautifulSoup(content, 'html.parser')\n",
        "  keywords=soup.findAll(\"div\", {\"class\": \"keyword\"})\n",
        "  for keyword in keywords:\n",
        "    keywordsres.append(keyword.get_text())\n",
        "  results = soup.find(id='abstracts')\n",
        "  \n",
        "  strR=''\n",
        "  for res in results:\n",
        "    strR+=(res.getText())\n",
        "  \n",
        "  abstract=strR.split('Abstract')[1]\n",
        "  Highlights=strR.split('Abstract')[0]\n",
        "  return abstract,Highlights.split('Highlights')[1],keywordsres"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLfX77CTKycM"
      },
      "source": [
        "Abstract={}\n",
        "Highlights={}\n",
        "OtherContent={}\n",
        "OtherContent_Dict={}\n",
        "Abstract_Dict={}\n",
        "Highlights_Dict={}\n",
        "Intro={}\n",
        "Intro_Dict={}\n",
        "keywordmain={}\n",
        "for pii_id in listPII:\n",
        "  pii_doc =FullDoc(sd_pii=pii_id)\n",
        "  docid=''\n",
        "  if pii_doc.read(client):\n",
        "      print (\"pii_doc.title: \", pii_doc.id)\n",
        "      docid=pii_doc.id\n",
        "      pii_doc.write()\n",
        "      abstract,highlight,keywords=getAbstractHighlightsKeywords(pii_id)\n",
        "      out = 'paper'\n",
        "      try:\n",
        "        sci = SciHub(docid[4:len(docid)], out).download(choose_scihub_url_index=1)\n",
        "        paperName=(pii_doc.title[0:len(pii_doc.title)-1])\n",
        "        punc=['/','?',':']\n",
        "        for p in punc:\n",
        "          if p in paperName:\n",
        "            paperName=paperName.replace(p,\" \")\n",
        "        paperName=paperName.replace(\"'\",\"’\")\n",
        "      \n",
        "        with open('paper/'+paperName+'.pdf','rb') as f:\n",
        "          extracted_text = slate.PDF(f)\n",
        "          text = [blk.replace(\"\\n\", \" \") for blk in extracted_text]\n",
        "          resText=\"\\r\\n\".join(text)\n",
        "        if len(resText.split('Introduction'))==2:\n",
        "          contText=(resText.split('Introduction')[1]).split('References')[0]\n",
        "        else:\n",
        "          contText=''\n",
        "          for t in range(len(resText.split('Introduction'))):\n",
        "            if t!=0:\n",
        "              contText+=resText.split('Introduction')[t]\n",
        "        intro=getIntroDictCont(contText)\n",
        "        if intro=='':\n",
        "          raise Exception\n",
        "        else:\n",
        "          Intro[pii_id]=getSentences(intro)\n",
        "          Intro_Dict[pii_id]=intro\n",
        "\n",
        "        OtherContent[pii_id]=getSentences(contText)\n",
        "        OtherContent_Dict[pii_id]=contText\n",
        "        \n",
        "        Abstract_Dict[pii_id]=(abstract)\n",
        "        Highlights_Dict[pii_id]=(highlight)\n",
        "        Abstract[pii_id]=getSentences(abstract)\n",
        "        Highlights[pii_id]=getSentences(highlight)\n",
        "        keywordmain[pii_id]=keywords\n",
        "      except:\n",
        "        print(\"Failed to read:\"+pii_doc.title[0:len(pii_doc.title)-1]+\"with pii_id=\"+pii_id)\n",
        "\n",
        "\n",
        "  else:\n",
        "      print (\"Read document failed.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}