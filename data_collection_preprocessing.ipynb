{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_collection_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8x0ZqwY2920"
      },
      "source": [
        "Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P6iYX3Z2ycC"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "import xml.etree.ElementTree as ET\n",
        "import urllib\n",
        "import urllib.request\n",
        "import shutil\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import http.cookiejar\n",
        "from bs4 import BeautifulSoup\n",
        "!pip install elsapy\n",
        "from elsapy.elsclient import ElsClient\n",
        "from elsapy.elsprofile import ElsAuthor, ElsAffil\n",
        "from elsapy.elsdoc import FullDoc, AbsDoc\n",
        "!pip install scidownl\n",
        "!pip install slate3k\n",
        "import slate3k as slate\n",
        "from scidownl.scihub import *\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3-1v5rF3DWX"
      },
      "source": [
        "The task was to get the urls of the articles which have an open access.\n",
        "\n",
        "Steps followed:\n",
        "\n",
        "Go to the following link:\n",
        "\n",
        "https://www.sciencedirect.com/search/advanced?qs=highlights&articleTypes=FLA&accessTypes=openaccess&show=100&offset=3600 \n",
        "\n",
        "\n",
        "*   This link is obtained by filtering out the documents with \"open access\" and keyword \"highlights\"\n",
        "\n",
        "*   Click on Export and select Export citations to text\n",
        "\n",
        "*   We get a list of urls\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o4wXtP73MR2"
      },
      "source": [
        "def extractURLs(fileContent):\n",
        "     urls = re.findall('\\(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', fileContent.lower())\n",
        "\n",
        "    #  for url in urls:\n",
        "    #         print(url)\n",
        "     return urls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HREAYeEERka"
      },
      "source": [
        "Getting the urls from the citation files and storing it in variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FfX6rEiEN0i"
      },
      "source": [
        "myFile = open(os.getcwd()+\"path\"+ \"file_name\",encoding=\"utf8\")\n",
        "fileContent = myFile.read()\n",
        "listURL=(extractURLs(fileContent))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtdQpmwnOClg"
      },
      "source": [
        "Pass the API key generated from Elsevier portal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxgIIoTTNTDb"
      },
      "source": [
        "key = 'API KEY'\n",
        "client = ElsClient(key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VBuY5RmHGi5"
      },
      "source": [
        "We pass the extracted URLs which are stored in the list and download the scientific articles in .xml format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMHeLbxeHFu4"
      },
      "source": [
        "listPII=[]\n",
        "for url in listURL:\n",
        "    if url.startswith(\"(http://www.sciencedirect.com\"):\n",
        "        pii = url.replace(\"(http://www.sciencedirect.com/science/article/pii/\", \"\").replace(\")\", \"\").replace(\"\\n\", \"\")\n",
        "        if not pii in listPII:\n",
        "            listPII.append(pii)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRLmHnjjLT10"
      },
      "source": [
        "Downloading the files in XML format\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIQ_s200Oi2x"
      },
      "source": [
        "for pii in listPII:\n",
        "    piir = \"http://api.elsevier.com/content/article/PII:\" + pii + \"?httpAccept=text/xml&APIKey=\" + key\n",
        "    local_filename, headers = urllib.request.urlretrieve(piir)\n",
        "    shutil.copy(local_filename, \"folder_path/\" +pii + \".xml\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAGPnCy8aG8g"
      },
      "source": [
        "Method to split the text into sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WNhu03YaGXu"
      },
      "source": [
        "def getSentences(text):\n",
        "  sentences=[]\n",
        "  intersent=''\n",
        "  for s in re.split('[.][\\s]([A-Z])',text):\n",
        "    if len(s)==1:\n",
        "      intersent=s\n",
        "    else:\n",
        "      if len(re.split('[\\s]',intersent+s))>4:  \n",
        "        if not '\\n' in  intersent+s:     \n",
        "          sentences.append(intersent+s)\n",
        "        intersent=''\n",
        "  return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEv5OqqsZYF2"
      },
      "source": [
        "Method to get the \"Abstract\" section from the papers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qym_XuxSYtkt"
      },
      "source": [
        "def getAbstract():\n",
        "  abstract_df = {}\n",
        "  abstract_dict={}\n",
        "  for filename in os.listdir(directory):\n",
        "    abstract = []\n",
        "    sentence_index = []\n",
        "    if filename.endswith(\"xml\") and filename in finalvalid:\n",
        "      tree = ET.parse(directory + \"/\" + filename)\n",
        "      root = tree.getroot()\n",
        "      abstract_text = \"\"\n",
        "      for tag in tree.iter():\n",
        "        if(tag.tag == '{http://purl.org/dc/elements/1.1/}description'):\n",
        "            abstract_text+= str(tag.text)  \n",
        "      \n",
        "      abstract_df[filename]=getSentences(abstract_text) \n",
        "      abstract_dict[filename]=abstract_text                                            \n",
        "  return abstract_df,abstract_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ECnFC9EZHEU"
      },
      "source": [
        "Method to get the \"Highlights\" section from the papers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLYiyyurYte2"
      },
      "source": [
        "def getHighlights():\n",
        "    highlight_df={}\n",
        "    for filename in os.listdir(directory):\n",
        "        all_files = []\n",
        "        if filename.endswith(\".xml\"):\n",
        "            isIntroFound = False\n",
        "            all_files.append(filename)\n",
        "            tree = ET.parse(directory + \"/\" + filename)\n",
        "            highlightswithoutpreprocess = \"\"\n",
        "            for tag in tree.iter():\n",
        "\n",
        "                if (tag.tag == '{http://www.elsevier.com/xml/common/dtd}abstract-sec'):\n",
        "                    for t in tag:\n",
        "                        if (t.tag == '{http://www.elsevier.com/xml/common/dtd}simple-para'):\n",
        "\n",
        "                            for tg in t:\n",
        "                                if (tg.tag == '{http://www.elsevier.com/xml/common/dtd}list'):\n",
        "\n",
        "                                    for g in tg:\n",
        "                                        if (g.tag == '{http://www.elsevier.com/xml/common/dtd}list-item'):\n",
        "\n",
        "                                            for ti in g:\n",
        "                                                if (ti.tag == '{http://www.elsevier.com/xml/common/dtd}para'):\n",
        "\n",
        "                                                    highlightswithoutpreprocess += str(ti.text)\n",
        "                                                    highlightswithoutpreprocess += \" \"\n",
        "            if(highlightswithoutpreprocess!=\"\"):\n",
        "                highlight_df[filename]=highlightswithoutpreprocess\n",
        "    return highlight_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsVJdZjIZfLP"
      },
      "source": [
        "Method to get the \"Introduction\" section from the papers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O15FNwS4Ytsr"
      },
      "source": [
        "def getIntro():\n",
        "    intro_df = {}\n",
        "    intro_dict={}\n",
        "    for filename in os.listdir(directory):\n",
        "        introduction = []\n",
        "        sentence_index = []\n",
        "        if filename.endswith(\".xml\") and filename in finalvalid:\n",
        "            isIntroFound = False\n",
        "            tree = ET.parse(directory + \"/\" + filename)\n",
        "            root = tree.getroot()\n",
        "            introText=\"\"\n",
        "            for child in root.iter():\n",
        "                if child.tag == '{http://www.elsevier.com/xml/common/dtd}section' and isIntroFound == False:\n",
        "                    for introtag in child:\n",
        "                      if introtag.tag=='{http://www.elsevier.com/xml/common/dtd}section-title' and introtag.text=='Introduction':\n",
        "                        isIntroFound=True\n",
        "                      if introtag.tag=='{http://www.elsevier.com/xml/common/dtd}para' or introtag.tag == '{http://www.elsevier.com/xml/common/dtd}section':\n",
        "                        introText+=\" \"+introtag.text\n",
        "                        for subintroTag in introtag:\n",
        "                            introText+=\" \"+str(subintroTag.text) \n",
        "                            introText+=\" \"+subintroTag.tail       \n",
        "            intro_df[filename]=getSentences(introText)\n",
        "            intro_dict[filename]=introText\n",
        "    return intro_df, intro_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-Sc4S4rZqnu"
      },
      "source": [
        "Method to get the \"All other\" section from the paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tlMcpGSYtvs"
      },
      "source": [
        "def getOtherContent():\n",
        "  other_content_df = {}\n",
        "  othercontent_dict={}\n",
        "  for filename in os.listdir(directory):\n",
        "    oc = []\n",
        "    sentence_index = []\n",
        "    if filename.endswith(\"xml\") and filename in finalvalid:\n",
        "      tree = ET.parse(directory + \"/\" + filename)\n",
        "      root = tree.getroot()\n",
        "      oc_text = \"\"\n",
        "      for child in root.iter():\n",
        "        if child.tag == '{http://www.elsevier.com/xml/common/dtd}section':\n",
        "            for introtag in child:\n",
        "              if introtag.tag=='{http://www.elsevier.com/xml/common/dtd}para':\n",
        "                oc_text+=\" \"+introtag.text\n",
        "                for subintroTag in introtag:\n",
        "                    oc_text+=\" \"+str(subintroTag.text) \n",
        "                    oc_text+=\" \"+subintroTag.tail   \n",
        "      other_content_df[filename] = getSentences(oc_text)\n",
        "      othercontent_dict[filename]= oc_text\n",
        "  return other_content_df, othercontent_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECwhjBQUalrP"
      },
      "source": [
        "Extract keywords, font specific words and title words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhxYBiO2akKt"
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "stop_words = set(stopwords.words('english'))\n",
        "def getkeywords():\n",
        "  bold_words = {}\n",
        "  italic_words = {}\n",
        "  keyword_main = {}\n",
        "  title_words = {}\n",
        "  punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~\\n'''\n",
        "  all_files = []\n",
        "  for filename in os.listdir(directory):\n",
        "      title = []\n",
        "      bold = []\n",
        "      italics = []\n",
        "      keywords = []\n",
        "      if filename.endswith(\".xml\"):\n",
        "          all_files.append(filename)\n",
        "          tree = ET.parse(directory + \"/\" + filename)\n",
        "          for tag in tree.iter():\n",
        "              if (tag.tag == '{http://www.elsevier.com/xml/common/dtd}bold'): #get bold words\n",
        "                  bold.append(tag.text)\n",
        "\n",
        "                  bold_words[filename] = (bold)\n",
        "\n",
        "          for tag in tree.iter():\n",
        "              if (tag.tag == '{http://www.elsevier.com/xml/common/dtd}italic'): #get italic words\n",
        "                  italics.append(tag.text)\n",
        "                  italic_words[filename] = (italics)\n",
        "\n",
        "          for tag in tree.iter():\n",
        "              if (tag.tag == '{http://www.elsevier.com/xml/common/dtd}keyword'):  # get keywords\n",
        "                  for t in tag:\n",
        "                      if (t.tag == '{http://www.elsevier.com/xml/common/dtd}text'):\n",
        "                          keywords.append(t.text)\n",
        "                          \n",
        "\n",
        "          for tag in tree.iter():\n",
        "              if(tag.tag == '{http://purl.org/dc/elements/1.1/}title'): # title\n",
        "                for word in (tag.text).split(\" \"):\n",
        "                  if word not in punctuations and word not in stop_words and len(word)>3:\n",
        "                    keywords.append(word)\n",
        "          keyword_main[filename] = (keywords)\n",
        "  return bold_words, italic_words, keyword_main"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0amAYwSa7a1"
      },
      "source": [
        "Method for tokenisation, removing stopwords and lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ie4cGvybAvc"
      },
      "source": [
        "def getValidTokens(sent):\n",
        "  stopWords = set(stopwords.words('english'))\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  words = tokenizer.tokenize(sent)\n",
        "  wordsFiltered = []\n",
        "\n",
        "  for w in words:\n",
        "      w = str(w).lower()\n",
        "      if w not in stopWords:\n",
        "          wordsFiltered.append((w))\n",
        "  wordnet_lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_words = [wordnet_lemmatizer.lemmatize(token, pos=\"v\") for token in wordsFiltered]\n",
        "  return lemmatized_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxHfEgxGO2bE"
      },
      "source": [
        "Downloading the papers as the pdf files and separting the content of paper into \"Abstract\", \"Highlights\", \"Introduction\" and \"All other\" section\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA1vA6SSzlHJ"
      },
      "source": [
        "def getAbstractHighlightsKeywords(pii_id):\n",
        "  keywordsres=[]\n",
        "  xml_url='https://www.sciencedirect.com/science/article/abs/pii/'+pii_id\n",
        "\n",
        "\n",
        "  site= \"http://www.nseindia.com/live_market/dynaContent/live_watch/get_quote/getHistoricalData.jsp?symbol=JPASSOCIAT&fromDate=1-JAN-2012&toDate=1-AUG-2012&datePeriod=unselected&hiddDwnld=true\"\n",
        "  hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "        'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
        "        'Accept-Encoding': 'none',\n",
        "        'Accept-Language': 'en-US,en;q=0.8',\n",
        "        'Connection': 'keep-alive'}\n",
        "\n",
        "  req = urllib.request.Request(xml_url, headers=hdr)\n",
        "\n",
        "  try:\n",
        "      page = urllib.request.urlopen(req)\n",
        "  except (urllib.HTTPError, e):\n",
        "      print (e.fp.read())\n",
        "\n",
        "  content = page.read()\n",
        "  soup = BeautifulSoup(content, 'html.parser')\n",
        "  keywords=soup.findAll(\"div\", {\"class\": \"keyword\"})\n",
        "  for keyword in keywords:\n",
        "    keywordsres.append(keyword.get_text())\n",
        "  results = soup.find(id='abstracts')\n",
        "  \n",
        "  strR=''\n",
        "  for res in results:\n",
        "    strR+=(res.getText())\n",
        "  \n",
        "  abstract=strR.split('Abstract')[1]\n",
        "  Highlights=strR.split('Abstract')[0]\n",
        "  return abstract,Highlights.split('Highlights')[1],keywordsres"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLfX77CTKycM"
      },
      "source": [
        "Abstract={}\n",
        "Highlights={}\n",
        "OtherContent={}\n",
        "OtherContent_Dict={}\n",
        "Abstract_Dict={}\n",
        "Highlights_Dict={}\n",
        "Intro={}\n",
        "Intro_Dict={}\n",
        "keywordmain={}\n",
        "for pii_id in listPII:\n",
        "  pii_doc =FullDoc(sd_pii=pii_id)\n",
        "  docid=''\n",
        "  if pii_doc.read(client):\n",
        "      print (\"pii_doc.title: \", pii_doc.id)\n",
        "      docid=pii_doc.id\n",
        "      pii_doc.write()\n",
        "      abstract,highlight,keywords=getAbstractHighlightsKeywords(pii_id)\n",
        "      out = 'paper'\n",
        "      try:\n",
        "        sci = SciHub(docid[4:len(docid)], out).download(choose_scihub_url_index=1)\n",
        "        paperName=(pii_doc.title[0:len(pii_doc.title)-1])\n",
        "        punc=['/','?',':']\n",
        "        for p in punc:\n",
        "          if p in paperName:\n",
        "            paperName=paperName.replace(p,\" \")\n",
        "        paperName=paperName.replace(\"'\",\"â€™\")\n",
        "      \n",
        "        with open('paper/'+paperName+'.pdf','rb') as f:\n",
        "          extracted_text = slate.PDF(f)\n",
        "          text = [blk.replace(\"\\n\", \" \") for blk in extracted_text]\n",
        "          resText=\"\\r\\n\".join(text)\n",
        "        if len(resText.split('Introduction'))==2:\n",
        "          contText=(resText.split('Introduction')[1]).split('References')[0]\n",
        "        else:\n",
        "          contText=''\n",
        "          for t in range(len(resText.split('Introduction'))):\n",
        "            if t!=0:\n",
        "              contText+=resText.split('Introduction')[t]\n",
        "        intro=getIntroDictCont(contText)\n",
        "        if intro=='':\n",
        "          raise Exception\n",
        "        else:\n",
        "          Intro[pii_id]=getSentences(intro)\n",
        "          Intro_Dict[pii_id]=intro\n",
        "\n",
        "        OtherContent[pii_id]=getSentences(contText)\n",
        "        OtherContent_Dict[pii_id]=contText\n",
        "        \n",
        "        Abstract_Dict[pii_id]=(abstract)\n",
        "        Highlights_Dict[pii_id]=(highlight)\n",
        "        Abstract[pii_id]=getSentences(abstract)\n",
        "        Highlights[pii_id]=getSentences(highlight)\n",
        "        keywordmain[pii_id]=keywords\n",
        "      except:\n",
        "        print(\"Failed to read:\"+pii_doc.title[0:len(pii_doc.title)-1]+\"with pii_id=\"+pii_id)\n",
        "\n",
        "\n",
        "  else:\n",
        "      print (\"Read document failed.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}